repo_owner,repo_name,commit_hash,author_name,author_email,author_date,committer_name,committer_email,committer_date,message,filenames,touches_rmd,touches_r,touches_r_or_rmd,is_merge,added,deleted,changed,diff
Economic-and-Financial-Data-Discovery,imfdatapy,9c22f361b0b710cce1df9dcfcd9e05a1909d07ce,Sou-Cheng Choi,schoi32@iit.edu,2023-05-08T14:27:21Z,Sou-Cheng Choi,schoi32@iit.edu,2023-05-08T14:27:21Z,Fix a unit test,makefile;tests/test_ifs.py,False,False,False,False,3,3,6,"---FILE: makefile---
@@ -54,7 +54,7 @@ fasttests:
 	coverage run -m pytest tests/test_dot.py
 
 longtests:
-        #coverage run -m pytest 
+    #coverage run -m pytest
 	coverage run -m pytest tests/test_ifs.py
 
 coverage:
@@ -66,4 +66,4 @@ coverage:
 fasttests_cov: fasttests coverage
 
 
-longtests_cov: longtests coverage
+longtests_cov: longtests coverage
\ No newline at end of file

---FILE: tests/test_ifs.py---
@@ -140,7 +140,7 @@ def test_ifs_eg15(self):
         # test for getting all countries for a given IFS code
         ifs = IFS(search_terms=[""NGDP_R_SA_XDC""], countries=None, period=""Q"", start_date=None, end_date=None)
         df = ifs.download_data()
-        self.assertGreaterEqual(df.shape[0], 7580)
+        self.assertGreaterEqual(df.shape[0], 7508)
         self.assertGreaterEqual(df.shape[1], 10)
         meta_df = ifs.get_meta()
         self.assertGreaterEqual(meta_df.shape[0], 1)"
Economic-and-Financial-Data-Discovery,imfdatapy,daf664927d6b594de29206227f13d64b144d294e,Sou-Cheng Choi,schoi32@iit.edu,2023-05-07T19:22:55Z,Sou-Cheng Choi,schoi32@iit.edu,2023-05-07T19:22:55Z,Fix a typo,demo/mermaid/imfdatapy_architecture_sequence_diagram.html,False,False,False,False,1,1,2,"---FILE: demo/mermaid/imfdatapy_architecture_sequence_diagram.html---
@@ -11,7 +11,7 @@
         participant IMF server
         imfdatapy->>IMF server: Meta data request for series
         IMF server-->>imfdatapy: JSON
-        imfdatapy->>IMF server: mMta data request for dimensions
+        imfdatapy->>IMF server: Meta data request for dimensions
         IMF server-->>imfdatapy: JSON
         imfdatapy->>IMF server:  Data request for series
         IMF server-->>imfdatapy: JSON"
Economic-and-Financial-Data-Discovery,imfdatapy,0aa68477b2a7b77a815efa672e9f19e666fac2c4,Irina Klein,iklein@hawk.iit.edu,2023-02-04T02:15:18Z,Irina Klein,iklein@hawk.iit.edu,2023-02-04T02:15:18Z,fix search term for indicator issue,src/imfdatapy/imf.py,False,False,False,False,9,3,12,"---FILE: src/imfdatapy/imf.py---
@@ -74,11 +74,14 @@ def __init__(self, series='IFS', search_terms=None, countries=None, period='Q',
         self.des_list = []
         self.id_list = []
 
+        self._indicator_dim_position = 2
+
         # Control for rate limits, `https://datahelp.imf.org/knowledgebase/articles/630877-data-services`
         self._max_requests = 3
         self._sleep_sec = 2
         self._max_indicators = 5
 
+
         # Doesn't create new directory in colab
         self.outdir = outdir if outdir is not None else f""..{os.sep}out{os.sep}""
         if not path.exists(self.outdir):
@@ -328,6 +331,8 @@ def get_dimensions(self):
                 is_output = True
                 try:
                     dim_name = self.dimension_list[n]['@codelist']
+                    if 'INDICATOR' in dim_name:
+                        self._indicator_dim_position = n
                     key = f""CodeList/{dim_name}""
                     if dim_name[:7] in [""CL_FREQ""]:
                         def _get_metadata(series='IFS'):
@@ -430,7 +435,7 @@ def download_meta(self):
             logger.debug(dim_meta_df.head())
 
             # download  meta data
-            key = f""CodeList/{self.dimension_list[2]['@codelist']}""
+            key = f""CodeList/{self.dimension_list[self._indicator_dim_position]['@codelist']}""
             json = self.repeat_request(url=f'{self.url}{key}')
             if json is not None:
                 code_list = json['Structure']['CodeLists']['CodeList']['Code']
@@ -442,7 +447,7 @@ def download_meta(self):
 
         # finds the indicators by the search words
         if self.search_terms is not None:
-            key = f""CodeList/{self.dimension_list[2]['@codelist']}""
+            key = f""CodeList/{self.dimension_list[self._indicator_dim_position]['@codelist']}""
             json = self.repeat_request(url=f'{self.url}{key}')
             if json is not None:
                 code_list = json['Structure']['CodeLists']['CodeList']['Code']
@@ -553,7 +558,8 @@ def download_data(self):
 
         for cont, indicators in itertools.product(self.countries,  dcn_sa_list):
             logger.debug(""Current country"", cont)
-            url = f""{base}{self.period}.{cont}.{'+'.join(indicators)}{time}""
+            url = f""{base}{self.period}.{cont}{'.'*(self._indicator_dim_position-1)}{'+'.join(indicators)}{time}""
+            logger.info(f""URL is {url}"")
             # url = f""{base}{period}..{'+'.join(indicators)}.{time}""
             logger.debug(f""{url = }"")
             json = self.repeat_request(url)"
Economic-and-Financial-Data-Discovery,imfdatapy,845e3467ca75ea9819661545e9eb8e403be657c7,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T23:10:04Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T23:10:04Z,fix automated tests (almost there!),.github/workflows/imfdatapy_test.yml,False,False,False,False,1,0,1,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -36,4 +36,5 @@ jobs:
         run: |
           conda list
           pip install pytest pytest-cov
+          pip install -e .
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,0decd27eb150c983b20431025918b3f85605337c,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T23:04:33Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T23:04:33Z,fix automated tests (almost there!),.github/workflows/imfdatapy_test.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -35,5 +35,5 @@ jobs:
       - name: Test with pytest
         run: |
           conda list
-          conda install pytest pytest-cov
+          pip install pytest pytest-cov
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,c4ec11f7eafffa58272b308aa55242143f76b43f,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:57:36Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:57:36Z,fix automated tests (almost there!),.github/workflows/imfdatapy_test.yml,False,False,False,False,1,0,1,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -35,4 +35,5 @@ jobs:
       - name: Test with pytest
         run: |
           conda list
+          conda install pytest pytest-cov
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,d80d324e03dd0bf8f3d5d9b0bfb0da46dcf2a2a8,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:51:52Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:51:52Z,fix automated tests,.github/workflows/imfdatapy_test.yml,False,False,False,False,2,3,5,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -30,10 +30,9 @@ jobs:
           miniforge-variant: ${{ matrix.miniforge-variant }}
           miniforge-version: ${{ matrix.miniforge-version }}
           use-mamba: true
+          activate-environment: test
+          auto-activate-base: false
       - name: Test with pytest
         run: |
-          source $CONDA/etc/profile.d/conda.sh
-          conda info --envs
-          conda activate test
           conda list
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,58ef4183ea6a0ae5d51660a89b58d3a680c0860c,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:36:42Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:36:42Z,Fix automated tests,.github/workflows/imfdatapy_test.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -32,7 +32,7 @@ jobs:
           use-mamba: true
       - name: Test with pytest
         run: |
-          source /Users/runner/miniconda3/etc/profile.d/conda.sh
+          source $CONDA/etc/profile.d/conda.sh
           conda info --envs
           conda activate test
           conda list"
Economic-and-Financial-Data-Discovery,imfdatapy,1ff052b8d9e295b7597db1ec4e537f33d4fa2181,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:29:42Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:29:42Z,Fix automated tests,.github/workflows/imfdatapy_test.yml,False,False,False,False,2,3,5,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -32,9 +32,8 @@ jobs:
           use-mamba: true
       - name: Test with pytest
         run: |
-          conda init bash
+          source /Users/runner/miniconda3/etc/profile.d/conda.sh
           conda info --envs
-          conda deactivate
-          conda activate imfdatapy
+          conda activate test
           conda list
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,003317ea76217f72232fa3227b763421a005f38b,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:20:50Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T22:20:50Z,Fix automated tests,.github/workflows/imfdatapy_test.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -32,7 +32,7 @@ jobs:
           use-mamba: true
       - name: Test with pytest
         run: |
-          source shell_script.sh
+          conda init bash
           conda info --envs
           conda deactivate
           conda activate imfdatapy"
Economic-and-Financial-Data-Discovery,imfdatapy,ec971954ce2aac9723d705bc35b8a6cc80703dde,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T21:57:51Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T21:57:51Z,Fix automated tests,.github/workflows/imfdatapy_test.yml,False,False,False,False,2,0,2,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -32,6 +32,8 @@ jobs:
           use-mamba: true
       - name: Test with pytest
         run: |
+          source shell_script.sh
+          conda info --envs
           conda deactivate
           conda activate imfdatapy
           conda list"
Economic-and-Financial-Data-Discovery,imfdatapy,f7af8807091e2085ea0c0e545675e81cb3182e73,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T21:50:38Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T21:50:38Z,Fix automated tests,.github/workflows/imfdatapy_test.yml,False,False,False,False,3,0,3,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -32,4 +32,7 @@ jobs:
           use-mamba: true
       - name: Test with pytest
         run: |
+          conda deactivate
+          conda activate imfdatapy
+          conda list
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,e75b5453ae10045ad72c713b24321436eb21a6ff,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T21:46:41Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T21:46:41Z,Fix automated tests,.github/workflows/imfdatapy_test.yml;environment.yml,False,False,False,False,1,2,3,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -32,6 +32,4 @@ jobs:
           use-mamba: true
       - name: Test with pytest
         run: |
-          conda install pytest
-          conda install pytest-cov
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file

---FILE: environment.yml---
@@ -13,6 +13,7 @@ dependencies:
   - coverage=6.3.2
   - pip=22.2.2
   - pytest=7.1.2
+  - pytest-cov=4.0.0
   - pylint=2.14.5
   - requests=2.28.1
   - pyquery=1.4.3"
Economic-and-Financial-Data-Discovery,imfdatapy,44919bcc28f9fd1e08de1e2e86100579808ddce9,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T20:12:40Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T20:12:40Z,Fix automated tests on GitHub and add windows/Mac,.github/workflows/imfdatapy_test.yml,False,False,False,False,1,3,4,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -19,8 +19,7 @@ jobs:
             environment-file: environment.yml
             miniforge-variant: Mambaforge-pypy3
           - os: windows
-            environment-file: etc/example-explicit.Windows.conda.lock
-            condarc-file: environment.yml
+            environment-file: environment.yml
             miniforge-variant: Mambaforge
     steps:
       - uses: actions/checkout@v2
@@ -33,7 +32,6 @@ jobs:
           use-mamba: true
       - name: Test with pytest
         run: |
-          pip install -e .
           conda install pytest
           conda install pytest-cov
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,2fb3a5665f358dabec474eb11c10bffd1c97a5af,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T20:06:08Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T20:06:08Z,Fix automated tests on GitHub and add windows/Mac,.github/workflows/imfdatapy_test.yml,False,False,False,False,1,3,4,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -31,11 +31,9 @@ jobs:
           miniforge-variant: ${{ matrix.miniforge-variant }}
           miniforge-version: ${{ matrix.miniforge-version }}
           use-mamba: true
-      - name: Install imfdatapy
-        run: |
-          pip install -e .
       - name: Test with pytest
         run: |
+          pip install -e .
           conda install pytest
           conda install pytest-cov
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,afcf47ad82e702ec2f55c2b0db94f9e0e149d541,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T19:14:27Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T19:14:27Z,Fix automated tests on GitHub and add windows/Mac,.github/workflows/imfdatapy_test.yml,False,False,False,False,24,15,39,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -3,25 +3,34 @@ name: imfdatapy
 on: [push]
 
 jobs:
-  example-3:
-    name: Ex3 Linux
-    runs-on: ""ubuntu-latest""
-    defaults:
-      run:
-        shell: bash -el {0}
+  example-10-mambaforge:
+    name: Ex10 (${{ matrix.os }}, Mambaforge)
+    runs-on: ${{ matrix.os }}-latest
+    strategy:
+      fail-fast: false
+      matrix:
+        os: [""ubuntu"", ""macos"", ""windows""]
+        include:
+          - os: ubuntu
+            environment-file: environment.yml
+            miniforge-variant: Mambaforge
+            miniforge-version: 4.9.2-4
+          - os: macos
+            environment-file: environment.yml
+            miniforge-variant: Mambaforge-pypy3
+          - os: windows
+            environment-file: etc/example-explicit.Windows.conda.lock
+            condarc-file: environment.yml
+            miniforge-variant: Mambaforge
     steps:
       - uses: actions/checkout@v2
       - uses: conda-incubator/setup-miniconda@v2
         with:
-          activate-environment: anaconda-client-env
-          environment-file: environment.yml
-          python-version: 3.10
-          condarc-file: etc/example-condarc.yml
-          auto-activate-base: false
-      - run: |
-          conda info
-          conda list
-          conda activate imfdatapy
+          condarc-file: ${{ matrix.condarc-file }}
+          environment-file: ${{ matrix.environment-file }}
+          miniforge-variant: ${{ matrix.miniforge-variant }}
+          miniforge-version: ${{ matrix.miniforge-version }}
+          use-mamba: true
       - name: Install imfdatapy
         run: |
           pip install -e ."
Economic-and-Financial-Data-Discovery,imfdatapy,88ae35d9115f05e04500103a43b22c67a0f146e1,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T17:39:59Z,Sou-Cheng Choi,schoi32@iit.edu,2023-02-02T17:39:59Z,Fix automated tests on GitHub,.github/workflows/imfdatapy_test.yml;setup.py,False,False,False,False,21,17,38,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -3,26 +3,30 @@ name: imfdatapy
 on: [push]
 
 jobs:
-  build:
-
-    runs-on: ubuntu-latest
-    strategy:
-      matrix:
-        python-version: [""3.8""]
-
+  example-3:
+    name: Ex3 Linux
+    runs-on: ""ubuntu-latest""
+    defaults:
+      run:
+        shell: bash -el {0}
     steps:
-      - uses: actions/checkout@v3
-      - name: Set up Python ${{ matrix.python-version }}
-        uses: actions/setup-python@v4
+      - uses: actions/checkout@v2
+      - uses: conda-incubator/setup-miniconda@v2
         with:
-          python-version: ${{ matrix.python-version }}
-      - name: Install dependencies
+          activate-environment: anaconda-client-env
+          environment-file: environment.yml
+          python-version: 3.10
+          condarc-file: etc/example-condarc.yml
+          auto-activate-base: false
+      - run: |
+          conda info
+          conda list
+          conda activate imfdatapy
+      - name: Install imfdatapy
         run: |
-          python -m pip install --upgrade pip
-          pip install -r requirements.txt
           pip install -e .
       - name: Test with pytest
         run: |
-          pip install pytest
-          pip install pytest-cov
+          conda install pytest
+          conda install pytest-cov
           pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file

---FILE: setup.py---
@@ -12,7 +12,7 @@ def run(self):
         try:
             os.system('pip install -e .')
         except:
-            print('Problem installing imf')
+            print('Problem installing imfdatapy')
         # compile files used for docuemtnation
         #try:
         #    os.system('make _doc')"
Economic-and-Financial-Data-Discovery,imfdatapy,36016bbd0cd19444f09192a180a398a9d2531091,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T21:45:06Z,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T21:45:06Z,Fix readthedocs issue,docs/_build/doctrees/environment.pickle;docs/conf.py;environment.yml,False,False,False,False,4,1,5,"---FILE: docs/conf.py---
@@ -9,6 +9,7 @@
 project = u""imfdatapy""
 copyright = u""2022, Irina Klein, Sou-Cheng Choi""
 author = u""Irina Klein, Sou-Cheng Choi""
+version = u""1.0""
 
 # -- General configuration ---------------------------------------------------
 

---FILE: environment.yml---
@@ -18,4 +18,6 @@ dependencies:
   - pyquery=1.4.3
   - graphviz=2.50.0
   - sphinx=5.3.0
-  - sphinx_rtd_theme=1.1.1
\ No newline at end of file
+  - sphinx_rtd_theme=1.1.1
+  - pip:
+    - myst-nb==0.16.0
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,b854181c3610dce0e90a5c92b5fa37478b79e72b,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T21:30:12Z,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T21:30:12Z,Fix readthedocs failure,environment.yml,False,False,False,False,0,1,1,"---FILE: environment.yml---
@@ -18,5 +18,4 @@ dependencies:
   - pyquery=1.4.3
   - graphviz=2.50.0
   - sphinx=5.3.0
-  - myst_nb=0.17.1
   - sphinx_rtd_theme=1.1.1
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,253c7e93411bde7bd88052988cd7b29bd24e5e8e,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T21:28:24Z,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T21:28:24Z,Fix documentation,README.md;docs/_build/doctrees/environment.pickle;docs/_build/doctrees/index.doctree;docs/_build/html/_sources/index.md.txt;docs/_build/html/index.html;docs/_build/html/searchindex.js;docs/index.md;environment.yml;makefile,False,False,False,False,14,19,33,"---FILE: README.md---
@@ -26,7 +26,7 @@ df = bop.download_data()
 
 ## Contributing
 
-Interested in contributing? Check out the contributing guidelines. Please note that this project is released with a [Code of Conduct](CONDUCT.md). By contributing to this project, you agree to abide by its terms.
+Interested in contributing? Check out the contributing guidelines. Please note that this project is released with a [Code of Conduct](conduct.md). By contributing to this project, you agree to abide by its terms.
 
 ## License
 

---FILE: docs/_build/html/_sources/index.md.txt---
@@ -5,9 +5,9 @@
 :maxdepth: 1
 :hidden:
 
-example.ipynb
+
 changelog.md
 contributing.md
 conduct.md
-autoapi/index
+
 ```
\ No newline at end of file

---FILE: docs/_build/html/index.html---
@@ -82,21 +82,20 @@ <h2>Installation<a class=""headerlink"" href=""#installation"" title=""Permalink to t
 <section id=""usage"">
 <h2>Usage<a class=""headerlink"" href=""#usage"" title=""Permalink to this heading""></a></h2>
 <p><code class=""docutils literal notranslate""><span class=""pre"">imfdatapy</span></code> can be used to search through and extract data as follows. The examples below show how to search through the IFS (International Financial Statistics) and BOP (Balance of Payments) using <code class=""docutils literal notranslate""><span class=""pre"">serach_terms</span></code> and download all the data with matching economic indicator names.</p>
-<div class=""highlight-python notranslate""><div class=""highlight""><pre><span></span><span class=""kn"">from</span> <span class=""nn"">src</span> <span class=""kn"">import</span> <span class=""o"">*</span>
-
+<div class=""highlight-python notranslate""><div class=""highlight""><pre><span></span><span class=""kn"">from</span> <span class=""nn"">imfdatapy.imf</span> <span class=""kn"">import</span> <span class=""o"">*</span>
 <span class=""n"">ifs</span> <span class=""o"">=</span> <span class=""n"">IFS</span><span class=""p"">(</span><span class=""n"">search_terms</span><span class=""o"">=</span><span class=""p"">[</span><span class=""s2"">&quot;gross domestic product, real&quot;</span><span class=""p"">],</span> <span class=""n"">countries</span><span class=""o"">=</span><span class=""p"">[</span><span class=""s2"">&quot;US&quot;</span><span class=""p"">],</span> <span class=""n"">period</span><span class=""o"">=</span><span class=""s1"">&#39;Q&#39;</span><span class=""p"">,</span>
-          <span class=""n"">start_date</span><span class=""o"">=</span><span class=""s2"">&quot;2000&quot;</span><span class=""p"">,</span> <span class=""n"">end_date</span><span class=""o"">=</span><span class=""s2"">&quot;2022&quot;</span><span class=""p"">)</span>
+<span class=""n"">start_date</span><span class=""o"">=</span><span class=""s2"">&quot;2000&quot;</span><span class=""p"">,</span> <span class=""n"">end_date</span><span class=""o"">=</span><span class=""s2"">&quot;2022&quot;</span><span class=""p"">)</span>
 <span class=""n"">df</span> <span class=""o"">=</span> <span class=""n"">ifs</span><span class=""o"">.</span><span class=""n"">download_data</span><span class=""p"">()</span>
 
 <span class=""n"">bop</span> <span class=""o"">=</span> <span class=""n"">BOP</span><span class=""p"">(</span><span class=""n"">search_terms</span><span class=""o"">=</span><span class=""p"">[</span><span class=""s2"">&quot;current account, total, credit&quot;</span><span class=""p"">],</span> <span class=""n"">countries</span><span class=""o"">=</span><span class=""p"">[</span><span class=""s2"">&quot;US&quot;</span><span class=""p"">],</span> <span class=""n"">period</span><span class=""o"">=</span><span class=""s1"">&#39;Q&#39;</span><span class=""p"">,</span>
-          <span class=""n"">start_date</span><span class=""o"">=</span><span class=""s2"">&quot;2000&quot;</span><span class=""p"">,</span> <span class=""n"">end_date</span><span class=""o"">=</span><span class=""s2"">&quot;2022&quot;</span><span class=""p"">)</span>
+<span class=""n"">start_date</span><span class=""o"">=</span><span class=""s2"">&quot;2000&quot;</span><span class=""p"">,</span> <span class=""n"">end_date</span><span class=""o"">=</span><span class=""s2"">&quot;2022&quot;</span><span class=""p"">)</span>
 <span class=""n"">df</span> <span class=""o"">=</span> <span class=""n"">bop</span><span class=""o"">.</span><span class=""n"">download_data</span><span class=""p"">()</span>
 </pre></div>
 </div>
 </section>
 <section id=""contributing"">
 <h2>Contributing<a class=""headerlink"" href=""#contributing"" title=""Permalink to this heading""></a></h2>
-<p>Interested in contributing? Check out the contributing guidelines. Please note that this project is released with a <span class=""xref myst"">Code of Conduct</span>. By contributing to this project, you agree to abide by its terms.</p>
+<p>Interested in contributing? Check out the contributing guidelines. Please note that this project is released with a <a class=""reference internal"" href=""conduct.html""><span class=""doc std std-doc"">Code of Conduct</span></a>. By contributing to this project, you agree to abide by its terms.</p>
 </section>
 <section id=""license"">
 <h2>License<a class=""headerlink"" href=""#license"" title=""Permalink to this heading""></a></h2>

---FILE: docs/_build/html/searchindex.js---
@@ -1 +1 @@
-Search.setIndex({""docnames"": [""changelog"", ""conduct"", ""contributing"", ""index""], ""filenames"": [""changelog.md"", ""conduct.md"", ""contributing.md"", ""index.md""], ""titles"": [""Changelog"", ""Code of Conduct"", ""Contributing"", ""imfdatapy""], ""terms"": {""resolv"": 0, ""dependeci"": 0, ""issu"": [0, 1, 2], ""mkdir"": 0, ""befor"": [0, 2], ""panda"": 0, ""to_csv"": 0, ""chang"": [0, 2], ""sourc"": [0, 3], ""code"": [0, 3], ""oo"": 0, ""version"": [0, 1, 2], ""search"": [0, 3], ""data"": [0, 3], ""extract"": [0, 3], ""first"": 0, ""releas"": [0, 2, 3], ""imfdatapi"": [0, 2], ""reserv"": 0, ""name"": [0, 2, 3], ""pypi"": 0, ""In"": 1, ""interest"": [1, 3], ""foster"": 1, ""an"": 1, ""open"": [1, 2], ""welcom"": [1, 2], ""environ"": 1, ""we"": 1, ""contributor"": 1, ""maintain"": 1, ""make"": [1, 2], ""particip"": 1, ""project"": [1, 2, 3], ""commun"": 1, ""harass"": 1, ""free"": [1, 2], ""experi"": 1, ""everyon"": 1, ""regardless"": 1, ""ag"": 1, ""bodi"": 1, ""size"": 1, ""disabl"": 1, ""ethnic"": 1, ""gender"": 1, ""ident"": 1, ""express"": 1, ""level"": 1, ""nation"": 1, ""person"": 1, ""appear"": 1, ""race"": 1, ""religion"": 1, ""sexual"": 1, ""orient"": 1, ""exampl"": [1, 3], ""behavior"": 1, ""contribut"": 1, ""creat"": [1, 2, 3], ""posit"": 1, ""includ"": [1, 2], ""us"": [1, 2, 3], ""inclus"": 1, ""languag"": 1, ""Being"": 1, ""respect"": 1, ""differ"": 1, ""viewpoint"": 1, ""gracefulli"": 1, ""accept"": 1, ""construct"": 1, ""critic"": 1, ""focus"": 1, ""what"": 1, ""i"": [1, 2, 3], ""best"": 1, ""show"": [1, 3], ""empathi"": 1, ""toward"": 1, ""other"": 1, ""member"": 1, ""unaccept"": 1, ""The"": [1, 2, 3], ""imageri"": 1, ""unwelcom"": 1, ""attent"": [1, 3], ""advanc"": 1, ""troll"": 1, ""insult"": 1, ""derogatori"": 1, ""comment"": 1, ""polit"": 1, ""attack"": 1, ""public"": 1, ""privat"": 1, ""publish"": 1, ""inform"": 1, ""physic"": 1, ""electron"": 1, ""address"": 1, ""without"": 1, ""explicit"": 1, ""permiss"": 1, ""which"": 1, ""could"": 1, ""reason"": 1, ""consid"": 1, ""inappropri"": 1, ""profession"": 1, ""set"": [1, 2], ""ar"": [1, 2], ""clarifi"": 1, ""expect"": 1, ""take"": 1, ""appropri"": [1, 2], ""fair"": 1, ""correct"": 1, ""action"": 1, ""ani"": [1, 2], ""instanc"": 1, ""have"": [1, 2], ""right"": 1, ""remov"": 1, ""edit"": 1, ""reject"": 1, ""commit"": [1, 2], ""wiki"": 1, ""align"": 1, ""thi"": [1, 2, 3], ""ban"": 1, ""temporarili"": 1, ""perman"": 1, ""thei"": [1, 2], ""deem"": 1, ""threaten"": 1, ""offens"": 1, ""harm"": 1, ""appli"": 1, ""both"": 1, ""within"": 1, ""space"": 1, ""when"": [1, 2], ""individu"": 1, ""repres"": 1, ""its"": [1, 2, 3], ""offici"": [1, 2], ""e"": 1, ""mail"": 1, ""post"": [1, 2], ""via"": 1, ""social"": 1, ""media"": 1, ""account"": [1, 3], ""act"": 1, ""appoint"": 1, ""onlin"": 1, ""offlin"": 1, ""event"": 1, ""represent"": 1, ""mai"": 1, ""further"": 1, ""defin"": 1, ""abus"": 1, ""otherwis"": 1, ""report"": 1, ""contact"": 1, ""team"": 1, ""review"": 1, ""investig"": 1, ""all"": [1, 2, 3], ""complaint"": 1, ""respond"": 1, ""wai"": 1, ""circumst"": 1, ""oblig"": 1, ""confidenti"": 1, ""regard"": [1, 3], ""incid"": 1, ""detail"": [1, 2], ""specif"": 1, ""polici"": 1, ""separ"": 1, ""who"": 1, ""do"": 1, ""follow"": [1, 3], ""good"": 1, ""faith"": 1, ""face"": 1, ""temporari"": 1, ""repercuss"": 1, ""determin"": 1, """": [1, 2, 3], ""leadership"": 1, ""adapt"": 1, ""from"": [1, 3], ""coven"": 1, ""homepag"": 1, ""1"": 1, ""4"": 1, ""greatli"": 2, ""appreci"": 2, ""everi"": 2, ""littl"": 2, ""bit"": 2, ""help"": 2, ""credit"": 2, ""alwai"": 2, ""given"": 2, ""If"": 2, ""you"": [2, 3], ""pleas"": [2, 3], ""your"": 2, ""oper"": 2, ""system"": 2, ""about"": 2, ""local"": 2, ""setup"": 2, ""might"": 2, ""troubleshoot"": 2, ""step"": 2, ""reproduc"": 2, ""look"": 2, ""through"": [2, 3], ""github"": 2, ""anyth"": 2, ""tag"": 2, ""want"": 2, ""whoever"": 2, ""enhanc"": 2, ""can"": [2, 3], ""never"": 2, ""enough"": 2, ""feel"": 2, ""part"": 2, ""doc"": 2, ""docstr"": 2, ""even"": 2, ""web"": 2, ""blog"": 2, ""articl"": 2, ""propos"": 2, ""explain"": 2, ""how"": [2, 3], ""would"": 2, ""work"": 2, ""keep"": 2, ""scope"": 2, ""narrow"": 2, ""possibl"": 2, ""easier"": 2, ""rememb"": 2, ""volunt"": 2, ""driven"": 2, ""readi"": 2, ""here"": 2, ""up"": 2, ""develop"": 2, ""todo"": 2, ""download"": [2, 3], ""copi"": 2, ""install_imf"": 2, ""instal"": 2, ""poetri"": 2, ""git"": 2, ""similar"": 2, ""branch"": 2, ""checkout"": 2, ""b"": 2, ""bugfix"": 2, ""re"": 2, ""done"": 2, ""check"": [2, 3], ""conform"": 2, ""format"": 2, ""requir"": 2, ""pass"": 2, ""test"": 2, ""meet"": 2, ""should"": 2, ""addit"": 2, ""add"": 2, ""function"": 2, ""updat"": 2, ""current"": [2, 3], ""support"": 2, ""python"": [2, 3], ""note"": [2, 3], ""By"": [2, 3], ""agre"": [2, 3], ""abid"": [2, 3], ""term"": [2, 3], ""A"": 3, ""packag"": 3, ""discoveri"": 3, ""intern"": 3, ""monetari"": 3, ""fund"": 3, ""imf"": 3, ""repositori"": 3, ""contain"": 3, ""jupyt"": 3, ""notebook"": 3, ""pip"": 3, ""below"": 3, ""IFS"": 3, ""financi"": 3, ""statist"": 3, ""bop"": 3, ""balanc"": 3, ""payment"": 3, ""serach_term"": 3, ""match"": 3, ""econom"": 3, ""indic"": 3, ""src"": 3, ""import"": 3, ""ifs"": 3, ""search_term"": 3, ""gross"": 3, ""domest"": 3, ""product"": 3, ""real"": 3, ""countri"": 3, ""u"": 3, ""period"": 3, ""q"": 3, ""start_dat"": 3, ""2000"": 3, ""end_dat"": 3, ""2022"": 3, ""df"": 3, ""download_data"": 3, ""total"": 3, ""out"": 3, ""guidelin"": 3, ""conduct"": 3, ""wa"": 3, ""sou"": 3, ""cheng"": 3, ""t"": 3, ""choi"": 3, ""irina"": 3, ""klein"": 3, ""illinoi"": 3, ""institut"": 3, ""technologi"": 3, ""It"": 3, ""under"": 3, ""apach"": 3, ""v2"": 3, ""0"": 3, ""With"": 3, ""refer"": 3, ""copyright"": 3, ""pai"": 3, ""special"": 3, ""section"": 3, ""AND"": 3, ""condit"": 3, ""pertain"": 3, ""TO"": 3, ""THE"": 3, ""OF"": 3, ""cookiecutt"": 3, ""py"": 3, ""pkg"": 3, ""templat"": 3}, ""objects"": {}, ""objtypes"": {}, ""objnames"": {}, ""titleterms"": {""changelog"": 0, ""v0"": 0, ""1"": 0, ""5"": 0, ""26"": 0, ""11"": 0, ""2022"": 0, ""25"": 0, ""0"": 0, ""24"": 0, ""code"": [1, 2], ""conduct"": [1, 2], ""our"": 1, ""pledg"": 1, ""standard"": 1, ""respons"": 1, ""scope"": 1, ""enforc"": 1, ""attribut"": 1, ""contribut"": [2, 3], ""type"": 2, ""report"": 2, ""bug"": 2, ""fix"": 2, ""implement"": 2, ""featur"": 2, ""write"": 2, ""document"": 2, ""submit"": 2, ""feedback"": 2, ""get"": 2, ""start"": 2, ""pull"": 2, ""request"": 2, ""guidelin"": 2, ""imfdatapi"": 3, ""instal"": 3, ""usag"": 3, ""licens"": 3, ""credit"": 3}, ""envversion"": {""sphinx.domains.c"": 2, ""sphinx.domains.changeset"": 1, ""sphinx.domains.citation"": 1, ""sphinx.domains.cpp"": 8, ""sphinx.domains.index"": 1, ""sphinx.domains.javascript"": 2, ""sphinx.domains.math"": 2, ""sphinx.domains.python"": 3, ""sphinx.domains.rst"": 2, ""sphinx.domains.std"": 2, ""sphinx.ext.viewcode"": 1, ""sphinx"": 57}, ""alltitles"": {""Changelog"": [[0, ""changelog""]], ""v0.1.5 (26/11/2022)"": [[0, ""v0-1-5-26-11-2022""]], ""v0.1.1 (25/11/2022)"": [[0, ""v0-1-1-25-11-2022""]], ""v0.1.0 (24/11/2022)"": [[0, ""v0-1-0-24-11-2022""]], ""Code of Conduct"": [[1, ""code-of-conduct""], [2, ""code-of-conduct""]], ""Our Pledge"": [[1, ""our-pledge""]], ""Our Standards"": [[1, ""our-standards""]], ""Our Responsibilities"": [[1, ""our-responsibilities""]], ""Scope"": [[1, ""scope""]], ""Enforcement"": [[1, ""enforcement""]], ""Attribution"": [[1, ""attribution""]], ""Contributing"": [[2, ""contributing""], [3, ""contributing""]], ""Types of Contributions"": [[2, ""types-of-contributions""]], ""Report Bugs"": [[2, ""report-bugs""]], ""Fix Bugs"": [[2, ""fix-bugs""]], ""Implement Features"": [[2, ""implement-features""]], ""Write Documentation"": [[2, ""write-documentation""]], ""Submit Feedback"": [[2, ""submit-feedback""]], ""Get Started!"": [[2, ""get-started""]], ""Pull Request Guidelines"": [[2, ""pull-request-guidelines""]], ""imfdatapy"": [[3, ""imfdatapy""]], ""Installation"": [[3, ""installation""]], ""Usage"": [[3, ""usage""]], ""License"": [[3, ""license""]], ""Credits"": [[3, ""credits""]]}, ""indexentries"": {}})
\ No newline at end of file
+Search.setIndex({""docnames"": [""changelog"", ""conduct"", ""contributing"", ""index""], ""filenames"": [""changelog.md"", ""conduct.md"", ""contributing.md"", ""index.md""], ""titles"": [""Changelog"", ""Code of Conduct"", ""Contributing"", ""imfdatapy""], ""terms"": {""resolv"": 0, ""dependeci"": 0, ""issu"": [0, 1, 2], ""mkdir"": 0, ""befor"": [0, 2], ""panda"": 0, ""to_csv"": 0, ""chang"": [0, 2], ""sourc"": [0, 3], ""code"": [0, 3], ""oo"": 0, ""version"": [0, 1, 2], ""search"": [0, 3], ""data"": [0, 3], ""extract"": [0, 3], ""first"": 0, ""releas"": [0, 2, 3], ""imfdatapi"": [0, 2], ""reserv"": 0, ""name"": [0, 2, 3], ""pypi"": 0, ""In"": 1, ""interest"": [1, 3], ""foster"": 1, ""an"": 1, ""open"": [1, 2], ""welcom"": [1, 2], ""environ"": 1, ""we"": 1, ""contributor"": 1, ""maintain"": 1, ""make"": [1, 2], ""particip"": 1, ""project"": [1, 2, 3], ""commun"": 1, ""harass"": 1, ""free"": [1, 2], ""experi"": 1, ""everyon"": 1, ""regardless"": 1, ""ag"": 1, ""bodi"": 1, ""size"": 1, ""disabl"": 1, ""ethnic"": 1, ""gender"": 1, ""ident"": 1, ""express"": 1, ""level"": 1, ""nation"": 1, ""person"": 1, ""appear"": 1, ""race"": 1, ""religion"": 1, ""sexual"": 1, ""orient"": 1, ""exampl"": [1, 3], ""behavior"": 1, ""contribut"": 1, ""creat"": [1, 2, 3], ""posit"": 1, ""includ"": [1, 2], ""us"": [1, 2, 3], ""inclus"": 1, ""languag"": 1, ""Being"": 1, ""respect"": 1, ""differ"": 1, ""viewpoint"": 1, ""gracefulli"": 1, ""accept"": 1, ""construct"": 1, ""critic"": 1, ""focus"": 1, ""what"": 1, ""i"": [1, 2, 3], ""best"": 1, ""show"": [1, 3], ""empathi"": 1, ""toward"": 1, ""other"": 1, ""member"": 1, ""unaccept"": 1, ""The"": [1, 2, 3], ""imageri"": 1, ""unwelcom"": 1, ""attent"": [1, 3], ""advanc"": 1, ""troll"": 1, ""insult"": 1, ""derogatori"": 1, ""comment"": 1, ""polit"": 1, ""attack"": 1, ""public"": 1, ""privat"": 1, ""publish"": 1, ""inform"": 1, ""physic"": 1, ""electron"": 1, ""address"": 1, ""without"": 1, ""explicit"": 1, ""permiss"": 1, ""which"": 1, ""could"": 1, ""reason"": 1, ""consid"": 1, ""inappropri"": 1, ""profession"": 1, ""set"": [1, 2], ""ar"": [1, 2], ""clarifi"": 1, ""expect"": 1, ""take"": 1, ""appropri"": [1, 2], ""fair"": 1, ""correct"": 1, ""action"": 1, ""ani"": [1, 2], ""instanc"": 1, ""have"": [1, 2], ""right"": 1, ""remov"": 1, ""edit"": 1, ""reject"": 1, ""commit"": [1, 2], ""wiki"": 1, ""align"": 1, ""thi"": [1, 2, 3], ""ban"": 1, ""temporarili"": 1, ""perman"": 1, ""thei"": [1, 2], ""deem"": 1, ""threaten"": 1, ""offens"": 1, ""harm"": 1, ""appli"": 1, ""both"": 1, ""within"": 1, ""space"": 1, ""when"": [1, 2], ""individu"": 1, ""repres"": 1, ""its"": [1, 2, 3], ""offici"": [1, 2], ""e"": 1, ""mail"": 1, ""post"": [1, 2], ""via"": 1, ""social"": 1, ""media"": 1, ""account"": [1, 3], ""act"": 1, ""appoint"": 1, ""onlin"": 1, ""offlin"": 1, ""event"": 1, ""represent"": 1, ""mai"": 1, ""further"": 1, ""defin"": 1, ""abus"": 1, ""otherwis"": 1, ""report"": 1, ""contact"": 1, ""team"": 1, ""review"": 1, ""investig"": 1, ""all"": [1, 2, 3], ""complaint"": 1, ""respond"": 1, ""wai"": 1, ""circumst"": 1, ""oblig"": 1, ""confidenti"": 1, ""regard"": [1, 3], ""incid"": 1, ""detail"": [1, 2], ""specif"": 1, ""polici"": 1, ""separ"": 1, ""who"": 1, ""do"": 1, ""follow"": [1, 3], ""good"": 1, ""faith"": 1, ""face"": 1, ""temporari"": 1, ""repercuss"": 1, ""determin"": 1, """": [1, 2, 3], ""leadership"": 1, ""adapt"": 1, ""from"": [1, 3], ""coven"": 1, ""homepag"": 1, ""1"": 1, ""4"": 1, ""greatli"": 2, ""appreci"": 2, ""everi"": 2, ""littl"": 2, ""bit"": 2, ""help"": 2, ""credit"": 2, ""alwai"": 2, ""given"": 2, ""If"": 2, ""you"": [2, 3], ""pleas"": [2, 3], ""your"": 2, ""oper"": 2, ""system"": 2, ""about"": 2, ""local"": 2, ""setup"": 2, ""might"": 2, ""troubleshoot"": 2, ""step"": 2, ""reproduc"": 2, ""look"": 2, ""through"": [2, 3], ""github"": 2, ""anyth"": 2, ""tag"": 2, ""want"": 2, ""whoever"": 2, ""enhanc"": 2, ""can"": [2, 3], ""never"": 2, ""enough"": 2, ""feel"": 2, ""part"": 2, ""doc"": 2, ""docstr"": 2, ""even"": 2, ""web"": 2, ""blog"": 2, ""articl"": 2, ""propos"": 2, ""explain"": 2, ""how"": [2, 3], ""would"": 2, ""work"": 2, ""keep"": 2, ""scope"": 2, ""narrow"": 2, ""possibl"": 2, ""easier"": 2, ""rememb"": 2, ""volunt"": 2, ""driven"": 2, ""readi"": 2, ""here"": 2, ""up"": 2, ""develop"": 2, ""todo"": 2, ""download"": [2, 3], ""copi"": 2, ""install_imf"": 2, ""instal"": 2, ""poetri"": 2, ""git"": 2, ""similar"": 2, ""branch"": 2, ""checkout"": 2, ""b"": 2, ""bugfix"": 2, ""re"": 2, ""done"": 2, ""check"": [2, 3], ""conform"": 2, ""format"": 2, ""requir"": 2, ""pass"": 2, ""test"": 2, ""meet"": 2, ""should"": 2, ""addit"": 2, ""add"": 2, ""function"": 2, ""updat"": 2, ""current"": [2, 3], ""support"": 2, ""python"": [2, 3], ""note"": [2, 3], ""By"": [2, 3], ""agre"": [2, 3], ""abid"": [2, 3], ""term"": [2, 3], ""A"": 3, ""packag"": 3, ""discoveri"": 3, ""intern"": 3, ""monetari"": 3, ""fund"": 3, ""imf"": 3, ""repositori"": 3, ""contain"": 3, ""jupyt"": 3, ""notebook"": 3, ""pip"": 3, ""below"": 3, ""IFS"": 3, ""financi"": 3, ""statist"": 3, ""bop"": 3, ""balanc"": 3, ""payment"": 3, ""serach_term"": 3, ""match"": 3, ""econom"": 3, ""indic"": 3, ""import"": 3, ""ifs"": 3, ""search_term"": 3, ""gross"": 3, ""domest"": 3, ""product"": 3, ""real"": 3, ""countri"": 3, ""u"": 3, ""period"": 3, ""q"": 3, ""start_dat"": 3, ""2000"": 3, ""end_dat"": 3, ""2022"": 3, ""df"": 3, ""download_data"": 3, ""total"": 3, ""out"": 3, ""guidelin"": 3, ""conduct"": 3, ""wa"": 3, ""sou"": 3, ""cheng"": 3, ""t"": 3, ""choi"": 3, ""irina"": 3, ""klein"": 3, ""illinoi"": 3, ""institut"": 3, ""technologi"": 3, ""It"": 3, ""under"": 3, ""apach"": 3, ""v2"": 3, ""0"": 3, ""With"": 3, ""refer"": 3, ""copyright"": 3, ""pai"": 3, ""special"": 3, ""section"": 3, ""AND"": 3, ""condit"": 3, ""pertain"": 3, ""TO"": 3, ""THE"": 3, ""OF"": 3, ""cookiecutt"": 3, ""py"": 3, ""pkg"": 3, ""templat"": 3}, ""objects"": {}, ""objtypes"": {}, ""objnames"": {}, ""titleterms"": {""changelog"": 0, ""v0"": 0, ""1"": 0, ""5"": 0, ""26"": 0, ""11"": 0, ""2022"": 0, ""25"": 0, ""0"": 0, ""24"": 0, ""code"": [1, 2], ""conduct"": [1, 2], ""our"": 1, ""pledg"": 1, ""standard"": 1, ""respons"": 1, ""scope"": 1, ""enforc"": 1, ""attribut"": 1, ""contribut"": [2, 3], ""type"": 2, ""report"": 2, ""bug"": 2, ""fix"": 2, ""implement"": 2, ""featur"": 2, ""write"": 2, ""document"": 2, ""submit"": 2, ""feedback"": 2, ""get"": 2, ""start"": 2, ""pull"": 2, ""request"": 2, ""guidelin"": 2, ""imfdatapi"": 3, ""instal"": 3, ""usag"": 3, ""licens"": 3, ""credit"": 3}, ""envversion"": {""sphinx.domains.c"": 2, ""sphinx.domains.changeset"": 1, ""sphinx.domains.citation"": 1, ""sphinx.domains.cpp"": 8, ""sphinx.domains.index"": 1, ""sphinx.domains.javascript"": 2, ""sphinx.domains.math"": 2, ""sphinx.domains.python"": 3, ""sphinx.domains.rst"": 2, ""sphinx.domains.std"": 2, ""sphinx.ext.viewcode"": 1, ""sphinx"": 57}, ""alltitles"": {""Changelog"": [[0, ""changelog""]], ""v0.1.5 (26/11/2022)"": [[0, ""v0-1-5-26-11-2022""]], ""v0.1.1 (25/11/2022)"": [[0, ""v0-1-1-25-11-2022""]], ""v0.1.0 (24/11/2022)"": [[0, ""v0-1-0-24-11-2022""]], ""Code of Conduct"": [[1, ""code-of-conduct""], [2, ""code-of-conduct""]], ""Our Pledge"": [[1, ""our-pledge""]], ""Our Standards"": [[1, ""our-standards""]], ""Our Responsibilities"": [[1, ""our-responsibilities""]], ""Scope"": [[1, ""scope""]], ""Enforcement"": [[1, ""enforcement""]], ""Attribution"": [[1, ""attribution""]], ""Contributing"": [[2, ""contributing""], [3, ""contributing""]], ""Types of Contributions"": [[2, ""types-of-contributions""]], ""Report Bugs"": [[2, ""report-bugs""]], ""Fix Bugs"": [[2, ""fix-bugs""]], ""Implement Features"": [[2, ""implement-features""]], ""Write Documentation"": [[2, ""write-documentation""]], ""Submit Feedback"": [[2, ""submit-feedback""]], ""Get Started!"": [[2, ""get-started""]], ""Pull Request Guidelines"": [[2, ""pull-request-guidelines""]], ""imfdatapy"": [[3, ""imfdatapy""]], ""Installation"": [[3, ""installation""]], ""Usage"": [[3, ""usage""]], ""License"": [[3, ""license""]], ""Credits"": [[3, ""credits""]]}, ""indexentries"": {}})
\ No newline at end of file

---FILE: docs/index.md---
@@ -5,9 +5,9 @@
 :maxdepth: 1
 :hidden:
 
-example.ipynb
+
 changelog.md
 contributing.md
 conduct.md
-autoapi/index
+
 ```
\ No newline at end of file

---FILE: environment.yml---
@@ -19,5 +19,4 @@ dependencies:
   - graphviz=2.50.0
   - sphinx=5.3.0
   - myst_nb=0.17.1
-  - autoapi=2.0.1
   - sphinx_rtd_theme=1.1.1
\ No newline at end of file

---FILE: makefile---
@@ -21,16 +21,13 @@ _uml:
 	pyreverse src/imfdatapy/imf.py -o png
 	mv classes.png docs/imfdatapy_classes_members.png
 
-
-
 doc_html:
-	sphinx-build -b html doc doc/build
+	sphinx-build -b html docs docs/_build
 
 doc_pdf:
-	sphinx-build -b latex doc doc/build -W --keep-going  2>/dev/null
-	cd doc/build/
-	latex doc/build/imfdatapy.tex
-	cd ../..
+	cd docs
+	make latexpdf
+	cd ..
 
 tests:
 	python -W ignore -m coverage run --append --source=./ -m unittest discover -s tests/ 1>/dev/null
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,dc8b4ca5ad3c20f97465aa1279cd35f11827deac,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T19:01:17Z,Sou-Cheng Choi,schoi32@iit.edu,2023-01-28T19:01:17Z,Fix logger,src/imfdatapy/imf.py;src/imfdatapy/imf_log.py,False,False,False,False,45,15,60,"---FILE: src/imfdatapy/imf.py---
@@ -4,13 +4,11 @@
 Creation Date: Jun 15, 2022
 
 """"""
-import itertools
-import logging
-import logging.config
+
 import os
 import time as tm
 from abc import ABC, abstractmethod
-from datetime import datetime
+
 from os import mkdir
 from os import path
 import itertools
@@ -20,17 +18,13 @@
 
 MAX_FILENAME_LEN = 260
 
-# Create logger
-log_file_path = path.join(path.dirname(path.abspath(__file__)), 'logging.conf')
-logging.config.fileConfig(log_file_path, disable_existing_loggers=False)
-logger = logging.getLogger(__name__) # TODO is it logger or logging later
-time_stamp = str(datetime.now())[:16].replace("" "", ""-"").replace("":"", ""-"")
-# Doesn't create new directory in colab
-logdir = '../../log'
-if not path.exists(logdir):
-    mkdir(logdir)
-logging.basicConfig(filename=f'{logdir}/imf_{time_stamp}.log', encoding='utf-8', level=logging.info)
-
+try:
+    from .imf_log import *
+except:
+    try:
+        from imf_log import *
+    except:
+        print(f""WARN: Failed to create log file."")
 
 # abstract class
 class Series(ABC):

---FILE: src/imfdatapy/imf_log.py---
@@ -0,0 +1,36 @@
+
+import logging.config
+from datetime import datetime
+import os
+
+
+time_stamp = str(datetime.now())[:16].replace("" "", ""-"").replace("":"", ""-"")
+cwd = os.getcwd()
+if (cwd[-3:]==""src"") or (cwd[-4:]==""demo"") or (cwd[-5:]==""tests""):
+    logdir = ""../log""
+elif cwd[-9:] == ""imfdatapy"":
+    logdir = ""../../log""
+else:
+    print(""WARN: Contact the IM"")
+
+if not os.path.exists(logdir):
+    print(f""WARN: Creating log directory {logdir}"")
+    os.mkdir(logdir)
+log_file = f""{logdir}/imfdatapy_{time_stamp}.log""
+log_format = ""%(asctime)s %(filename)s:%(lineno)d - %(levelname)s - %(message)s""
+
+rootLogger = logging.getLogger()
+
+file_handler = logging.FileHandler(log_file)
+file_handler.setFormatter(logging.Formatter(log_format))
+
+consoleHandler = logging.StreamHandler()
+consoleHandler.setFormatter(logging.Formatter(log_format))
+rootLogger.addHandler(consoleHandler)
+
+logger = logging.getLogger(""imfdatapy_log"")
+
+logger.addHandler(file_handler)
+logger.setLevel(logging.INFO)
+logger.info(f""Current directory {os.getcwd()}"")
+logger.info(f""Started log {log_file}"")
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,2dc3426ed38a9dfd12099f8a69ec749f83276d79,Sou-Cheng Choi,schoi32@iit.edu,2022-12-15T15:09:45Z,Sou-Cheng Choi,schoi32@iit.edu,2022-12-15T15:09:45Z,Fix test failure,.github/workflows/imfdatapy_test.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/imfdatapy_test.yml---
@@ -25,4 +25,4 @@ jobs:
         run: |
           pip install pytest
           pip install pytest-cov
-          pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov=com --cov-report=xml --cov-report=html
\ No newline at end of file
+          pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov-report=xml --cov-report=html
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,dff1c46f5a4294375639102421191b2438801b7c,Sou-Cheng Choi,schoi32@iit.edu,2022-12-12T22:21:31Z,Sou-Cheng Choi,schoi32@iit.edu,2022-12-12T22:21:31Z,Fix a test failure,src/imfdatapy/imf.py;tests/test_gfsr.py,False,False,False,False,63,60,123,"---FILE: src/imfdatapy/imf.py---
@@ -80,6 +80,7 @@ def __init__(self, series='IFS', search_terms=None, countries=None, period='Q',
         self.des_list = []
         self.id_list = []
 
+        # Control for rate limits, `https://datahelp.imf.org/knowledgebase/articles/630877-data-services`
         self._max_requests = 3
         self._sleep_sec = 2
         self._max_indicators = 5
@@ -473,7 +474,7 @@ def download_meta(self):
             self.meta_df = self.clean_column_names(self.meta_df)
 
             # deduplicate
-            self.meta_df = self.meta_df.drop_duplicates(subset=[""ID""], keep='last')
+            self.meta_df = self.meta_df.drop_duplicates(keep='last')
 
             self.output_meta(indicator="""")
         else:
@@ -547,33 +548,68 @@ def download_data(self):
         dcn_sa = list(self.meta_df[""ID""].values)
         temp = pd.DataFrame()
 
-        dcn_sa_list = [dcn_sa[x:x + self._max_indicators] for x in range(0, len(dcn_sa), self._max_indicators)]
-        area_list = [self.countries[x:x + 1] for x in range(0, len(self.countries), 1)]
-        for cont in self.countries:
-            for indicators in dcn_sa_list:
-                logger.debug(""Current country"", cont)
-                url = f""{base}{self.period}.{cont}.{'+'.join(indicators)}{time}""
-                # url = f""{base}{period}..{'+'.join(indicators)}.{time}""
-                logger.debug(f""{url = }"")
-                json = self.repeat_request(url)
-                if json is not None:
-                    try:
-                        response = json
-                        series = response['CompactData']['DataSet']['Series']
-                        temp_df = pd.DataFrame()
-                        if isinstance(series, dict):
-                            if isinstance(series.get(""Obs""), list):
-                                temp_df = pd.concat([temp_df, pd.json_normalize(series.get(""Obs""))])
-                            for k in series.keys():
+        if self.countries[0] == '':
+            dcn_sa_list = [dcn_sa[x:x + self._max_indicators] for x in range(0, len(dcn_sa), self._max_indicators)]
+        else:
+            dcn_sa_list = [dcn_sa[x:x + 1] for x in range(0, len(dcn_sa), 1)]
+
+        for cont, indicators in itertools.product(self.countries,  dcn_sa_list):
+            logger.debug(""Current country"", cont)
+            url = f""{base}{self.period}.{cont}.{'+'.join(indicators)}{time}""
+            # url = f""{base}{period}..{'+'.join(indicators)}.{time}""
+            logger.debug(f""{url = }"")
+            json = self.repeat_request(url)
+            if json is not None:
+                try:
+                    response = json
+                    series = response['CompactData']['DataSet']['Series']
+                    temp_df = pd.DataFrame()
+                    if isinstance(series, dict):
+                        if isinstance(series.get(""Obs""), list):
+                            temp_df = pd.concat([temp_df, pd.json_normalize(series.get(""Obs""))])
+                        for k in series.keys():
+                            if k != ""Obs"":
+                                temp_df[k] = series.get(k)
+
+                        if temp_df.shape[0] > 0:
+                            temp_df = temp_df.rename(
+                                columns={
+                                    '@OBS_VALUE': 'Value',
+                                    '@INDICATOR': 'ID',
+                                    '@INDICATOR_CODE': 'ID',
+                                    '@REF_AREA': 'Country'
+                                }
+                            )
+                            temp_df['Period'] = pd.to_datetime(
+                                [row.replace('-', '') for row in temp_df['@TIME_PERIOD']]
+                            )
+                            temp_df.drop('@TIME_PERIOD', axis=1, inplace=True)
+
+                            temp = pd.concat([temp, temp_df], axis=0)
+                    elif isinstance(series, list):
+                        series_len = len(series)
+                        for n in range(0, series_len):
+                            temp_dic = series[n].get('Obs')
+
+                            temp_df = pd.DataFrame.from_dict(
+                                temp_dic
+                            ).rename(
+                                columns={
+                                    '@OBS_VALUE': 'Value',
+                                    '@OBS_STATUS': 'Status'
+                                }
+                            )
+
+                            for k in series[n].keys():
                                 if k != ""Obs"":
-                                    temp_df[k] = series.get(k)
+                                    temp_df[k] = series[n].get(k)
 
                             if temp_df.shape[0] > 0:
                                 temp_df = temp_df.rename(
                                     columns={
                                         '@OBS_VALUE': 'Value',
                                         '@INDICATOR': 'ID',
-                                        '@INDICATOR_CODE': 'ID',
+                                        '@REF_SECTOR': 'ID',  # for GFSR
                                         '@REF_AREA': 'Country'
                                     }
                                 )
@@ -583,42 +619,9 @@ def download_data(self):
                                 temp_df.drop('@TIME_PERIOD', axis=1, inplace=True)
 
                                 temp = pd.concat([temp, temp_df], axis=0)
-                        elif isinstance(series, list):
-                            series_len = len(series)
-                            for n in range(0, series_len):
-                                temp_dic = series[n].get('Obs')
-
-                                temp_df = pd.DataFrame.from_dict(
-                                    temp_dic
-                                ).rename(
-                                    columns={
-                                        '@OBS_VALUE': 'Value',
-                                        '@OBS_STATUS': 'Status'
-                                    }
-                                )
-
-                                for k in series[n].keys():
-                                    if k != ""Obs"":
-                                        temp_df[k] = series[n].get(k)
-
-                                if temp_df.shape[0] > 0:
-                                    temp_df = temp_df.rename(
-                                        columns={
-                                            '@OBS_VALUE': 'Value',
-                                            '@INDICATOR': 'ID',
-                                            '@REF_SECTOR': 'ID',  # for GFSR
-                                            '@REF_AREA': 'Country'
-                                        }
-                                    )
-                                    temp_df['Period'] = pd.to_datetime(
-                                        [row.replace('-', '') for row in temp_df['@TIME_PERIOD']]
-                                    )
-                                    temp_df.drop('@TIME_PERIOD', axis=1, inplace=True)
-
-                                    temp = pd.concat([temp, temp_df], axis=0)
-                    except:
-                        logger.warning(f""Request for IMF data failed for area code, {cont}: {url = }."")
-                        pass
+                except:
+                    logger.warning(f""Request for IMF data failed for area code, {cont}: {url = }."")
+                    pass
 
         is_output = True
         if temp.shape[0] == 0:
@@ -638,7 +641,7 @@ def download_data(self):
         self.data_df = pd.merge(self.data_df, self.meta_df, on=""ID"", how=""left"")
 
         # deduplicate
-        self.data_df =  self.data_df.drop_duplicates(subset=['ID', 'Country', 'Period'], keep='last')
+        self.data_df = self.data_df.drop_duplicates(keep='last')
 
         # sorting
         self.data_df.sort_values(by=[""ID"", ""Country"", 'Period'], axis=0, inplace=True)
@@ -695,7 +698,7 @@ def _validate_date(date, date_des):
         if (area_key in self.dim_dict.keys()) and (self.dim_dict[area_key] is not None):
             valid_countries = self.dim_dict[area_key][""VALUE""].values
             if self.countries is None:
-                self.countries = valid_countries
+                self.countries = ['']
             rm_countries = []
             for c in self.countries:
                 if not (c in valid_countries):

---FILE: tests/test_gfsr.py---
@@ -6,7 +6,7 @@ class MyTestCase(unittest.TestCase):
     def test_imf_gfsr_eg1(self):
         gfsr = GFSR(search_terms=[""central government""], countries=[""US""], period='A', start_date=""2000"", end_date=""2022"")
         df = gfsr.download_data()
-        self.assertGreaterEqual(df.shape[0], 14784)
+        self.assertGreaterEqual(df.shape[0], 11088)
         meta_df = gfsr.get_meta()
         self.assertGreaterEqual(meta_df.shape[0], 4)
 "
Economic-and-Financial-Data-Discovery,imfdatapy,4f0b964ff8d9b9fb59da50af1b29ee92161f7e0b,Sou-Cheng Choi,schoi32@iit.edu,2022-12-10T22:29:18Z,Sou-Cheng Choi,schoi32@iit.edu,2022-12-10T22:29:18Z,Fix tests for IFS,tests/test_ifs.py,False,False,False,False,4,4,8,"---FILE: tests/test_ifs.py---
@@ -5,11 +5,11 @@
 class MyTestCase(unittest.TestCase):
 
     def test_imf_oo(self):
-        ifs = IMF()
+        ifs = IFS()
         self.assertEqual(ifs.start_date, None)
 
     def test_imf_ifs_eg1(self):
-        ifs = IMF(search_terms=[""gross domestic product, real""], countries=[""US""], period='Q', start_date=""2000"",
+        ifs = IFS(search_terms=[""gross domestic product, real""], countries=[""US""], period='Q', start_date=""2000"",
                   end_date=""2022"")
         df = ifs.download_data()
         self.assertGreaterEqual(df.shape[0], 174)
@@ -20,7 +20,7 @@ def test_imf_ifs_eg1(self):
 
 
     def test_imf_ifs_eg2(self):
-        ifs = IMF(search_terms=[""gross Domestic Product, Real""], countries=[""CA"", ""RU""],
+        ifs = IFS(search_terms=[""gross Domestic Product, Real""], countries=[""CA"", ""RU""],
                   period='Q', start_date=""1970"", end_date=""2022"")
         df = ifs.download_data()
         self.assertGreaterEqual(df.shape[0], 361)
@@ -29,7 +29,7 @@ def test_imf_ifs_eg2(self):
 
 
     def test_imf_ifs_eg3(self):
-        ifs = IMF(countries=[""US""], period='Q', start_date=""2000"", end_date=""2022"")
+        ifs = IFS(countries=[""US""], period='Q', start_date=""2000"", end_date=""2022"")
         df = ifs.download_data()
         self.assertGreaterEqual(df.shape[0], 174)
         meta_df = ifs.get_meta()"
Economic-and-Financial-Data-Discovery,imfdatapy,ffdcc2a1c906a3121b66a59e5f193dc2a9ca21a0,Sou-Cheng Choi,schoi32@iit.edu,2022-12-08T23:54:49Z,Sou-Cheng Choi,schoi32@iit.edu,2022-12-08T23:54:49Z,"Multiple improvements

(1) Added Google style documentation and inline comments.
(2) Fixed a but on getting meta data related to frequency.
(3) Generate data csv filename up to 260 characters.
(4) Default start and end dates are set to get earliest and latest data.
(5) Add output directory as user input
(6) When not getting valid responses from IMF data server, load local data files if exist.
(7) Added a few subclasses of IMF.",src/imfdatapy/imf.py;tests/test_afrreo.py;tests/test_bop.py;tests/test_dot.py;tests/test_fsi.py;tests/test_gfsr.py;tests/test_hpdd.py;tests/test_ifs.py,False,False,False,False,468,232,700,"---FILE: src/imfdatapy/imf.py---
@@ -1,72 +1,80 @@
 """"""
-Authors: Sou-Cheng Choi and Irina Klein, Illinois Institute of Technology
-Updated Date: Nov 24, 2022
+Authors: Sou-Cheng T. Choi and Irina Klein, Illinois Institute of Technology
+Updated Date: Dec 7, 2022
 Creation Date: Jun 15, 2022
+
 """"""
+import itertools
+import logging
+import logging.config
+import os
+import time as tm
 from abc import ABC, abstractmethod
-
-from os import path
+from datetime import datetime
 from os import mkdir
-
-import time as tm
+from os import path
 
 import pandas as pd
-
-import logging
-import logging.config
 import requests
-import itertools
 
-# Create logger TODO add filename and line number
+MAX_FILENAME_LEN = 260
+
+# Create logger
 log_file_path = path.join(path.dirname(path.abspath(__file__)), 'logging.conf')
 logging.config.fileConfig(log_file_path, disable_existing_loggers=False)
 logger = logging.getLogger(__name__)
-logging.basicConfig(filename='../../log/imf.log', encoding='utf-8', level=logging.info)
+time_stamp = str(datetime.now())[:16].replace("" "", ""-"").replace("":"", ""-"")
+# Doesn't create new directory in colab
+logdir = '../../log'
+if not path.exists(logdir):
+    mkdir(logdir)
+logging.basicConfig(filename=f'{logdir}/imf_{time_stamp}.log', encoding='utf-8', level=logging.info)
 
 
 # abstract class
 class Series(ABC):
 
-    def __init__(self, series='IFS', search_terms=None, countries=None, period='Q',
-                 start_date=""2000-01-01"", end_date=""2022-10-20""):
+    def __init__(self, series='IFS', search_terms=None, countries=None, period='Q', start_date=None, end_date=None, outdir=None):
         """"""
-
-        :param series: IMF series name, e.g., ""IFS"", ""DOT""
-        :param search_terms: list of strings to find in indicator names in the series
-        :param countries: list of strings containing ISO-2 code of country names
-        :param period: frequency of time series
-        :param start_date: start date of time series
-        :param end_date: end date of time series
+        This function initializes the IMF class, which is used to download data from the IMF's Data API.
+
+        Args:
+          series: The name of the series you want to download. Defaults to IFS
+          search_terms: list of strings to find in indicator names in the series
+          countries: list of strings containing ISO-2 code of country names
+          period: frequency of time series, defaults to Q
+          start_date: The start date of the time series. Defaults to 2000-01-01
+          end_date: The end date of the time series. Defaults to 2022-10-20
+          outdir: the directory where the data will be saved
         """"""
         self.series = series
         self.search_terms = [""Gross Domestic Product, Real""] if search_terms is None else search_terms
         self.countries = [""US""] if countries is None else countries
         self.start_date = start_date
         self.end_date = end_date
         self.period = period
-        self.start_time, self.end_time = start_date[:4], end_date[:4]
+        self.start_time = start_date[:4] if isinstance(start_date, str) else start_date
+        self.end_time = end_date[:4] if isinstance(end_date, str) else end_date
         self.url = 'http://dataservices.imf.org/REST/SDMX_JSON.svc/'
         countries_str = "", "".join(self.countries)
-       
-        #logging.info(f""{countries_str = }, {self.start_date = }, {self.end_date = }"")
+        logging.debug(f""countries = {countries_str}, {start_date = }, {end_date = }"")
 
         self.meta_df = pd.DataFrame()
         self.series_df = pd.DataFrame()
         self.data_df = pd.DataFrame()
         self.dim_dict = {}
+        self.des_list = []
+        self.id_list = []
+
+        self._max_requests = 3
+        self._sleep_sec = 2
 
-        self._max_requests = 10
-        self._sleep_sec = 1
-        
         # Doesn't create new directory in colab
-        outdir = '../out'
-        if not path.exists(outdir):
-            mkdir(outdir)
+        self.outdir = outdir if outdir is not None else f""..{os.sep}out{os.sep}""
+        if not path.exists(self.outdir):
+            mkdir(self.outdir)
 
     def get_series_names(self):
-        """"""
-        TODO
-        """"""
         pass
 
     @abstractmethod
@@ -98,193 +106,288 @@ def describe_data(self):
 class IMF(Series):
 
     def __init__(self, series='IFS', search_terms=None, countries=None, period='Q',
-                 start_date=""2000-01-01"", end_date=""2022-10-20""):
-        super().__init__(series, search_terms, countries, period, start_date, end_date)
+                 start_date=None, end_date=None, outdir=None):
+        super().__init__(series, search_terms, countries, period, start_date, end_date, outdir)
 
     def output_series(self, series=None):
-        """""" Method to output all or some IMF series names to a .csv file in 'out'.
+        """"""
+        This function outputs all or some IMF series dataframe to a csv file, and logs its file path.
 
-        :param series: Series code as a string
 
+        Args:
+          series: Series code as a string. If `series` is `None`, then the function outputs all series to a csv file. If `series` is not `None`, then the function outputs only the series that contain the string `series` to a csv file.
         """"""
+
         # output to csv file
         if series is None:
-            outfile_path = f""../out/series_imf.csv""
+            outfile_path = f""{self.outdir}series_imf.csv""
         else:
-            outfile_path = f""../out/series_{series.lower()}.csv""
-        self.series_df.to_csv(outfile_path, index=False)
-        if series is None:
-            logging.info(f""Output all IMF series table to {outfile_path}"")
+            outfile_path = f""{self.outdir}series_{series.lower()}.csv""
+        if (self.series_df.shape[0] > 0) and (self.series_df.shape[1] > 0):
+            self.series_df.to_csv(outfile_path, index=False)
+            if series is None:
+                logging.info(f""Output all IMF series in a {self.series_df.shape} table to {outfile_path}"")
+            else:
+                logging.info(f""Output series containing '{series}' in a {self.series_df.shape} table to {outfile_path}"")
         else:
-            logging.info(f""Output series containing '{series}' table to {outfile_path}"")
+            logging.warning(f""No series data to be output"")
 
-    def output_dim(self, dim_name=None):
-        """""" Method to output all or some dimension tables to a .csv file in 'out'.
 
-        :param dim_name: Dimension name as a string
+    def output_dim(self, dim_name=None):
+        """"""
+        This function outputs all or some dimension tables to a .csv file in 'out'
 
+        Args:
+          dim_name: Dimension name as a string
         """"""
         # output to csv file
         if dim_name is not None:
             for key in self.dim_dict.keys():
-                outfile_path = f""../out/dim_{key.lower()}.csv""
-                self.dim_dict[key].to_csv(outfile_path, index=False)
-                logging.info(f""Output dimension {key} table to {outfile_path}"")
+                outfile_path = f""{self.outdir}dim_{key.lower()}.csv""
+                if (self.dim_dict[key].shape[0] > 0) and (self.dim_dict[key].shape[1] > 0):
+                    self.dim_dict[key].to_csv(outfile_path, index=False)
+                    logging.info(f""Output dimension {key} in a {self.dim_dict[key].shape} table to {outfile_path}"")
+                else:
+                    logging.warning(f""No dimension {key} data to be output."")
 
     def output_meta(self, indicator=None):
-        """""" Method to output all or some indicators in a tables to a .csv file in 'out'.
-
-        :param indicator: Indicator code as a string
+        """"""
+        Method to output all or some indicators in a tables to a .csv file in 'out'
 
+        Args:
+          indicator: Indicator code as a string
         """"""
+
         # output to csv file
         if indicator is None:
-            outfile_path = f""../out/meta_{self.series.lower()}.csv""
+            outfile_path = f""{self.outdir}meta_{self.series.lower()}.csv""
         else:
-            st_str = ""_"".join(self.search_terms)
-            ctry_str = ""_"".join(self.countries)
-            outfile_path = f""../out/meta_{st_str}_{ctry_str}_{self.period}_{self.start_time}_{self.end_time}.csv""
-        self.meta_df.to_csv(outfile_path, index=False)
-        if indicator is None:
-            logging.info(f""Output meta data of {self.series} table to {outfile_path}"")
+            csv_filename, st_str = self.gen_data_filename(is_meta=True)
+            outfile_path = f""{self.outdir}{csv_filename}""
+        if (self.meta_df.shape[0] > 0) and (self.meta_df.shape[1] > 0):
+            self.meta_df.to_csv(outfile_path, index=False)
+            if indicator is None:
+                logging.info(f""Output meta data of {self.series} in a {self.meta_df.shape} table to {outfile_path}"")
+            else:
+                logging.info(f""Output meta data of {self.series} containing '{st_str}' in a {self.meta_df.shape} table to {outfile_path}"")
         else:
-            logging.info(f""Output meta data of {self.series} containing '{st_str}' table to {outfile_path}"")
-
-    def output_data(self, data=None):
-        """""" Method to output downloaded time series data to a .csv file in 'out'.
-
-        :param data: TODO
+            logging.warning(f""No meta data to be output."")
 
+    def output_data(self, is_gen_filename=False):
         """"""
+        This function outputs the data to a csv file.
 
+        Args:
+          is_gen_filename: generate the csv file name from user inputs if `True`, defaults to `False`
+        """"""
         # output to csv file
-        if data is None:
-            outfile_path = f""../out/data_{self.series.lower()}.csv""
+        if not is_gen_filename:
+            outfile_path = f""{self.outdir}data_{self.series.lower()}.csv""
+        else:
+            csvfilename, st_str = self.gen_data_filename()
+            outfile_path = f""{self.outdir}{csvfilename}""
+
+        if (self.data_df.shape[0] > 0) and (self.data_df.shape[1] > 0):
+            self.data_df.to_csv(outfile_path, index=False)
+            if not is_gen_filename:
+                logging.info(f""Output data of {self.series} in a {self.data_df.shape} table to {outfile_path}"")
+            else:
+                logging.info(f""Output data of {self.series} containing '{st_str}' in a {self.data_df.shape} table to {outfile_path}"")
         else:
-            st_str = ""_"".join(self.search_terms)
-            ctry_str = ""_"".join(self.countries)
-            outfile_path = f""../out/data_{st_str}_{ctry_str}_{self.period}_{self.start_time}_{self.end_time}.csv""
-        self.data_df.to_csv(outfile_path, index=False)
-        if data is None:
-            logging.info(f""Output data of {self.series} table to {outfile_path}"")
+            logging.warning(f""No data to be output."")
+
+    def gen_data_filename(self, is_meta=False):
+        """"""
+        It takes the search terms, countries, period, start time, and end time, and creates a filename for
+        the data up to 250 characters
+
+        Args:
+          is_meta: whether to generate a filename for the meta data or the actual data, defaults to False
+        (optional). Defaults to False
+
+        Returns:
+          The filename and the search terms
+        """"""
+
+        st_str = ""_"".join(self.search_terms)
+        ctry_str = ""_"".join(self.countries)
+
+        time = ''
+        if self.start_time is not None and self.end_time is not None:
+            time = f'_{self.start_time}_{self.end_time}'
+        if self.start_time is None and self.end_time is not None:
+            time = f'_{self.end_time}'
+        if self.start_time is not None and self.end_time is None:
+            time = f'_{self.start_time}_'
+        if not is_meta:
+            csv_filename = f""data_{st_str}_{ctry_str}_{self.period}{time}""[:MAX_FILENAME_LEN]
         else:
-            logging.info(f""Output data of {self.series} containing '{st_str}' table to {outfile_path}"")
+            csv_filename = f""meta_{st_str}_{ctry_str}_{self.period}{time}""[:MAX_FILENAME_LEN]
+        csv_filename = f""{csv_filename}.csv""
+        return csv_filename, st_str
 
     # overriding abstract methods
     def get_series_names(self):
         """"""
+        It takes a list of series names, and returns a dataframe with the series names and their
+        corresponding IDs
 
-        :return: relevant IMF series names
+        Returns:
+          A dataframe with the series names and their corresponding IDs.
         """"""
+
         # searches through series:
         key = 'Dataflow'  # Method with series information
-        search_terms = self.series  # Term to find in series names
-        rq = self.repeat_request(url=f'{self.url}{key}')
-        if rq.status_code == 200:
-            series_list = rq.json()['Structure']['Dataflows']['Dataflow']
-            # find all series names
+        search_terms = self.series  # search terms to find in series names
+        # define the URL we want to use, make a request to the IMF data server
+        json = self.repeat_request(url=f'{self.url}{key}')
+        # check if the request was successful
+        if json is not None:
+            # found series names
+            series_list = json['Structure']['Dataflows']['Dataflow']
+            # normalize the JSON data
             self.series_df = pd.json_normalize(series_list)
             self.series_df = self.series_df.sort_values(""KeyFamilyRef.KeyFamilyID"")
+            self.series_df = self.clean_column_names(self.series_df)
+            # output the data to a CSV file
             self.output_series()
+            is_output = True
         else:
-            logging.warning(f""Failed to download series."")
+            infile = f""{self.outdir}series_{self.series.lower()}.csv""
+            self.series_df = pd.read_csv(infile)
+            logging.info(f""Read series names from historical data {infile}"")
+            is_output = False
 
-        # Filter series names
+        # Filter the dataframe to only include the series names we want
         self.series_df[""search_found""] = False
         string_columns = self.series_df.select_dtypes(include=object).columns
         for col, search_term in itertools.product(string_columns, [search_terms]):
-            #logging.debug(f""{col = }, {search_term = }"")
+            logging.debug(f""{col = }, {search_term = }"")
             self.series_df[""search_found""] = self.series_df[""search_found""] | self.series_df[col].str.lower().str.contains(
                 search_term.lower())
-            #logging.debug(self.series_df[""search_found""].describe())
+            logging.debug(self.series_df[""search_found""].describe())
         self.series_df = self.series_df[self.series_df[""search_found""]]
         self.series_df = self.series_df.drop(['search_found'], axis=1)
-        self.output_series(series=self.series)
-        # Use dict keys to navigate through results:
-        """"""
-        des_list, id_list = [], []
-        for s in series_list:
-            if search_term in s['Name']['#text']:
-                des_list.extend([s['Name']['#text']])
-                id_list.extend([s['KeyFamilyRef']['KeyFamilyID']])
-                # logging.debug(f""{series['Name']['#text']}: {series['KeyFamilyRef']['KeyFamilyID']}"")
-        self.series_names_df = pd.DataFrame(list(zip(des_list, id_list)), columns=['Description', 'ID'])
-        self.series_names_df = self.series_names_df.sort_values(""ID"")
-        """"""
+        if is_output:
+            # output the data to a CSV file
+            self.output_series(series=self.series)
+
         logging.debug(self.series_df.head())
         return self.series_df
 
+
     def get_dimensions(self):
         """"""
-
-        :return: TODO
+        It downloads the dimensions of the series, and then downloads the details of each dimension
         """"""
+
         # finds the dimensions in the series. We need the indicators.
-        des_list, id_list = [], []
         key = f'DataStructure/{self.series}'  # DataStructure Method / series
-        rq = self.repeat_request(url=f'{self.url}{key}')
-        if rq.status_code == 200:
-            dimension_list = rq.json()['Structure']['KeyFamilies']['KeyFamily']['Components']['Dimension']
-            self.dim_df = pd.json_normalize(dimension_list)
+        json = self.repeat_request(url=f'{self.url}{key}')
+        if json is not None:
+            self.dimension_list = json['Structure']['KeyFamilies']['KeyFamily']['Components']['Dimension']
+            self.dim_df = pd.json_normalize(self.dimension_list)
 
             # finds the indicators by the search words:
-            for n, dimension in enumerate(dimension_list):
-                des_list.extend([n + 1])
-                id_list.extend([dimension['@codelist']])
+            for n, dimension in enumerate(self.dimension_list):
+                self.des_list.extend([n + 1])
+                self.id_list.extend([dimension['@codelist']])
                 logging.debug(f""Dimension {n + 1}: {dimension['@codelist']}"")
-            self.dim_meta_df = pd.DataFrame(list(zip(des_list, id_list)), columns=['Dimension', 'ID'])
+            self.dim_meta_df = pd.DataFrame(list(zip(self.des_list, self.id_list)), columns=['Dimension', 'ID'])
             self.dim_meta_df = self.dim_meta_df.sort_values(""Dimension"")
 
-            for n in range(0, len(dimension_list)):
+            for n in range(0, len(self.dimension_list)):
                 try:
-                    dim_name = dimension_list[n]['@codelist']
+                    dim_name = self.dimension_list[n]['@codelist']
                     key = f""CodeList/{dim_name}""
-                    rq = self.repeat_request(url=f'{self.url}{key}')
-                    if rq.status_code == 200:
-                        code_list = rq.json()['Structure']['CodeLists']['CodeList']['Code']
-                        code_df = pd.json_normalize(code_list)
+                    if dim_name[:7] in [""CL_FREQ""]:
+                        def _get_metadata(series='IFS'):
+                            url = 'http://dataservices.imf.org/REST/SDMX_JSON.svc/'
+                            key = f'GenericMetadata/{series}'
+                            metadata = requests.get(f'{self.url}{key}').json()['GenericMetadata']['MetadataSet']['AttributeValueSet']
+                            # TODO use my request function
+                            return metadata
+
+                        def _print_metadata(metadata, indicator='FREQ'):
+                            des_list, value_list = [], []
+                            for i in range(len(metadata)):
+                                ind = metadata[i]['ReportedAttribute'][1]['@conceptID']
+                                if ind == indicator:
+                                    output = metadata[i]['ReportedAttribute'][1]['ReportedAttribute']
+                                    logging.debug(output[0]['Value']['#text'], "": "", output[2]['Value']['#text'])
+                                    des_list.extend([output[0]['Value']['#text']])
+                                    value_list.extend([output[2]['Value']['#text']])
+
+                            df = pd.DataFrame.from_dict({""Value"": value_list, ""Description"": des_list})
+                            return df
+
+                        _metadata = _get_metadata(series=self.series)
+                        code_df = _print_metadata(metadata=_metadata, indicator='FREQ')
+                        code_df = self.clean_column_names(code_df)
+                        if dim_name[-4:].lower() != self.series.lower():
+                            dim_name = ""_"".join([dim_name, self.series])
                         self.dim_dict[dim_name] = code_df
                         logging.debug(f""Dimension {dim_name} details: \n{code_df}"")
                     else:
-                        logging.warning(f""Failed to download {dim_name}."")
+                        json = self.repeat_request(url=f'{self.url}{key}')
+                        if json is not None:
+                            code_list = json['Structure']['CodeLists']['CodeList']['Code']
+                            code_df = pd.json_normalize(code_list)
+                            code_df = self.clean_column_names(code_df)
+                            self.dim_dict[dim_name] = code_df
+                            logging.debug(f""Dimension {dim_name} details: \n{code_df}"")
+                        else:
+                            logging.warning(f""Failed to download {dim_name}."")
                 except:
-                    logging.error(f""Cannot extract dimension {dim_name} details."")
+                    pass
 
             self.output_dim("""")
         else:
             logging.warning(f""Failed to download dimensions."")
 
-        return des_list, dimension_list, id_list
-
-    def download_meta(self, des_list, dimension_list, id_list):
+    def download_meta(self):
         """"""
+        The function downloads the meta data of the time series from the IMF API
 
-        :param des_list:  TODO
-        :param dimension_list:
-        :param id_list:
-        :return: meta data of time series
+        Returns:
+          The meta data of the time series.
         """"""
-        # finds the indicators by the search words:
-        for n, dimension in enumerate(dimension_list):
-            des_list.extend([n + 1])
-            id_list.extend([dimension['@codelist']])
-            # logging.debug(f""Dimension {n + 1}: {dimension['@codelist']}"")
-        dim_meta_df = pd.DataFrame(list(zip(des_list, id_list)), columns=['Dimension', 'ID'])
-        dim_meta_df = dim_meta_df.sort_values(""Dimension"")
-        logging.debug(dim_meta_df.head())
+
+        if len(self.dimension_list) == 0:
+            self.read_meta_df()
+        else:
+            # finds the indicators by the search words:
+            for n, dimension in enumerate(self.dimension_list):
+                self.des_list.extend([n + 1])
+                self.id_list.extend([dimension['@codelist']])
+                # logging.debug(f""Dimension {n + 1}: {dimension['@codelist']}"")
+            dim_meta_df = pd.DataFrame(list(zip(self.des_list, self.id_list)), columns=['Dimension', 'ID'])
+            dim_meta_df = dim_meta_df.sort_values(""Dimension"")
+            dim_meta_df = self.clean_column_names(dim_meta_df)
+            logging.debug(dim_meta_df.head())
+
+            # download  meta data
+            key = f""CodeList/{self.dimension_list[2]['@codelist']}""
+            json = self.repeat_request(url=f'{self.url}{key}')
+            if json is not None:
+                code_list = json['Structure']['CodeLists']['CodeList']['Code']
+                self.meta_df = pd.json_normalize(code_list)
+                self.meta_df = self.clean_column_names(self.meta_df)
+                self.output_meta()
+            else:
+                self.read_meta_df()
 
         # finds the indicators by the search words
-        key = f""CodeList/{dimension_list[2]['@codelist']}""
-        rq = self.repeat_request(url=f'{self.url}{key}')
-        if rq.status_code == 200:
-            code_list = rq.json()['Structure']['CodeLists']['CodeList']['Code']
+        key = f""CodeList/{self.dimension_list[2]['@codelist']}""
+        json = self.repeat_request(url=f'{self.url}{key}')
+        if json is not None:
+            code_list = json['Structure']['CodeLists']['CodeList']['Code']
             self.meta_df = pd.json_normalize(code_list)
-            self.output_meta()
 
             self.meta_df[""search_found""] = False
             string_columns = self.meta_df.select_dtypes(include=object).columns
             for col, search_term in itertools.product(string_columns, self.search_terms):
-                #logging.debug(f""{col = }, {search_term = }"")
+                logging.debug(f""{col = }, {search_term = }"")
                 self.meta_df[""search_found""] = self.meta_df[""search_found""] | self.meta_df[
                     col].str.lower().str.contains(
                     search_term.lower())
@@ -297,41 +400,65 @@ def download_meta(self, des_list, dimension_list, id_list):
                 self.meta_df.columns = [""ID"", *list(self.meta_df.columns)[1:]]
             if ""Description"" not in self.meta_df.columns:
                 self.meta_df.columns = [*list(self.meta_df.columns)[:-1], ""Description""]
-            #logging.info(f""{self.meta_df.shape = }"")
+            logging.debug(f""{self.meta_df.shape = }"")
+            self.meta_df = self.clean_column_names(self.meta_df)
             self.output_meta(indicator="""")
         else:
             logging.warning(f""Failed to download meta data."")
 
         return self.meta_df
 
+    def read_meta_df(self):
+        """"""
+        The function reads a csv file into a Pandas dataframe, renames the columns, and then cleans the column names.
+
+        """"""
+        infile = f""{self.outdir}meta_{self.series.lower()}.csv""
+        self.meta_df = pd.read_csv(infile)
+        self.meta_df = self.meta_df.rename(
+            columns={
+                '@value': 'ID',
+                'Description.#text': 'Description'
+            }
+        )
+        self.meta_df = self.clean_column_names(self.meta_df)
+        logging.info(f""Read meta information from historical data {infile}"")
+
+
     # overriding abstract method
     def download_data(self):
         """"""
+        It downloads data and its meta data from the IMF web server, and saves it to a csv file.
 
-        :return: time series data downloaded
+        Returns:
+          The data is being returned as a pandas dataframe.
         """"""
         self.get_series_names()
-
-        des_list, dimension_list, id_list = self.get_dimensions()  # TODO simplify
-        self.meta_df = self.download_meta(des_list, dimension_list, id_list)  # TODO simplify
+        self.get_dimensions()
+        self.meta_df = self.download_meta()
 
         base = f'{self.url}CompactData/{self.series}/'
-        time = f'?startPeriod={self.start_time}&endPeriod={self.end_time}'
+        time = ''
+        if self.start_time is not None and self.end_time is not None:
+            time = f'.?startPeriod={self.start_time}&endPeriod={self.end_time}'
+        if self.start_time is None and self.end_time is not None:
+            time = f'.?endPeriod={self.end_time}'
+        if self.start_time is not None and self.end_time is None:
+            time = f'.?startPeriod={self.start_time}'
+
         self.data_df = pd.DataFrame()
         # sometimes a big list of country codes results in an error, try splitting it into 2 lists and running this and next cell twice.
         dcn_sa = list(self.meta_df[""ID""].values)
         temp = pd.DataFrame()
         for cont in self.countries:
-
-            # logging.debug(""Current country"", cont)
-            url = f""{base}{self.period}.{cont}.{'+'.join(dcn_sa)}.{time}""
+            logging.debug(""Current country"", cont)
+            url = f""{base}{self.period}.{cont}.{'+'.join(dcn_sa)}{time}""
             # url = f""{base}{period}..{'+'.join(dcn_sa)}.{time}""
-            # logging.debug('url',url)
-            rq = self.repeat_request(url)
-            # logging.debug('rq',rq)
-            if rq.status_code == 200:
+            logger.debug(f""{url = }"")
+            json = self.repeat_request(url)
+            if json is not None:
                 try:
-                    response = rq.json()
+                    response = json
                     series = response['CompactData']['DataSet']['Series']
                     temp_df = pd.DataFrame()
                     if isinstance(series, dict):
@@ -369,8 +496,6 @@ def download_data(self):
                                 }
                             )
 
-                            # temp_df['Country'] = series[n].get('@REF_AREA')
-                            # temp_df['ID'] = series[n].get('@INDICATOR')
                             for k in series[n].keys():
                                 if k != ""Obs"":
                                     temp_df[k] = series[n].get(k)
@@ -389,98 +514,212 @@ def download_data(self):
                             temp_df.drop('@TIME_PERIOD', axis=1, inplace=True)
 
                             temp = pd.concat([temp, temp_df], axis=0)
+
                 except:
-                    logging.error(url)
                     pass
 
+        is_output = True
+        if temp.shape[0] == 0:
+            csv_filename, _ = self.gen_data_filename()
+            outfile_path=  f""{self.outdir}{csv_filename}""
+            temp = pd.read_csv(outfile_path)
+            logging.warning(f""Read data from historical file {outfile_path}"")
+            if ""Description"" in temp.columns:
+                temp = temp.drop(['Description'], axis=1)
+            is_output = False
+
         self.data_df = pd.concat([temp, self.data_df], axis=0)
         self.data_df = pd.merge(self.data_df, self.meta_df, on=""ID"", how=""left"")
-        self.data_df = self.data_df[[""Description"", ""Country"", 'Period', ""Value"", ""ID""]]
+
         # sorting
         self.data_df.sort_values(by=[""ID"", ""Country"", 'Period'], axis=0, inplace=True)
-        #logging.info(f""{self.data_df.shape = }"")
-        self.output_data(data="""")
+        logging.debug(f""data_df.shape = {self.data_df.shape}"")
+
+        # remove special characters in column names
+        self.data_df = self.clean_column_names(self.data_df)
+
+        if is_output:
+            self.output_data(is_gen_filename=True)
 
         return self.data_df
 
+    def clean_column_names(self, df):
+        """"""
+        It replaces special characters in all column names and makes them upper case
+
+        Args:
+          df: The Pandas dataframe to be cleaned
+
+        Returns:
+          The Pandas dataframe with the cleaned column names.
+        """"""
+
+        df.columns = df.columns.str.replace('[@]',   '',  regex=True).  \
+                                str.replace('[#,:]', '_', regex=True).  \
+                                str.replace(""._"",    '.', regex=False). \
+                                str.upper()
+        return df
+
     def repeat_request(self, url):
-        rq = None
+        """"""
+        It will try to get a response from the IMF data server for a given url, and if it doesn't get a
+        response, it will wait a few seconds and try again
+
+        Args:
+          url: the url to request
+
+        Returns:
+          The json object is being returned. It will returns `None` if it does not get a valid response after making a few requests to the IMF data server.
+        """"""
+
+        json = None
         for _ in range(self._max_requests):
+            tm.sleep(self._sleep_sec)
             rq = requests.get(url)
             if rq.status_code == 200:
-                return rq
+                try:
+                    json = rq.json()
+                except:
+                    # print a warning message to the console if it does not gets a valid response from the IMF data server
+                    logger.warning(f""Response received from IMF data server for {url = }."")
+                    return json
 
-            tm.sleep(self._sleep_sec)
+        if not (isinstance(json, dict) and (len(json) >= 1)):
+            # check that the JSON object is a dictionary, and that it has more than one key, otherwise gives a warning
+            logger.warning(f""No response received from IMF data server for {url = } after {self._max_requests} trials."")
+            json = None
 
-        return rq
+        return json
 
     # overriding abstract method
     def get_meta(self):
         """"""
+        This function returns the meta data of the IMF economic indicator(s).
 
-        :return: meta data of time series data
+        Returns:
+          The meta data of the time series data
         """"""
         return self.meta_df
 
     # overriding abstract method
     def get_data(self):
         """"""
+        > This function returns the time series data.
 
-        :return: time series data
+        Returns:
+          The dataframe of the time series data.
         """"""
+
         return self.data_df
 
     def describe_data(self):
         """"""
+        The function returns a summary of data.
 
-        :return: summary of time series data
+        Returns:
+          The describe_data method returns a Pandas dataframe that contains the summary statistics of the data.
         """"""
-        # TODO groupby countries summary
-        return
+
+        self.data_summary_df = self.data_df.groupby([""ID"", ""COUNTRY""]).describe(include=""all"", datetime_is_numeric=True).T
+        return self.data_summary_df
 
     def describe_meta(self):
         """"""
+        This function takes the meta data dataframe and returns a summary of the meta data.
 
         :return: summary of meta data
         """"""
+        self.meta_summary_df  = self.meta_df.describe(include=""all"", datetime_is_numeric=True)
+        return self.meta_summary_df
 
-        return self.meta_df.describe(include=""all"")
 
+class AFRREO(IMF):
+    """"""
+    Sub-Saharan Africa Regional Economic Outlook (AFRREO). A child class of IMF.
+    """"""
+    def __init__(self, series='AFRREO', search_terms=None, countries=None, period='Q',
+                 start_date=None, end_date=None, outdir=None):
+        super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
+                         start_date=start_date, end_date=end_date, outdir=outdir)
 
 class IFS(IMF):
+    """"""
+    International Financial Statistics (IFS). A child class of IMF.
+    """"""
     def __init__(self, series='IFS', search_terms=None, countries=None, period='Q',
-                 start_date=""2000-01-01"", end_date=""2022-10-20""):
+                 start_date=None, end_date=None, outdir=None):
         super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
-                         start_date=start_date, end_date=end_date)
+                         start_date=start_date, end_date=end_date, outdir=outdir)
 
 
 class DOT(IMF):
+    """""" Direction of Trade Statistics (DOT). A child class of IMF.
+
+    """"""
     def __init__(self, series='DOT', search_terms=None, countries=None, period='Q',
-                 start_date=""2000-01-01"", end_date=""2022-10-20""):
+                 start_date=None, end_date=None, outdir=None):
         super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
-                         start_date=start_date, end_date=end_date)
+                         start_date=start_date, end_date=end_date, outdir=outdir)
 
 
 class BOP(IMF):
+    """"""
+    Balance of Payments (BOP). A child class of IMF.
+    """"""
     def __init__(self, series='BOP', search_terms=None, countries=None, period='Q',
-                 start_date=""2000-01-01"", end_date=""2022-10-20""):
+                 start_date=None, end_date=None, outdir=None):
         super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
-                         start_date=start_date, end_date=end_date)
+                         start_date=start_date, end_date=end_date, outdir=outdir)
 
 
 class FSI(IMF):
-    def __init__(self, series='FSI', search_terms=None, countries=None, period='Q',
-                 start_date=""2000-01-01"", end_date=""2022-10-20""):
+    """"""
+    Financial Soundness Indicators (FSIs). A child class of IMF.
+
+    """"""
+    def __init__(self, series='FSI', search_terms=None, countries=None, period='M',
+                 start_date=None, end_date=None, outdir=None):
         super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
-                         start_date=start_date, end_date=end_date)
+                         start_date=start_date, end_date=end_date, outdir=outdir)
 
 
 class GFSR(IMF):
     """"""
-    TODO What is GFSR
+    Government Finance Statistics (GFS), Revenue. A child class of IMF.
     """"""
 
     def __init__(self, series='GFSR', search_terms=None, countries=None, period='A',
-                 start_date=""2000-01-01"", end_date=""2022-10-20""):
+                 start_date=None, end_date=None, outdir=None):
         super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
-                         start_date=start_date, end_date=end_date)
+                         start_date=start_date, end_date=end_date, outdir=outdir)
+
+
+class COFOG(IMF):
+    """"""
+    Government Finance Statistics (GFS), Expenditure by Function of Government (COFOG). A child class of IMF.
+    """"""
+
+    def __init__(self, series='COFOG', search_terms=None, countries=None, period='A',
+                 start_date=None, end_date=None, outdir=None):
+        super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
+                         start_date=start_date, end_date=end_date, outdir=outdir)
+
+
+# > The `HPDD` class is a subclass of the `IMF` class. It inherits all of the methods and attributes of the `IMF` class,
+# and adds a few of its own
+class HPDD(IMF):
+    """"""
+     Historical Public Debt Database (HPDD). A child class of IMF.
+    """"""
+    def __init__(self, series='HPDD', search_terms=None, countries=None, period='A',
+                 start_date=None, end_date=None, outdir=None):
+        super().__init__(series=series, search_terms=search_terms, countries=countries, period=period,
+                         start_date=start_date, end_date=end_date, outdir=outdir)
+
+
+#
+if __name__ == '__main__':
+    ifs = IFS(search_terms=[""Gross Domestic Product, Real""], countries=[""US"", ""DE""],
+                      period='Q', start_date=""2000"", end_date=""2022"", outdir = f""..{os.sep}..{os.sep}out{os.sep}"")
+    df = ifs.download_data()
+    df_summary = ifs.describe_data()
\ No newline at end of file

---FILE: tests/test_afrreo.py---
@@ -0,0 +1,16 @@
+import unittest
+from imfdatapy.imf import *
+
+class MyTestCase(unittest.TestCase):
+
+
+    def test_imf_afrreo_eg1(self):
+        affreo = AFRREO(search_terms=[""total expenditure""], countries=[""CF""], period='A', start_date=None, end_date=None)
+        df = affreo.download_data()
+        self.assertGreaterEqual(df.shape[0], 20)
+        meta_df = affreo.get_meta()
+        self.assertGreaterEqual(meta_df.shape[0], 1)
+
+
+if __name__ == '__main__':
+    unittest.main()
\ No newline at end of file

---FILE: tests/test_bop.py---
@@ -1,6 +1,4 @@
-import logging.config
 import unittest
-
 from imfdatapy.imf import *
 
 class MyTestCase(unittest.TestCase):
@@ -16,11 +14,5 @@ def test_imf_bop_eg1(self):
         self.assertGreaterEqual(meta_df.shape[0], 6)
 
 
-
-
 if __name__ == '__main__':
-    unittest.main()
-
-""""""
-
-""""""
\ No newline at end of file
+    unittest.main()
\ No newline at end of file

---FILE: tests/test_dot.py---
@@ -1,15 +1,9 @@
-import logging.config
 import unittest
-
 from imfdatapy.imf import *
 
-
-
 class MyTestCase(unittest.TestCase):
 
-
     def test_imf_dot_eg1(self):
-
         dot = DOT(search_terms=[""trade""], countries=[""US""], period='Q', start_date=""2000"",
                   end_date=""2022"")
         df = dot.download_data()
@@ -18,11 +12,5 @@ def test_imf_dot_eg1(self):
         self.assertGreaterEqual(meta_df.shape[0], 1)
 
 
-
-
 if __name__ == '__main__':
-    unittest.main()
-
-""""""
-
-""""""
\ No newline at end of file
+    unittest.main()
\ No newline at end of file

---FILE: tests/test_fsi.py---
@@ -1,10 +1,6 @@
-import logging.config
 import unittest
-
-
 from imfdatapy.imf import *
 
-
 class MyTestCase(unittest.TestCase):
 
     def test_imf_fsi_eg1(self):
@@ -16,10 +12,5 @@ def test_imf_fsi_eg1(self):
         self.assertGreaterEqual(meta_df.shape[0], 6)
 
 
-
 if __name__ == '__main__':
-    unittest.main()
-
-""""""
-
-""""""
\ No newline at end of file
+    unittest.main()
\ No newline at end of file

---FILE: tests/test_gfsr.py---
@@ -1,10 +1,6 @@
-import logging.config
 import unittest
-
-
 from imfdatapy.imf import *
 
-
 class MyTestCase(unittest.TestCase):
 
     def test_imf_gfsr_eg1(self):
@@ -15,11 +11,5 @@ def test_imf_gfsr_eg1(self):
         self.assertGreaterEqual(meta_df.shape[0], 4)
 
 
-
-
 if __name__ == '__main__':
-    unittest.main()
-
-""""""
-
-""""""
\ No newline at end of file
+    unittest.main()
\ No newline at end of file

---FILE: tests/test_hpdd.py---
@@ -0,0 +1,16 @@
+import unittest
+from imfdatapy.imf import *
+
+class MyTestCase(unittest.TestCase):
+
+    def test_imf_hpdd_eg1(self):
+        hpdd = HPDD(search_terms=[""GDP""], countries=[""US""], period='A', start_date=None,
+                  end_date=None)
+        df = hpdd.download_data()
+        self.assertGreaterEqual(df.shape[0], 213)
+        meta_df = hpdd.get_meta()
+        self.assertGreaterEqual(meta_df.shape[0], 1)
+
+
+if __name__ == '__main__':
+    unittest.main()
\ No newline at end of file

---FILE: tests/test_ifs.py---
@@ -1,23 +1,24 @@
-import logging.config
+# The IMF class is a wrapper for the IMF's Data API
+# The IMF class is a wrapper for the IMF's Data API
+# The IMF class is a wrapper for the IMF's Data API
 import unittest
-
-
 from imfdatapy.imf import *
 
 class MyTestCase(unittest.TestCase):
 
     def test_imf_oo(self):
         ifs = IMF()
-        self.assertEqual(ifs.start_date, ""2000-01-01"")
+        self.assertEqual(ifs.start_date, None)
 
     def test_imf_ifs_eg1(self):
         ifs = IMF(search_terms=[""gross domestic product, real""], countries=[""US""], period='Q', start_date=""2000"",
                   end_date=""2022"")
         df = ifs.download_data()
         self.assertGreaterEqual(df.shape[0], 174)
+        self.assertGreaterEqual(df.shape[1], 10)
         meta_df = ifs.get_meta()
         self.assertGreaterEqual(meta_df.shape[0], 3)
-
+        self.assertGreaterEqual(meta_df.shape[1], 3)
 
 
     def test_imf_ifs_eg2(self):
@@ -44,11 +45,14 @@ def test_imf_ifs_eg4(self):
         meta_df = ifs.get_meta()
         self.assertGreaterEqual(meta_df.shape[0], 3)
 
-
+    def test_imf_ifs_eg5(self):
+        ifs = IFS(countries=[""US""], period='Q', start_date=None, end_date=None)
+        df = ifs.download_data()
+        self.assertGreaterEqual(df.shape[0], 374)
+        self.assertGreaterEqual(df.shape[1], 5)
+        meta_df = ifs.get_meta()
+        self.assertGreaterEqual(meta_df.shape[0], 3)
+        self.assertGreaterEqual(meta_df.shape[1], 3)
 
 if __name__ == '__main__':
-    unittest.main()
-
-""""""
-
-""""""
\ No newline at end of file
+    unittest.main()
\ No newline at end of file"
Economic-and-Financial-Data-Discovery,imfdatapy,a349a416035ab52e51d5ab4a2d034cfe5b95056c,Sou-Cheng Choi,schoi32@iit.edu,2022-11-26T12:48:11Z,Sou-Cheng Choi,schoi32@iit.edu,2022-11-26T12:48:11Z,Fix pypi release issue,README.md;environment.yml;setup.py,False,False,False,False,10,10,20,"---FILE: README.md---
@@ -3,8 +3,6 @@
 A package for data discovery and extraction from the International Monetary Fund (IMF)!
 This repository contains Python source code and Jupyter notebooks with examples on how to extract data from the IMF.
 
-TODO data license warning
-
 ## Installation
 
 ```bash
@@ -34,6 +32,10 @@ Interested in contributing? Check out the contributing guidelines. Please note t
 
 `imfdatapy` was created by Sou-Cheng T. Choi and Irina Klein, Illinois Institute of Technology. It is licensed under the terms of the Apache License, v2.0.
 
+With regard to the terms for using IMF data, please refer to IMF's [Copyright and Usage](https://www.imf.org/external/terms.htm) and pay special attention to the 
+section _SPECIAL TERMS AND CONDITIONS PERTAINING TO THE USE OF DATA_.  
+
+
 ## Credits
 
 `imfdatapy` was created with [`cookiecutter`](https://cookiecutter.readthedocs.io/en/latest/) and the `py-pkgs-cookiecutter` [template](https://github.com/py-pkgs/py-pkgs-cookiecutter).
\ No newline at end of file

---FILE: environment.yml---
@@ -15,6 +15,4 @@ dependencies:
   - pylint=2.14.5
   - requests=2.28.1
   - pyquery=1.4.3
-  - coverage=6.3.2
-  - pylint=2.14.5
   - graphviz=2.50.0
\ No newline at end of file

---FILE: setup.py---
@@ -32,7 +32,7 @@ def finalize_options(self):
         pass
 
     def run(self):
-        os.system(""rm -vrf ./build ./dist ./*.pyc ./imf/imf.egg-info"")
+        os.system(""rm -vrf ./build ./dist ./*.pyc ./imfdatapy/imfdatapy.egg-info"")
 
 
 try:
@@ -42,19 +42,19 @@ def run(self):
     long_description = ""IMF Data Discovery""
 
 packages = [
-    'imf']
+    'imfdatapy']
 
 setuptools.setup(
-    name=""imf"",
+    name=""imfdatapy"",
     version=""1.0"",
     author=""Sou-Cheng T. Choi and Irina Klein"",
     author_email=""schoi32@iit.edu"",
     license='Apache license 2.0',
     description=""IMF Data Discovery API in Python 3"",
     long_description=long_description,
     long_description_content_type=""text/markdown"",
-    url=""https://github.com/kloiks/IMF_data_discovery"",
-    download_url=""https://github.com/kloiks/IMF_data_discovery/releases/tag/v1.0.gz"",
+    url=""https://github.com/Economic-and-Financial-Data-Discovery/imfdatapy"",
+    download_url=""https://github.com/Economic-and-Financial-Data-Discovery/imfdatapy/releases/tag/v1.0.gz"",
     packages=packages,
     install_requires=[
         'requests >= 2.28.1',
@@ -63,7 +63,7 @@ def run(self):
         ""Programming Language :: Python :: 3"",
         ""License :: OSI Approved :: Apache Software License"",
         ""Operating System :: OS Independent""],
-    keywords=[""imf"", ""data""],
+    keywords=[""IMF data"", ""JSON API""],
     python_requires="">=3.5"",
     include_package_data=True,
     cmdclass={"
