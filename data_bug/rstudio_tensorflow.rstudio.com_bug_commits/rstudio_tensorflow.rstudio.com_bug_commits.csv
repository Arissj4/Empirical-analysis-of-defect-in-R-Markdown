repo_owner,repo_name,commit_hash,author_name,author_email,author_date,committer_name,committer_email,committer_date,message,filenames,touches_rmd,touches_r,touches_r_or_rmd,is_merge,added,deleted,changed,diff
rstudio,tensorflow.rstudio.com,5f479cd2423ea507fe90b2b52127a76657e798c9,Tomasz Kalinowski,tomasz@posit.co,2023-02-15T20:30:11Z,Tomasz Kalinowski,tomasz@posit.co,2023-02-15T20:30:11Z,fix reference formatting,reference/tensorflow/install_tensorflow.qmd,True,False,True,False,5,10,15,"---FILE: reference/tensorflow/install_tensorflow.qmd---
@@ -50,26 +50,22 @@ install_tensorflow(
 
 ## Custom Installation
 
-  `install_tensorflow()` or `keras::install_keras()` isn't required to use tensorflow with the package. If you manually configure a python environment with the required dependencies, you can tell R to use it by pointing reticulate at it, commonly by setting an environment variable:```
-
+  `install_tensorflow()` or `keras::install_keras()` isn't required to use tensorflow with the package. If you manually configure a python environment with the required dependencies, you can tell R to use it by pointing reticulate at it, commonly by setting an environment variable:
+  
+```
 Sys.setenv(""RETICULATE_PYTHON"" = ""~/path/to/python-env/bin/python"") 
-
 ``` 
 
 ## Apple Silicon
 
-  Tensorflow on Apple Silicon is not officially supported by the Tensorflow maintainers. However Apple has published a custom version of Tensorflow compatible with Arm Macs. `install_tensorflow()` will install the special packages `tensorflow-macos` and `tensorflow-metal` on Arm Macs. See [https://developer.apple.com/metal/tensorflow-plugin/](https://developer.apple.com/metal/tensorflow-plugin/) for instructions on how to do the equivalent manually. Please note that this is an experimental build of both Python and Tensorflow, with known issues. In particular, certain operations will cause errors, but can often be remedied by pinning them to the CPU. For example:```
+  Tensorflow on Apple Silicon is not officially supported by the Tensorflow maintainers. However Apple has published a custom version of Tensorflow compatible with Arm Macs. `install_tensorflow()` will install the special packages `tensorflow-macos` and `tensorflow-metal` on Arm Macs. See [https://developer.apple.com/metal/tensorflow-plugin/](https://developer.apple.com/metal/tensorflow-plugin/) for instructions on how to do the equivalent manually. Please note that this is an experimental build of both Python and Tensorflow, with known issues. In particular, certain operations will cause errors, but can often be remedied by pinning them to the CPU. For example:
 
+```
 x <- array(runif(64*64), c(1, 64, 64)) 
-
 keras::layer_random_rotation(x, .5)  # Error: 
-
 # No registered 'RngReadAndSkip' OpKernel for 'GPU' devices 
-
 # Pin the operation to the CPU to avoid the error 
-
 with(tf$device(""CPU""), keras::layer_random_rotation(x, .5) ) # No Error 
-
 ``` 
 
 ## Additional Packages
@@ -81,4 +77,3 @@ with(tf$device(""CPU""), keras::layer_random_rotation(x, .5) ) # No Error
 
 ## See Also
  `keras::install_keras()` 
-"
rstudio,tensorflow.rstudio.com,8ea17c60b891cb11d1d8c6486939609f14b2ea5d,Tomasz Kalinowski,tomasz@posit.co,2023-02-13T03:22:25Z,Tomasz Kalinowski,tomasz@posit.co,2023-02-13T03:22:25Z,"fix target array shape

training target was being simplified to a vector,
causing an error in the `evaluate()` call below.",examples/timeseries_classification_transformer.qmd,True,False,True,False,5,6,11,"---FILE: examples/timeseries_classification_transformer.qmd---
@@ -5,7 +5,7 @@ authors:
   - ""[terrytangyuan](https://github.com/terrytangyuan) - R adaptation""
   - ""[t-kalinowski](https://github.com/t-kalinowski) - R adaptation""
 date-created: 2022/12/12
-date-last-modified: 2022/12/12
+date-last-modified: 2023/2/12
 description: ""This notebook demonstrates how to do timeseries classification using a Transformer model.""
 categories: [timeseries]
 aliases:
@@ -51,11 +51,11 @@ y_test <- as.matrix(test_df[, 1])
 n_classes <- length(unique(y_train))
 
 shuffle_ind <- sample(nrow(x_train))
-x_train <- x_train[shuffle_ind,]
-y_train <- y_train[shuffle_ind,]
+x_train <- x_train[shuffle_ind, , drop = FALSE]
+y_train <- y_train[shuffle_ind, , drop = FALSE]
 
 y_train[y_train == -1] <- 0
-y_test[y_test == -1] <- 0
+y_test [y_test  == -1] <- 0
 
 dim(x_train) <- c(dim(x_train), 1)
 dim(x_test) <- c(dim(x_test), 1)
@@ -75,7 +75,6 @@ resulting layer can be stacked multiple times. The projection layers are
 implemented through `layer_conv_1d()`.
 
 ```{r}
-
 transformer_encoder <- function(inputs,
                                 head_size,
                                 num_heads,
@@ -163,7 +162,7 @@ model <- build_model(
 model %>% compile(
   loss = ""sparse_categorical_crossentropy"",
   optimizer = optimizer_adam(learning_rate = 1e-4),
-  metrics = c(""sparse_categorical_accuracy""),
+  metrics = c(""sparse_categorical_accuracy"")
 )
 
 model"
rstudio,tensorflow.rstudio.com,8a0556a44236c5e30736654415a73afedbebc984,Tomasz Kalinowski,tomasz@posit.co,2023-02-13T01:21:47Z,Tomasz Kalinowski,tomasz@posit.co,2023-02-13T01:21:47Z,render + error fixes + style tweaks,.gitignore;_freeze/examples/image_classification_from_scratch/execute-results/html.json;_freeze/examples/image_classification_from_scratch/figure-html/unnamed-chunk-13-1.png;_freeze/examples/image_classification_from_scratch/figure-html/unnamed-chunk-14-1.png;_freeze/examples/image_classification_from_scratch/figure-html/unnamed-chunk-15-1.png;_freeze/examples/image_classification_from_scratch/figure-html/unnamed-chunk-6-1.png;_freeze/examples/image_classification_from_scratch/figure-html/unnamed-chunk-8-1.png;examples/image_classification_from_scratch.qmd,True,False,True,False,193,139,332,"---FILE: .gitignore---
@@ -18,3 +18,9 @@ guides/keras/logs/
 examples/creditcardfraud.zip
 examples/fraud_model_at*
 examples/creditcard.csv
+examples/aclImdb_v1.tar.gz
+examples/aclImdb/*
+examples/PetImages
+examples/kagglecatsanddogs_5340.zip
+examples/readme\[1\].txt
+examples/save_at_*.keras

---FILE: _freeze/examples/image_classification_from_scratch/execute-results/html.json---
@@ -0,0 +1,16 @@
+{
+  ""hash"": ""3910da906502ff87e26231a585de7998"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: \""Image classification from scratch\""\nauthors:\n  - \""[fchollet](https://twitter.com/fchollet)\""\n  - \""[terrytangyuan](https://github.com/terrytangyuan) - R adaptation\""\n  - \""[t-kalinowski](https://github.com/t-kalinowski) - R adaptation\""\ndate-created: 2022/12/19\ndate-last-modified: !expr Sys.Date()\ndescription: \""Training an image classifier from scratch on the Kaggle Cats vs Dogs dataset.\""\ncategories: [vision]\naliases:\n  - ../guide/keras/examples/image_classification_from_scratch/index.html\n---\n\n\n## Introduction\n\nThis example shows how to do image classification from scratch, starting\nfrom JPEG image files on disk, without leveraging pre-trained weights or\na pre-made Keras Application model. We demonstrate the workflow on the\nKaggle Cats vs Dogs binary classification dataset.\n\nWe use the `image_dataset_from_directory` utility to generate the\ndatasets, and we use Keras image preprocessing layers for image\nstandardization and data augmentation.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nset.seed(1234)\n```\n:::\n\n\n## Load the data: the Cats vs Dogs dataset\n\n### Raw data download\n\nFirst, let's download the 786M ZIP archive of the raw data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \""https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\""\noptions(timeout = 60 * 5) # 5 minutes\ndownload.file(url, destfile = \""kagglecatsanddogs_5340.zip\"") # (786.7 MB)\n## To see a list of everything in the zip file:\n# zip::zip_list(\""kagglecatsanddogs_5340.zip\"") |> tibble::as_tibble()\nzip::unzip(\""kagglecatsanddogs_5340.zip\"")\n```\n:::\n\n\nNow we have a `PetImages` folder which contain two subfolders, `Cat` and\n`Dog`. Each subfolder contains image files for each category.\n\n\n::: {.cell paged.print='false'}\n\n```{.r .cell-code}\nfs::dir_info(\""PetImages\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 18\n  path         type   size permiss…¹ modification_time   user  group devic…²\n  <fs::path>   <fct> <fs:> <fs::per> <dttm>              <chr> <chr>   <dbl>\n1 …tImages/Cat dire…  272K rwx------ 2023-02-11 18:23:59 toma… toma…   66306\n2 …tImages/Dog dire…  272K rwx------ 2023-02-11 18:23:59 toma… toma…   66306\n# … with 10 more variables: hard_links <dbl>, special_device_id <dbl>,\n#   inode <dbl>, block_size <dbl>, blocks <dbl>, flags <int>,\n#   generation <dbl>, access_time <dttm>, change_time <dttm>,\n#   birth_time <dttm>, and abbreviated variable names ¹​permissions,\n#   ²​device_id\n```\n:::\n\n```{.r .cell-code}\nfs::dir_info(\""PetImages\"", recurse = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 23,202 × 18\n   path       type    size permiss…¹ modification_time   user  group devic…²\n   <fs::path> <fct> <fs::> <fs::per> <dttm>              <chr> <chr>   <dbl>\n 1 …mages/Cat dire…   272K rwx------ 2023-02-11 18:23:59 toma… toma…   66306\n 2 …Cat/0.jpg file   11.9K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n 3 …Cat/1.jpg file   16.5K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n 4 …at/10.jpg file   34.5K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n 5 …t/100.jpg file     30K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n 6 …/1000.jpg file   25.7K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n 7 …10000.jpg file  127.2K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n 8 …10001.jpg file   26.4K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n 9 …10002.jpg file   25.6K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n10 …10003.jpg file   27.9K rw------- 2017-02-10 09:54:54 toma… toma…   66306\n# … with 23,192 more rows, 10 more variables: hard_links <dbl>,\n#   special_device_id <dbl>, inode <dbl>, block_size <dbl>, blocks <dbl>,\n#   flags <int>, generation <dbl>, access_time <dttm>, change_time <dttm>,\n#   birth_time <dttm>, and abbreviated variable names ¹​permissions,\n#   ²​device_id\n```\n:::\n:::\n\n\n### Filter out corrupted images\n\nWhen working with lots of real-world image data, corrupted images are a\ncommon occurrence. Let's filter out badly-encoded images that do not\nfeature the string \""JFIF\"" in their header.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_deleted <- 0L\nfor(filepath in list.files(\""PetImages\"", pattern = \""\\\\.jpg$\"",\n                           recursive = TRUE, full.names = TRUE)) {\n  header <- readBin(filepath, what = \""raw\"", n = 10)\n  if(!identical(header[7:10], charToRaw(\""JFIF\""))) {\n    n_deleted <- n_deleted + 1L\n    unlink(filepath)\n  }\n}\n\ncat(sprintf(\""Deleted %d images\\n\"", n_deleted))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDeleted 0 images\n```\n:::\n:::\n\n\n## Generate a `Dataset`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimage_size <- c(180, 180)\nbatch_size <- 32\n\ntrain_ds <- image_dataset_from_directory(\n    \""PetImages\"",\n    validation_split = 0.2,\n    subset = \""training\"",\n    seed = 1337,\n    image_size = image_size,\n    batch_size = batch_size,\n)\nval_ds <- image_dataset_from_directory(\n    \""PetImages\"",\n    validation_split = 0.2,\n    subset = \""validation\"",\n    seed = 1337,\n    image_size = image_size,\n    batch_size = batch_size,\n)\n```\n:::\n\n\n## Visualize the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch <- train_ds %>%\n  as_iterator() %>%\n  iter_next()\n\nstr(batch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ :<tf.Tensor: shape=(32, 180, 180, 3), dtype=float32, numpy=…>\n $ :<tf.Tensor: shape=(32), dtype=int32, numpy=…>\n```\n:::\n\n```{.r .cell-code}\nc(images, labels) %<-% batch\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndisplay_image_tensor <- function(x, ..., max = 255,\n                                 plot_margins = c(0, 0, 0, 0)) {\n  if(!is.null(plot_margins))\n    par(mar = plot_margins)\n\n  x %>%\n    as.array() %>%\n    drop() %>%\n    as.raster(max = max) %>%\n    plot(..., interpolate = FALSE)\n}\n\npar(mfrow = c(3, 3))\nfor (i in 1:9)\n  display_image_tensor(images[i,,,],\n                       plot_margins = rep(.5, 4))\n```\n\n::: {.cell-output-display}\n![](image_classification_from_scratch_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Using image data augmentation\n\nWhen you don't have a large image dataset, it's a good practice to\nartificially introduce sample diversity by applying random yet realistic\ntransformations to the training images, such as random horizontal\nflipping or small random rotations. This helps expose the model to\ndifferent aspects of the training data while slowing down overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If you are on an M1 mac, you may need to wrap this model definition in\n# with(tf$device(\""CPU\""), { ... })\n# https://stackoverflow.com/questions/69088577/apple-m1-i-got-no-registered-rngreadandskip-opkernel-for-gpu-devices-comp\n\ndata_augmentation <-\n  keras_model_sequential(input_shape = c(image_size, 3)) %>%\n  layer_random_flip(\""horizontal\"") %>%\n  layer_random_rotation(factor = 0.1)\n```\n:::\n\n\nLet's visualize what the augmented samples look like, by applying\n`data_augmentation` repeatedly to the first image in the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(3, 3))\nfor (i in 1:9) {\n    images[4, , , , drop = FALSE] %>%\n    data_augmentation() %>%\n    display_image_tensor()\n}\n```\n\n::: {.cell-output-display}\n![](image_classification_from_scratch_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Standardizing the data\n\nOur image are already in a standard size (180x180), as they are being\nyielded as contiguous `float32` batches by our dataset. However, their\nRGB channel values are in the `[0, 255]` range. This is not ideal for a\nneural network; in general you should seek to make your input values\nsmall. Here, we will standardize values to be in the `[0, 1]` by using a\n`Rescaling` layer at the start of our model.\n\n## Two options to preprocess the data\n\nThere are two ways you could be using the `data_augmentation`\npreprocessor: **Option 1: Make it part of the model**, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- layer_input(shape = input_shape) %>%\n  data_augmentation() %>%\n  layer_rescaling(1./255)\n...  # Rest of the model\n```\n:::\n\n\nWith this option, your data augmentation will happen *on device*,\nsynchronously with the rest of the model execution, meaning that it will\nbenefit from GPU acceleration.\n\nNote that data augmentation is inactive at test time, so the input\nsamples will only be augmented during `fit()`, not when calling\n`evaluate()` or `predict()`.\n\nIf you're training on GPU, this may be a good option.\n\n**Option 2: apply it to the dataset**, so as to obtain a dataset that\nyields batches of augmented images, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugmented_train_ds <- train_ds %>%\n  dataset_map(function(x, y) {\n    x <- data_augmentation(x, training = TRUE)\n    list(x, y)\n  })\n```\n:::\n\n\nWith this option, your data augmentation will happen **on CPU**,\nasynchronously, and will be buffered before going into the model (this\nis because all TF Dataset operations, include any defined in\n`dataset_map()`, are pinned to the CPU).\n\nIf you're training on CPU, this is the better option, since it makes\ndata augmentation asynchronous and non-blocking.\n\nIn our case, we'll go with the second option. If you're not sure which\none to pick, this second option (asynchronous preprocessing) is always a\nsolid choice.\n\n## Configure the dataset for performance\n\nLet's apply data augmentation to our training dataset, and let's make\nsure to use buffered prefetching so we can yield data from disk without\nhaving I/O becoming blocking:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply `data_augmentation` to the training images.\ntrain_ds <- train_ds %>%\n  dataset_map(function(images, labels) {\n    list(data_augmentation(images, training = TRUE),\n         labels)\n  })\n\n# Prefetching samples in GPU memory helps maximize GPU utilization.\ntrain_ds %<>% dataset_prefetch()\nval_ds   %<>% dataset_prefetch()\n```\n:::\n\n\n## Build a model\n\nWe'll build a small version of the Xception network. We haven't\nparticularly tried to optimize the architecture; if you want to do a\nsystematic search for the best model configuration, consider using\n[KerasTuner](https://github.com/keras-team/keras-tuner).\n\nNote that:\n\n-   We start the model with the `data_augmentation` preprocessor,\n    followed by a `Rescaling` layer.\n-   We include a `Dropout` layer before the final classification layer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_model <- function(input_shape, num_classes) {\n\n  inputs <- layer_input(shape = input_shape)\n\n  x <- inputs %>%\n    # data augmentation() ? %>%\n    layer_rescaling(1.0 / 255)\n\n  x <- x %>%\n    layer_conv_2d(128, 3, strides = 2, padding = \""same\"") %>%\n    layer_batch_normalization() %>%\n    layer_activation(\""relu\"")\n\n  previous_block_activation <- x  # Set aside residual\n  for (size in c(256, 512, 728)) {\n    x <- x %>%\n      layer_activation(\""relu\"") %>%\n      layer_separable_conv_2d(size, 3, padding = \""same\"") %>%\n      layer_batch_normalization() %>%\n\n      layer_activation(\""relu\"") %>%\n      layer_separable_conv_2d(size, 3, padding = \""same\"") %>%\n      layer_batch_normalization() %>%\n\n      layer_max_pooling_2d(3, strides = 2, padding = \""same\"")\n\n    # Project residual\n    residual <- previous_block_activation %>%\n      layer_conv_2d(filters = size, kernel_size = 1, strides = 2,\n                    padding = \""same\"")\n\n    x <- tf$keras$layers$add(list(x, residual))  # Add back residual\n    previous_block_activation <- x  # Set aside next residual\n  }\n\n  x <- x %>%\n    layer_separable_conv_2d(1024, 3, padding = \""same\"") %>%\n    layer_batch_normalization() %>%\n    layer_activation(\""relu\"") %>%\n    layer_global_average_pooling_2d()\n\n  if (num_classes == 2) {\n    activation <- \""sigmoid\""\n    units <- 1\n  } else {\n    activation <- \""softmax\""\n    units <- num_classes\n  }\n\n  outputs <- x %>%\n    layer_dropout(0.5) %>%\n    layer_dense(units, activation = activation)\n\n  return(keras_model(inputs, outputs))\n}\n\nmodel <- make_model(input_shape = c(image_size, 3), num_classes = 2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](image_classification_from_scratch_files/figure-html/unnamed-chunk-13-1.png){width=576}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nepochs <- 25\n\ncallbacks <- list(callback_model_checkpoint(\""save_at_{epoch}.keras\""))\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-3),\n  loss = \""binary_crossentropy\"",\n  metrics = list(\""accuracy\""),\n)\nhistory <- model %>% fit(\n    train_ds,\n    epochs = epochs,\n    callbacks = callbacks,\n    validation_data = val_ds,\n)\nplot(history)\n```\n\n::: {.cell-output-display}\n![](image_classification_from_scratch_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nWe get to \\>90% validation accuracy after training for 25 epochs on the\nfull dataset (in practice, you can train for 50+ epochs before\nvalidation performance starts degrading).\n\n## Run inference on new data\n\nYou can reload one of the models saved by the checkpoint callback like\nthis:\n\n``` r\nmodel <- load_model_tf(\""save_at_25.keras\"")\n```\n\nNote that data augmentation and dropout are inactive at inference time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load an image as a tensor\nimg_tensor <-\n  \""PetImages/Cat/6779.jpg\"" %>%\n  tf$io$read_file() %>%\n  tf$io$decode_image() %>%\n  tf$image$resize(as.integer(image_size)) %>%\n  tf$expand_dims(0L)  # Create batch axis\n\nscore <- model %>% predict(img_tensor)\n\ndisplay_image_tensor(img_tensor)\n```\n\n::: {.cell-output-display}\n![](image_classification_from_scratch_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsprintf(\""This image is %.2f%% cat and %.2f%% dog.\"", 100 * (1 - score), 100 * score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \""This image is 69.22% cat and 30.78% dog.\""\n```\n:::\n:::\n"",
+    ""supporting"": [
+      ""image_classification_from_scratch_files""
+    ],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: examples/image_classification_from_scratch.qmd---
@@ -5,7 +5,7 @@ authors:
   - ""[terrytangyuan](https://github.com/terrytangyuan) - R adaptation""
   - ""[t-kalinowski](https://github.com/t-kalinowski) - R adaptation""
 date-created: 2022/12/19
-date-last-modified: 2022/12/19
+date-last-modified: !expr Sys.Date()
 description: ""Training an image classifier from scratch on the Kaggle Cats vs Dogs dataset.""
 categories: [vision]
 aliases:
@@ -14,17 +14,18 @@ aliases:
 
 ## Introduction
 
-This example shows how to do image classification from scratch, starting from JPEG
-image files on disk, without leveraging pre-trained weights or a pre-made Keras
-Application model. We demonstrate the workflow on the Kaggle Cats vs Dogs binary
-classification dataset.
- 
-We use the `image_dataset_from_directory` utility to generate the datasets, and
-we use Keras image preprocessing layers for image standardization and data augmentation.
+This example shows how to do image classification from scratch, starting
+from JPEG image files on disk, without leveraging pre-trained weights or
+a pre-made Keras Application model. We demonstrate the workflow on the
+Kaggle Cats vs Dogs binary classification dataset.
+
+We use the `image_dataset_from_directory` utility to generate the
+datasets, and we use Keras image preprocessing layers for image
+standardization and data augmentation.
 
 ## Setup
 
-```{r}
+```{r setup}
 library(tensorflow)
 library(keras)
 library(tfdatasets)
@@ -37,55 +38,48 @@ set.seed(1234)
 
 First, let's download the 786M ZIP archive of the raw data:
 
-```{bash}
-curl -O https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip
-```
-
-```{bash}
-unzip -q kagglecatsanddogs_5340.zip
-ls
+```{r, eval = !file.exists(""kagglecatsanddogs_5340.zip"")}
+url <- ""https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip""
+options(timeout = 60 * 5) # 5 minutes
+download.file(url, destfile = ""kagglecatsanddogs_5340.zip"") # (786.7 MB)
+## To see a list of everything in the zip file:
+# zip::zip_list(""kagglecatsanddogs_5340.zip"") |> tibble::as_tibble()
+zip::unzip(""kagglecatsanddogs_5340.zip"")
 ```
 
-Now we have a `PetImages` folder which contain two subfolders, `Cat` and `Dog`.
-Each subfolder contains image files for each category.
+Now we have a `PetImages` folder which contain two subfolders, `Cat` and
+`Dog`. Each subfolder contains image files for each category.
 
-```{bash}
-ls PetImages
+```{r, paged.print = FALSE}
+fs::dir_info(""PetImages"")
+fs::dir_info(""PetImages"", recurse = TRUE)
 ```
 
 ### Filter out corrupted images
 
-When working with lots of real-world image data, corrupted images are a common
-occurrence. Let's filter out badly-encoded images that do not feature the string ""JFIF""
-in their header.
+When working with lots of real-world image data, corrupted images are a
+common occurrence. Let's filter out badly-encoded images that do not
+feature the string ""JFIF"" in their header.
 
 ```{r}
-num_skipped <- 0
-ex <- NULL
-for (folder_name in c(""Cat"", ""Dog"")) {
-  folder_path <- file.path(""PetImages"", folder_name)
-  for (fname in list.files(folder_path, pattern = ""*.jpg"")) {
-    f_name <- file.path(folder_path, fname)
-    f_obj <- file(f_name, ""rb"")
-    header <- readBin(f_obj, what = ""raw"", n = 10)
-    is_jfif <- grepl(paste(charToRaw(""JFIF""), collapse = """"), paste(header, collapse = """"))
-    if (!is_jfif) {
-      num_skipped <- num_skipped + 1
-      close(f_obj)
-      unlink(f_name)
-    } else {
-      close(f_obj)
-    }
+n_deleted <- 0L
+for(filepath in list.files(""PetImages"", pattern = ""\\.jpg$"",
+                           recursive = TRUE, full.names = TRUE)) {
+  header <- readBin(filepath, what = ""raw"", n = 10)
+  if(!identical(header[7:10], charToRaw(""JFIF""))) {
+    n_deleted <- n_deleted + 1L
+    unlink(filepath)
   }
 }
-sprintf(""Deleted %d images"", num_skipped)
+
+cat(sprintf(""Deleted %d images\n"", n_deleted))
 ```
 
 ## Generate a `Dataset`
 
 ```{r}
 image_size <- c(180, 180)
-batch_size <- 128
+batch_size <- 32
 
 train_ds <- image_dataset_from_directory(
     ""PetImages"",
@@ -108,189 +102,214 @@ val_ds <- image_dataset_from_directory(
 ## Visualize the data
 
 ```{r}
-train_ds %>% dataset_take(1)
-train_ds_iterator <- as_iterator(train_ds)
-batch <- iter_next(train_ds_iterator)
+batch <- train_ds %>%
+  as_iterator() %>%
+  iter_next()
+
+str(batch)
+
+c(images, labels) %<-% batch
+```
 
+```{r}
 display_image_tensor <- function(x, ..., max = 255,
-                                 plot_margins = c(0, 0, 0, 0), title = """") {
+                                 plot_margins = c(0, 0, 0, 0)) {
   if(!is.null(plot_margins))
     par(mar = plot_margins)
-  
+
   x %>%
     as.array() %>%
     drop() %>%
     as.raster(max = max) %>%
-    plot(..., interpolate = FALSE, main = title, xlab = title)
+    plot(..., interpolate = FALSE)
 }
+
 par(mfrow = c(3, 3))
-for (i in 1:9) {
-  display_image_tensor(batch[[1]][[i]], title = as.character(batch[[2]][[i]]), plot_margins = c(0.5, 0.5, 0.5, 0.5))
-}
+for (i in 1:9)
+  display_image_tensor(images[i,,,],
+                       plot_margins = rep(.5, 4))
 ```
 
 ## Using image data augmentation
 
-When you don't have a large image dataset, it's a good practice to artificially
-introduce sample diversity by applying random yet realistic transformations to the
-training images, such as random horizontal flipping or small random rotations. This
-helps expose the model to different aspects of the training data while slowing down
-overfitting.
+When you don't have a large image dataset, it's a good practice to
+artificially introduce sample diversity by applying random yet realistic
+transformations to the training images, such as random horizontal
+flipping or small random rotations. This helps expose the model to
+different aspects of the training data while slowing down overfitting.
 
 ```{r}
-train_ds %>% dataset_take(1)
-train_ds_iterator <- as_iterator(train_ds)
-batch <- iter_next(train_ds_iterator)
-# Certain kernels are not supported on Mac M1 so switching to CPU: https://stackoverflow.com/questions/69088577/apple-m1-i-got-no-registered-rngreadandskip-opkernel-for-gpu-devices-comp
-with(tf$device(""CPU""), {
-  data_augmentation <- keras_model_sequential() %>% 
-  layer_random_flip(""horizontal"") %>% 
-  layer_random_rotation(factor = 0.1)
-})
+# If you are on an M1 mac, you may need to wrap this model definition in
+# with(tf$device(""CPU""), { ... })
+# https://stackoverflow.com/questions/69088577/apple-m1-i-got-no-registered-rngreadandskip-opkernel-for-gpu-devices-comp
 
+data_augmentation <-
+  keras_model_sequential(input_shape = c(image_size, 3)) %>%
+  layer_random_flip(""horizontal"") %>%
+  layer_random_rotation(factor = 0.1)
 ```
 
-Let's visualize what the augmented samples look like, by applying `data_augmentation`
-repeatedly to the first image in the dataset:
+Let's visualize what the augmented samples look like, by applying
+`data_augmentation` repeatedly to the first image in the dataset:
 
 ```{r}
 par(mfrow = c(3, 3))
 for (i in 1:9) {
-  display_image_tensor(data_augmentation(batch[[1]])[[1]], title = as.character(batch[[2]][[i]]), plot_margins = c(0.5, 0.5, 0.5, 0.5))
+    images[4, , , , drop = FALSE] %>%
+    data_augmentation() %>%
+    display_image_tensor()
 }
 ```
 
 ## Standardizing the data
 
-Our image are already in a standard size (180x180), as they are being yielded as
-contiguous `float32` batches by our dataset. However, their RGB channel values are in
-the `[0, 255]` range. This is not ideal for a neural network;
-in general you should seek to make your input values small. Here, we will
-standardize values to be in the `[0, 1]` by using a `Rescaling` layer at the start of
-our model.
+Our image are already in a standard size (180x180), as they are being
+yielded as contiguous `float32` batches by our dataset. However, their
+RGB channel values are in the `[0, 255]` range. This is not ideal for a
+neural network; in general you should seek to make your input values
+small. Here, we will standardize values to be in the `[0, 1]` by using a
+`Rescaling` layer at the start of our model.
 
 ## Two options to preprocess the data
 
-There are two ways you could be using the `data_augmentation` preprocessor:
-**Option 1: Make it part of the model**, like this:
+There are two ways you could be using the `data_augmentation`
+preprocessor: **Option 1: Make it part of the model**, like this:
 
 ```{r, eval = FALSE}
-x <- layer_input(shape = input_shape) %>% 
-  data_augmentation() %>% 
+x <- layer_input(shape = input_shape) %>%
+  data_augmentation() %>%
   layer_rescaling(1./255)
 ...  # Rest of the model
 ```
 
-With this option, your data augmentation will happen *on device*, synchronously
-with the rest of the model execution, meaning that it will benefit from GPU
-acceleration.
+With this option, your data augmentation will happen *on device*,
+synchronously with the rest of the model execution, meaning that it will
+benefit from GPU acceleration.
 
-Note that data augmentation is inactive at test time, so the input samples will only be
-augmented during `fit()`, not when calling `evaluate()` or `predict()`.
+Note that data augmentation is inactive at test time, so the input
+samples will only be augmented during `fit()`, not when calling
+`evaluate()` or `predict()`.
 
 If you're training on GPU, this may be a good option.
 
-**Option 2: apply it to the dataset**, so as to obtain a dataset that yields batches of
-augmented images, like this:
+**Option 2: apply it to the dataset**, so as to obtain a dataset that
+yields batches of augmented images, like this:
 
 ```{r, eval = FALSE}
-augmented_train_ds <- train_ds %>% map(function(x, y) {
- return(tuple(data_augmentation(x, training = TRUE), y)) 
-})
+augmented_train_ds <- train_ds %>%
+  dataset_map(function(x, y) {
+    x <- data_augmentation(x, training = TRUE)
+    list(x, y)
+  })
 ```
 
-With this option, your data augmentation will happen **on CPU**, asynchronously, and will
-be buffered before going into the model.
+With this option, your data augmentation will happen **on CPU**,
+asynchronously, and will be buffered before going into the model (this
+is because all TF Dataset operations, include any defined in
+`dataset_map()`, are pinned to the CPU).
 
-If you're training on CPU, this is the better option, since it makes data augmentation
-asynchronous and non-blocking.
+If you're training on CPU, this is the better option, since it makes
+data augmentation asynchronous and non-blocking.
 
-In our case, we'll go with the second option. If you're not sure
-which one to pick, this second option (asynchronous preprocessing) is always a solid choice.
+In our case, we'll go with the second option. If you're not sure which
+one to pick, this second option (asynchronous preprocessing) is always a
+solid choice.
 
 ## Configure the dataset for performance
 
-Let's apply data augmentation to our training dataset,
-and let's make sure to use buffered prefetching so we can yield data from disk without
+Let's apply data augmentation to our training dataset, and let's make
+sure to use buffered prefetching so we can yield data from disk without
 having I/O becoming blocking:
 
 ```{r}
 # Apply `data_augmentation` to the training images.
-train_ds <- train_ds %>% dataset_map(function(img, label) {
-  return(tuple(data_augmentation(img), label))
-}, num_parallel_calls = tf$data$AUTOTUNE)
+train_ds <- train_ds %>%
+  dataset_map(function(images, labels) {
+    list(data_augmentation(images, training = TRUE),
+         labels)
+  })
 
 # Prefetching samples in GPU memory helps maximize GPU utilization.
-train_ds <- train_ds %>% dataset_prefetch(tf$data$AUTOTUNE)
-val_ds <- val_ds %>% dataset_prefetch(tf$data$AUTOTUNE)
+train_ds %<>% dataset_prefetch()
+val_ds   %<>% dataset_prefetch()
 ```
 
 ## Build a model
 
-We'll build a small version of the Xception network. We haven't particularly tried to
-optimize the architecture; if you want to do a systematic search for the best model
-configuration, consider using
+We'll build a small version of the Xception network. We haven't
+particularly tried to optimize the architecture; if you want to do a
+systematic search for the best model configuration, consider using
 [KerasTuner](https://github.com/keras-team/keras-tuner).
 
 Note that:
 
-- We start the model with the `data_augmentation` preprocessor, followed by a
- `Rescaling` layer.
-- We include a `Dropout` layer before the final classification layer.
+-   We start the model with the `data_augmentation` preprocessor,
+    followed by a `Rescaling` layer.
+-   We include a `Dropout` layer before the final classification layer.
 
 ```{r}
 make_model <- function(input_shape, num_classes) {
+
   inputs <- layer_input(shape = input_shape)
-  x <- inputs %>% 
-    layer_rescaling(1.0 / 255) %>% 
-    layer_conv_2d(128, 3, strides = 2, padding = ""same"") %>% 
-    layer_batch_normalization() %>% 
+
+  x <- inputs %>%
+    # data augmentation() ? %>%
+    layer_rescaling(1.0 / 255)
+
+  x <- x %>%
+    layer_conv_2d(128, 3, strides = 2, padding = ""same"") %>%
+    layer_batch_normalization() %>%
     layer_activation(""relu"")
 
   previous_block_activation <- x  # Set aside residual
   for (size in c(256, 512, 728)) {
-    x <- x %>% 
-      layer_activation(""relu"") %>% 
-      layer_separable_conv_2d(size, 3, padding = ""same"") %>% 
-      layer_batch_normalization() %>% 
-      
-      layer_activation(""relu"") %>% 
-      layer_separable_conv_2d(size, 3, padding = ""same"") %>% 
-      layer_batch_normalization() %>% 
-      
+    x <- x %>%
+      layer_activation(""relu"") %>%
+      layer_separable_conv_2d(size, 3, padding = ""same"") %>%
+      layer_batch_normalization() %>%
+
+      layer_activation(""relu"") %>%
+      layer_separable_conv_2d(size, 3, padding = ""same"") %>%
+      layer_batch_normalization() %>%
+
       layer_max_pooling_2d(3, strides = 2, padding = ""same"")
-    
+
     # Project residual
-    residual <- layer_conv_2d(filters = size, kernel_size = 1, strides = 2, padding = ""same"")(previous_block_activation)
+    residual <- previous_block_activation %>%
+      layer_conv_2d(filters = size, kernel_size = 1, strides = 2,
+                    padding = ""same"")
+
     x <- tf$keras$layers$add(list(x, residual))  # Add back residual
     previous_block_activation <- x  # Set aside next residual
   }
-  
-  x <- x %>% 
-    layer_separable_conv_2d(1024, 3, padding = ""same"") %>% 
-    layer_batch_normalization() %>% 
-    layer_activation(""relu"") %>% 
+
+  x <- x %>%
+    layer_separable_conv_2d(1024, 3, padding = ""same"") %>%
+    layer_batch_normalization() %>%
+    layer_activation(""relu"") %>%
     layer_global_average_pooling_2d()
-  
+
   if (num_classes == 2) {
     activation <- ""sigmoid""
     units <- 1
   } else {
     activation <- ""softmax""
     units <- num_classes
   }
-  
-  outputs <- x %>% 
-    layer_dropout(0.5) %>% 
+
+  outputs <- x %>%
+    layer_dropout(0.5) %>%
     layer_dense(units, activation = activation)
-  
+
   return(keras_model(inputs, outputs))
 }
 
 model <- make_model(input_shape = c(image_size, 3), num_classes = 2)
 ```
 
-## Train the model
+```{r, fig.height=18, fig.width = 6}
+plot(model)
+```
 
 ```{r}
 epochs <- 25
@@ -307,22 +326,35 @@ history <- model %>% fit(
     callbacks = callbacks,
     validation_data = val_ds,
 )
+plot(history)
 ```
 
-We get to >90% validation accuracy after training for 25 epochs on the full dataset
-(in practice, you can train for 50+ epochs before validation performance starts degrading).
+We get to \>90% validation accuracy after training for 25 epochs on the
+full dataset (in practice, you can train for 50+ epochs before
+validation performance starts degrading).
 
 ## Run inference on new data
 
+You can reload one of the models saved by the checkpoint callback like
+this:
+
+``` r
+model <- load_model_tf(""save_at_25.keras"")
+```
+
 Note that data augmentation and dropout are inactive at inference time.
 
 ```{r}
-img <- image_load(""PetImages/Cat/6779.jpg"", target_size = image_size)
-img_array <- image_to_array(img) %>% 
+# load an image as a tensor
+img_tensor <-
+  ""PetImages/Cat/6779.jpg"" %>%
+  tf$io$read_file() %>%
+  tf$io$decode_image() %>%
+  tf$image$resize(as.integer(image_size)) %>%
   tf$expand_dims(0L)  # Create batch axis
 
-score <- model %>%  
-  predict(img_array) %>% 
-  .[[1]]
+score <- model %>% predict(img_tensor)
+
+display_image_tensor(img_tensor)
 sprintf(""This image is %.2f%% cat and %.2f%% dog."", 100 * (1 - score), 100 * score)
 ```"
rstudio,tensorflow.rstudio.com,6cf211332f19649e77450f7f4ad4d9734422d4b5,Yuan Tang,terrytangyuan@gmail.com,2022-12-08T02:50:58Z,Yuan Tang,terrytangyuan@gmail.com,2022-12-08T02:50:58Z,"Fix regex

Signed-off-by: Yuan Tang <terrytangyuan@gmail.com>",examples/text_classification_from_scratch.qmd,True,False,True,False,2,1,3,"---FILE: examples/text_classification_from_scratch.qmd---
@@ -129,7 +129,8 @@ custom_standardization <- function(input_data) {
   return(
     tf$strings$regex_replace(
       stripped_html, 
-      ""[{re.escape(string.punctuation)}]"",
+      # This is an escaped form for the regex that contains all sets of punctuation: !""#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~
+      '[{!""\\#\\$%\\&\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~}]',
       """")
   )
 }"
rstudio,tensorflow.rstudio.com,5416480f1688773a81959df3a801face8f22280f,Yuan Tang,terrytangyuan@gmail.com,2022-11-21T17:39:45Z,Yuan Tang,terrytangyuan@gmail.com,2022-11-21T17:39:45Z,"Fix Quarto metadata and index

Signed-off-by: Yuan Tang <terrytangyuan@gmail.com>",_freeze/examples/deep_dream/execute-results/html.json;reference/index.qmd;tutorials/keras/_metadata.yml,True,False,True,False,4,7,11,"---FILE: _freeze/examples/deep_dream/execute-results/html.json---
@@ -1,10 +1,8 @@
 {
   ""hash"": ""45184354e80bdea8cc680942ef65e7e5"",
   ""result"": {
-    ""markdown"": ""---\ntitle: Deep Dream\nauthors: \n  - \""[fchollet](https://twitter.com/fchollet)\""\n  - \""[dfalbel](https://github.com/dfalbel) - R translation\""\ndate-created: 2016/01/13\ndate-last-modified: 2020/05/02\ndescription: Generating Deep Dreams with Keras.\ncategories: [generative]\naliases: \n  - ../guide/keras/examples/deep_dream/index.html\n---\n\n\n## Introduction\n\n\""Deep dream\"" is an image-filtering technique which consists of taking an image\nclassification model, and running gradient ascent over an input image to\ntry to maximize the activations of specific layers (and sometimes, specific units in\nspecific layers) for this input. It produces hallucination-like visuals.\n\nIt was first introduced by Alexander Mordvintsev from Google in July 2015.\n\nProcess:\n\n- Load the original image.\n- Define a number of processing scales (\""octaves\""),\nfrom smallest to largest.\n- Resize the original image to the smallest scale.\n- For every scale, starting with(the smallest (i$e. current one), {    })\n    - Run gradient ascent\n    - Upscale image to the next scale\n    - Reinject the detail that was lost at upscaling time\n- Stop when we are back to the original size.\nTo obtain the detail lost during upscaling, we simply\ntake the original image, shrink it down, upscale it,\nand compare the result to the (resized) original image.\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\nbase_image_path <- get_file(\""sky.jpg\"", \""https://i.imgur.com/aGBdQyK.jpg\"")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nresult_prefix <- \""sky_dream\""\n\n# These are the names of the layers\n# for which we try to maximize activation,\n# as well as their weight in the final loss\n# we try to maximize.\n# You can tweak these setting to obtain new visual effects.\nlayer_settings <- list(\n  \""mixed4\"" = 1.0,\n  \""mixed5\"" = 1.5,\n  \""mixed6\"" = 2.0,\n  \""mixed7\"" = 2.5\n)\n\n# Playing with these hyperparameters will also allow you to achieve new effects\n\nstep <- 0.01  # Gradient ascent step size\nnum_octave <- 3  # Number of scales at which to run gradient ascent\noctave_scale <- 1.4  # Size ratio between scales\niterations <- 20  # Number of ascent steps per scale\nmax_loss <- 15.0\n```\n:::\n\n\nThis is our base image:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_image <- function(img) {\n  img %>%   \n    as.raster(max = 255) %>% \n    plot()\n}\n\nbase_image_path %>% \n  image_load() %>% \n  image_to_array() %>% \n  plot_image()\n```\n\n::: {.cell-output-display}\n![](deep_dream_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLet's set up some image preprocessing/deprocessing utilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreprocess_image <- function(image_path) {\n  # Util function to open, resize and format pictures\n  # into appropriate arrays.\n  img <- image_path %>% \n    image_load() %>% \n    image_to_array()\n  dim(img) <- c(1, dim(img))\n  inception_v3_preprocess_input(img)\n}\n\ndeprocess_image <- function(x) {\n  dim(x) <- dim(x)[-1]\n  # Undo inception v3 preprocessing\n  x <- x/2.0\n  x <- x + 0.5\n  x <- x*255.0\n  x[] <- raster::clamp(as.numeric(x), 0, 255)\n  x\n}\n```\n:::\n\n\n## Compute the Deep Dream loss\n\n\nFirst, build a feature extraction model to retrieve the activations of our target layers\ngiven an input image.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build an InceptionV3 model loaded with pre-trained ImageNet weights\nmodel <- application_inception_v3(weights = \""imagenet\"", include_top = FALSE)\n\n# Get the symbolic outputs of each \""key\"" layer (we gave them unique names).\noutputs_dict <- purrr::imap(layer_settings, function(v, name) {\n  layer <- get_layer(model, name)\n  layer$output\n})\n\n# Set up a model that returns the activation values for every target layer\n# (as a dict)\n\nfeature_extractor <- keras_model(inputs = model$inputs, outputs = outputs_dict)\n```\n:::\n\n\nThe actual loss computation is very simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_loss <- function(input_image) {\n  features <- feature_extractor(input_image)\n  # Initialize the loss\n  loss <- tf$zeros(shape = shape())\n  \n  layer_settings %>% \n    purrr::imap(function(coeff, name) {\n      activation <- features[[name]]\n      scaling <- tf$reduce_prod(tf$cast(tf$shape(activation), \""float32\""))\n      # We avoid border artifacts by only involving non-border pixels in the loss.\n      coeff * tf$reduce_sum(tf$square(activation[, 3:-2, 3:-2, ])) / scaling\n    }) %>% \n    purrr::reduce(tf$add)\n}\n```\n:::\n\n\n## Set up the gradient ascent loop for one octave\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngradient_ascent_step <- tf_function(function(img, learning_rate) {\n  with(tf$GradientTape() %as% tape, {    \n    tape$watch(img)\n    loss <- compute_loss(img)\n  })\n  \n  # Compute gradients.\n  grads <- tape$gradient(loss, img)\n  # Normalize gradients.\n  grads <- grads/tf$maximum(tf$reduce_mean(tf$abs(grads)), 1e-6)\n  img <- img + learning_rate * grads\n  list(loss, img)\n})\n\n\ngradient_ascent_loop <- function(img, iterations, learning_rate, max_loss = NULL) {\n  for (i in seq_len(iterations)) {\n    c(loss, img) %<-% gradient_ascent_step(img, learning_rate)\n    if (!is.null(max_loss) && as.logical(loss > max_loss)) {\n      break\n    }\n    cat(\""... Loss value at step \"", i, \"": \"", as.numeric(loss), \""\\n\"")\n  }\n  img\n}\n```\n:::\n\n\n## Run the training loop, iterating over different octaves\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_img <- preprocess_image(base_image_path)\noriginal_shape <- dim(original_img)[2:3]\n\nsuccessive_shapes <- list(original_shape)\nfor (i in seq_len(num_octave - 1)) {\n  shape <- as.integer(original_shape / octave_scale^i)\n  successive_shapes[[i+1]] <- shape\n}\nsuccessive_shapes <- rev(successive_shapes)\n\nshrunk_original_img <- tf$image$resize(original_img, successive_shapes[[1]])\nimg <- tf$identity(original_img)  # Make a copy\nfor (i in seq_along(successive_shapes)) {\n  shape <- successive_shapes[[i]]\n  \n  cat(\""Processing octave \"", i, \""with shape:\"", shape, \""\\n\"")\n  \n  img <- tf$image$resize(img, shape)\n  img <- gradient_ascent_loop(\n    img, iterations = iterations, learning_rate = step, max_loss = max_loss\n  )\n  upscaled_shrunk_original_img <- tf$image$resize(shrunk_original_img, shape)\n  same_size_original <- tf$image$resize(original_img, shape)\n  lost_detail <- same_size_original - upscaled_shrunk_original_img\n  \n  img <- img + lost_detail\n  shrunk_original_img <- tf$image$resize(original_img, shape)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing octave  1 with shape: 326 489 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Negative numbers are interpreted python-style when subsetting tensorflow tensors.\nSee: ?`[.tensorflow.tensor` for details.\nTo turn off this warning, set `options(tensorflow.extract.warn_negatives_pythonic = FALSE)`\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n... Loss value at step  1 :  0.5014444 \n... Loss value at step  2 :  0.7033239 \n... Loss value at step  3 :  0.9975789 \n... Loss value at step  4 :  1.339841 \n... Loss value at step  5 :  1.69038 \n... Loss value at step  6 :  2.055832 \n... Loss value at step  7 :  2.368303 \n... Loss value at step  8 :  2.700009 \n... Loss value at step  9 :  3.018286 \n... Loss value at step  10 :  3.355762 \n... Loss value at step  11 :  3.659731 \n... Loss value at step  12 :  3.970093 \n... Loss value at step  13 :  4.239055 \n... Loss value at step  14 :  4.570164 \n... Loss value at step  15 :  4.875012 \n... Loss value at step  16 :  5.180275 \n... Loss value at step  17 :  5.479495 \n... Loss value at step  18 :  5.700634 \n... Loss value at step  19 :  6.022943 \n... Loss value at step  20 :  6.264833 \nProcessing octave  2 with shape: 457 685 \n... Loss value at step  1 :  1.214347 \n... Loss value at step  2 :  1.962548 \n... Loss value at step  3 :  2.586394 \n... Loss value at step  4 :  3.085848 \n... Loss value at step  5 :  3.57484 \n... Loss value at step  6 :  4.002796 \n... Loss value at step  7 :  4.40269 \n... Loss value at step  8 :  4.807469 \n... Loss value at step  9 :  5.220548 \n... Loss value at step  10 :  5.58031 \n... Loss value at step  11 :  5.953027 \n... Loss value at step  12 :  6.311125 \n... Loss value at step  13 :  6.666377 \n... Loss value at step  14 :  7.01564 \n... Loss value at step  15 :  7.373053 \n... Loss value at step  16 :  7.687048 \n... Loss value at step  17 :  8.022481 \n... Loss value at step  18 :  8.353153 \n... Loss value at step  19 :  8.676409 \n... Loss value at step  20 :  8.996731 \nProcessing octave  3 with shape: 640 960 \n... Loss value at step  1 :  1.34736 \n... Loss value at step  2 :  2.137553 \n... Loss value at step  3 :  2.785597 \n... Loss value at step  4 :  3.325019 \n... Loss value at step  5 :  3.836637 \n... Loss value at step  6 :  4.312602 \n... Loss value at step  7 :  4.782893 \n... Loss value at step  8 :  5.230842 \n... Loss value at step  9 :  5.646795 \n... Loss value at step  10 :  6.066389 \n... Loss value at step  11 :  6.450614 \n... Loss value at step  12 :  6.861366 \n... Loss value at step  13 :  7.217497 \n... Loss value at step  14 :  7.596985 \n... Loss value at step  15 :  7.9565 \n... Loss value at step  16 :  8.316831 \n... Loss value at step  17 :  8.604964 \n... Loss value at step  18 :  8.966898 \n... Loss value at step  19 :  9.21804 \n... Loss value at step  20 :  9.593339 \n```\n:::\n:::\n\n\nDisplay the result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimg %>% \n  as.array() %>% \n  deprocess_image() %>% \n  plot_image()\n```\n\n::: {.cell-output-display}\n![](deep_dream_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n"",
-    ""supporting"": [
-      ""deep_dream_files""
-    ],
+    ""markdown"": ""---\ntitle: Deep Dream\nauthors: \n  - \""[fchollet](https://twitter.com/fchollet)\""\n  - \""[dfalbel](https://github.com/dfalbel) - R translation\""\ndate-created: 2016/01/13\ndate-last-modified: 2020/05/02\ndescription: Generating Deep Dreams with Keras.\ncategories: [generative]\naliases: \n  - ../guide/keras/examples/deep_dream/index.html\n---\n\n\n## Introduction\n\n\""Deep dream\"" is an image-filtering technique which consists of taking an image\nclassification model, and running gradient ascent over an input image to\ntry to maximize the activations of specific layers (and sometimes, specific units in\nspecific layers) for this input. It produces hallucination-like visuals.\n\nIt was first introduced by Alexander Mordvintsev from Google in July 2015.\n\nProcess:\n\n- Load the original image.\n- Define a number of processing scales (\""octaves\""),\nfrom smallest to largest.\n- Resize the original image to the smallest scale.\n- For every scale, starting with(the smallest (i$e. current one), {    })\n    - Run gradient ascent\n    - Upscale image to the next scale\n    - Reinject the detail that was lost at upscaling time\n- Stop when we are back to the original size.\nTo obtain the detail lost during upscaling, we simply\ntake the original image, shrink it down, upscale it,\nand compare the result to the (resized) original image.\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n\nbase_image_path <- get_file(\""sky.jpg\"", \""https://i.imgur.com/aGBdQyK.jpg\"")\nresult_prefix <- \""sky_dream\""\n\n# These are the names of the layers\n# for which we try to maximize activation,\n# as well as their weight in the final loss\n# we try to maximize.\n# You can tweak these setting to obtain new visual effects.\nlayer_settings <- list(\n  \""mixed4\"" = 1.0,\n  \""mixed5\"" = 1.5,\n  \""mixed6\"" = 2.0,\n  \""mixed7\"" = 2.5\n)\n\n# Playing with these hyperparameters will also allow you to achieve new effects\n\nstep <- 0.01  # Gradient ascent step size\nnum_octave <- 3  # Number of scales at which to run gradient ascent\noctave_scale <- 1.4  # Size ratio between scales\niterations <- 20  # Number of ascent steps per scale\nmax_loss <- 15.0\n```\n:::\n\n\nThis is our base image:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_image <- function(img) {\n  img %>%   \n    as.raster(max = 255) %>% \n    plot()\n}\n\nbase_image_path %>% \n  image_load() %>% \n  image_to_array() %>% \n  plot_image()\n```\n:::\n\n\nLet's set up some image preprocessing/deprocessing utilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreprocess_image <- function(image_path) {\n  # Util function to open, resize and format pictures\n  # into appropriate arrays.\n  img <- image_path %>% \n    image_load() %>% \n    image_to_array()\n  dim(img) <- c(1, dim(img))\n  inception_v3_preprocess_input(img)\n}\n\ndeprocess_image <- function(x) {\n  dim(x) <- dim(x)[-1]\n  # Undo inception v3 preprocessing\n  x <- x/2.0\n  x <- x + 0.5\n  x <- x*255.0\n  x[] <- raster::clamp(as.numeric(x), 0, 255)\n  x\n}\n```\n:::\n\n\n## Compute the Deep Dream loss\n\n\nFirst, build a feature extraction model to retrieve the activations of our target layers\ngiven an input image.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build an InceptionV3 model loaded with pre-trained ImageNet weights\nmodel <- application_inception_v3(weights = \""imagenet\"", include_top = FALSE)\n\n# Get the symbolic outputs of each \""key\"" layer (we gave them unique names).\noutputs_dict <- purrr::imap(layer_settings, function(v, name) {\n  layer <- get_layer(model, name)\n  layer$output\n})\n\n# Set up a model that returns the activation values for every target layer\n# (as a dict)\n\nfeature_extractor <- keras_model(inputs = model$inputs, outputs = outputs_dict)\n```\n:::\n\n\nThe actual loss computation is very simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_loss <- function(input_image) {\n  features <- feature_extractor(input_image)\n  # Initialize the loss\n  loss <- tf$zeros(shape = shape())\n  \n  layer_settings %>% \n    purrr::imap(function(coeff, name) {\n      activation <- features[[name]]\n      scaling <- tf$reduce_prod(tf$cast(tf$shape(activation), \""float32\""))\n      # We avoid border artifacts by only involving non-border pixels in the loss.\n      coeff * tf$reduce_sum(tf$square(activation[, 3:-2, 3:-2, ])) / scaling\n    }) %>% \n    purrr::reduce(tf$add)\n}\n```\n:::\n\n\n## Set up the gradient ascent loop for one octave\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngradient_ascent_step <- tf_function(function(img, learning_rate) {\n  with(tf$GradientTape() %as% tape, {    \n    tape$watch(img)\n    loss <- compute_loss(img)\n  })\n  \n  # Compute gradients.\n  grads <- tape$gradient(loss, img)\n  # Normalize gradients.\n  grads <- grads/tf$maximum(tf$reduce_mean(tf$abs(grads)), 1e-6)\n  img <- img + learning_rate * grads\n  list(loss, img)\n})\n\n\ngradient_ascent_loop <- function(img, iterations, learning_rate, max_loss = NULL) {\n  for (i in seq_len(iterations)) {\n    c(loss, img) %<-% gradient_ascent_step(img, learning_rate)\n    if (!is.null(max_loss) && as.logical(loss > max_loss)) {\n      break\n    }\n    cat(\""... Loss value at step \"", i, \"": \"", as.numeric(loss), \""\\n\"")\n  }\n  img\n}\n```\n:::\n\n\n## Run the training loop, iterating over different octaves\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_img <- preprocess_image(base_image_path)\noriginal_shape <- dim(original_img)[2:3]\n\nsuccessive_shapes <- list(original_shape)\nfor (i in seq_len(num_octave - 1)) {\n  shape <- as.integer(original_shape / octave_scale^i)\n  successive_shapes[[i+1]] <- shape\n}\nsuccessive_shapes <- rev(successive_shapes)\n\nshrunk_original_img <- tf$image$resize(original_img, successive_shapes[[1]])\nimg <- tf$identity(original_img)  # Make a copy\nfor (i in seq_along(successive_shapes)) {\n  shape <- successive_shapes[[i]]\n  \n  cat(\""Processing octave \"", i, \""with shape:\"", shape, \""\\n\"")\n  \n  img <- tf$image$resize(img, shape)\n  img <- gradient_ascent_loop(\n    img, iterations = iterations, learning_rate = step, max_loss = max_loss\n  )\n  upscaled_shrunk_original_img <- tf$image$resize(shrunk_original_img, shape)\n  same_size_original <- tf$image$resize(original_img, shape)\n  lost_detail <- same_size_original - upscaled_shrunk_original_img\n  \n  img <- img + lost_detail\n  shrunk_original_img <- tf$image$resize(original_img, shape)\n}\n```\n:::\n\n\nDisplay the result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimg %>% \n  as.array() %>% \n  deprocess_image() %>% \n  plot_image()\n```\n:::\n"",
+    ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: reference/index.qmd---
@@ -2,7 +2,6 @@
 title: Reference
 listing:
   type: grid
-  sort: """"
   fields: [title, description]
   sort: '' # don't sort
   contents:

---FILE: tutorials/keras/_metadata.yml---
@@ -1,2 +1,2 @@
-# execute:
-#   eval: false
+execute:
+  eval: false"
rstudio,tensorflow.rstudio.com,b7a14ed9278d0153d8c101c54b87b60119961c3f,Tomasz Kalinowski,tomasz@rstudio.com,2022-09-21T00:10:49Z,Tomasz Kalinowski,tomasz@rstudio.com,2022-09-21T00:10:49Z,render basic regression tutorial,_freeze/tutorials/keras/regression/execute-results/html.json;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-21-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-24-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-30-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-36-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-38-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-42-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-45-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-46-1.png;index.qmd;tutorials/_metadata.yml,True,False,True,False,5,3,8,"---FILE: _freeze/tutorials/keras/regression/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""e5ebbb0990d86477362b0f13eb0f41ed"",
+  ""hash"": ""03312b5c7c508c46f11ebfc9a9aa3e1e"",
   ""result"": {
-    ""markdown"": ""---\ntitle: Basic Regression\ndescription: Train a neural netowrk to predict a continous value.\naliases:\n  - ../beginners/basic-ml/tutorial_basic_regression/index.html\n  - ../../articles/tutorial_basic_regression.html\n---\n\n\nIn a *regression* problem, the aim is to predict the output of a continuous value, like a price or a probability.\nContrast this with a *classification* problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n\nThis tutorial uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles.\nTo do this, you will provide the models with a description of many automobiles from that time period.\nThis description includes attributes like cylinders, displacement, horsepower, and weight.\n\nThis example uses the Keras API.\n(Visit the Keras [tutorials](../keras) and [guides](../../guides/keras) to learn more.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────── tidymodels 1.0.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.0     ✔ rsample      1.0.0\n✔ dials        1.0.0     ✔ tune         1.0.0\n✔ infer        1.0.2     ✔ workflows    1.0.0\n✔ modeldata    1.0.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.0     ✔ yardstick    1.0.0\n✔ recipes      1.0.1     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ yardstick::get_weights() masks keras::get_weights()\n✖ dplyr::lag()             masks stats::lag()\n✖ yardstick::spec()        masks readr::spec()\n✖ recipes::step()          masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n:::\n\n\n## The Auto MPG dataset\n\nThe dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n\n### Get the data\n\nFirst download and import the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\""\ncol_names <- c(\""mpg\"",\""cylinders\"",\""displacement\"",\""horsepower\"",\""weight\"",\""acceleration\"",\""model_year\"", \""origin\"",\""car_name\"")\n\nraw_dataset <- read.table(\n  url,\n  header = T,\n  col.names = col_names,\n  na.strings = \""?\""\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- raw_dataset %>% select(-car_name)\ntail(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    mpg cylinders displacement horsepower weight acceleration model_year\n392  27         4          151         90   2950         17.3         82\n393  27         4          140         86   2790         15.6         82\n394  44         4           97         52   2130         24.6         82\n395  32         4          135         84   2295         11.6         82\n396  28         4          120         79   2625         18.6         82\n397  31         4          119         82   2720         19.4         82\n    origin\n392      1\n393      1\n394      2\n395      1\n396      1\n397      1\n```\n:::\n:::\n\n\n### Clean the data\n\nThe dataset contains a few unknown values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlapply(dataset, function(x) sum(is.na(x))) %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 8\n $ mpg         : int 0\n $ cylinders   : int 0\n $ displacement: int 0\n $ horsepower  : int 6\n $ weight      : int 0\n $ acceleration: int 0\n $ model_year  : int 0\n $ origin      : int 0\n```\n:::\n:::\n\n\nDrop those rows to keep this initial tutorial simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- na.omit(dataset)\n```\n:::\n\n\nThe `\""origin\""` column is categorical, not numeric.\nSo the next step is to one-hot encode the values in the column with the `recipes` package.\n\nNote: You can set up the `keras_model()` to do this kind of transformation for you but that's beyond the scope of this tutorial.\nCheck out the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) or [Load CSV data](../load_data/csv.qmd) tutorials for examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\ndataset <- recipe(mpg ~ ., dataset) %>%\n  step_num2factor(origin, levels = c(\""USA\"", \""Europe\"", \""Japan\"")) %>%\n  step_dummy(origin, one_hot = TRUE) %>%\n  prep() %>%\n  bake(new_data = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 391\nColumns: 10\n$ cylinders     <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4…\n$ displacement  <dbl> 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…\n$ horsepower    <dbl> 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…\n$ weight        <dbl> 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 3850…\n$ acceleration  <dbl> 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, 1…\n$ model_year    <int> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ mpg           <dbl> 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 14, …\n$ origin_USA    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0…\n$ origin_Europe <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ origin_Japan  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…\n```\n:::\n:::\n\n\n### Split the data into training and test sets\n\nNow, split the dataset into a training set and a test set.\nYou will use the test set in the final evaluation of your models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- initial_split(dataset, 0.8)\ntrain_dataset <- training(split)\ntest_dataset <- testing(split)\n```\n:::\n\n\n### Inspect the data\n\nReview the joint distribution of a few pairs of columns from the training set.\n\nThe top row suggests that the fuel efficiency (MPG) is a function of all the other parameters.\nThe other rows indicate they are functions of each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_dataset %>%\n  select(mpg, cylinders, displacement, weight) %>%\n  GGally::ggpairs()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLet's also check the overall statistics.\nNote how each feature covers a very different range:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(train_dataset)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |train_dataset |\n|Number of rows           |312           |\n|Number of columns        |10            |\n|_______________________  |              |\n|Column type frequency:   |              |\n|numeric                  |10            |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|     sd|   p0|     p25|     p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|-------:|------:|----:|-------:|-------:|------:|------:|:-----|\n|cylinders     |         0|             1|    5.50|   1.72|    3|    4.00|    4.00|    8.0|    8.0|▇▁▃▁▅ |\n|displacement  |         0|             1|  196.99| 105.81|   68|  105.00|  151.00|  302.0|  455.0|▇▂▂▃▁ |\n|horsepower    |         0|             1|  104.93|  38.85|   46|   76.75|   92.00|  126.0|  230.0|▆▇▃▂▁ |\n|weight        |         0|             1| 2995.04| 844.26| 1613| 2232.50| 2849.00| 3622.5| 4997.0|▇▇▆▅▂ |\n|acceleration  |         0|             1|   15.57|   2.83|    8|   13.88|   15.50|   17.2|   24.8|▁▆▇▂▁ |\n|model_year    |         0|             1|   76.06|   3.71|   70|   73.00|   76.00|   79.0|   82.0|▇▆▇▆▇ |\n|mpg           |         0|             1|   23.48|   7.91|    9|   17.00|   22.15|   29.0|   46.6|▆▇▆▃▁ |\n|origin_USA    |         0|             1|    0.62|   0.48|    0|    0.00|    1.00|    1.0|    1.0|▅▁▁▁▇ |\n|origin_Europe |         0|             1|    0.17|   0.38|    0|    0.00|    0.00|    0.0|    1.0|▇▁▁▁▂ |\n|origin_Japan  |         0|             1|    0.21|   0.40|    0|    0.00|    0.00|    0.0|    1.0|▇▁▁▁▂ |\n:::\n:::\n\n\n### Split features from labels\n\nSeparate the target value---the \""label\""---from the features.\nThis label is the value that you will train the model to predict.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_features <- train_dataset %>% select(-mpg)\ntest_features <- test_dataset %>% select(-mpg)\n\ntrain_labels <- train_dataset %>% select(mpg)\ntest_labels <- test_dataset %>% select(mpg)\n```\n:::\n\n\n## Normalization\n\nIn the table of statistics it's easy to see how different the ranges of each feature are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_skim <- skimr::skim_with(numeric = skimr::sfl(mean, sd))\ntrain_dataset %>%\n  select(where(~is.numeric(.x))) %>%\n  pivot_longer(\n    cols = everything(), names_to = \""variable\"", values_to = \""values\"") %>%\n  group_by(variable) %>%\n  summarise(mean = mean(values), sd = sd(values))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n   variable          mean      sd\n   <chr>            <dbl>   <dbl>\n 1 acceleration    15.6     2.83 \n 2 cylinders        5.5     1.72 \n 3 displacement   197.    106.   \n 4 horsepower     105.     38.9  \n 5 model_year      76.1     3.71 \n 6 mpg             23.5     7.91 \n 7 origin_Europe    0.170   0.376\n 8 origin_Japan     0.205   0.404\n 9 origin_USA       0.625   0.485\n10 weight        2995.    844.   \n```\n:::\n:::\n\n\nIt is good practice to normalize features that use different scales and ranges.\n\nOne reason this is important is because the features are multiplied by the model weights.\nSo, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n\nAlthough a model *might* converge without feature normalization, normalization makes training much more stable.\n\nNote: There is no advantage to normalizing the one-hot features---it is done here for simplicity.\nFor more details on how to use the preprocessing layers, refer to the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) tutorial.\n\n### The Normalization layer\n\nThe `layer_normalization()` is a clean and simple way to add feature normalization into your model.\n\nThe first step is to create the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer <- layer_normalization(axis = -1L)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n:::\n\n\nThen, fit the state of the preprocessing layer to the data by calling `adapt()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer %>% adapt(as.matrix(train_features))\n```\n:::\n\n\nCalculate the mean and variance, and store them in the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(normalizer$mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[5.5000000e+00 1.9699361e+02 1.0493270e+02 2.9950388e+03 1.5573077e+01\n  7.6060890e+01 6.2500000e-01 1.6987181e-01 2.0512822e-01]], shape=(1, 9), dtype=float32)\n```\n:::\n:::\n\n\nWhen the layer is called, it returns the input data, with each feature independently normalized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst <- as.matrix(train_features[1,])\n\ncat('First example:', first)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst example: 8 318 150 4077 14 72 1 0 0\n```\n:::\n\n```{.r .cell-code}\ncat('Normalized:', as.matrix(normalizer(first)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNormalized: 1.452718 1.145492 1.161751 1.283603 -0.5567052 -1.095416 0.7745967 -0.4523641 -0.5080006\n```\n:::\n:::\n\n\n## Linear regression\n\nBefore building a deep neural network model, start with linear regression using one and several variables.\n\n### Linear regression with one variable\n\nBegin with a single-variable linear regression to predict `'mpg'` from `'horsepower'`.\n\nTraining a model with Keras typically starts by defining the model architecture.\nUse a Sequential model, which [represents a sequence of steps](https://www.tensorflow.org/guide/keras/sequential_model).\n\nThere are two steps in your single-variable linear regression model:\n\n-   Normalize the `'horsepower'` input features using the `normalization` preprocessing layer.\n-   Apply a linear transformation ($y = mx+b$) to produce 1 output using a linear layer (`dense`).\n\nThe number of *inputs* can either be set by the `input_shape` argument, or automatically when the model is run for the first time.\n\nFirst, create a matrix made of the `'horsepower'` features.\nThen, instantiate the `layer_normalization` and fit its state to the `horsepower` data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower <- matrix(train_features$horsepower)\nhorsepower_normalizer <- layer_normalization(input_shape = shape(1), axis = NULL)\nhorsepower_normalizer %>% adapt(horsepower)\n```\n:::\n\n\nBuild the Keras Sequential model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model <- keras_model_sequential() %>%\n  horsepower_normalizer() %>%\n  layer_dense(units = 1)\n\nsummary(horsepower_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense (Dense)               (None, 1)                 2         Y          \n============================================================================\nTotal params: 5\nTrainable params: 2\nNon-trainable params: 3\n____________________________________________________________________________\n```\n:::\n:::\n\n\nThis model will predict `'mpg'` from `'horsepower'`.\n\nRun the untrained model on the first 10 'horsepower' values.\nThe output won't be good, but notice that it has the expected shape of `(10, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(horsepower_model, horsepower[1:10,])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1]\n [1,] -1.245993376\n [2,] -1.245993376\n [3,]  0.246965900\n [4,]  0.302260667\n [5,]  0.827561140\n [6,]  0.219318494\n [7,] -0.001860639\n [8,] -0.140097603\n [9,] -0.140097603\n[10,]  0.606382012\n```\n:::\n:::\n\n\nOnce the model is built, configure the training procedure using the Keras `compile()` method.\nThe most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized (`mean_absolute_error`) and how (using the `optimizer_adam`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n\nUse Keras `fit()` to execute the training for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nVisualize the model's training progress using the stats stored in the `history` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results <- list()\ntest_results[[\""horsepower_model\""]] <- horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\nSince this is a single variable regression, it's easy to view the model's predictions as a function of the input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 250, length.out = 251)\ny <- predict(horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n### Linear regression with multiple inputs\n\nYou can use an almost identical setup to make predictions based on multiple inputs.\nThis model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.\n\nCreate a two-step Keras Sequential model again with the first layer being `normalizer` (`layer_normalization(axis = -1)`) you defined earlier and adapted to the whole dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- keras_model_sequential() %>%\n  normalizer() %>%\n  layer_dense(units = 1)\n```\n:::\n\n\nWhen you call `predict()` on a batch of inputs, it produces `units = 1` outputs for each example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(linear_model, as.matrix(train_features[1:10, ]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n [1,] -1.4724398\n [2,] -1.6605439\n [3,]  2.3765769\n [4,]  1.9531747\n [5,]  2.3478329\n [6,]  1.6735287\n [7,] -0.7476962\n [8,] -1.1660424\n [9,] -1.0990586\n[10,] -0.2849139\n```\n:::\n:::\n\n\nWhen you call the model, its weight matrices will be built---check that the `kernel` weights (the $m$ in $y = mx+b$) have a shape of `(9, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model$layers[[2]]$kernel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'dense_1/kernel:0' shape=(9, 1) dtype=float32, numpy=\narray([[-0.43921676],\n       [ 0.01068735],\n       [-0.16628778],\n       [ 0.13414735],\n       [-0.00095809],\n       [ 0.15732205],\n       [-0.3785252 ],\n       [ 0.05685097],\n       [ 0.6592699 ]], dtype=float32)>\n```\n:::\n:::\n\n\nConfigure the model with Keras `compile()` and train with `fit()` for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- linear_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nUsing all the inputs in this regression model achieves a much lower training and validation error than the `horsepower_model`, which had one input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['linear_model']] <- linear_model %>%\n  evaluate(\n    as.matrix(test_features),\n    as.matrix(test_labels),\n    verbose = 0\n  )\n```\n:::\n\n\n## Regression with a deep neural network (DNN)\n\nIn the previous section, you implemented two linear models for single and multiple inputs.\n\nHere, you will implement single-input and multiple-input DNN models.\n\nThe code is basically the same except the model is expanded to include some \""hidden\"" non-linear layers.\nThe name \""hidden\"" here just means not directly connected to the inputs or outputs.\n\nThese models will contain a few more layers than the linear model:\n\n-   The normalization layer, as before (with `horsepower_normalizer` for a single-input model and `normalizer` for a multiple-input model).\n-   Two hidden, non-linear, `Dense` layers with the ReLU (`relu`) activation function nonlinearity.\n-   A linear `Dense` single-output layer.\n\nBoth models will use the same training procedure so the `compile` method is included in the `build_and_compile_model` function below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuild_and_compile_model <- function(norm) {\n  model <- keras_model_sequential() %>%\n    norm() %>%\n    layer_dense(64, activation = 'relu') %>%\n    layer_dense(64, activation = 'relu') %>%\n    layer_dense(1)\n\n  model %>% compile(\n    loss = 'mean_absolute_error',\n    optimizer = optimizer_adam(0.001)\n  )\n\n  model\n}\n```\n:::\n\n\n### Regression using a DNN and a single input\n\nCreate a DNN model with only `'Horsepower'` as input and `horsepower_normalizer` (defined earlier) as the normalization layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_horsepower_model <- build_and_compile_model(horsepower_normalizer)\n```\n:::\n\n\nThis model has quite a few more trainable parameters than the linear models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dnn_horsepower_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_2\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense_4 (Dense)             (None, 64)                128       Y          \n dense_3 (Dense)             (None, 64)                4160      Y          \n dense_2 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,356\nTrainable params: 4,353\nNon-trainable params: 3\n____________________________________________________________________________\n```\n:::\n:::\n\n\nTrain the model with Keras `Model$fit`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0,\n  epochs = 100\n)\n```\n:::\n\n\nThis model does slightly better than the linear single-input `horsepower_model`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\nIf you plot the predictions as a function of `'horsepower'`, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0.0, 250, length.out = 251)\ny <- predict(dnn_horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_horsepower_model']] <- dnn_horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\n### Regression using a DNN and multiple inputs\n\nRepeat the previous process using all the inputs.\nThe model's performance slightly improves on the validation dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_model <- build_and_compile_model(normalizer)\nsummary(dnn_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_3\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization (Normalizatio  (None, 9)                19        Y          \n n)                                                                         \n dense_7 (Dense)             (None, 64)                640       Y          \n dense_6 (Dense)             (None, 64)                4160      Y          \n dense_5 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,884\nTrainable params: 4,865\nNon-trainable params: 19\n____________________________________________________________________________\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0,\n  epochs = 100\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_model']] <- dnn_model %>% evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\n## Performance\n\nSince all models have been trained, you can review their test set performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    horsepower_model.loss         linear_model.loss \n                 3.566376                  2.510710 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.103988                  1.688091 \n```\n:::\n:::\n\n\nThese results match the validation error observed during training.\n\n### Make predictions\n\nYou can now make predictions with the `dnn_model` on the test set using Keras `predict()` and review the loss:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(dnn_model, as.matrix(test_features))\nggplot(data.frame(pred = as.numeric(test_predictions), mpg = test_labels$mpg)) +\n  geom_point(aes(x = pred, y = mpg)) +\n  geom_abline(intercept = 0, slope = 1, color = \""blue\"")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\nIt appears that the model predicts reasonably well.\n\nNow, check the error distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(test_predictions - test_labels$mpg, geom = \""density\"")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n\n```{.r .cell-code}\nerror <- test_predictions - test_labels\n```\n:::\n\n\nIf you're happy with the model, save it for later use with `Model$save`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_tf(dnn_model, 'dnn_model')\n```\n:::\n\n\nIf you reload the model, it gives identical output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- load_model_tf('dnn_model')\n\ntest_results[['reloaded']] <- reloaded %>% evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    horsepower_model.loss         linear_model.loss \n                 3.566376                  2.510710 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.103988                  1.688091 \n            reloaded.loss \n                 1.688091 \n```\n:::\n:::\n\n\n## Conclusion\n\nThis notebook introduced a few techniques to handle a regression problem.\nHere are a few more tips that may help:\n\n-   Mean squared error (MSE) (`loss_mean_squared_error()`) and mean absolute error (MAE) (`loss_mean_absolute_error()`) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.\n-   Similarly, evaluation metrics used for regression differ from classification.\n-   When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n-   Overfitting is a common problem for DNN models, though it wasn't a problem for this tutorial. Visit the [Overfit and underfit](overfit_and_underfit.qmd) tutorial for more help with this.\n"",
+    ""markdown"": ""---\ntitle: Basic Regression\ndescription: Train a neural network to predict a continous value.\naliases:\n  - ../beginners/basic-ml/tutorial_basic_regression/index.html\n  - ../../articles/tutorial_basic_regression.html\n---\n\n\nIn a *regression* problem, the aim is to predict the output of a continuous value, like a price or a probability.\nContrast this with a *classification* problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n\nThis tutorial uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles.\nTo do this, you will provide the models with a description of many automobiles from that time period.\nThis description includes attributes like cylinders, displacement, horsepower, and weight.\n\nThis example uses the Keras API.\n(Visit the Keras [tutorials](../keras) and [guides](../../guides/keras) to learn more.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.0     ✔ rsample      1.1.0\n✔ dials        1.0.0     ✔ tune         1.0.0\n✔ infer        1.0.3     ✔ workflows    1.0.0\n✔ modeldata    1.0.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.1     ✔ yardstick    1.0.0\n✔ recipes      1.0.1     \n── Conflicts ───────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ yardstick::get_weights() masks keras::get_weights()\n✖ dplyr::lag()             masks stats::lag()\n✖ yardstick::spec()        masks readr::spec()\n✖ recipes::step()          masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n:::\n\n\n## The Auto MPG dataset\n\nThe dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n\n### Get the data\n\nFirst download and import the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\""\ncol_names <- c(\""mpg\"",\""cylinders\"",\""displacement\"",\""horsepower\"",\""weight\"",\""acceleration\"",\""model_year\"", \""origin\"",\""car_name\"")\n\nraw_dataset <- read.table(\n  url,\n  header = T,\n  col.names = col_names,\n  na.strings = \""?\""\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- raw_dataset %>% select(-car_name)\ntail(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    mpg cylinders displacement horsepower weight acceleration model_year\n392  27         4          151         90   2950         17.3         82\n393  27         4          140         86   2790         15.6         82\n394  44         4           97         52   2130         24.6         82\n395  32         4          135         84   2295         11.6         82\n396  28         4          120         79   2625         18.6         82\n397  31         4          119         82   2720         19.4         82\n    origin\n392      1\n393      1\n394      2\n395      1\n396      1\n397      1\n```\n:::\n:::\n\n\n### Clean the data\n\nThe dataset contains a few unknown values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlapply(dataset, function(x) sum(is.na(x))) %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 8\n $ mpg         : int 0\n $ cylinders   : int 0\n $ displacement: int 0\n $ horsepower  : int 6\n $ weight      : int 0\n $ acceleration: int 0\n $ model_year  : int 0\n $ origin      : int 0\n```\n:::\n:::\n\n\nDrop those rows to keep this initial tutorial simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- na.omit(dataset)\n```\n:::\n\n\nThe `\""origin\""` column is categorical, not numeric.\nSo the next step is to one-hot encode the values in the column with the `recipes` package.\n\nNote: You can set up the `keras_model()` to do this kind of transformation for you but that's beyond the scope of this tutorial.\nCheck out the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) or [Load CSV data](../load_data/csv.qmd) tutorials for examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\ndataset <- recipe(mpg ~ ., dataset) %>%\n  step_num2factor(origin, levels = c(\""USA\"", \""Europe\"", \""Japan\"")) %>%\n  step_dummy(origin, one_hot = TRUE) %>%\n  prep() %>%\n  bake(new_data = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 391\nColumns: 10\n$ cylinders     <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4…\n$ displacement  <dbl> 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…\n$ horsepower    <dbl> 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…\n$ weight        <dbl> 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 3850…\n$ acceleration  <dbl> 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, 1…\n$ model_year    <int> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ mpg           <dbl> 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 14, …\n$ origin_USA    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0…\n$ origin_Europe <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ origin_Japan  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…\n```\n:::\n:::\n\n\n### Split the data into training and test sets\n\nNow, split the dataset into a training set and a test set.\nYou will use the test set in the final evaluation of your models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- initial_split(dataset, 0.8)\ntrain_dataset <- training(split)\ntest_dataset <- testing(split)\n```\n:::\n\n\n### Inspect the data\n\nReview the joint distribution of a few pairs of columns from the training set.\n\nThe top row suggests that the fuel efficiency (MPG) is a function of all the other parameters.\nThe other rows indicate they are functions of each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_dataset %>%\n  select(mpg, cylinders, displacement, weight) %>%\n  GGally::ggpairs()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLet's also check the overall statistics.\nNote how each feature covers a very different range:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(train_dataset)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |train_dataset |\n|Number of rows           |312           |\n|Number of columns        |10            |\n|_______________________  |              |\n|Column type frequency:   |              |\n|numeric                  |10            |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|     sd|   p0|     p25|     p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|-------:|------:|----:|-------:|-------:|------:|------:|:-----|\n|cylinders     |         0|             1|    5.50|   1.72|    3|    4.00|    4.00|    8.0|    8.0|▇▁▃▁▅ |\n|displacement  |         0|             1|  196.99| 105.81|   68|  105.00|  151.00|  302.0|  455.0|▇▂▂▃▁ |\n|horsepower    |         0|             1|  104.93|  38.85|   46|   76.75|   92.00|  126.0|  230.0|▆▇▃▂▁ |\n|weight        |         0|             1| 2995.04| 844.26| 1613| 2232.50| 2849.00| 3622.5| 4997.0|▇▇▆▅▂ |\n|acceleration  |         0|             1|   15.57|   2.83|    8|   13.88|   15.50|   17.2|   24.8|▁▆▇▂▁ |\n|model_year    |         0|             1|   76.06|   3.71|   70|   73.00|   76.00|   79.0|   82.0|▇▆▇▆▇ |\n|mpg           |         0|             1|   23.48|   7.91|    9|   17.00|   22.15|   29.0|   46.6|▆▇▆▃▁ |\n|origin_USA    |         0|             1|    0.62|   0.48|    0|    0.00|    1.00|    1.0|    1.0|▅▁▁▁▇ |\n|origin_Europe |         0|             1|    0.17|   0.38|    0|    0.00|    0.00|    0.0|    1.0|▇▁▁▁▂ |\n|origin_Japan  |         0|             1|    0.21|   0.40|    0|    0.00|    0.00|    0.0|    1.0|▇▁▁▁▂ |\n:::\n:::\n\n\n### Split features from labels\n\nSeparate the target value---the \""label\""---from the features.\nThis label is the value that you will train the model to predict.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_features <- train_dataset %>% select(-mpg)\ntest_features <- test_dataset %>% select(-mpg)\n\ntrain_labels <- train_dataset %>% select(mpg)\ntest_labels <- test_dataset %>% select(mpg)\n```\n:::\n\n\n## Normalization\n\nIn the table of statistics it's easy to see how different the ranges of each feature are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_skim <- skimr::skim_with(numeric = skimr::sfl(mean, sd))\ntrain_dataset %>%\n  select(where(~is.numeric(.x))) %>%\n  pivot_longer(\n    cols = everything(), names_to = \""variable\"", values_to = \""values\"") %>%\n  group_by(variable) %>%\n  summarise(mean = mean(values), sd = sd(values))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n   variable          mean      sd\n   <chr>            <dbl>   <dbl>\n 1 acceleration    15.6     2.83 \n 2 cylinders        5.5     1.72 \n 3 displacement   197.    106.   \n 4 horsepower     105.     38.9  \n 5 model_year      76.1     3.71 \n 6 mpg             23.5     7.91 \n 7 origin_Europe    0.170   0.376\n 8 origin_Japan     0.205   0.404\n 9 origin_USA       0.625   0.485\n10 weight        2995.    844.   \n```\n:::\n:::\n\n\nIt is good practice to normalize features that use different scales and ranges.\n\nOne reason this is important is because the features are multiplied by the model weights.\nSo, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n\nAlthough a model *might* converge without feature normalization, normalization makes training much more stable.\n\nNote: There is no advantage to normalizing the one-hot features---it is done here for simplicity.\nFor more details on how to use the preprocessing layers, refer to the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) tutorial.\n\n### The Normalization layer\n\nThe `layer_normalization()` is a clean and simple way to add feature normalization into your model.\n\nThe first step is to create the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer <- layer_normalization(axis = -1L)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded TensorFlow version 2.10.0\n```\n:::\n:::\n\n\nThen, fit the state of the preprocessing layer to the data by calling `adapt()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer %>% adapt(as.matrix(train_features))\n```\n:::\n\n\nCalculate the mean and variance, and store them in the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(normalizer$mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[5.5000000e+00 1.9699361e+02 1.0493270e+02 2.9950388e+03 1.5573077e+01\n  7.6060890e+01 6.2500000e-01 1.6987181e-01 2.0512822e-01]], shape=(1, 9), dtype=float32)\n```\n:::\n:::\n\n\nWhen the layer is called, it returns the input data, with each feature independently normalized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst <- as.matrix(train_features[1,])\n\ncat('First example:', first)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst example: 8 318 150 4077 14 72 1 0 0\n```\n:::\n\n```{.r .cell-code}\ncat('Normalized:', as.matrix(normalizer(first)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNormalized: 1.452718 1.145492 1.161751 1.283603 -0.5567052 -1.095416 0.7745967 -0.4523641 -0.5080006\n```\n:::\n:::\n\n\n## Linear regression\n\nBefore building a deep neural network model, start with linear regression using one and several variables.\n\n### Linear regression with one variable\n\nBegin with a single-variable linear regression to predict `'mpg'` from `'horsepower'`.\n\nTraining a model with Keras typically starts by defining the model architecture.\nUse a Sequential model, which [represents a sequence of steps](https://www.tensorflow.org/guide/keras/sequential_model).\n\nThere are two steps in your single-variable linear regression model:\n\n-   Normalize the `'horsepower'` input features using the `normalization` preprocessing layer.\n-   Apply a linear transformation ($y = mx+b$) to produce 1 output using a linear layer (`dense`).\n\nThe number of *inputs* can either be set by the `input_shape` argument, or automatically when the model is run for the first time.\n\nFirst, create a matrix made of the `'horsepower'` features.\nThen, instantiate the `layer_normalization` and fit its state to the `horsepower` data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower <- matrix(train_features$horsepower)\nhorsepower_normalizer <- layer_normalization(input_shape = shape(1), axis = NULL)\nhorsepower_normalizer %>% adapt(horsepower)\n```\n:::\n\n\nBuild the Keras Sequential model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model <- keras_model_sequential() %>%\n  horsepower_normalizer() %>%\n  layer_dense(units = 1)\n\nsummary(horsepower_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense (Dense)               (None, 1)                 2         Y          \n============================================================================\nTotal params: 5\nTrainable params: 2\nNon-trainable params: 3\n____________________________________________________________________________\n```\n:::\n:::\n\n\nThis model will predict `'mpg'` from `'horsepower'`.\n\nRun the untrained model on the first 10 'horsepower' values.\nThe output won't be good, but notice that it has the expected shape of `(10, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(horsepower_model, horsepower[1:10,])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1]\n [1,]  1.136952281\n [2,]  1.136952281\n [3,] -0.225353062\n [4,] -0.275808811\n [5,] -0.755138457\n [6,] -0.200125188\n [7,]  0.001697808\n [8,]  0.127837196\n [9,]  0.127837196\n[10,] -0.553315461\n```\n:::\n:::\n\n\nOnce the model is built, configure the training procedure using the Keras `compile()` method.\nThe most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized (`mean_absolute_error`) and how (using the `optimizer_adam`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n\nUse Keras `fit()` to execute the training for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nVisualize the model's training progress using the stats stored in the `history` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results <- list()\ntest_results[[\""horsepower_model\""]] <- horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\nSince this is a single variable regression, it's easy to view the model's predictions as a function of the input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 250, length.out = 251)\ny <- predict(horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n### Linear regression with multiple inputs\n\nYou can use an almost identical setup to make predictions based on multiple inputs.\nThis model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.\n\nCreate a two-step Keras Sequential model again with the first layer being `normalizer` (`layer_normalization(axis = -1)`) you defined earlier and adapted to the whole dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- keras_model_sequential() %>%\n  normalizer() %>%\n  layer_dense(units = 1)\n```\n:::\n\n\nWhen you call `predict()` on a batch of inputs, it produces `units = 1` outputs for each example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(linear_model, as.matrix(train_features[1:10, ]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n [1,] -2.4510198\n [2,] -2.1960301\n [3,]  0.5865564\n [4,]  0.7901620\n [5,]  1.0799651\n [6,] -0.3309186\n [7,] -0.7099205\n [8,] -1.4144585\n [9,] -1.6733706\n[10,]  0.6978542\n```\n:::\n:::\n\n\nWhen you call the model, its weight matrices will be built---check that the `kernel` weights (the $m$ in $y = mx+b$) have a shape of `(9, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model$layers[[2]]$kernel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'dense_1/kernel:0' shape=(9, 1) dtype=float32, numpy=\narray([[-0.6548641 ],\n       [ 0.28214037],\n       [-0.40660018],\n       [-0.6446194 ],\n       [ 0.23400044],\n       [ 0.06394339],\n       [-0.42258593],\n       [ 0.30855668],\n       [-0.28376827]], dtype=float32)>\n```\n:::\n:::\n\n\nConfigure the model with Keras `compile()` and train with `fit()` for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- linear_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nUsing all the inputs in this regression model achieves a much lower training and validation error than the `horsepower_model`, which had one input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['linear_model']] <- linear_model %>%\n  evaluate(\n    as.matrix(test_features),\n    as.matrix(test_labels),\n    verbose = 0\n  )\n```\n:::\n\n\n## Regression with a deep neural network (DNN)\n\nIn the previous section, you implemented two linear models for single and multiple inputs.\n\nHere, you will implement single-input and multiple-input DNN models.\n\nThe code is basically the same except the model is expanded to include some \""hidden\"" non-linear layers.\nThe name \""hidden\"" here just means not directly connected to the inputs or outputs.\n\nThese models will contain a few more layers than the linear model:\n\n-   The normalization layer, as before (with `horsepower_normalizer` for a single-input model and `normalizer` for a multiple-input model).\n-   Two hidden, non-linear, `Dense` layers with the ReLU (`relu`) activation function nonlinearity.\n-   A linear `Dense` single-output layer.\n\nBoth models will use the same training procedure so the `compile` method is included in the `build_and_compile_model` function below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuild_and_compile_model <- function(norm) {\n  model <- keras_model_sequential() %>%\n    norm() %>%\n    layer_dense(64, activation = 'relu') %>%\n    layer_dense(64, activation = 'relu') %>%\n    layer_dense(1)\n\n  model %>% compile(\n    loss = 'mean_absolute_error',\n    optimizer = optimizer_adam(0.001)\n  )\n\n  model\n}\n```\n:::\n\n\n### Regression using a DNN and a single input\n\nCreate a DNN model with only `'Horsepower'` as input and `horsepower_normalizer` (defined earlier) as the normalization layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_horsepower_model <- build_and_compile_model(horsepower_normalizer)\n```\n:::\n\n\nThis model has quite a few more trainable parameters than the linear models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dnn_horsepower_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_2\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense_4 (Dense)             (None, 64)                128       Y          \n dense_3 (Dense)             (None, 64)                4160      Y          \n dense_2 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,356\nTrainable params: 4,353\nNon-trainable params: 3\n____________________________________________________________________________\n```\n:::\n:::\n\n\nTrain the model with Keras `Model$fit`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0,\n  epochs = 100\n)\n```\n:::\n\n\nThis model does slightly better than the linear single-input `horsepower_model`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\nIf you plot the predictions as a function of `'horsepower'`, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0.0, 250, length.out = 251)\ny <- predict(dnn_horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_horsepower_model']] <- dnn_horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\n### Regression using a DNN and multiple inputs\n\nRepeat the previous process using all the inputs.\nThe model's performance slightly improves on the validation dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_model <- build_and_compile_model(normalizer)\nsummary(dnn_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_3\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization (Normalizatio  (None, 9)                19        Y          \n n)                                                                         \n dense_7 (Dense)             (None, 64)                640       Y          \n dense_6 (Dense)             (None, 64)                4160      Y          \n dense_5 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,884\nTrainable params: 4,865\nNon-trainable params: 19\n____________________________________________________________________________\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0,\n  epochs = 100\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_model']] <- dnn_model %>% evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\n## Performance\n\nSince all models have been trained, you can review their test set performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    horsepower_model.loss         linear_model.loss \n                 3.571319                  2.421681 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.111937                  1.736351 \n```\n:::\n:::\n\n\nThese results match the validation error observed during training.\n\n### Make predictions\n\nYou can now make predictions with the `dnn_model` on the test set using Keras `predict()` and review the loss:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(dnn_model, as.matrix(test_features))\nggplot(data.frame(pred = as.numeric(test_predictions), mpg = test_labels$mpg)) +\n  geom_point(aes(x = pred, y = mpg)) +\n  geom_abline(intercept = 0, slope = 1, color = \""blue\"")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\nIt appears that the model predicts reasonably well.\n\nNow, check the error distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(test_predictions - test_labels$mpg, geom = \""density\"")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n\n```{.r .cell-code}\nerror <- test_predictions - test_labels\n```\n:::\n\n\nIf you're happy with the model, save it for later use with `Model$save`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_tf(dnn_model, 'dnn_model')\n```\n:::\n\n\nIf you reload the model, it gives identical output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- load_model_tf('dnn_model')\n\ntest_results[['reloaded']] <- reloaded %>% evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    horsepower_model.loss         linear_model.loss \n                 3.571319                  2.421681 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.111937                  1.736351 \n            reloaded.loss \n                 1.736351 \n```\n:::\n:::\n\n\n## Conclusion\n\nThis notebook introduced a few techniques to handle a regression problem.\nHere are a few more tips that may help:\n\n-   Mean squared error (MSE) (`loss_mean_squared_error()`) and mean absolute error (MAE) (`loss_mean_absolute_error()`) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.\n-   Similarly, evaluation metrics used for regression differ from classification.\n-   When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n-   Overfitting is a common problem for DNN models, though it wasn't a problem for this tutorial. Visit the [Overfit and underfit](overfit_and_underfit.qmd) tutorial for more help with this.\n"",
     ""supporting"": [
       ""regression_files""
     ],

---FILE: index.qmd---
@@ -19,7 +19,7 @@ css: _css/index.css
 
 <br> <br> <br>
 
--   Build and train deep learning models easily with high-level API's like Keras and TF Datasets.
+-   Build and train deep learning models easily with high-level APIs like Keras and TF Datasets.
 -   Iterate rapidly and debug easily with eager execution.
 -   Scale computations to accelerators like GPUs, TPUs, and clusters with graph execution.
 -   Deploy models to the cloud, on-prem, in the browser, or on-device.

---FILE: tutorials/_metadata.yml---
@@ -0,0 +1,2 @@
+website:
+  repo-actions: [edit, source, issue]"
rstudio,tensorflow.rstudio.com,3c3848211a844234827563ea97e6def1c3390f78,Alex Linneman,51831372+alinneman@users.noreply.github.com,2022-09-20T12:47:16Z,GitHub,noreply@github.com,2022-09-20T12:47:16Z,Fix network typo,tutorials/keras/regression.qmd,True,False,True,False,1,1,2,"---FILE: tutorials/keras/regression.qmd---
@@ -1,6 +1,6 @@
 ---
 title: Basic Regression
-description: Train a neural netowrk to predict a continous value.
+description: Train a neural network to predict a continous value.
 aliases:
   - ../beginners/basic-ml/tutorial_basic_regression/index.html
   - ../../articles/tutorial_basic_regression.html"
rstudio,tensorflow.rstudio.com,334f3fa6df17ce5b1dc02bc920def0bc9e2363e0,Edgar Ruiz,edgararuiz@gmail.com,2022-08-26T19:43:53Z,Edgar Ruiz,edgararuiz@gmail.com,2022-08-26T19:43:53Z,"Removes edit button, adds line number of the function to source button, fixes when the function is a method, rerenders",.ecodown;_dont_run_examples/cloudml/cloudml_train.R;_dont_run_examples/cloudml/gcloud_exec.R;_dont_run_examples/cloudml/gcloud_install.R;_dont_run_examples/keras/KerasCallback.R;_dont_run_examples/keras/KerasConstraint.R;_dont_run_examples/keras/Layer.R;_dont_run_examples/keras/application_resnet.R;_dont_run_examples/keras/application_vgg.R;_dont_run_examples/keras/freeze_weights.R;_dont_run_examples/keras/grapes-py_class-grapes.R;_dont_run_examples/keras/is_keras_available.R;_dont_run_examples/keras/keras_model.R;_dont_run_examples/keras/keras_model_sequential.R;_dont_run_examples/keras/multi_gpu_model.R;_dont_run_examples/keras/save_text_tokenizer.R;_dont_run_examples/keras/use_implementation.R;_dont_run_examples/keras/with_custom_object_scope.R;_dont_run_examples/tensorflow/all_dims.R;_dont_run_examples/tensorflow/as_tensor.R;_dont_run_examples/tensorflow/parse_flags.R;_dont_run_examples/tensorflow/shape.R;_dont_run_examples/tensorflow/sub-.tensorflow.tensor.R;_dont_run_examples/tensorflow/tf.R;_dont_run_examples/tensorflow/tf_extract_opts.R;_dont_run_examples/tensorflow/tf_probability.R;_dont_run_examples/tensorflow/tfe_enable_eager_execution.R;_dont_run_examples/tensorflow/use_compat.R;_dont_run_examples/tensorflow/use_session_with_seed.R;_dont_run_examples/tfautograph/ag_if_vars.R;_dont_run_examples/tfautograph/ag_loop_vars.R;_dont_run_examples/tfautograph/ag_name.R;_dont_run_examples/tfautograph/ag_while_opts.R;_dont_run_examples/tfautograph/sub-subset-.tensorflow.python.ops.tensor_array_ops.TensorArray.R;_dont_run_examples/tfautograph/tf_assert.R;_dont_run_examples/tfautograph/tf_case.R;_dont_run_examples/tfautograph/tf_cond.R;_dont_run_examples/tfautograph/tf_switch.R;_dont_run_examples/tfautograph/view_function_graph.R;_dont_run_examples/tfdatasets/choose_from_datasets.R;_dont_run_examples/tfdatasets/dataset_bucket_by_sequence_length.R;_dont_run_examples/tfdatasets/dataset_enumerate.R;_dont_run_examples/tfdatasets/dataset_filter.R;_dont_run_examples/tfdatasets/dataset_interleave.R;_dont_run_examples/tfdatasets/dataset_options.R;_dont_run_examples/tfdatasets/dataset_padded_batch.R;_dont_run_examples/tfdatasets/dataset_rejection_resample.R;_dont_run_examples/tfdatasets/dataset_scan.R;_dont_run_examples/tfdatasets/dataset_unique.R;_dont_run_examples/tfdatasets/dataset_use_spec.R;_dont_run_examples/tfdatasets/feature_spec.R;_dont_run_examples/tfdatasets/fit.FeatureSpec.R;_dont_run_examples/tfdatasets/layer_input_from_dataset.R;_dont_run_examples/tfdatasets/length.tf_dataset.R;_dont_run_examples/tfdatasets/next_batch.R;_dont_run_examples/tfdatasets/step_bucketized_column.R;_dont_run_examples/tfdatasets/step_categorical_column_with_hash_bucket.R;_dont_run_examples/tfdatasets/step_categorical_column_with_identity.R;_dont_run_examples/tfdatasets/step_categorical_column_with_vocabulary_file.R;_dont_run_examples/tfdatasets/step_categorical_column_with_vocabulary_list.R;_dont_run_examples/tfdatasets/step_crossed_column.R;_dont_run_examples/tfdatasets/step_embedding_column.R;_dont_run_examples/tfdatasets/step_indicator_column.R;_dont_run_examples/tfdatasets/step_numeric_column.R;_dont_run_examples/tfdatasets/step_remove_column.R;_dont_run_examples/tfdatasets/tfrecord_dataset.R;_dont_run_examples/tfdatasets/until_out_of_range.R;_dont_run_examples/tfdatasets/with_dataset.R;_dont_run_examples/tfhub/hub_load.R;_dont_run_examples/tfhub/layer_hub.R;_dont_run_examples/tfhub/step_pretrained_text_embedding.R;_dont_run_examples/tfruns/clean_runs.R;_dont_run_examples/tfruns/copy_run.R;_dont_run_examples/tfruns/flags.R;_dont_run_examples/tfruns/tuning_run.R;_freeze/reference/cloudml/cloudml_train/execute-results/html.json;_freeze/reference/cloudml/gcloud_exec/execute-results/html.json;_freeze/reference/cloudml/gcloud_install/execute-results/html.json;_freeze/reference/keras/application_resnet/execute-results/html.json;_freeze/reference/keras/application_vgg/execute-results/html.json;_freeze/reference/keras/freeze_weights/execute-results/html.json;_freeze/reference/keras/grapes-py_class-grapes/execute-results/html.json;_freeze/reference/keras/grapes-set-active-grapes/execute-results/html.json;_freeze/reference/keras/is_keras_available/execute-results/html.json;_freeze/reference/keras/keras_model/execute-results/html.json;_freeze/reference/keras/keras_model_sequential/execute-results/html.json;_freeze/reference/keras/kerascallback/execute-results/html.json;_freeze/reference/keras/kerasconstraint/execute-results/html.json;_freeze/reference/keras/layer/execute-results/html.json;_freeze/reference/keras/multi_gpu_model/execute-results/html.json;_freeze/reference/keras/save_text_tokenizer/execute-results/html.json;_freeze/reference/keras/use_implementation/execute-results/html.json;_freeze/reference/keras/with_custom_object_scope/execute-results/html.json;_freeze/reference/keras/zip_lists/execute-results/html.json;_freeze/reference/tensorflow/all_dims/execute-results/html.json;_freeze/reference/tensorflow/as_tensor/execute-results/html.json;_freeze/reference/tensorflow/parse_flags/execute-results/html.json;_freeze/reference/tensorflow/shape/execute-results/html.json;_freeze/reference/tensorflow/sub-.tensorflow.tensor/execute-results/html.json;_freeze/reference/tensorflow/tf/execute-results/html.json;_freeze/reference/tensorflow/tf_extract_opts/execute-results/html.json;_freeze/reference/tensorflow/tf_probability/execute-results/html.json;_freeze/reference/tensorflow/tfe_enable_eager_execution/execute-results/html.json;_freeze/reference/tensorflow/use_compat/execute-results/html.json;_freeze/reference/tensorflow/use_session_with_seed/execute-results/html.json;_freeze/reference/tfautograph/ag_if_vars/execute-results/html.json;_freeze/reference/tfautograph/ag_loop_vars/execute-results/html.json;_freeze/reference/tfautograph/ag_name/execute-results/html.json;_freeze/reference/tfautograph/ag_while_opts/execute-results/html.json;_freeze/reference/tfautograph/sub-subset-.tensorflow.python.ops.tensor_array_ops.tensorarray/execute-results/html.json;_freeze/reference/tfautograph/tf_assert/execute-results/html.json;_freeze/reference/tfautograph/tf_case/execute-results/html.json;_freeze/reference/tfautograph/tf_cond/execute-results/html.json;_freeze/reference/tfautograph/tf_switch/execute-results/html.json;_freeze/reference/tfautograph/view_function_graph/execute-results/html.json;_freeze/reference/tfdatasets/choose_from_datasets/execute-results/html.json;_freeze/reference/tfdatasets/dataset_bucket_by_sequence_length/execute-results/html.json;_freeze/reference/tfdatasets/dataset_enumerate/execute-results/html.json;_freeze/reference/tfdatasets/dataset_filter/execute-results/html.json;_freeze/reference/tfdatasets/dataset_interleave/execute-results/html.json;_freeze/reference/tfdatasets/dataset_options/execute-results/html.json;_freeze/reference/tfdatasets/dataset_padded_batch/execute-results/html.json;_freeze/reference/tfdatasets/dataset_rejection_resample/execute-results/html.json;_freeze/reference/tfdatasets/dataset_scan/execute-results/html.json;_freeze/reference/tfdatasets/dataset_unique/execute-results/html.json;_freeze/reference/tfdatasets/dataset_use_spec/execute-results/html.json;_freeze/reference/tfdatasets/feature_spec/execute-results/html.json;_freeze/reference/tfdatasets/fit.featurespec/execute-results/html.json;_freeze/reference/tfdatasets/layer_input_from_dataset/execute-results/html.json;_freeze/reference/tfdatasets/length.tf_dataset/execute-results/html.json;_freeze/reference/tfdatasets/next_batch/execute-results/html.json;_freeze/reference/tfdatasets/step_bucketized_column/execute-results/html.json;_freeze/reference/tfdatasets/step_categorical_column_with_hash_bucket/execute-results/html.json;_freeze/reference/tfdatasets/step_categorical_column_with_identity/execute-results/html.json;_freeze/reference/tfdatasets/step_categorical_column_with_vocabulary_file/execute-results/html.json;_freeze/reference/tfdatasets/step_categorical_column_with_vocabulary_list/execute-results/html.json;_freeze/reference/tfdatasets/step_crossed_column/execute-results/html.json;_freeze/reference/tfdatasets/step_embedding_column/execute-results/html.json;_freeze/reference/tfdatasets/step_indicator_column/execute-results/html.json;_freeze/reference/tfdatasets/step_numeric_column/execute-results/html.json;_freeze/reference/tfdatasets/step_remove_column/execute-results/html.json;_freeze/reference/tfdatasets/tfrecord_dataset/execute-results/html.json;_freeze/reference/tfdatasets/until_out_of_range/execute-results/html.json;_freeze/reference/tfdatasets/with_dataset/execute-results/html.json;_freeze/reference/tfhub/hub_load/execute-results/html.json;_freeze/reference/tfhub/layer_hub/execute-results/html.json;_freeze/reference/tfhub/step_pretrained_text_embedding/execute-results/html.json;_freeze/reference/tfruns/clean_runs/execute-results/html.json;_freeze/reference/tfruns/copy_run/execute-results/html.json;_freeze/reference/tfruns/flags/execute-results/html.json;_freeze/reference/tfruns/tuning_run/execute-results/html.json;_tests/tfhub/hub_load.R;_tests/tfhub/layer_hub.R;_tests/tfruns/step_pretrained_text_embedding.R;reference/assets/_reference.qmd;reference/cloudml/cloudml-package.qmd;reference/cloudml/cloudml_deploy.qmd;reference/cloudml/cloudml_predict.qmd;reference/cloudml/cloudml_train.qmd;reference/cloudml/gcloud_exec.qmd;reference/cloudml/gcloud_init.qmd;reference/cloudml/gcloud_install.qmd;reference/cloudml/gcloud_terminal.qmd;reference/cloudml/gcloud_version.qmd;reference/cloudml/gs_copy.qmd;reference/cloudml/gs_data_dir.qmd;reference/cloudml/gs_data_dir_local.qmd;reference/cloudml/gs_local_dir.qmd;reference/cloudml/gs_rsync.qmd;reference/cloudml/gsutil_exec.qmd;reference/cloudml/job_cancel.qmd;reference/cloudml/job_collect.qmd;reference/cloudml/job_list.qmd;reference/cloudml/job_status.qmd;reference/cloudml/job_stream_logs.qmd;reference/cloudml/job_trials.qmd;reference/keras/activation_relu.qmd;reference/keras/adapt.qmd;reference/keras/application_densenet.qmd;reference/keras/application_efficientnet.qmd;reference/keras/application_inception_resnet_v2.qmd;reference/keras/application_inception_v3.qmd;reference/keras/application_mobilenet.qmd;reference/keras/application_mobilenet_v2.qmd;reference/keras/application_mobilenet_v3.qmd;reference/keras/application_nasnet.qmd;reference/keras/application_resnet.qmd;reference/keras/application_vgg.qmd;reference/keras/application_xception.qmd;reference/keras/backend.qmd;reference/keras/bidirectional.qmd;reference/keras/callback_backup_and_restore.qmd;reference/keras/callback_csv_logger.qmd;reference/keras/callback_early_stopping.qmd;reference/keras/callback_lambda.qmd;reference/keras/callback_learning_rate_scheduler.qmd;reference/keras/callback_model_checkpoint.qmd;reference/keras/callback_progbar_logger.qmd;reference/keras/callback_reduce_lr_on_plateau.qmd;reference/keras/callback_remote_monitor.qmd;reference/keras/callback_tensorboard.qmd;reference/keras/callback_terminate_on_naan.qmd;reference/keras/clone_model.qmd;reference/keras/compile.keras.engine.training.model.qmd;reference/keras/constraints.qmd;reference/keras/count_params.qmd;reference/keras/create_layer.qmd;reference/keras/create_layer_wrapper.qmd;reference/keras/create_wrapper.qmd;reference/keras/custom_metric.qmd;reference/keras/dataset_boston_housing.qmd;reference/keras/dataset_cifar10.qmd;reference/keras/dataset_cifar100.qmd;reference/keras/dataset_fashion_mnist.qmd;reference/keras/dataset_imdb.qmd;reference/keras/dataset_mnist.qmd;reference/keras/dataset_reuters.qmd;reference/keras/evaluate.keras.engine.training.model.qmd;reference/keras/evaluate_generator.qmd;reference/keras/export_savedmodel.keras.engine.training.model.qmd;reference/keras/fit.keras.engine.training.model.qmd;reference/keras/fit_generator.qmd;reference/keras/fit_image_data_generator.qmd;reference/keras/fit_text_tokenizer.qmd;reference/keras/flow_images_from_data.qmd;reference/keras/flow_images_from_dataframe.qmd;reference/keras/flow_images_from_directory.qmd;reference/keras/freeze_weights.qmd;reference/keras/generator_next.qmd;reference/keras/get_config.qmd;reference/keras/get_file.qmd;reference/keras/get_input_at.qmd;reference/keras/get_layer.qmd;reference/keras/get_weights.qmd;reference/keras/grapes-py_class-grapes.qmd;reference/keras/grapes-set-active-grapes.qmd;reference/keras/hdf5_matrix.qmd;reference/keras/image_data_generator.qmd;reference/keras/image_dataset_from_directory.qmd;reference/keras/image_load.qmd;reference/keras/image_to_array.qmd;reference/keras/imagenet_decode_predictions.qmd;reference/keras/imagenet_preprocess_input.qmd;reference/keras/implementation.qmd;reference/keras/initializer_constant.qmd;reference/keras/initializer_glorot_normal.qmd;reference/keras/initializer_glorot_uniform.qmd;reference/keras/initializer_he_normal.qmd;reference/keras/initializer_he_uniform.qmd;reference/keras/initializer_identity.qmd;reference/keras/initializer_lecun_normal.qmd;reference/keras/initializer_lecun_uniform.qmd;reference/keras/initializer_ones.qmd;reference/keras/initializer_orthogonal.qmd;reference/keras/initializer_random_normal.qmd;reference/keras/initializer_random_uniform.qmd;reference/keras/initializer_truncated_normal.qmd;reference/keras/initializer_variance_scaling.qmd;reference/keras/initializer_zeros.qmd;reference/keras/install_keras.qmd;reference/keras/is_keras_available.qmd;reference/keras/k_abs.qmd;reference/keras/k_all.qmd;reference/keras/k_any.qmd;reference/keras/k_arange.qmd;reference/keras/k_argmax.qmd;reference/keras/k_argmin.qmd;reference/keras/k_backend.qmd;reference/keras/k_batch_dot.qmd;reference/keras/k_batch_flatten.qmd;reference/keras/k_batch_get_value.qmd;reference/keras/k_batch_normalization.qmd;reference/keras/k_batch_set_value.qmd;reference/keras/k_bias_add.qmd;reference/keras/k_binary_crossentropy.qmd;reference/keras/k_cast.qmd;reference/keras/k_cast_to_floatx.qmd;reference/keras/k_categorical_crossentropy.qmd;reference/keras/k_clear_session.qmd;reference/keras/k_clip.qmd;reference/keras/k_concatenate.qmd;reference/keras/k_constant.qmd;reference/keras/k_conv1d.qmd;reference/keras/k_conv2d.qmd;reference/keras/k_conv2d_transpose.qmd;reference/keras/k_conv3d.qmd;reference/keras/k_conv3d_transpose.qmd;reference/keras/k_cos.qmd;reference/keras/k_count_params.qmd;reference/keras/k_ctc_batch_cost.qmd;reference/keras/k_ctc_decode.qmd;reference/keras/k_ctc_label_dense_to_sparse.qmd;reference/keras/k_cumprod.qmd;reference/keras/k_cumsum.qmd;reference/keras/k_depthwise_conv2d.qmd;reference/keras/k_dot.qmd;reference/keras/k_dropout.qmd;reference/keras/k_dtype.qmd;reference/keras/k_elu.qmd;reference/keras/k_epsilon.qmd,True,True,True,False,644,680,1324,"---FILE: .ecodown---
@@ -1 +1 @@
-bb424ecd1a8413632e2097342c16590901929ef4
+a03f8db13f3f238f14e1b9934a3d5af55a1cf89a

---FILE: _dont_run_examples/cloudml/gcloud_exec.R---
@@ -1 +1,2 @@
+library(cloudml)
 gcloud_exec(""help"", ""info"") 

---FILE: _dont_run_examples/keras/KerasConstraint.R---
@@ -1,3 +1,4 @@
+library(keras)
 CustomNonNegConstraint <- R6::R6Class( 
   ""CustomNonNegConstraint"", 
   inherit = KerasConstraint, 

---FILE: _dont_run_examples/keras/Layer.R---
@@ -1,3 +1,4 @@
+library(keras)
 layer_dense2 <- Layer( 
   ""Dense2"", 
   initialize = function(units) { 

---FILE: _dont_run_examples/keras/freeze_weights.R---
@@ -1,3 +1,4 @@
+library(keras)
 conv_base <- application_vgg16( 
   weights = ""imagenet"", 
   include_top = FALSE, 

---FILE: _dont_run_examples/keras/grapes-py_class-grapes.R---
@@ -1,3 +1,4 @@
+library(keras)
 MyClass %py_class% { 
   initialize <- function(x) { 
     print(""Hi from MyClass$initialize()!"") 

---FILE: _dont_run_examples/keras/is_keras_available.R---
@@ -1,3 +1,4 @@
+library(keras)
 # testthat utilty for skipping tests when Keras isn't available 
 skip_if_no_keras <- function(version = NULL) { 
   if (!is_keras_available(version)) 

---FILE: _dont_run_examples/keras/save_text_tokenizer.R---
@@ -1,3 +1,4 @@
+library(keras)
 # vectorize texts then save for use in prediction 
 tokenizer <- text_tokenizer(num_words = 10000) %>% 
 fit_text_tokenizer(tokenizer, texts) 

---FILE: _dont_run_examples/keras/with_custom_object_scope.R---
@@ -1,3 +1,4 @@
+library(keras)
 # define custom metric 
 metric_top_3_categorical_accuracy <- 
   custom_metric(""top_3_categorical_accuracy"", function(y_true, y_pred) { 

---FILE: _dont_run_examples/tensorflow/all_dims.R---
@@ -1,3 +1,4 @@
+library(tensorflow)
 # in python, if x is a numpy array or tensorflow tensor 
 x[..., i] 
 # the ellipsis means ""expand to match number of dimension of x"". 

---FILE: _dont_run_examples/tensorflow/as_tensor.R---
@@ -1,2 +1,3 @@
+library(tensorflow)
 as_tensor(42, ""int32"") 
 as_tensor(as_tensor(42)) 

---FILE: _dont_run_examples/tensorflow/parse_flags.R---
@@ -1,3 +1,4 @@
+library(tensorflow)
 # examine an example configuration file provided by tensorflow 
 file <- system.file(""examples/config/flags.yml"", package = ""tensorflow"") 
 cat(readLines(file), sep = ""\n"") 

---FILE: _dont_run_examples/tensorflow/shape.R---
@@ -1,3 +1,4 @@
+library(tensorflow)
 # --- construct --- 
 shape()       # tf.TensorShape()       # scalar 
 shape(NULL)   # tf.TensorShape([None]) # 1-D array of unknown length 

---FILE: _dont_run_examples/tensorflow/sub-.tensorflow.tensor.R---
@@ -1,3 +1,4 @@
+library(tensorflow)
 x <- as_tensor(array(1:15, dim = c(3, 5))) 
 x 
 # by default, numerics supplied to [...] are interpreted R style 

---FILE: _dont_run_examples/tensorflow/tf_extract_opts.R---
@@ -1,3 +1,4 @@
+library(tensorflow)
 x <- tf$constant(1:10) 
 opts <-  tf_extract_opts(""R"") 
 x[1, options = opts] 

---FILE: _dont_run_examples/tfautograph/ag_if_vars.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 # these examples only have an effect in graph mode 
 # to enter graph mode easily we'll create a few helpers 
 ag <- autograph 

---FILE: _dont_run_examples/tfautograph/ag_loop_vars.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 i <- tf$constant(0L) 
 autograph({ 
   ag_loop_vars(x, i) 

---FILE: _dont_run_examples/tfautograph/ag_name.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 ## when you're in graph mode. (e.g, tf$executing_eagerly == FALSE) 
 ag_name(""main-training-loop"") 
 for(elem in dataset) ... 

---FILE: _dont_run_examples/tfautograph/ag_while_opts.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 ## use tf_function() to enter graph mode: 
 tf_function(autograph(function(n) { 
   ag_name(""silly-example"") 

---FILE: _dont_run_examples/tfautograph/sub-subset-.tensorflow.python.ops.tensor_array_ops.TensorArray.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 ta <- tf$TensorArray(tf$float32, size = 5L) 
 for(i in 0:4) 
   ta[[i]] <- i 

---FILE: _dont_run_examples/tfautograph/tf_assert.R---
@@ -1,2 +1,3 @@
+library(tfautograph)
 x <- tf$constant(-1) 
 try(tf_assert(x > 0, ""oopsies! x must be greater than 0"")) 

---FILE: _dont_run_examples/tfautograph/tf_case.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 fizz_buzz_one <- function(x) { 
   tf_case( 
     x %% 15 == 0 ~ ""FizzBuzz"", 

---FILE: _dont_run_examples/tfautograph/tf_cond.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 ## square if positive 
 # using tf$cond directly: 
 raw <- function(x) tf$cond(x > 0, function() x * x, function() x) 

---FILE: _dont_run_examples/tfautograph/tf_switch.R---
@@ -1,3 +1,4 @@
+library(tfautograph)
 tf_pow <- tf_function(function(x, pow) { 
    tf_switch(pow, 
    0 ~ 1, 

---FILE: _dont_run_examples/tfautograph/view_function_graph.R---
@@ -1,2 +1,3 @@
+library(tfautograph)
 fn <- tf_function(function(x) autograph(if(x > 0) x * x else x)) 
 view_function_graph(fn, list(tf$constant(5))) 

---FILE: _dont_run_examples/tfdatasets/choose_from_datasets.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 datasets <- list(tensors_dataset(""foo"") %>% dataset_repeat(), 
                  tensors_dataset(""bar"") %>% dataset_repeat(), 
                  tensors_dataset(""baz"") %>% dataset_repeat()) 

---FILE: _dont_run_examples/tfdatasets/dataset_bucket_by_sequence_length.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 dataset <- list(c(0), 
                 c(1, 2, 3, 4), 
                 c(5, 6, 7), 

---FILE: _dont_run_examples/tfdatasets/dataset_enumerate.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 dataset <- tensor_slices_dataset(100:103) %>% 
   dataset_enumerate() 
 iterator <- reticulate::as_iterator(dataset) 

---FILE: _dont_run_examples/tfdatasets/dataset_filter.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 dataset <- text_line_dataset(""mtcars.csv"", record_spec = mtcars_spec) %>% 
   dataset_filter(function(record) { 
     record$mpg >= 20 

---FILE: _dont_run_examples/tfdatasets/dataset_interleave.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 dataset <- tensor_slices_dataset(c(1,2,3,4,5)) %>% 
  dataset_interleave(cycle_length = 2, block_length = 4, function(x) { 
    tensors_dataset(x) %>% 

---FILE: _dont_run_examples/tfdatasets/dataset_options.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 # pass options directly: 
 range_dataset(0, 10) %>% 
   dataset_options( 

---FILE: _dont_run_examples/tfdatasets/dataset_padded_batch.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 A <- range_dataset(1, 5, dtype = tf$int32) %>% 
   dataset_map(function(x) tf$fill(list(x), x)) 
 # Pad to the smallest per-batch size that fits all elements. 

---FILE: _dont_run_examples/tfdatasets/dataset_rejection_resample.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 initial_dist <- c(.5, .5) 
 target_dist <- c(.6, .4) 
 num_classes <- length(initial_dist) 

---FILE: _dont_run_examples/tfdatasets/dataset_scan.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 initial_state <- as_tensor(0, dtype=""int64"") 
 scan_func <- function(state, i) list(state + i, state + i) 
 dataset <- range_dataset(0, 10) %>% 

---FILE: _dont_run_examples/tfdatasets/dataset_unique.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 c(0, 37, 2, 37, 2, 1) %>% as_tensor(""int32"") %>% 
   tensor_slices_dataset() %>% 
   dataset_unique() %>% 

---FILE: _dont_run_examples/tfdatasets/length.tf_dataset.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 range_dataset(0, 42) %>% length() 
 # 42 
 range_dataset(0, 42) %>% dataset_repeat() %>% length() 

---FILE: _dont_run_examples/tfdatasets/tfrecord_dataset.R---
@@ -1,3 +1,4 @@
+library(tfdatasets)
 # Creates a dataset that reads all of the examples from two files, and extracts 
 # the image and label features. 
 filenames <- c(""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord"") 

---FILE: _dont_run_examples/tfhub/hub_load.R---
@@ -1 +1,2 @@
+library(tfhub)
 model <- hub_load('https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4') 

---FILE: _dont_run_examples/tfhub/layer_hub.R---
@@ -1,3 +1,4 @@
+library(tfhub)
 library(keras) 
 model <- keras_model_sequential() %>% 
  layer_hub( 

---FILE: _dont_run_examples/tfhub/step_pretrained_text_embedding.R---
@@ -1,3 +1,4 @@
+library(tfhub)
 library(tibble) 
 library(recipes) 
 df <- tibble(text = c('hi', ""heello"", ""goodbye""), y = 0) 

---FILE: _dont_run_examples/tfruns/clean_runs.R---
@@ -0,0 +1,2 @@
+library(tfruns)
+clean_runs(ls_runs(completed == FALSE)) 

---FILE: _dont_run_examples/tfruns/copy_run.R---
@@ -0,0 +1,9 @@
+library(tfruns)
+# export a run directory to the current working directory 
+copy_run(""runs/2017-09-24T10-54-00Z"") 
+# export to the current working directory then rename 
+copy_run(""runs/2017-09-24T10-54-00Z"", rename = ""best-run"") 
+# export artifact files only to the current working directory then rename 
+copy_run_files(""runs/2017-09-24T10-54-00Z"", rename = ""best-model"") 
+# export 3 best eval_acc to a ""best-runs"" directory 
+copy_run(ls_runs(order = eval_acc)[1:3,], to = ""best-runs"") 

---FILE: _dont_run_examples/tfruns/flags.R---
@@ -0,0 +1,8 @@
+library(tfruns) 
+# define flags and parse flag values from flags.yml and the command line 
+FLAGS <- flags( 
+  flag_numeric('learning_rate', 0.01, 'Initial learning rate.'), 
+  flag_integer('max_steps', 5000, 'Number of steps to run trainer.'), 
+  flag_string('data_dir', 'MNIST-data', 'Directory for training data'), 
+  flag_boolean('fake_data', FALSE, 'If true, use fake data for testing') 
+) 

---FILE: _dont_run_examples/tfruns/tuning_run.R---
@@ -0,0 +1,23 @@
+library(tfruns) 
+# using a list as input to the flags argument 
+runs <- tuning_run( 
+  system.file(""examples/mnist_mlp/mnist_mlp.R"", package = ""tfruns""), 
+  flags = list( 
+    dropout1 = c(0.2, 0.3, 0.4), 
+    dropout2 = c(0.2, 0.3, 0.4) 
+  ) 
+) 
+runs[order(runs$eval_acc, decreasing = TRUE), ] 
+# using a data frame as input to the flags argument 
+# resulting in the same combinations above, but remove those 
+# where the combined dropout rate exceeds 1 
+grid <- expand.grid( 
+  dropout1 = c(0.2, 0.3, 0.4), 
+  dropout2 = c(0.2, 0.3, 0.4) 
+) 
+grid$combined_droput <- grid$dropout1 + grid$dropout2 
+grid <- grid[grid$combined_droput <= 1, ] 
+runs <- tuning_run( 
+  system.file(""examples/mnist_mlp/mnist_mlp.R"", package = ""tfruns""), 
+  flags = grid[, c(""dropout1"", ""dropout2"")] 
+) 

---FILE: _freeze/reference/cloudml/cloudml_train/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""d9115c256d7d541ced5772839967574c"",
+  ""hash"": ""76f33d709ae2c50782c757728e2d76f1"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/jobs.R*\n\n# cloudml_train\n\n## Train a model using Cloud ML\n\n## Description\nUpload a TensorFlow application to Google Cloud, and use that application to train a model. \n\n\n## Usage\n```r\ncloudml_train(file = \""train.R\"", master_type = NULL, flags = NULL, \n  region = NULL, config = NULL, collect = \""ask\"", dry_run = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| file | File to be used as entrypoint for training. |\n| master_type | Training master node machine type. \""standard\"" provides a basic machine configuration suitable for training simple models with small to moderate datasets. See the documentation at [https://cloud.google.com/ml-engine/docs/tensorflow/machine-types#machine_type_table](https://cloud.google.com/ml-engine/docs/tensorflow/machine-types#machine_type_table)<br>for details on available machine types. |\n| flags | Named list with flag values (see `flags()`) or path to YAML file containing flag values. |\n| region | The region to be used for training. |\n| config | A list, `YAML` or `JSON` configuration file as described [https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs). |\n| collect | Logical. If TRUE, collect job when training is completed (blocks waiting for the job to complete). The default (`\""ask\""`) will interactively prompt the user whether to collect the results or not. |\n| dry_run | Triggers a local dry run over the deployment phase to validate packages and packing work as expected. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cloudml) \ngcloud_install() \njob <- cloudml_train(\""train.R\"") \n```\n:::\n\n\n## See Also\n\n`job_status()`, `job_collect()`, `job_cancel()`\n\nOther CloudML functions: `cloudml_deploy`,   `cloudml_predict`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](NULL/blob/main/R/jobs.R#L42) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# cloudml_train\n\n## Train a model using Cloud ML\n\n## Description\nUpload a TensorFlow application to Google Cloud, and use that application to train a model. \n\n\n## Usage\n```r\ncloudml_train(file = \""train.R\"", master_type = NULL, flags = NULL, \n  region = NULL, config = NULL, collect = \""ask\"", dry_run = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| file | File to be used as entrypoint for training. |\n| master_type | Training master node machine type. \""standard\"" provides a basic machine configuration suitable for training simple models with small to moderate datasets. See the documentation at [https://cloud.google.com/ml-engine/docs/tensorflow/machine-types#machine_type_table](https://cloud.google.com/ml-engine/docs/tensorflow/machine-types#machine_type_table)<br>for details on available machine types. |\n| flags | Named list with flag values (see `flags()`) or path to YAML file containing flag values. |\n| region | The region to be used for training. |\n| config | A list, `YAML` or `JSON` configuration file as described [https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs). |\n| collect | Logical. If TRUE, collect job when training is completed (blocks waiting for the job to complete). The default (`\""ask\""`) will interactively prompt the user whether to collect the results or not. |\n| dry_run | Triggers a local dry run over the deployment phase to validate packages and packing work as expected. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cloudml) \ngcloud_install() \njob <- cloudml_train(\""train.R\"") \n```\n:::\n\n\n## See Also\n\n`job_status()`, `job_collect()`, `job_cancel()`\n\nOther CloudML functions: `cloudml_deploy`,   `cloudml_predict`\n\n"",
+    ""supporting"": [
+      ""cloudml_train_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/cloudml/gcloud_exec/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""057ab654c5d704d88c6226e749872a66"",
+  ""hash"": ""6f5dc60cc692f60d2b6081685b920ef7"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](NULL/blob/main/R/gcloud-exec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-exec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/gcloud-exec.R*\n\n# gcloud_exec\n\n## Executes a Google Cloud Command\n\n## Description\nExecutes a Google Cloud command with the given parameters. \n\n\n## Usage\n```r\ngcloud_exec(..., args = NULL, echo = TRUE, dry_run = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Parameters to use specified based on position. |\n| args | Parameters to use specified as a list. |\n| echo | Echo command output to console. |\n| dry_run | Echo but not execute the command? |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ngcloud_exec(\""help\"", \""info\"") \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](NULL/blob/main/R/gcloud-exec.R#L74) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# gcloud_exec\n\n## Executes a Google Cloud Command\n\n## Description\nExecutes a Google Cloud command with the given parameters. \n\n\n## Usage\n```r\ngcloud_exec(..., args = NULL, echo = TRUE, dry_run = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Parameters to use specified based on position. |\n| args | Parameters to use specified as a list. |\n| echo | Echo command output to console. |\n| dry_run | Echo but not execute the command? |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cloudml)\ngcloud_exec(\""help\"", \""info\"") \n```\n:::\n"",
+    ""supporting"": [
+      ""gcloud_exec_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/cloudml/gcloud_install/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""99151fb70ff919d79c9d3150a9cd3e05"",
+  ""hash"": ""875eeb5e45a35d742f76b4ada1dc4200"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](NULL/blob/main/R/gcloud-install.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-install.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/gcloud-install.R*\n\n# gcloud_install\n\n## Install the Google Cloud SDK\n\n## Description\nInstalls the Google Cloud SDK which enables CloudML operations. \n\n\n## Usage\n```r\ngcloud_install(update = TRUE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| update | Attempt to update an existing installation. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cloudml) \ngcloud_install() \n```\n:::\n\n\n## See Also\nOther Google Cloud SDK functions: `gcloud_init`,   `gcloud_terminal`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](NULL/blob/main/R/gcloud-install.R#L75) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# gcloud_install\n\n## Install the Google Cloud SDK\n\n## Description\nInstalls the Google Cloud SDK which enables CloudML operations. \n\n\n## Usage\n```r\ngcloud_install(update = TRUE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| update | Attempt to update an existing installation. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cloudml) \ngcloud_install() \n```\n:::\n\n\n## See Also\nOther Google Cloud SDK functions: `gcloud_init`,   `gcloud_terminal`\n\n"",
+    ""supporting"": [
+      ""gcloud_install_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/application_resnet/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""8f8339f84fecb72a31f0399edf8621b5"",
+  ""hash"": ""b1f7dc4a0a8ce2316cfb9f9b655541ef"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/applications.R*\n\n# application_resnet\n\n## Instantiates the ResNet architecture\n\n## Description\nInstantiates the ResNet architecture \n\n\n## Usage\n```r\napplication_resnet50( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  ... \n) \napplication_resnet101( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  ... \n) \napplication_resnet152( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  ... \n) \napplication_resnet50_v2( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"", \n  ... \n) \napplication_resnet101_v2( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"", \n  ... \n) \napplication_resnet152_v2( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"", \n  ... \n) \nresnet_preprocess_input(x) \nresnet_v2_preprocess_input(x) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| include_top | Whether to include the fully-connected layer at the top of the network. Defaults to `TRUE`. |\n| weights | One of `NULL` (random initialization), `'imagenet'` (pre-training on ImageNet), or the path to the weights file to be loaded. Defaults to `'imagenet'`. |\n| input_tensor | Optional Keras tensor (i.e. output of `layer_input()`) to use as image input for the model. |\n| input_shape | optional shape list, only to be specified if `include_top` is FALSE (otherwise the input shape has to be `c(224, 224, 3)` (with `'channels_last'` data format) or `c(3, 224, 224)` (with `'channels_first'` data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. `c(200, 200, 3)` would be one valid value. |\n| pooling | Optional pooling mode for feature extraction when `include_top` is `FALSE`. Defaults to `NULL`. <br>- `NULL` means that the output of the model will be the 4D tensor output of the last convolutional layer. <br>- `'avg'` means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor. <br>- `'max'` means that global max pooling will be applied.  |\n| classes | Optional number of classes to classify images into, only to be specified if `include_top` is TRUE, and if no `weights` argument is specified. Defaults to 1000 (number of ImageNet classes). |\n| ... | For backwards and forwards compatibility |\n| classifier_activation | A string or callable. The activation function to use on the \""top\"" layer. Ignored unless `include_top = TRUE`. Set `classifier_activation = NULL` to return the logits of the \""top\"" layer. Defaults to `'softmax'`. When loading pretrained weights, `classifier_activation` can only be `NULL` or `\""softmax\""`. |\n| x | `preprocess_input()` takes an array or floating point tensor, 3D or 4D with 3 color channels, with values in the range `[0, 255]`. |\n\n## Details\n\nReference: \n\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (CVPR 2015) \n\nFor image classification use cases, see [this page for detailed examples](https://keras.io/api/applications/#usage-examples-for-image-classification-models). \n\nFor transfer learning use cases, make sure to read the [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/). \n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call `tf.keras.applications.resnet.preprocess_input` on your inputs before passing them to the model. `resnet.preprocess_input` will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \n# instantiate the model \nmodel <- application_resnet50(weights = 'imagenet') \n# load the image \nimg_path <- \""elephant.jpg\"" \nimg <- image_load(img_path, target_size = c(224,224)) \nx <- image_to_array(img) \n# ensure we have a 4d tensor with single element in the batch dimension, \n# the preprocess the input for prediction using resnet50 \nx <- array_reshape(x, c(1, dim(x))) \nx <- imagenet_preprocess_input(x) \n# make predictions then decode and print them \npreds <- model %>% predict(x) \nimagenet_decode_predictions(preds, top = 3)[[1]] \n```\n:::\n\n\n## See Also\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet101](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet101)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet152](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet152)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet50V2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet50V2)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet101V2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet101V2)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet152V2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet152V2)\n\n- [https://keras.io/api/applications/](https://keras.io/api/applications/)\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L200) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# application_resnet\n\n## Instantiates the ResNet architecture\n\n## Description\nInstantiates the ResNet architecture \n\n\n## Usage\n```r\napplication_resnet50( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  ... \n) \n\napplication_resnet101( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  ... \n) \n\napplication_resnet152( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  ... \n) \n\napplication_resnet50_v2( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"", \n  ... \n) \n\napplication_resnet101_v2( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"", \n  ... \n) \n\napplication_resnet152_v2( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"", \n  ... \n) \n\nresnet_preprocess_input(x) \n\nresnet_v2_preprocess_input(x) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| include_top | Whether to include the fully-connected layer at the top of the network. Defaults to `TRUE`. |\n| weights | One of `NULL` (random initialization), `'imagenet'` (pre-training on ImageNet), or the path to the weights file to be loaded. Defaults to `'imagenet'`. |\n| input_tensor | Optional Keras tensor (i.e. output of `layer_input()`) to use as image input for the model. |\n| input_shape | optional shape list, only to be specified if `include_top` is FALSE (otherwise the input shape has to be `c(224, 224, 3)` (with `'channels_last'` data format) or `c(3, 224, 224)` (with `'channels_first'` data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. `c(200, 200, 3)` would be one valid value. |\n| pooling | Optional pooling mode for feature extraction when `include_top` is `FALSE`. Defaults to `NULL`. <br>- `NULL` means that the output of the model will be the 4D tensor output of the last convolutional layer. <br>- `'avg'` means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor. <br>- `'max'` means that global max pooling will be applied.  |\n| classes | Optional number of classes to classify images into, only to be specified if `include_top` is TRUE, and if no `weights` argument is specified. Defaults to 1000 (number of ImageNet classes). |\n| ... | For backwards and forwards compatibility |\n| classifier_activation | A string or callable. The activation function to use on the \""top\"" layer. Ignored unless `include_top = TRUE`. Set `classifier_activation = NULL` to return the logits of the \""top\"" layer. Defaults to `'softmax'`. When loading pretrained weights, `classifier_activation` can only be `NULL` or `\""softmax\""`. |\n| x | `preprocess_input()` takes an array or floating point tensor, 3D or 4D with 3 color channels, with values in the range `[0, 255]`. |\n\n## Details\n\nReference: \n\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (CVPR 2015) \n\nFor image classification use cases, see [this page for detailed examples](https://keras.io/api/applications/#usage-examples-for-image-classification-models). \n\nFor transfer learning use cases, make sure to read the [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/). \n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call `tf.keras.applications.resnet.preprocess_input` on your inputs before passing them to the model. `resnet.preprocess_input` will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \n# instantiate the model \nmodel <- application_resnet50(weights = 'imagenet') \n# load the image \nimg_path <- \""elephant.jpg\"" \nimg <- image_load(img_path, target_size = c(224,224)) \nx <- image_to_array(img) \n# ensure we have a 4d tensor with single element in the batch dimension, \n# the preprocess the input for prediction using resnet50 \nx <- array_reshape(x, c(1, dim(x))) \nx <- imagenet_preprocess_input(x) \n# make predictions then decode and print them \npreds <- model %>% predict(x) \nimagenet_decode_predictions(preds, top = 3)[[1]] \n```\n:::\n\n\n## See Also\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet101](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet101)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet152](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet152)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet50V2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet50V2)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet101V2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet101V2)\n\n- [https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet152V2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/ResNet152V2)\n\n- [https://keras.io/api/applications/](https://keras.io/api/applications/)\n\n"",
+    ""supporting"": [
+      ""application_resnet_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/application_vgg/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""10021790bf2dce2adfb1f44ae40d21b9"",
+  ""hash"": ""ec3f0ac3b26e64b7d522df8703981fba"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/applications.R*\n\n# application_vgg\n\n## VGG16 and VGG19 models for Keras.\n\n## Description\nVGG16 and VGG19 models for Keras. \n\n\n## Usage\n```r\napplication_vgg16( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"" \n) \napplication_vgg19( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| include_top | whether to include the 3 fully-connected layers at the top of the network. |\n| weights | One of `NULL` (random initialization), `'imagenet'` (pre-training on ImageNet), or the path to the weights file to be loaded. Defaults to `'imagenet'`. |\n| input_tensor | Optional Keras tensor (i.e. output of `layer_input()`) to use as image input for the model. |\n| input_shape | optional shape list, only to be specified if `include_top`<br>is FALSE (otherwise the input shape has to be `(224, 224, 3)` It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. `(200, 200, 3)` would be one valid value. |\n| pooling | Optional pooling mode for feature extraction when `include_top` is `FALSE`. Defaults to `NULL`. <br>- `NULL` means that the output of the model will be the 4D tensor output of the last convolutional layer. <br>- `'avg'` means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor. <br>- `'max'` means that global max pooling will be applied.  |\n| classes | Optional number of classes to classify images into, only to be specified if `include_top` is TRUE, and if no `weights` argument is specified. Defaults to 1000 (number of ImageNet classes). |\n| classifier_activation | A string or callable. The activation function to use on the \""top\"" layer. Ignored unless `include_top = TRUE`. Set `classifier_activation = NULL` to return the logits of the \""top\"" layer. Defaults to `'softmax'`. When loading pretrained weights, `classifier_activation` can only be `NULL` or `\""softmax\""`. |\n\n## Details\n\nOptionally loads weights pre-trained on ImageNet. \n\nThe `imagenet_preprocess_input()` function should be used for image preprocessing. \n\n## Section\n\n## Reference\n\n - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n## Value\nKeras model instance. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nmodel <- application_vgg16(weights = 'imagenet', include_top = FALSE) \nimg_path <- \""elephant.jpg\"" \nimg <- image_load(img_path, target_size = c(224,224)) \nx <- image_to_array(img) \nx <- array_reshape(x, c(1, dim(x))) \nx <- imagenet_preprocess_input(x) \nfeatures <- model %>% predict(x) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L119) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# application_vgg\n\n## VGG16 and VGG19 models for Keras.\n\n## Description\nVGG16 and VGG19 models for Keras. \n\n\n## Usage\n```r\napplication_vgg16( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"" \n) \n\napplication_vgg19( \n  include_top = TRUE, \n  weights = \""imagenet\"", \n  input_tensor = NULL, \n  input_shape = NULL, \n  pooling = NULL, \n  classes = 1000, \n  classifier_activation = \""softmax\"" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| include_top | whether to include the 3 fully-connected layers at the top of the network. |\n| weights | One of `NULL` (random initialization), `'imagenet'` (pre-training on ImageNet), or the path to the weights file to be loaded. Defaults to `'imagenet'`. |\n| input_tensor | Optional Keras tensor (i.e. output of `layer_input()`) to use as image input for the model. |\n| input_shape | optional shape list, only to be specified if `include_top`<br>is FALSE (otherwise the input shape has to be `(224, 224, 3)` It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. `(200, 200, 3)` would be one valid value. |\n| pooling | Optional pooling mode for feature extraction when `include_top` is `FALSE`. Defaults to `NULL`. <br>- `NULL` means that the output of the model will be the 4D tensor output of the last convolutional layer. <br>- `'avg'` means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor. <br>- `'max'` means that global max pooling will be applied.  |\n| classes | Optional number of classes to classify images into, only to be specified if `include_top` is TRUE, and if no `weights` argument is specified. Defaults to 1000 (number of ImageNet classes). |\n| classifier_activation | A string or callable. The activation function to use on the \""top\"" layer. Ignored unless `include_top = TRUE`. Set `classifier_activation = NULL` to return the logits of the \""top\"" layer. Defaults to `'softmax'`. When loading pretrained weights, `classifier_activation` can only be `NULL` or `\""softmax\""`. |\n\n## Details\n\nOptionally loads weights pre-trained on ImageNet. \n\nThe `imagenet_preprocess_input()` function should be used for image preprocessing. \n\n## Section\n\n## Reference\n\n - [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n## Value\nKeras model instance. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nmodel <- application_vgg16(weights = 'imagenet', include_top = FALSE) \nimg_path <- \""elephant.jpg\"" \nimg <- image_load(img_path, target_size = c(224,224)) \nx <- image_to_array(img) \nx <- array_reshape(x, c(1, dim(x))) \nx <- imagenet_preprocess_input(x) \nfeatures <- model %>% predict(x) \n```\n:::\n"",
+    ""supporting"": [
+      ""application_vgg_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/freeze_weights/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""6f149be162c80e7f5bd2a3b21bc3844d"",
+  ""hash"": ""de37fc1802b350ad7b57c491e2444d88"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/freeze.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/freeze.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/freeze.R*\n\n# freeze_weights\n\n## Freeze and unfreeze weights\n\n## Description\nFreeze weights in a model or layer so that they are no longer trainable. \n\n\n## Usage\n```r\nfreeze_weights(object, from = NULL, to = NULL, which = NULL) \nunfreeze_weights(object, from = NULL, to = NULL, which = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Keras model or layer object |\n| from | Layer instance, layer name, or layer index within model |\n| to | Layer instance, layer name, or layer index within model |\n| which | layer names, integer positions, layers, logical vector (of `length(object$layers)`), or a function returning a logical vector. |\n\n\n\n\n## Note\n\nThe `from` and `to` layer arguments are both inclusive. \n\nWhen applied to a model, the freeze or unfreeze is a global operation over all layers in the model (i.e. layers not within the specified range will be set to the opposite value, e.g. unfrozen for a call to freeze). \n\nModels must be compiled again after weights are frozen or unfrozen. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nconv_base <- application_vgg16( \n  weights = \""imagenet\"", \n  include_top = FALSE, \n  input_shape = c(150, 150, 3) \n) \n# freeze it's weights \nfreeze_weights(conv_base) \nconv_base \n# create a composite model that includes the base + more layers \nmodel <- keras_model_sequential() %>% \n  conv_base() %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \""relu\"") %>% \n  layer_dense(units = 1, activation = \""sigmoid\"") \n# compile \nmodel %>% compile( \n  loss = \""binary_crossentropy\"", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\""accuracy\"") \n) \nmodel \nprint(model, expand_nested = TRUE) \n# unfreeze weights from \""block5_conv1\"" on \nunfreeze_weights(conv_base, from = \""block5_conv1\"") \n# compile again since we froze or unfroze weights \nmodel %>% compile( \n  loss = \""binary_crossentropy\"", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\""accuracy\"") \n) \nconv_base \nprint(model, expand_nested = TRUE) \n# freeze only the last 5 layers \nfreeze_weights(conv_base, from = -5) \nconv_base \n# equivalently, also freeze only the last 5 layers \nunfreeze_weights(conv_base, to = -6) \nconv_base \n# Freeze only layers of a certain type, e.g, BatchNorm layers \nbatch_norm_layer_class_name <- class(layer_batch_normalization())[1] \nis_batch_norm_layer <- function(x) inherits(x, batch_norm_layer_class_name) \nmodel <- application_efficientnet_b0() \nfreeze_weights(model, which = is_batch_norm_layer) \nmodel \n# equivalent to: \nfor(layer in model$layers) { \n  if(is_batch_norm_layer(layer)) \n    layer$trainable <- FALSE \n  else \n    layer$trainable <- TRUE \n} \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/freeze.R#L89) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# freeze_weights\n\n## Freeze and unfreeze weights\n\n## Description\nFreeze weights in a model or layer so that they are no longer trainable. \n\n\n## Usage\n```r\nfreeze_weights(object, from = NULL, to = NULL, which = NULL) \n\nunfreeze_weights(object, from = NULL, to = NULL, which = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Keras model or layer object |\n| from | Layer instance, layer name, or layer index within model |\n| to | Layer instance, layer name, or layer index within model |\n| which | layer names, integer positions, layers, logical vector (of `length(object$layers)`), or a function returning a logical vector. |\n\n\n\n\n## Note\n\nThe `from` and `to` layer arguments are both inclusive. \n\nWhen applied to a model, the freeze or unfreeze is a global operation over all layers in the model (i.e. layers not within the specified range will be set to the opposite value, e.g. unfrozen for a call to freeze). \n\nModels must be compiled again after weights are frozen or unfrozen. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nconv_base <- application_vgg16( \n  weights = \""imagenet\"", \n  include_top = FALSE, \n  input_shape = c(150, 150, 3) \n) \n# freeze it's weights \nfreeze_weights(conv_base) \nconv_base \n# create a composite model that includes the base + more layers \nmodel <- keras_model_sequential() %>% \n  conv_base() %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \""relu\"") %>% \n  layer_dense(units = 1, activation = \""sigmoid\"") \n# compile \nmodel %>% compile( \n  loss = \""binary_crossentropy\"", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\""accuracy\"") \n) \nmodel \nprint(model, expand_nested = TRUE) \n# unfreeze weights from \""block5_conv1\"" on \nunfreeze_weights(conv_base, from = \""block5_conv1\"") \n# compile again since we froze or unfroze weights \nmodel %>% compile( \n  loss = \""binary_crossentropy\"", \n  optimizer = optimizer_rmsprop(lr = 2e-5), \n  metrics = c(\""accuracy\"") \n) \nconv_base \nprint(model, expand_nested = TRUE) \n# freeze only the last 5 layers \nfreeze_weights(conv_base, from = -5) \nconv_base \n# equivalently, also freeze only the last 5 layers \nunfreeze_weights(conv_base, to = -6) \nconv_base \n# Freeze only layers of a certain type, e.g, BatchNorm layers \nbatch_norm_layer_class_name <- class(layer_batch_normalization())[1] \nis_batch_norm_layer <- function(x) inherits(x, batch_norm_layer_class_name) \nmodel <- application_efficientnet_b0() \nfreeze_weights(model, which = is_batch_norm_layer) \nmodel \n# equivalent to: \nfor(layer in model$layers) { \n  if(is_batch_norm_layer(layer)) \n    layer$trainable <- FALSE \n  else \n    layer$trainable <- TRUE \n} \n```\n:::\n"",
+    ""supporting"": [
+      ""freeze_weights_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/grapes-py_class-grapes/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""cb7e05386085b10f82ef83ff1026cf77"",
+  ""hash"": ""648e7dcdb5bc12158250bc21909cc738"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/py-classes.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/py-classes.R*\n\n# %py_class%\n\n## Make a python class constructor\n\n## Description\nMake a python class constructor \n\n\n## Usage\n```r\nspec %py_class% body \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | a bare symbol `MyClassName`, or a call `MyClassName(SuperClass)` |\n| body | an expression that can be evaluated to construct the class methods. |\n\n\n\n## Value\nThe python class constructor, invisibly. Note, the same constructor is also assigned in the parent frame. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nMyClass %py_class% { \n  initialize <- function(x) { \n    print(\""Hi from MyClass$initialize()!\"") \n    self$x <- x \n  } \n  my_method <- function() { \n    self$x \n  } \n} \nmy_class_instance <- MyClass(42) \nmy_class_instance$my_method() \nMyClass2(MyClass) %py_class% { \n  \""This will be a __doc__ string for MyClass2\"" \n  initialize <- function(...) { \n    \""This will be the __doc__ string for the MyClass2.__init__() method\"" \n    print(\""Hi from MyClass2$initialize()!\"") \n    super$initialize(...) \n  } \n} \nmy_class_instance2 <- MyClass2(42) \nmy_class_instance2$my_method() \nreticulate::py_help(MyClass2) # see the __doc__ strings and more! \n# In addition to `self`, there is also `private` available. \n# This is an R environment unique to each class instance, where you can \n# store objects that you don't want converted to Python, but still want \n# available from methods. You can also assign methods to private, and \n# `self` and `private` will be available in private methods. \nMyClass %py_class% { \n  initialize <- function(x) { \n    print(\""Hi from MyClass$initialize()!\"") \n    private$y <- paste(\""A Private field:\"", x) \n  } \n  get_private_field <- function() { \n    private$y \n  } \n  private$a_private_method <- function() { \n    cat(\""a_private_method() was called.\\n\"") \n    cat(\""private$y is \"", sQuote(private$y), \""\\n\"") \n  } \n  call_private_method <- function() \n    private$a_private_method() \n} \ninst1 <- MyClass(1) \ninst2 <- MyClass(2) \ninst1$get_private_field() \ninst2$get_private_field() \ninst1$call_private_method() \ninst2$call_private_method() \n```\n:::\n\n\n## See Also\n[https://keras.rstudio.com/articles/new-guides/python_subclasses.html](https://keras.rstudio.com/articles/new-guides/python_subclasses.html)\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R#L) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# %py_class%\n\n## Make a python class constructor\n\n## Description\nMake a python class constructor \n\n\n## Usage\n```r\nspec %py_class% body \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | a bare symbol `MyClassName`, or a call `MyClassName(SuperClass)` |\n| body | an expression that can be evaluated to construct the class methods. |\n\n\n\n## Value\nThe python class constructor, invisibly. Note, the same constructor is also assigned in the parent frame. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nMyClass %py_class% { \n  initialize <- function(x) { \n    print(\""Hi from MyClass$initialize()!\"") \n    self$x <- x \n  } \n  my_method <- function() { \n    self$x \n  } \n} \nmy_class_instance <- MyClass(42) \nmy_class_instance$my_method() \nMyClass2(MyClass) %py_class% { \n  \""This will be a __doc__ string for MyClass2\"" \n  initialize <- function(...) { \n    \""This will be the __doc__ string for the MyClass2.__init__() method\"" \n    print(\""Hi from MyClass2$initialize()!\"") \n    super$initialize(...) \n  } \n} \nmy_class_instance2 <- MyClass2(42) \nmy_class_instance2$my_method() \nreticulate::py_help(MyClass2) # see the __doc__ strings and more! \n# In addition to `self`, there is also `private` available. \n# This is an R environment unique to each class instance, where you can \n# store objects that you don't want converted to Python, but still want \n# available from methods. You can also assign methods to private, and \n# `self` and `private` will be available in private methods. \nMyClass %py_class% { \n  initialize <- function(x) { \n    print(\""Hi from MyClass$initialize()!\"") \n    private$y <- paste(\""A Private field:\"", x) \n  } \n  get_private_field <- function() { \n    private$y \n  } \n  private$a_private_method <- function() { \n    cat(\""a_private_method() was called.\\n\"") \n    cat(\""private$y is \"", sQuote(private$y), \""\\n\"") \n  } \n  call_private_method <- function() \n    private$a_private_method() \n} \ninst1 <- MyClass(1) \ninst2 <- MyClass(2) \ninst1$get_private_field() \ninst2$get_private_field() \ninst1$call_private_method() \ninst2$call_private_method() \n```\n:::\n\n\n## See Also\n[https://keras.rstudio.com/articles/new-guides/python_subclasses.html](https://keras.rstudio.com/articles/new-guides/python_subclasses.html)\n\n"",
+    ""supporting"": [
+      ""grapes-py_class-grapes_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/grapes-set-active-grapes/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""3b29568c66ef11950f8835aebc829eeb"",
+  ""hash"": ""fc39511458f189b654c2231e99eb0e58"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/py-classes.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/py-classes.R*\n\n# %<-active%\n\n## Make an Active Binding\n\n## Description\nMake an Active Binding \n\n\n## Usage\n```r\nsym %<-active% value \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| sym | symbol to bind |\n| value | A function to call when the value of `sym` is accessed. |\n\n## Details\nActive bindings defined in a `%py_class%` are converted to `@property` decorated methods. \n\n\n## Value\n`value`, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234) \nx %<-active% function(value) { \n  message(\""Evaluating function of active binding\"") \n  if(missing(value)) \n    runif(1) \n  else \n   message(\""Received: \"", value) \n} \nx \nx \nx <- \""foo\"" \nx <- \""foo\"" \nx \nrm(x) # cleanup \n```\n:::\n\n\n## See Also\n`makeActiveBinding()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R#L) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# %<-active%\n\n## Make an Active Binding\n\n## Description\nMake an Active Binding \n\n\n## Usage\n```r\nsym %<-active% value \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| sym | symbol to bind |\n| value | A function to call when the value of `sym` is accessed. |\n\n## Details\nActive bindings defined in a `%py_class%` are converted to `@property` decorated methods. \n\n\n## Value\n`value`, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nset.seed(1234) \nx %<-active% function(value) { \n  message(\""Evaluating function of active binding\"") \n  if(missing(value)) \n    runif(1) \n  else \n   message(\""Received: \"", value) \n} \nx \nx \nx <- \""foo\"" \nx <- \""foo\"" \nx \nrm(x) # cleanup \n```\n:::\n\n\n## See Also\n`makeActiveBinding()`\n\n"",
+    ""supporting"": [
+      ""grapes-set-active-grapes_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/is_keras_available/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""4ef6345d4e539e2bca4ac3e6c85ed897"",
+  ""hash"": ""3cccd41236823cd105edca4319ba37e1"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/utils.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/utils.R*\n\n# is_keras_available\n\n## Check if Keras is Available\n\n## Description\nProbe to see whether the Keras Python package is available in the current system environment. \n\n\n## Usage\n```r\nis_keras_available(version = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| version | Minimum required version of Keras (defaults to `NULL`, no required version). |\n\n\n\n## Value\nLogical indicating whether Keras (or the specified minimum version of Keras) is available. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# testthat utilty for skipping tests when Keras isn't available \nskip_if_no_keras <- function(version = NULL) { \n  if (!is_keras_available(version)) \n    skip(\""Required keras version not available for testing\"") \n} \n# use the function within a test \ntest_that(\""keras function works correctly\"", { \n  skip_if_no_keras() \n  # test code here \n}) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R#L343) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# is_keras_available\n\n## Check if Keras is Available\n\n## Description\nProbe to see whether the Keras Python package is available in the current system environment. \n\n\n## Usage\n```r\nis_keras_available(version = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| version | Minimum required version of Keras (defaults to `NULL`, no required version). |\n\n\n\n## Value\nLogical indicating whether Keras (or the specified minimum version of Keras) is available. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n# testthat utilty for skipping tests when Keras isn't available \nskip_if_no_keras <- function(version = NULL) { \n  if (!is_keras_available(version)) \n    skip(\""Required keras version not available for testing\"") \n} \n# use the function within a test \ntest_that(\""keras function works correctly\"", { \n  skip_if_no_keras() \n  # test code here \n}) \n```\n:::\n"",
+    ""supporting"": [
+      ""is_keras_available_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/keras_model/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""06a3ad2e5f9f4e4b545877fab7232940"",
+  ""hash"": ""da499c84bc209946346de55944a827c6"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/model.R*\n\n# keras_model\n\n## Keras Model\n\n## Description\nA model is a directed acyclic graph of layers. \n\n\n## Usage\n```r\nkeras_model(inputs, outputs = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| inputs | Input layer |\n| outputs | Output layer |\n| ... | Any additional arguments |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \n# input layer \ninputs <- layer_input(shape = c(784)) \n# outputs compose input + dense layers \npredictions <- inputs %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 10, activation = 'softmax') \n# create and compile model \nmodel <- keras_model(inputs = inputs, outputs = predictions) \nmodel %>% compile( \n  optimizer = 'rmsprop', \n  loss = 'categorical_crossentropy', \n  metrics = c('accuracy') \n) \n```\n:::\n\n\n## See Also\nOther model functions:  `compile.keras.engine.training.Model()`, `evaluate.keras.engine.training.Model()`, `evaluate_generator()`, `fit.keras.engine.training.Model()`, `fit_generator()`, `get_config()`, `get_layer()`, `keras_model_sequential()`, `multi_gpu_model()`, `pop_layer()`, `predict.keras.engine.training.Model()`, `predict_generator()`, `predict_on_batch()`, `predict_proba()`, `summary.keras.engine.training.Model()`, `train_on_batch()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L33) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# keras_model\n\n## Keras Model\n\n## Description\nA model is a directed acyclic graph of layers. \n\n\n## Usage\n```r\nkeras_model(inputs, outputs = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| inputs | Input layer |\n| outputs | Output layer |\n| ... | Any additional arguments |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \n# input layer \ninputs <- layer_input(shape = c(784)) \n# outputs compose input + dense layers \npredictions <- inputs %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 10, activation = 'softmax') \n# create and compile model \nmodel <- keras_model(inputs = inputs, outputs = predictions) \nmodel %>% compile( \n  optimizer = 'rmsprop', \n  loss = 'categorical_crossentropy', \n  metrics = c('accuracy') \n) \n```\n:::\n\n\n## See Also\nOther model functions:  `compile.keras.engine.training.Model()`, `evaluate.keras.engine.training.Model()`, `evaluate_generator()`, `fit.keras.engine.training.Model()`, `fit_generator()`, `get_config()`, `get_layer()`, `keras_model_sequential()`, `multi_gpu_model()`, `pop_layer()`, `predict.keras.engine.training.Model()`, `predict_generator()`, `predict_on_batch()`, `predict_proba()`, `summary.keras.engine.training.Model()`, `train_on_batch()`\n\n"",
+    ""supporting"": [
+      ""keras_model_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/keras_model_sequential/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""ac77110faf438e05f8ec5dbaf89d993b"",
+  ""hash"": ""a8135cb693974105173f50a4af56fa14"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/model.R*\n\n# keras_model_sequential\n\n## Keras Model composed of a linear stack of layers\n\n## Description\nKeras Model composed of a linear stack of layers \n\n\n## Usage\n```r\nkeras_model_sequential(layers = NULL, name = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| layers | List of layers to add to the model |\n| name | Name of model |\n| ... |   Arguments passed on to `sequential_model_input_layer`<br>  <br>    <br>`input_shape`<br>an integer vector of dimensions (not including the batch <br>    <br>`batch_size`<br>Optional input batch size (integer or NULL).<br>    <br>`dtype`<br>Optional datatype of the input. When not provided, the Keras <br>    <br>`input_tensor`<br>Optional tensor to use as layer input. If set, the layer <br>    <br>`sparse`<br>Boolean, whether the placeholder created is meant to be sparse. <br>    <br>`ragged`<br>Boolean, whether the placeholder created is meant to be ragged. <br>    <br>`type_spec`<br>A <br>    <br>`input_layer_name,name`<br>Optional name of the input layer (string).<br>   |\n\n\n\n\n## Note\n\nIf any arguments are provided to `...`, then the sequential model is initialized with a `InputLayer` instance. If not, then the first layer passed to a Sequential model should have a defined input shape. What that means is that it should have received an `input_shape` or `batch_input_shape`\n\nargument, or for some type of layers (recurrent, Dense...) an `input_dim`\n\nargument. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 32, input_shape = c(784)) %>% \n  layer_activation('relu') %>% \n  layer_dense(units = 10) %>% \n  layer_activation('softmax') \nmodel %>% compile( \n  optimizer = 'rmsprop', \n  loss = 'categorical_crossentropy', \n  metrics = c('accuracy') \n) \n# alternative way to provide input shape \nmodel <- keras_model_sequential(input_shape = c(784)) %>% \n  layer_dense(units = 32) %>% \n  layer_activation('relu') %>% \n  layer_dense(units = 10) %>% \n  layer_activation('softmax') \n```\n:::\n\n\n## See Also\nOther model functions:  `compile.keras.engine.training.Model()`, `evaluate.keras.engine.training.Model()`, `evaluate_generator()`, `fit.keras.engine.training.Model()`, `fit_generator()`, `get_config()`, `get_layer()`, `keras_model()`, `multi_gpu_model()`, `pop_layer()`, `predict.keras.engine.training.Model()`, `predict_generator()`, `predict_on_batch()`, `predict_proba()`, `summary.keras.engine.training.Model()`, `train_on_batch()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L85) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# keras_model_sequential\n\n## Keras Model composed of a linear stack of layers\n\n## Description\nKeras Model composed of a linear stack of layers \n\n\n## Usage\n```r\nkeras_model_sequential(layers = NULL, name = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| layers | List of layers to add to the model |\n| name | Name of model |\n| ... |   Arguments passed on to `sequential_model_input_layer`<br>  <br>    <br>`input_shape`<br>an integer vector of dimensions (not including the batch <br>    <br>`batch_size`<br>Optional input batch size (integer or NULL).<br>    <br>`dtype`<br>Optional datatype of the input. When not provided, the Keras <br>    <br>`input_tensor`<br>Optional tensor to use as layer input. If set, the layer <br>    <br>`sparse`<br>Boolean, whether the placeholder created is meant to be sparse. <br>    <br>`ragged`<br>Boolean, whether the placeholder created is meant to be ragged. <br>    <br>`type_spec`<br>A <br>    <br>`input_layer_name,name`<br>Optional name of the input layer (string).<br>   |\n\n\n\n\n## Note\n\nIf any arguments are provided to `...`, then the sequential model is initialized with a `InputLayer` instance. If not, then the first layer passed to a Sequential model should have a defined input shape. What that means is that it should have received an `input_shape` or `batch_input_shape`\n\nargument, or for some type of layers (recurrent, Dense...) an `input_dim`\n\nargument. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 32, input_shape = c(784)) %>% \n  layer_activation('relu') %>% \n  layer_dense(units = 10) %>% \n  layer_activation('softmax') \nmodel %>% compile( \n  optimizer = 'rmsprop', \n  loss = 'categorical_crossentropy', \n  metrics = c('accuracy') \n) \n# alternative way to provide input shape \nmodel <- keras_model_sequential(input_shape = c(784)) %>% \n  layer_dense(units = 32) %>% \n  layer_activation('relu') %>% \n  layer_dense(units = 10) %>% \n  layer_activation('softmax') \n```\n:::\n\n\n## See Also\nOther model functions:  `compile.keras.engine.training.Model()`, `evaluate.keras.engine.training.Model()`, `evaluate_generator()`, `fit.keras.engine.training.Model()`, `fit_generator()`, `get_config()`, `get_layer()`, `keras_model()`, `multi_gpu_model()`, `pop_layer()`, `predict.keras.engine.training.Model()`, `predict_generator()`, `predict_on_batch()`, `predict_proba()`, `summary.keras.engine.training.Model()`, `train_on_batch()`\n\n"",
+    ""supporting"": [
+      ""keras_model_sequential_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/kerascallback/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""8afbb4458930f860b79c8e9fa5dedd48"",
+  ""hash"": ""2aa5e6a4ea7cfccbc634ad0b6c41e5fd"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/callbacks.R*\n\n# KerasCallback\n\n## (Deprecated) Base R6 class for Keras callbacks\n\n## Description\nNew custom callbacks implemented as R6 classes are encouraged to inherit from `keras$callbacks$Callback` directly. \n\n## Format\nAn R6Class generator object \n\n\n\n## Details\n\nThe `logs` named list that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. \n\nCurrently, the `fit.keras.engine.training.Model()` method for sequential models will include the following quantities in the `logs` that it passes to its callbacks: \n\n- `on_epoch_end`: logs include `acc` and `loss`, and optionally include `val_loss` (if validation is enabled in `fit`), and `val_acc` (if validation and accuracy monitoring are enabled). \n\n- `on_batch_begin`: logs include `size`, the number of samples in the current batch. \n\n- `on_batch_end`: logs include `loss`, and optionally `acc` (if accuracy monitoring is enabled). \n\n## Section\n\n## Fields\n\n`params`\n\nNamed list with training parameters (eg. verbosity, batch size, number of epochs...).\n\n`model`\n\nReference to the Keras model being trained.\n\n## Methods\n\n`on_epoch_begin(epoch, logs)`\n\nCalled at the beginning of each epoch.\n\n`on_epoch_end(epoch, logs)`\n\nCalled at the end of each epoch.\n\n`on_batch_begin(batch, logs)`\n\nCalled at the beginning of each batch.\n\n`on_batch_end(batch, logs)`\n\nCalled at the end of each batch.\n\n`on_train_begin(logs)`\n\nCalled at the beginning of training.\n\n`on_train_end(logs)`\n\nCalled at the end of training.\n\n## Value\nKerasCallback. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nLossHistory <- R6::R6Class(\""LossHistory\"", \n  inherit = KerasCallback, \n  public = list( \n    losses = NULL, \n    on_batch_end = function(batch, logs = list()) { \n      self$losses <- c(self$losses, logs[[\""loss\""]]) \n    } \n  ) \n) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L605) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# KerasCallback\n\n## (Deprecated) Base R6 class for Keras callbacks\n\n## Description\nNew custom callbacks implemented as R6 classes are encouraged to inherit from `keras$callbacks$Callback` directly. \n\n## Format\nAn R6Class generator object \n\n\n\n## Details\n\nThe `logs` named list that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. \n\nCurrently, the `fit.keras.engine.training.Model()` method for sequential models will include the following quantities in the `logs` that it passes to its callbacks: \n\n- `on_epoch_end`: logs include `acc` and `loss`, and optionally include `val_loss` (if validation is enabled in `fit`), and `val_acc` (if validation and accuracy monitoring are enabled). \n\n- `on_batch_begin`: logs include `size`, the number of samples in the current batch. \n\n- `on_batch_end`: logs include `loss`, and optionally `acc` (if accuracy monitoring is enabled). \n\n## Section\n\n## Fields\n\n`params`\n\nNamed list with training parameters (eg. verbosity, batch size, number of epochs...).\n\n`model`\n\nReference to the Keras model being trained.\n\n## Methods\n\n`on_epoch_begin(epoch, logs)`\n\nCalled at the beginning of each epoch.\n\n`on_epoch_end(epoch, logs)`\n\nCalled at the end of each epoch.\n\n`on_batch_begin(batch, logs)`\n\nCalled at the beginning of each batch.\n\n`on_batch_end(batch, logs)`\n\nCalled at the end of each batch.\n\n`on_train_begin(logs)`\n\nCalled at the beginning of training.\n\n`on_train_end(logs)`\n\nCalled at the end of training.\n\n## Value\nKerasCallback. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nLossHistory <- R6::R6Class(\""LossHistory\"", \n  inherit = KerasCallback, \n  public = list( \n    losses = NULL, \n    on_batch_end = function(batch, logs = list()) { \n      self$losses <- c(self$losses, logs[[\""loss\""]]) \n    } \n  ) \n) \n```\n:::\n"",
+    ""supporting"": [
+      ""kerascallback_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/kerasconstraint/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""a9cec199a45cf54ca511481af8a409dc"",
+  ""hash"": ""1b86da95d3fcba16d9250a36be506b1b"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/constraints.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/constraints.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/constraints.R*\n\n# KerasConstraint\n\n## (Deprecated) Base R6 class for Keras constraints\n\n## Description\nNew custom constraints are encouraged to subclass `keras$constraints$Constraint` directly. \n\n## Format\nAn R6Class generator object \n\n\n\n## Details\nYou can implement a custom constraint either by creating an R function that accepts a weights (`w`) parameter, or by creating an R6 class that derives from `KerasConstraint` and implements a `call` method. \n\n## Section\n\n## Methods\n\n`call(w)`\n\nConstrain the specified weights.\n\n\n## Note\nModels which use custom constraints cannot be serialized using `save_model_hdf5()`. Rather, the weights of the model should be saved and restored using `save_model_weights_hdf5()`. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomNonNegConstraint <- R6::R6Class( \n  \""CustomNonNegConstraint\"", \n  inherit = KerasConstraint, \n  public = list( \n    call = function(x) { \n       w * k_cast(k_greater_equal(w, 0), k_floatx()) \n    } \n  ) \n) \nlayer_dense(units = 32, input_shape = c(784), \n            kernel_constraint = CustomNonNegConstraint$new()) \n```\n:::\n\n\n## See Also\nconstraints\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/constraints.R#L127) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# KerasConstraint\n\n## (Deprecated) Base R6 class for Keras constraints\n\n## Description\nNew custom constraints are encouraged to subclass `keras$constraints$Constraint` directly. \n\n## Format\nAn R6Class generator object \n\n\n\n## Details\nYou can implement a custom constraint either by creating an R function that accepts a weights (`w`) parameter, or by creating an R6 class that derives from `KerasConstraint` and implements a `call` method. \n\n## Section\n\n## Methods\n\n`call(w)`\n\nConstrain the specified weights.\n\n\n## Note\nModels which use custom constraints cannot be serialized using `save_model_hdf5()`. Rather, the weights of the model should be saved and restored using `save_model_weights_hdf5()`. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nCustomNonNegConstraint <- R6::R6Class( \n  \""CustomNonNegConstraint\"", \n  inherit = KerasConstraint, \n  public = list( \n    call = function(x) { \n       w * k_cast(k_greater_equal(w, 0), k_floatx()) \n    } \n  ) \n) \nlayer_dense(units = 32, input_shape = c(784), \n            kernel_constraint = CustomNonNegConstraint$new()) \n```\n:::\n\n\n## See Also\nconstraints\n\n"",
+    ""supporting"": [
+      ""kerasconstraint_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/layer/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""636f6580bfb6eca92057b3500eeafbb5"",
+  ""hash"": ""1a26b56706ce85e87f586231ebbb9ae9"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/Layer.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/Layer.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/Layer.R*\n\n# Layer\n\n## (Deprecated) Create a custom Layer\n\n## Description\nThis function is maintained but deprecated. Please use `new_layer_class()` or `%py_class%` to define custom layers. \n\n\n## Usage\n```r\nLayer( \n  classname, \n  initialize, \n  build = NULL, \n  call = NULL, \n  compute_output_shape = NULL, \n  ..., \n  inherit = keras::keras$layers$Layer \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| classname | the name of the custom Layer. |\n| initialize | a function. This is where you define the arguments used to further build your layer. For example, a dense layer would take the `units` argument. You should always call `super()$`__init__()`` to initialize the base inherited layer. |\n| build | a function that takes `input_shape` as argument. This is where you will define your weights. Note that if your layer doesn't define trainable weights then you need not implement this method. |\n| call | This is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to `call`<br>(the input tensor). |\n| compute_output_shape | a function that takes `input_shape` as an argument. In case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference. If you don't modify the shape of the input then you need not implement this method. |\n| ... | Any other methods and/or attributes can be specified using named arguments. They will be added to the layer class. |\n| inherit | the Keras layer to inherit from. |\n\n\n\n## Value\nA function that wraps `create_layer`, similar to `keras::layer_dense`. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_dense2 <- Layer( \n  \""Dense2\"", \n  initialize = function(units) { \n    super()$`__init__`() \n    self$units <- as.integer(units) \n  }, \n  build = function(input_shape) { \n    print(class(input_shape)) \n    self$kernel <- self$add_weight( \n      name = \""kernel\"", \n      shape = list(input_shape[[2]], self$units), \n      initializer = \""uniform\"", \n      trainable = TRUE \n    ) \n  }, \n  call = function(x) { \n    tensorflow::tf$matmul(x, self$kernel) \n  }, \n  compute_output_shape = function(input_shape) { \n    list(input_shape[[1]], self$units) \n  } \n) \nl <- layer_dense2(units = 10) \nl(matrix(runif(10), ncol = 1)) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/Layer.R#L64) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# Layer\n\n## (Deprecated) Create a custom Layer\n\n## Description\nThis function is maintained but deprecated. Please use `new_layer_class()` or `%py_class%` to define custom layers. \n\n\n## Usage\n```r\nLayer( \n  classname, \n  initialize, \n  build = NULL, \n  call = NULL, \n  compute_output_shape = NULL, \n  ..., \n  inherit = keras::keras$layers$Layer \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| classname | the name of the custom Layer. |\n| initialize | a function. This is where you define the arguments used to further build your layer. For example, a dense layer would take the `units` argument. You should always call `super()$`__init__()`` to initialize the base inherited layer. |\n| build | a function that takes `input_shape` as argument. This is where you will define your weights. Note that if your layer doesn't define trainable weights then you need not implement this method. |\n| call | This is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to `call`<br>(the input tensor). |\n| compute_output_shape | a function that takes `input_shape` as an argument. In case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference. If you don't modify the shape of the input then you need not implement this method. |\n| ... | Any other methods and/or attributes can be specified using named arguments. They will be added to the layer class. |\n| inherit | the Keras layer to inherit from. |\n\n\n\n## Value\nA function that wraps `create_layer`, similar to `keras::layer_dense`. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlayer_dense2 <- Layer( \n  \""Dense2\"", \n  initialize = function(units) { \n    super()$`__init__`() \n    self$units <- as.integer(units) \n  }, \n  build = function(input_shape) { \n    print(class(input_shape)) \n    self$kernel <- self$add_weight( \n      name = \""kernel\"", \n      shape = list(input_shape[[2]], self$units), \n      initializer = \""uniform\"", \n      trainable = TRUE \n    ) \n  }, \n  call = function(x) { \n    tensorflow::tf$matmul(x, self$kernel) \n  }, \n  compute_output_shape = function(input_shape) { \n    list(input_shape[[1]], self$units) \n  } \n) \nl <- layer_dense2(units = 10) \nl(matrix(runif(10), ncol = 1)) \n```\n:::\n"",
+    ""supporting"": [
+      ""layer_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/multi_gpu_model/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""f4da388b80c7a770bd1e4d26e6e62762"",
+  ""hash"": ""6dee426999b184ac400734cc1b0e810b"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/model.R*\n\n# multi_gpu_model\n\n## (Deprecated) Replicates a model on different GPUs.\n\n## Description\n(Deprecated) Replicates a model on different GPUs. \n\n\n## Usage\n```r\nmulti_gpu_model(model, gpus = NULL, cpu_merge = TRUE, cpu_relocation = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| model | A Keras model instance. To avoid OOM errors, this model could have been built on CPU, for instance (see usage example below). |\n| gpus | `NULL` to use all available GPUs (default). Integer >= 2 or list of integers, number of GPUs or list of GPU IDs on which to create model replicas. |\n| cpu_merge | A boolean value to identify whether to force merging model weights under the scope of the CPU or not. |\n| cpu_relocation | A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. |\n\n## Details\n\nSpecifically, this function implements single-machine multi-GPU data parallelism. It works in the following way: \n\n- Divide the model's input(s) into multiple sub-batches. \n\n- Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU. \n\n- Concatenate the results (on CPU) into one big batch. \n\nE.g. if your `batch_size` is 64 and you use `gpus=2`, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples. \n\nThis induces quasi-linear speedup on up to 8 GPUs. \n\nThis function is only available with the TensorFlow backend for the time being. \n\n## Section\n\n## Model Saving\n\nTo save the multi-gpu model, use `save_model_hdf5()` or `save_model_weights_hdf5()` with the template model (the argument you passed to `multi_gpu_model`), rather than the model returned by `multi_gpu_model`. \n\n## Value\nA Keras model object which can be used just like the initial `model` argument, but which distributes its workload on multiple GPUs. \n\n## Note\n\nThis function is deprecated and has been removed from tensorflow on 2020-04-01. To distribute your training across all available GPUS, you can use `tensorflow::tf$distribute$MirroredStrategy()`\n\nby creating your model like this: \n\n```\n\nstrategy <- tensorflow::tf$distribute$MirroredStrategy() \n\nwith(strategy$scope(), { \n\n  model <- application_xception( \n\n    weights = NULL, \n\n    input_shape = c(height, width, 3), \n\n    classes = num_classes \n\n}) \n\n```\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nlibrary(tensorflow) \nnum_samples <- 1000 \nheight <- 224 \nwidth <- 224 \nnum_classes <- 1000 \n# Instantiate the base model (or \""template\"" model). \n# We recommend doing this with under a CPU device scope, \n# so that the model's weights are hosted on CPU memory. \n# Otherwise they may end up hosted on a GPU, which would \n# complicate weight sharing. \nwith(tf$device(\""/cpu:0\""), { \n  model <- application_xception( \n    weights = NULL, \n    input_shape = c(height, width, 3), \n    classes = num_classes \n  ) \n}) \n# Replicates the model on 8 GPUs. \n# This assumes that your machine has 8 available GPUs. \nparallel_model <- multi_gpu_model(model, gpus = 8) \nparallel_model %>% compile( \n  loss = \""categorical_crossentropy\"", \n  optimizer = \""rmsprop\"" \n) \n# Generate dummy data. \nx <- array(runif(num_samples * height * width*3), \n           dim = c(num_samples, height, width, 3)) \ny <- array(runif(num_samples * num_classes), \n           dim = c(num_samples, num_classes)) \n# This `fit` call will be distributed on 8 GPUs. \n# Since the batch size is 256, each GPU will process 32 samples. \nparallel_model %>% fit(x, y, epochs = 20, batch_size = 256) \n# Save model via the template model (which shares the same weights): \nmodel %>% save_model_hdf5(\""my_model.h5\"") \n```\n:::\n\n\n## See Also\nOther model functions:  `compile.keras.engine.training.Model()`, `evaluate.keras.engine.training.Model()`, `evaluate_generator()`, `fit.keras.engine.training.Model()`, `fit_generator()`, `get_config()`, `get_layer()`, `keras_model_sequential()`, `keras_model()`, `pop_layer()`, `predict.keras.engine.training.Model()`, `predict_generator()`, `predict_on_batch()`, `predict_proba()`, `summary.keras.engine.training.Model()`, `train_on_batch()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L252) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# multi_gpu_model\n\n## (Deprecated) Replicates a model on different GPUs.\n\n## Description\n(Deprecated) Replicates a model on different GPUs. \n\n\n## Usage\n```r\nmulti_gpu_model(model, gpus = NULL, cpu_merge = TRUE, cpu_relocation = FALSE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| model | A Keras model instance. To avoid OOM errors, this model could have been built on CPU, for instance (see usage example below). |\n| gpus | `NULL` to use all available GPUs (default). Integer >= 2 or list of integers, number of GPUs or list of GPU IDs on which to create model replicas. |\n| cpu_merge | A boolean value to identify whether to force merging model weights under the scope of the CPU or not. |\n| cpu_relocation | A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option. |\n\n## Details\n\nSpecifically, this function implements single-machine multi-GPU data parallelism. It works in the following way: \n\n- Divide the model's input(s) into multiple sub-batches. \n\n- Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU. \n\n- Concatenate the results (on CPU) into one big batch. \n\nE.g. if your `batch_size` is 64 and you use `gpus=2`, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples. \n\nThis induces quasi-linear speedup on up to 8 GPUs. \n\nThis function is only available with the TensorFlow backend for the time being. \n\n## Section\n\n## Model Saving\n\nTo save the multi-gpu model, use `save_model_hdf5()` or `save_model_weights_hdf5()` with the template model (the argument you passed to `multi_gpu_model`), rather than the model returned by `multi_gpu_model`. \n\n## Value\nA Keras model object which can be used just like the initial `model` argument, but which distributes its workload on multiple GPUs. \n\n## Note\n\nThis function is deprecated and has been removed from tensorflow on 2020-04-01. To distribute your training across all available GPUS, you can use `tensorflow::tf$distribute$MirroredStrategy()`\n\nby creating your model like this: \n\n```\n\nstrategy <- tensorflow::tf$distribute$MirroredStrategy() \n\nwith(strategy$scope(), { \n\n  model <- application_xception( \n\n    weights = NULL, \n\n    input_shape = c(height, width, 3), \n\n    classes = num_classes \n\n}) \n\n```\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nlibrary(tensorflow) \nnum_samples <- 1000 \nheight <- 224 \nwidth <- 224 \nnum_classes <- 1000 \n# Instantiate the base model (or \""template\"" model). \n# We recommend doing this with under a CPU device scope, \n# so that the model's weights are hosted on CPU memory. \n# Otherwise they may end up hosted on a GPU, which would \n# complicate weight sharing. \nwith(tf$device(\""/cpu:0\""), { \n  model <- application_xception( \n    weights = NULL, \n    input_shape = c(height, width, 3), \n    classes = num_classes \n  ) \n}) \n# Replicates the model on 8 GPUs. \n# This assumes that your machine has 8 available GPUs. \nparallel_model <- multi_gpu_model(model, gpus = 8) \nparallel_model %>% compile( \n  loss = \""categorical_crossentropy\"", \n  optimizer = \""rmsprop\"" \n) \n# Generate dummy data. \nx <- array(runif(num_samples * height * width*3), \n           dim = c(num_samples, height, width, 3)) \ny <- array(runif(num_samples * num_classes), \n           dim = c(num_samples, num_classes)) \n# This `fit` call will be distributed on 8 GPUs. \n# Since the batch size is 256, each GPU will process 32 samples. \nparallel_model %>% fit(x, y, epochs = 20, batch_size = 256) \n# Save model via the template model (which shares the same weights): \nmodel %>% save_model_hdf5(\""my_model.h5\"") \n```\n:::\n\n\n## See Also\nOther model functions:  `compile.keras.engine.training.Model()`, `evaluate.keras.engine.training.Model()`, `evaluate_generator()`, `fit.keras.engine.training.Model()`, `fit_generator()`, `get_config()`, `get_layer()`, `keras_model_sequential()`, `keras_model()`, `pop_layer()`, `predict.keras.engine.training.Model()`, `predict_generator()`, `predict_on_batch()`, `predict_proba()`, `summary.keras.engine.training.Model()`, `train_on_batch()`\n\n"",
+    ""supporting"": [
+      ""multi_gpu_model_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/save_text_tokenizer/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""50b1cca76190d11804614892df1db0a1"",
+  ""hash"": ""1339a9cda802511b9260e63d4d746f00"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/preprocessing.R*\n\n# save_text_tokenizer\n\n## Save a text tokenizer to an external file\n\n## Description\nEnables persistence of text tokenizers alongside saved models. \n\n\n## Usage\n```r\nsave_text_tokenizer(object, filename) \nload_text_tokenizer(filename) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Text tokenizer fit with `fit_text_tokenizer()` |\n| filename | File to save/load |\n\n## Details\n\nYou should always use the same text tokenizer for training and prediction. In many cases however prediction will occur in another session with a version of the model loaded via `load_model_hdf5()`. \n\nIn this case you need to save the text tokenizer object after training and then reload it prior to prediction. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# vectorize texts then save for use in prediction \ntokenizer <- text_tokenizer(num_words = 10000) %>% \nfit_text_tokenizer(tokenizer, texts) \nsave_text_tokenizer(tokenizer, \""tokenizer\"") \n# (train model, etc.) \n# ...later in another session \ntokenizer <- load_text_tokenizer(\""tokenizer\"") \n# (use tokenizer to preprocess data for prediction) \n```\n:::\n\n\n## See Also\nOther text tokenization:  `fit_text_tokenizer()`, `sequences_to_matrix()`, `text_tokenizer()`, `texts_to_matrix()`, `texts_to_sequences_generator()`, `texts_to_sequences()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L370) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# save_text_tokenizer\n\n## Save a text tokenizer to an external file\n\n## Description\nEnables persistence of text tokenizers alongside saved models. \n\n\n## Usage\n```r\nsave_text_tokenizer(object, filename) \n\nload_text_tokenizer(filename) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Text tokenizer fit with `fit_text_tokenizer()` |\n| filename | File to save/load |\n\n## Details\n\nYou should always use the same text tokenizer for training and prediction. In many cases however prediction will occur in another session with a version of the model loaded via `load_model_hdf5()`. \n\nIn this case you need to save the text tokenizer object after training and then reload it prior to prediction. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n# vectorize texts then save for use in prediction \ntokenizer <- text_tokenizer(num_words = 10000) %>% \nfit_text_tokenizer(tokenizer, texts) \nsave_text_tokenizer(tokenizer, \""tokenizer\"") \n# (train model, etc.) \n# ...later in another session \ntokenizer <- load_text_tokenizer(\""tokenizer\"") \n# (use tokenizer to preprocess data for prediction) \n```\n:::\n\n\n## See Also\nOther text tokenization:  `fit_text_tokenizer()`, `sequences_to_matrix()`, `text_tokenizer()`, `texts_to_matrix()`, `texts_to_sequences_generator()`, `texts_to_sequences()`\n\n"",
+    ""supporting"": [
+      ""save_text_tokenizer_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/use_implementation/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""e00e8566eb6edff3207a8c1898e0daec"",
+  ""hash"": ""e7c2e4456391706a1f5249ff06694c52"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/package.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/package.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/package.R*\n\n# use_implementation\n\n## Select a Keras implementation and backend\n\n## Description\nSelect a Keras implementation and backend \n\n\n## Usage\n```r\nuse_implementation(implementation = c(\""keras\"", \""tensorflow\"")) \nuse_backend(backend = c(\""tensorflow\"", \""cntk\"", \""theano\"", \""plaidml\"")) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| implementation | One of \""keras\"" or \""tensorflow\"" (defaults to \""keras\""). |\n| backend | One of \""tensorflow\"", \""cntk\"", or \""theano\"" (defaults to \""tensorflow\"") |\n\n## Details\n\nKeras has multiple implementations (the original keras implementation and the implementation native to TensorFlow) and supports multiple backends (\""tensorflow\"", \""cntk\"", \""theano\"", and \""plaidml\""). These functions allow switching between the various implementations and backends. \n\nThe functions should be called after `library(keras)` and before calling other functions within the package (see below for an example). \n\nThe default implementation and backend should be suitable for most use cases. The \""tensorflow\"" implementation is useful when using Keras in conjunction with TensorFlow Estimators (the `tfestimators`\n\nR package). \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# use the tensorflow implementation \nlibrary(keras) \nuse_implementation(\""tensorflow\"") \n# use the cntk backend \nlibrary(keras) \nuse_backend(\""theano\"") \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/package.R#L64) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# use_implementation\n\n## Select a Keras implementation and backend\n\n## Description\nSelect a Keras implementation and backend \n\n\n## Usage\n```r\nuse_implementation(implementation = c(\""keras\"", \""tensorflow\"")) \n\nuse_backend(backend = c(\""tensorflow\"", \""cntk\"", \""theano\"", \""plaidml\"")) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| implementation | One of \""keras\"" or \""tensorflow\"" (defaults to \""keras\""). |\n| backend | One of \""tensorflow\"", \""cntk\"", or \""theano\"" (defaults to \""tensorflow\"") |\n\n## Details\n\nKeras has multiple implementations (the original keras implementation and the implementation native to TensorFlow) and supports multiple backends (\""tensorflow\"", \""cntk\"", \""theano\"", and \""plaidml\""). These functions allow switching between the various implementations and backends. \n\nThe functions should be called after `library(keras)` and before calling other functions within the package (see below for an example). \n\nThe default implementation and backend should be suitable for most use cases. The \""tensorflow\"" implementation is useful when using Keras in conjunction with TensorFlow Estimators (the `tfestimators`\n\nR package). \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# use the tensorflow implementation \nlibrary(keras) \nuse_implementation(\""tensorflow\"") \n# use the cntk backend \nlibrary(keras) \nuse_backend(\""theano\"") \n```\n:::\n"",
+    ""supporting"": [
+      ""use_implementation_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/with_custom_object_scope/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""a8f2a203438031489ccbe79b4a2073ef"",
+  ""hash"": ""f407b297e30c48c0774784310d0e95de"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/utils.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/utils.R*\n\n# with_custom_object_scope\n\n## Provide a scope with mappings of names to custom objects\n\n## Description\nProvide a scope with mappings of names to custom objects \n\n\n## Usage\n```r\nwith_custom_object_scope(objects, expr) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| objects | Named list of objects |\n| expr | Expression to evaluate |\n\n## Details\n\nThere are many elements of Keras models that can be customized with user objects (e.g. losses, metrics, regularizers, etc.). When loading saved models that use these functions you typically need to explicitily map names to user objects via the `custom_objects`\n\nparmaeter. \n\nThe `with_custom_object_scope()` function provides an alternative that lets you create a named alias for a user object that applies to an entire block of code, and is automatically recognized when loading saved models. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# define custom metric \nmetric_top_3_categorical_accuracy <- \n  custom_metric(\""top_3_categorical_accuracy\"", function(y_true, y_pred) { \n    metric_top_k_categorical_accuracy(y_true, y_pred, k = 3) \n  }) \nwith_custom_object_scope(c(top_k_acc = sparse_top_k_cat_acc), { \n  # ...define model... \n  # compile model (refer to \""top_k_acc\"" by name) \n  model %>% compile( \n    loss = \""binary_crossentropy\"", \n    optimizer = optimizer_nadam(), \n    metrics = c(\""top_k_acc\"") \n  ) \n  # save the model \n  save_model_hdf5(\""my_model.h5\"") \n  # loading the model within the custom object scope doesn't \n  # require explicitly providing the custom_object \n  load_model_hdf5(\""my_model.h5\"") \n}) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R#L176) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# with_custom_object_scope\n\n## Provide a scope with mappings of names to custom objects\n\n## Description\nProvide a scope with mappings of names to custom objects \n\n\n## Usage\n```r\nwith_custom_object_scope(objects, expr) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| objects | Named list of objects |\n| expr | Expression to evaluate |\n\n## Details\n\nThere are many elements of Keras models that can be customized with user objects (e.g. losses, metrics, regularizers, etc.). When loading saved models that use these functions you typically need to explicitily map names to user objects via the `custom_objects`\n\nparmaeter. \n\nThe `with_custom_object_scope()` function provides an alternative that lets you create a named alias for a user object that applies to an entire block of code, and is automatically recognized when loading saved models. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n# define custom metric \nmetric_top_3_categorical_accuracy <- \n  custom_metric(\""top_3_categorical_accuracy\"", function(y_true, y_pred) { \n    metric_top_k_categorical_accuracy(y_true, y_pred, k = 3) \n  }) \nwith_custom_object_scope(c(top_k_acc = sparse_top_k_cat_acc), { \n  # ...define model... \n  # compile model (refer to \""top_k_acc\"" by name) \n  model %>% compile( \n    loss = \""binary_crossentropy\"", \n    optimizer = optimizer_nadam(), \n    metrics = c(\""top_k_acc\"") \n  ) \n  # save the model \n  save_model_hdf5(\""my_model.h5\"") \n  # loading the model within the custom object scope doesn't \n  # require explicitly providing the custom_object \n  load_model_hdf5(\""my_model.h5\"") \n}) \n```\n:::\n"",
+    ""supporting"": [
+      ""with_custom_object_scope_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/keras/zip_lists/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""6c3bf53561f4cfc25ad1d9b7f8788ce8"",
+  ""hash"": ""e63351fbc0754ae9469bc70be6191dbc"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/utils.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/utils.R*\n\n# zip_lists\n\n## zip lists\n\n## Description\nThis is conceptually similar to `zip()` in Python, or R functions `purrr::transpose()` and `data.table::transpose()` (albeit, accepting elements in `...` instead of a single list), with one crucial difference: if the provided objects are named, then matching is done by names, not positions. \n\n\n## Usage\n```r\nzip_lists(...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | R lists or atomic vectors, optionally named. |\n\n## Details\nAll arguments supplied must be of the same length. If positional matching is required, then all arguments provided must be unnamed. If matching by names, then all arguments must have the same set of names, but they can be in different orders. \n\n\n## Value\nA inverted list \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ngradients <- list(\""grad_for_wt_1\"", \""grad_for_wt_2\"", \""grad_for_wt_3\"") \nweights <- list(\""weight_1\"", \""weight_2\"", \""weight_3\"") \nstr(zip_lists(gradients, weights)) \nstr(zip_lists(gradient = gradients, weight = weights)) \nnames(gradients) <- names(weights) <- paste0(\""layer_\"", 1:3) \nstr(zip_lists(gradients, weights[c(3, 1, 2)])) \nnames(gradients) <- paste0(\""gradient_\"", 1:3) \ntry(zip_lists(gradients, weights)) # error, names don't match \n# call unname directly for positional matching \nzip_lists(unname(gradients), unname(weights)) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R#L625) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# zip_lists\n\n## zip lists\n\n## Description\nThis is conceptually similar to `zip()` in Python, or R functions `purrr::transpose()` and `data.table::transpose()` (albeit, accepting elements in `...` instead of a single list), with one crucial difference: if the provided objects are named, then matching is done by names, not positions. \n\n\n## Usage\n```r\nzip_lists(...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | R lists or atomic vectors, optionally named. |\n\n## Details\nAll arguments supplied must be of the same length. If positional matching is required, then all arguments provided must be unnamed. If matching by names, then all arguments must have the same set of names, but they can be in different orders. \n\n\n## Value\nA inverted list \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\ngradients <- list(\""grad_for_wt_1\"", \""grad_for_wt_2\"", \""grad_for_wt_3\"") \nweights <- list(\""weight_1\"", \""weight_2\"", \""weight_3\"") \nstr(zip_lists(gradients, weights)) \nstr(zip_lists(gradient = gradients, weight = weights)) \nnames(gradients) <- names(weights) <- paste0(\""layer_\"", 1:3) \nstr(zip_lists(gradients, weights[c(3, 1, 2)])) \nnames(gradients) <- paste0(\""gradient_\"", 1:3) \ntry(zip_lists(gradients, weights)) # error, names don't match \n# call unname directly for positional matching \nzip_lists(unname(gradients), unname(weights)) \n```\n:::\n"",
+    ""supporting"": [
+      ""zip_lists_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tensorflow/all_dims/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""db2e62fafce8d3d99ce722167e930169"",
+  ""hash"": ""0ec4843751a974ea3094e1b093b2b281"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/extract.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/extract.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/extract.R*\n\n# all_dims\n\n## All dims\n\n## Description\nThis function returns an object that can be used when subsetting tensors with `[`. If you are familiar with python,, this is equivalent to the python Ellipsis `...`, (not to be confused with `...` in `R`). \n\n\n## Usage\n```r\nall_dims() \n```\n\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# in python, if x is a numpy array or tensorflow tensor \nx[..., i] \n# the ellipsis means \""expand to match number of dimension of x\"". \n# to translate the above python expression to R, write: \nx[all_dims(), i] \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/extract.R#L263) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# all_dims\n\n## All dims\n\n## Description\nThis function returns an object that can be used when subsetting tensors with `[`. If you are familiar with python,, this is equivalent to the python Ellipsis `...`, (not to be confused with `...` in `R`). \n\n\n## Usage\n```r\nall_dims() \n```\n\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n# in python, if x is a numpy array or tensorflow tensor \nx[..., i] \n# the ellipsis means \""expand to match number of dimension of x\"". \n# to translate the above python expression to R, write: \nx[all_dims(), i] \n```\n:::\n"",
     ""supporting"": [
       ""all_dims_files""
     ],

---FILE: _freeze/reference/tensorflow/as_tensor/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""334abff930a272ace02ebff4005c816b"",
+  ""hash"": ""5b4d3e866e788619e3547c452b6fcb1a"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/generics.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/generics.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/generics.R*\n\n# as_tensor\n\n## as_tensor\n\n## Description\n\nCoerce objects to tensorflow tensors (potentially of a specific dtype or shape). The provided default methods will call [`tf$convert_to_tensor`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor). Depending on arguments supplied it may also call some combination of \n\n- [`tf$saturate_cast`](https://www.tensorflow.org/api_docs/python/tf/dtypes/saturate_cast) or [`tf$cast`](https://www.tensorflow.org/api_docs/python/tf/cast)\n\n- [`tf$fill`](https://www.tensorflow.org/api_docs/python/tf/fill) or [`tf$reshape`](https://www.tensorflow.org/api_docs/python/tf/reshape)\n\n\n## Usage\n```r\nas_tensor(x, dtype = NULL, ..., name = NULL) \n## S3 method for class 'default'\nas_tensor\n(x, dtype = NULL, ..., shape = NULL, name = NULL) \n## S3 method for class 'double'\nas_tensor\n(x, dtype = NULL, ..., name = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | object to convert |\n| dtype | `NULL`, a tensorflow dtype (`tf$int32`), or something coercible to one (e.g. a string `\""int32\""`) |\n| ...,  | ignored |\n| name | `NULL` or a string. Useful for debugging in graph mode, ignored while in eager mode. |\n| shape | an integer vector, tensor, or `tf.TensorShape`. Can contain up to 1 unspecified dimension, encoded as a `-1` or `NA`. This will reshape `x` using row-major (C-style) semantics. It will prefer reshaping using non-graph operations if possible, but will otherwise invoke `tf$reshape()`. If `x` is a scalar and the requested `shape` is fully defined or a tensor, the value of `x` will be recycled to fill a tensor of the requested shape (it will dispatch to `tf$fill()`). |\n\n\n\n## Value\na tensorflow tensor \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nas_tensor(42, \""int32\"") \nas_tensor(as_tensor(42)) \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/generics.R#L743) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# as_tensor\n\n## as_tensor\n\n## Description\n\nCoerce objects to tensorflow tensors (potentially of a specific dtype or shape). The provided default methods will call [`tf$convert_to_tensor`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor). Depending on arguments supplied it may also call some combination of \n\n- [`tf$saturate_cast`](https://www.tensorflow.org/api_docs/python/tf/dtypes/saturate_cast) or [`tf$cast`](https://www.tensorflow.org/api_docs/python/tf/cast)\n\n- [`tf$fill`](https://www.tensorflow.org/api_docs/python/tf/fill) or [`tf$reshape`](https://www.tensorflow.org/api_docs/python/tf/reshape)\n\n\n## Usage\n```r\nas_tensor(x, dtype = NULL, ..., name = NULL) \n\n## S3 method for class 'default'\nas_tensor(x, dtype = NULL, ..., shape = NULL, name = NULL) \n\n## S3 method for class 'double'\nas_tensor(x, dtype = NULL, ..., name = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | object to convert |\n| dtype | `NULL`, a tensorflow dtype (`tf$int32`), or something coercible to one (e.g. a string `\""int32\""`) |\n| ...,  | ignored |\n| name | `NULL` or a string. Useful for debugging in graph mode, ignored while in eager mode. |\n| shape | an integer vector, tensor, or `tf.TensorShape`. Can contain up to 1 unspecified dimension, encoded as a `-1` or `NA`. This will reshape `x` using row-major (C-style) semantics. It will prefer reshaping using non-graph operations if possible, but will otherwise invoke `tf$reshape()`. If `x` is a scalar and the requested `shape` is fully defined or a tensor, the value of `x` will be recycled to fill a tensor of the requested shape (it will dispatch to `tf$fill()`). |\n\n\n\n## Value\na tensorflow tensor \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nas_tensor(42, \""int32\"") \nas_tensor(as_tensor(42)) \n```\n:::\n"",
     ""supporting"": [
       ""as_tensor_files""
     ],

---FILE: _freeze/reference/tensorflow/parse_flags/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""706aa71162e15bfd0ce4df95dfa53775"",
+  ""hash"": ""48cf4848e9121d11bea13d5831a02da6"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/flags.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/flags.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/flags.R*\n\n# parse_flags\n\n## Parse Configuration Flags for a TensorFlow Application\n\n## Description\nParse configuration flags for a TensorFlow application. Use this to parse and unify the configuration(s) specified through a `flags.yml` configuration file, alongside other arguments set through the command line. \n\n\n## Usage\n```r\nparse_flags( \n  config = Sys.getenv(\""R_CONFIG_ACTIVE\"", unset = \""default\""), \n  file = \""flags.yml\"", \n  arguments = commandArgs(TRUE) \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| config | The configuration to use. Defaults to the active configuration for the current environment (as specified by the `R_CONFIG_ACTIVE` environment variable), or `default` when unset. |\n| file | The configuration file to read. |\n| arguments | The command line arguments (as a character vector) to be parsed. |\n\n\n\n## Value\nA named `R` list, mapping configuration keys to values. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# examine an example configuration file provided by tensorflow \nfile <- system.file(\""examples/config/flags.yml\"", package = \""tensorflow\"") \ncat(readLines(file), sep = \""\\n\"") \n# read the default configuration \nFLAGS <- tensorflow::parse_flags(\""default\"", file = file) \nstr(FLAGS) \n# read the alternate configuration: note that \n# the default configuration is inherited, but \n# we override the 'string' configuration here \nFLAGS <- tensorflow::parse_flags(\""alternate\"", file = file) \nstr(FLAGS) \n# override configuration values using command \n# line arguments (normally, these would be \n# passed in through the command line invocation \n# used to start the process) \nFLAGS <- tensorflow::parse_flags( \n  \""alternate\"", \n  file = file, \n  arguments = c(\""--foo=1\"") \n) \nstr(FLAGS) \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/flags.R#L47) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# parse_flags\n\n## Parse Configuration Flags for a TensorFlow Application\n\n## Description\nParse configuration flags for a TensorFlow application. Use this to parse and unify the configuration(s) specified through a `flags.yml` configuration file, alongside other arguments set through the command line. \n\n\n## Usage\n```r\nparse_flags( \n  config = Sys.getenv(\""R_CONFIG_ACTIVE\"", unset = \""default\""), \n  file = \""flags.yml\"", \n  arguments = commandArgs(TRUE) \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| config | The configuration to use. Defaults to the active configuration for the current environment (as specified by the `R_CONFIG_ACTIVE` environment variable), or `default` when unset. |\n| file | The configuration file to read. |\n| arguments | The command line arguments (as a character vector) to be parsed. |\n\n\n\n## Value\nA named `R` list, mapping configuration keys to values. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n# examine an example configuration file provided by tensorflow \nfile <- system.file(\""examples/config/flags.yml\"", package = \""tensorflow\"") \ncat(readLines(file), sep = \""\\n\"") \n# read the default configuration \nFLAGS <- tensorflow::parse_flags(\""default\"", file = file) \nstr(FLAGS) \n# read the alternate configuration: note that \n# the default configuration is inherited, but \n# we override the 'string' configuration here \nFLAGS <- tensorflow::parse_flags(\""alternate\"", file = file) \nstr(FLAGS) \n# override configuration values using command \n# line arguments (normally, these would be \n# passed in through the command line invocation \n# used to start the process) \nFLAGS <- tensorflow::parse_flags( \n  \""alternate\"", \n  file = file, \n  arguments = c(\""--foo=1\"") \n) \nstr(FLAGS) \n```\n:::\n"",
     ""supporting"": [
       ""parse_flags_files""
     ],

---FILE: _freeze/reference/tensorflow/shape/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""69ea556fc43d8c3bea5658b3ff73db79"",
+  ""hash"": ""80bc4a7c923daef5cd214440231dc516"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/shape.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/shape.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/shape.R*\n\n# shape\n\n## Create a `tf.TensorShape` object\n\n## Description\nCreate a `tf.TensorShape` object \n\n\n## Usage\n```r\nshape(..., dims = list(...)) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Tensor dimensions as integers or `NULL` for an unknown dimensions. `NA` and `-1` are synonyms for `NULL`. |\n| dims | Tensor dimensions as a vector. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- construct --- \nshape()       # tf.TensorShape()       # scalar \nshape(NULL)   # tf.TensorShape([None]) # 1-D array of unknown length \nshape(NA)     # tf.TensorShape([None]) # 1-D array of unknown length, NA is a synonym for NULL \nshape(dims = NULL) # TensorShape(None)    # Unknown rank, unknown size \nshape(3, 4)        # TensorShape([3, 4])  # 2-D array (matrix) with 3 rows, 4 columns \nshape(NA, 4)           # TensorShape([None, 4])  # 2-D array (matrix) with unknown rows, 4 columns \nshape(dims = c(NA, 4)) # TensorShape([None, 4]) # same as above; bypass ... and pass dims directly \n# --- inspect --- \nlength(shape(dims = NULL)) # NA_integer_ \nlength(shape(1,2,3,NA))    # 4L \n# ---convert --- \nx <- shape(dims = list(3L, 5L)) \nas.list(x)     # list(3L, 5L) \nas.integer(x)  # c(3L, 5L) \nas.numeric(x)  # c(3, 5) \nas.double(x)   # c(3, 5) # alias for as.numeric \nas_tensor(x)   # tf.Tensor([3 5], shape=(2,), dtype=int32) \n# convert partially undefined shapes \nx <- shape(NA, 3) \nas.list(x)     # list(NULL, 3L) \nas.integer(x)  # c(NA, 3L) \nas_tensor(x)   # tf.Tensor([-1  3], shape=(2,), dtype=int32) # unspecified dims default is -1 \n# as_tensor() converts undefined dimensions to -1, which is useful for \n# tf functions that only accept tensors for shapes, e.g, \ntf$reshape(tf$zeros(shape(8)), \n           as_tensor(shape(NA, 4))) \n# tf.Tensor([[0. 0. 0. 0.] \n#            [0. 0. 0. 0.]], shape=(2, 4), dtype=float32) \n# converting fully unknown shapes raises an error \ntry(as.list(shape(dims = NULL))) # ValueError: as_list() is not defined on an unknown TensorShape. \n# test for rank first if this a concern: \nas.list_or_null <- function(x) if(is.na(length(x))) NULL else as.list(x) \nas.list_or_null(shape(dims = NULL)) \n# --- compare --- \n# Fully known shapes return TRUE if and only if each element is equal \nshape(3, 4) == shape(3, 4) # TRUE \nshape(3, 4) == shape(4, 4) # FALSE \n# two unknown dimensions are treated as equal \nshape(NA, 4) == shape(NA, 4) # TRUE \nshape(NA, 4) == shape(3, 4)  # FALSE \n# Two unknown shapes, return TRUE \nshape(dims = NULL) == shape(dims = NULL) # TRUE \n# Comparing an unknown shape to a partially or fully defined shape returns FALSE \nshape(dims = NULL) == shape(NULL) # FALSE \nshape(dims = NULL) == shape(4)    # FALSE \nvalues of length greater than one supplied to `...`  are automatically flattened \nshape(1, c(2, 3), 4) # shape(1, 2, 3, 4) \nshape(1, shape(2, 3), 4) # shape(1, 2, 3, 4) \nshape(1, as_tensor(2, 3), 4) # shape(1, 2, 3, 4) \n# --- extract or replace --- \n# regular R-list semantics for `[`, `[[`, `[<-`, `[[<-` \nx <- shape(1, 2, 3) \nx[1]       # TensorShape([1]) \nx[[1]]     # 1L \nx[2:3]     # TensorShape([2, 3]) \nx[-1]      # TensorShape([2, 3]) \nx[1] <- 11        ; x # TensorShape([11, 2, 3]) \nx[1] <- shape(11) ; x # TensorShape([11, 2, 3]) \nx[1] <- list(11)  ; x # TensorShape([11, 2, 3]) \nx[[1]] <- 22            ; x # TensorShape([22, 2, 3]) \nx[1:2] <- c(NA, 99)     ; x # TensorShape([None, 99, 3]) \nx[1:2] <- shape(33, 44) ; x # TensorShape([33, 44, 3]) \n# --- concatenate --- \nc(shape(1), shape(2, 3), shape(4, NA)) # TensorShape([1, 2, 3, 4, None]) \n# --- merge --- \nmerge(shape(NA, 2), \n      shape(1 , 2)) # TensorShape([1, 2]) \ntry(merge(shape(2, 2), \n          shape(1, 2))) # ValueError: Shapes (2, 2) and (1, 2) are not compatible \nrm(x) # cleanup \n```\n:::\n\n\n## See Also\n[https://www.tensorflow.org/api_docs/python/tf/TensorShape](https://www.tensorflow.org/api_docs/python/tf/TensorShape)\n\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/shape.R#L105) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# shape\n\n## Create a `tf.TensorShape` object\n\n## Description\nCreate a `tf.TensorShape` object \n\n\n## Usage\n```r\nshape(..., dims = list(...)) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Tensor dimensions as integers or `NULL` for an unknown dimensions. `NA` and `-1` are synonyms for `NULL`. |\n| dims | Tensor dimensions as a vector. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n# --- construct --- \nshape()       # tf.TensorShape()       # scalar \nshape(NULL)   # tf.TensorShape([None]) # 1-D array of unknown length \nshape(NA)     # tf.TensorShape([None]) # 1-D array of unknown length, NA is a synonym for NULL \nshape(dims = NULL) # TensorShape(None)    # Unknown rank, unknown size \nshape(3, 4)        # TensorShape([3, 4])  # 2-D array (matrix) with 3 rows, 4 columns \nshape(NA, 4)           # TensorShape([None, 4])  # 2-D array (matrix) with unknown rows, 4 columns \nshape(dims = c(NA, 4)) # TensorShape([None, 4]) # same as above; bypass ... and pass dims directly \n# --- inspect --- \nlength(shape(dims = NULL)) # NA_integer_ \nlength(shape(1,2,3,NA))    # 4L \n# ---convert --- \nx <- shape(dims = list(3L, 5L)) \nas.list(x)     # list(3L, 5L) \nas.integer(x)  # c(3L, 5L) \nas.numeric(x)  # c(3, 5) \nas.double(x)   # c(3, 5) # alias for as.numeric \nas_tensor(x)   # tf.Tensor([3 5], shape=(2,), dtype=int32) \n# convert partially undefined shapes \nx <- shape(NA, 3) \nas.list(x)     # list(NULL, 3L) \nas.integer(x)  # c(NA, 3L) \nas_tensor(x)   # tf.Tensor([-1  3], shape=(2,), dtype=int32) # unspecified dims default is -1 \n# as_tensor() converts undefined dimensions to -1, which is useful for \n# tf functions that only accept tensors for shapes, e.g, \ntf$reshape(tf$zeros(shape(8)), \n           as_tensor(shape(NA, 4))) \n# tf.Tensor([[0. 0. 0. 0.] \n#            [0. 0. 0. 0.]], shape=(2, 4), dtype=float32) \n# converting fully unknown shapes raises an error \ntry(as.list(shape(dims = NULL))) # ValueError: as_list() is not defined on an unknown TensorShape. \n# test for rank first if this a concern: \nas.list_or_null <- function(x) if(is.na(length(x))) NULL else as.list(x) \nas.list_or_null(shape(dims = NULL)) \n# --- compare --- \n# Fully known shapes return TRUE if and only if each element is equal \nshape(3, 4) == shape(3, 4) # TRUE \nshape(3, 4) == shape(4, 4) # FALSE \n# two unknown dimensions are treated as equal \nshape(NA, 4) == shape(NA, 4) # TRUE \nshape(NA, 4) == shape(3, 4)  # FALSE \n# Two unknown shapes, return TRUE \nshape(dims = NULL) == shape(dims = NULL) # TRUE \n# Comparing an unknown shape to a partially or fully defined shape returns FALSE \nshape(dims = NULL) == shape(NULL) # FALSE \nshape(dims = NULL) == shape(4)    # FALSE \nvalues of length greater than one supplied to `...`  are automatically flattened \nshape(1, c(2, 3), 4) # shape(1, 2, 3, 4) \nshape(1, shape(2, 3), 4) # shape(1, 2, 3, 4) \nshape(1, as_tensor(2, 3), 4) # shape(1, 2, 3, 4) \n# --- extract or replace --- \n# regular R-list semantics for `[`, `[[`, `[<-`, `[[<-` \nx <- shape(1, 2, 3) \nx[1]       # TensorShape([1]) \nx[[1]]     # 1L \nx[2:3]     # TensorShape([2, 3]) \nx[-1]      # TensorShape([2, 3]) \nx[1] <- 11        ; x # TensorShape([11, 2, 3]) \nx[1] <- shape(11) ; x # TensorShape([11, 2, 3]) \nx[1] <- list(11)  ; x # TensorShape([11, 2, 3]) \nx[[1]] <- 22            ; x # TensorShape([22, 2, 3]) \nx[1:2] <- c(NA, 99)     ; x # TensorShape([None, 99, 3]) \nx[1:2] <- shape(33, 44) ; x # TensorShape([33, 44, 3]) \n# --- concatenate --- \nc(shape(1), shape(2, 3), shape(4, NA)) # TensorShape([1, 2, 3, 4, None]) \n# --- merge --- \nmerge(shape(NA, 2), \n      shape(1 , 2)) # TensorShape([1, 2]) \ntry(merge(shape(2, 2), \n          shape(1, 2))) # ValueError: Shapes (2, 2) and (1, 2) are not compatible \nrm(x) # cleanup \n```\n:::\n\n\n## See Also\n[https://www.tensorflow.org/api_docs/python/tf/TensorShape](https://www.tensorflow.org/api_docs/python/tf/TensorShape)\n\n"",
     ""supporting"": [
       ""shape_files""
     ],

---FILE: _freeze/reference/tensorflow/sub-.tensorflow.tensor/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""fdd445e314432ccb2d5f13934d6178e9"",
+  ""hash"": ""09e1ddb2af85eadfc2abb4d558432d4f"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/extract.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/extract.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/extract.R*\n\n# [.tensorflow.tensor\n\n## Subset tensors with `[`\n\n## Description\nSubset tensors with `[`\n\n\n## Usage\n```r\n## S3 method for class 'tensorflow.tensor'\n[\n( \n  x, \n  ..., \n  drop = TRUE, \n  style = getOption(\""tensorflow.extract.style\""), \n  options = tf_extract_opts(style) \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | Tensorflow tensor |\n| ... | slicing specs. See examples and details. |\n| drop | whether to drop scalar dimensions |\n| style | One of `\""python\""` or `\""R\""`. |\n| options | An object returned by `tf_extract_opts()` |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(array(1:15, dim = c(3, 5))) \nx \n# by default, numerics supplied to [...] are interpreted R style \nx[,1]    # first column \nx[1:2,]  # first two rows \nx[,1, drop = FALSE] # 1 column matrix \n# strided steps can be specified in R syntax or python syntax \nx[, seq(1, 5, by = 2)] \nx[, 1:5:2] \n# if you are unfamiliar with python-style strided steps, see: \n# https://numpy.org/doc/stable/reference/arrays.indexing.html#basic-slicing-and-indexing \n# missing arguments for python syntax are valid, but they must by backticked \n# or supplied as NULL \nx[, `::2`] \nx[, NULL:NULL:2] \nx[, `2:`] \n# all_dims() expands to the shape of the tensor \n# (equivalent to a python ellipsis `...`) \n# (not to be confused with R dots `...`) \ny <- as_tensor(array(1:(3^5), dim = c(3,3,3,3,3))) \nall.equal(y[all_dims(), 1], \n          y[, , , , 1]) \n# tf$newaxis are valid (equivalent to a NULL) \nx[,, tf$newaxis] \nx[,, NULL] \n# negative numbers are always interpreted python style \n# The first time a negative number is supplied to `[`, a warning is issued \n# about the non-standard behavior. \nx[-1,]  # last row, with a warning \nx[-1,]  # the warning is only issued once \n# specifying `style = 'python'` changes the following: \n# +  zero-based indexing is used \n# +  slice sequences in the form of `start:stop` do not include `stop` \n#    in the returned value \n# +  out-of-bounds indices in a slice are valid \n# The style argument can be supplied to individual calls of `[` or set \n# as a global option \n# example of zero based  indexing \nx[0, , style = 'python']  # first row \nx[1, , style = 'python']  # second row \n# example of slices with exclusive stop \noptions(tensorflow.extract.style = 'python') \nx[, 0:1]  # just the first column \nx[, 0:2]  # first and second column \n# example of out-of-bounds index \nx[, 0:10] \noptions(tensorflow.extract.style = NULL) \n# slicing with tensors is valid too, but note, tensors are never \n# translated and are always interpreted python-style. \n# A warning is issued the first time a tensor is passed to `[` \nx[, tf$constant(0L):tf$constant(2L)] \n# just as in python, only scalar tensors are valid \n# https://www.tensorflow.org/api_docs/python/tf/Tensor#__getitem__ \n# To silence the warnings about tensors being passed as-is and negative numbers \n# being interpreted python-style, set \noptions(tensorflow.extract.style = 'R') \n# clean up from examples \noptions(tensorflow.extract.style = NULL) \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/extract.R#L) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# [.tensorflow.tensor\n\n## Subset tensors with `[`\n\n## Description\nSubset tensors with `[`\n\n\n## Usage\n```r\n## S3 method for class 'tensorflow.tensor'\n[( \n  x, \n  ..., \n  drop = TRUE, \n  style = getOption(\""tensorflow.extract.style\""), \n  options = tf_extract_opts(style) \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | Tensorflow tensor |\n| ... | slicing specs. See examples and details. |\n| drop | whether to drop scalar dimensions |\n| style | One of `\""python\""` or `\""R\""`. |\n| options | An object returned by `tf_extract_opts()` |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nx <- as_tensor(array(1:15, dim = c(3, 5))) \nx \n# by default, numerics supplied to [...] are interpreted R style \nx[,1]    # first column \nx[1:2,]  # first two rows \nx[,1, drop = FALSE] # 1 column matrix \n# strided steps can be specified in R syntax or python syntax \nx[, seq(1, 5, by = 2)] \nx[, 1:5:2] \n# if you are unfamiliar with python-style strided steps, see: \n# https://numpy.org/doc/stable/reference/arrays.indexing.html#basic-slicing-and-indexing \n# missing arguments for python syntax are valid, but they must by backticked \n# or supplied as NULL \nx[, `::2`] \nx[, NULL:NULL:2] \nx[, `2:`] \n# all_dims() expands to the shape of the tensor \n# (equivalent to a python ellipsis `...`) \n# (not to be confused with R dots `...`) \ny <- as_tensor(array(1:(3^5), dim = c(3,3,3,3,3))) \nall.equal(y[all_dims(), 1], \n          y[, , , , 1]) \n# tf$newaxis are valid (equivalent to a NULL) \nx[,, tf$newaxis] \nx[,, NULL] \n# negative numbers are always interpreted python style \n# The first time a negative number is supplied to `[`, a warning is issued \n# about the non-standard behavior. \nx[-1,]  # last row, with a warning \nx[-1,]  # the warning is only issued once \n# specifying `style = 'python'` changes the following: \n# +  zero-based indexing is used \n# +  slice sequences in the form of `start:stop` do not include `stop` \n#    in the returned value \n# +  out-of-bounds indices in a slice are valid \n# The style argument can be supplied to individual calls of `[` or set \n# as a global option \n# example of zero based  indexing \nx[0, , style = 'python']  # first row \nx[1, , style = 'python']  # second row \n# example of slices with exclusive stop \noptions(tensorflow.extract.style = 'python') \nx[, 0:1]  # just the first column \nx[, 0:2]  # first and second column \n# example of out-of-bounds index \nx[, 0:10] \noptions(tensorflow.extract.style = NULL) \n# slicing with tensors is valid too, but note, tensors are never \n# translated and are always interpreted python-style. \n# A warning is issued the first time a tensor is passed to `[` \nx[, tf$constant(0L):tf$constant(2L)] \n# just as in python, only scalar tensors are valid \n# https://www.tensorflow.org/api_docs/python/tf/Tensor#__getitem__ \n# To silence the warnings about tensors being passed as-is and negative numbers \n# being interpreted python-style, set \noptions(tensorflow.extract.style = 'R') \n# clean up from examples \noptions(tensorflow.extract.style = NULL) \n```\n:::\n"",
     ""supporting"": [
       ""sub-.tensorflow.tensor_files""
     ],

---FILE: _freeze/reference/tensorflow/tf/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""97f0f2d88479522e586c63792780f0e0"",
+  ""hash"": ""1704adbcd66346595c8a1a4bb434ba1e"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/modules.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/modules.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/modules.R*\n\n# tf\n\n## Main TensorFlow module\n\n## Description\nInterface to main TensorFlow  module. Provides access to top level classes and functions as well as sub-modules (e.g. `tf$nn`, `tf$contrib$learn`, etc.). \n\n## Format\nTensorFlow module \n\n## Usage\n```r\ntf \n```\n\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \nhello <- tf$constant('Hello, TensorFlow!') \nzeros <- tf$Variable(tf$zeros(shape(1L))) \ntf$print(hello) \ntf$print(zeros) \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/modules.R#L21) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tf\n\n## Main TensorFlow module\n\n## Description\nInterface to main TensorFlow  module. Provides access to top level classes and functions as well as sub-modules (e.g. `tf$nn`, `tf$contrib$learn`, etc.). \n\n## Format\nTensorFlow module \n\n## Usage\n```r\ntf \n```\n\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \nhello <- tf$constant('Hello, TensorFlow!') \nzeros <- tf$Variable(tf$zeros(shape(1L))) \ntf$print(hello) \ntf$print(zeros) \n```\n:::\n"",
     ""supporting"": [
       ""tf_files""
     ],

---FILE: _freeze/reference/tensorflow/tf_extract_opts/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""19826b9a010b90db82bc3462da78a024"",
+  ""hash"": ""74a018ff69e70f5a6060bf96c57003fc"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/extract.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/extract.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/extract.R*\n\n# tf_extract_opts\n\n## Tensor extract options\n\n## Description\nTensor extract options \n\n\n## Usage\n```r\ntf_extract_opts( \n  style = getOption(\""tensorflow.extract.style\""), \n  ..., \n  one_based = getOption(\""tensorflow.extract.one_based\"", TRUE), \n  inclusive_stop = getOption(\""tensorflow.extract.inclusive_stop\"", TRUE), \n  disallow_out_of_bounds = getOption(\""tensorflow.extract.dissallow_out_of_bounds\"", \n    TRUE), \n  warn_tensors_passed_asis = getOption(\""tensorflow.extract.warn_tensors_passed_asis\"", \n    TRUE), \n  warn_negatives_pythonic = getOption(\""tensorflow.extract.warn_negatives_pythonic\"", \n    TRUE) \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| style | one of `NULL` (the default) `\""R\""` or `\""python\""`. If supplied, this overrides all other options. `\""python\""` is equivalent to all the other arguments being `FALSE`. `\""R\""` is equivalent to `warn_tensors_passed_asis` and `warn_negatives_pythonic`<br>set to `FALSE` |\n| ... | ignored |\n| one_based | TRUE or FALSE, if one-based indexing should be used |\n| inclusive_stop | TRUE or FALSE, if slices like `start:stop` should be inclusive of `stop` |\n| disallow_out_of_bounds | TRUE or FALSE, whether checks are performed on the slicing index to ensure it is within bounds. |\n| warn_tensors_passed_asis | TRUE or FALSE, whether to emit a warning the first time a tensor is supplied to `[` that tensors are passed as-is, with no R to python translation |\n| warn_negatives_pythonic | TRUE or FALSE, whether to emit a warning the first time a negative number is supplied to `[` about the non-standard (python-style) interpretation |\n\n\n\n## Value\nan object with class \""tf_extract_opts\"", suitable for passing to `[.tensorflow.tensor()`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$constant(1:10) \nopts <-  tf_extract_opts(\""R\"") \nx[1, options = opts] \n# or for more fine-grained control \nopts <- tf_extract_opts( \n    one_based = FALSE, \n    warn_tensors_passed_asis = FALSE, \n    warn_negatives_pythonic = FALSE \n) \nx[0:2, options = opts] \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/extract.R#L178) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tf_extract_opts\n\n## Tensor extract options\n\n## Description\nTensor extract options \n\n\n## Usage\n```r\ntf_extract_opts( \n  style = getOption(\""tensorflow.extract.style\""), \n  ..., \n  one_based = getOption(\""tensorflow.extract.one_based\"", TRUE), \n  inclusive_stop = getOption(\""tensorflow.extract.inclusive_stop\"", TRUE), \n  disallow_out_of_bounds = getOption(\""tensorflow.extract.dissallow_out_of_bounds\"", \n    TRUE), \n  warn_tensors_passed_asis = getOption(\""tensorflow.extract.warn_tensors_passed_asis\"", \n    TRUE), \n  warn_negatives_pythonic = getOption(\""tensorflow.extract.warn_negatives_pythonic\"", \n    TRUE) \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| style | one of `NULL` (the default) `\""R\""` or `\""python\""`. If supplied, this overrides all other options. `\""python\""` is equivalent to all the other arguments being `FALSE`. `\""R\""` is equivalent to `warn_tensors_passed_asis` and `warn_negatives_pythonic`<br>set to `FALSE` |\n| ... | ignored |\n| one_based | TRUE or FALSE, if one-based indexing should be used |\n| inclusive_stop | TRUE or FALSE, if slices like `start:stop` should be inclusive of `stop` |\n| disallow_out_of_bounds | TRUE or FALSE, whether checks are performed on the slicing index to ensure it is within bounds. |\n| warn_tensors_passed_asis | TRUE or FALSE, whether to emit a warning the first time a tensor is supplied to `[` that tensors are passed as-is, with no R to python translation |\n| warn_negatives_pythonic | TRUE or FALSE, whether to emit a warning the first time a negative number is supplied to `[` about the non-standard (python-style) interpretation |\n\n\n\n## Value\nan object with class \""tf_extract_opts\"", suitable for passing to `[.tensorflow.tensor()`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nx <- tf$constant(1:10) \nopts <-  tf_extract_opts(\""R\"") \nx[1, options = opts] \n# or for more fine-grained control \nopts <- tf_extract_opts( \n    one_based = FALSE, \n    warn_tensors_passed_asis = FALSE, \n    warn_negatives_pythonic = FALSE \n) \nx[0:2, options = opts] \n```\n:::\n"",
     ""supporting"": [
       ""tf_extract_opts_files""
     ],

---FILE: _freeze/reference/tensorflow/tf_probability/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""df7d31577592ae37ba5d1750117cec93"",
+  ""hash"": ""4c89631faa8434358fd723b751ab4d9b"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/probability.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/probability.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/probability.R*\n\n# tf_probability\n\n## TensorFlow Probability Module\n\n## Description\nTensorFlow Probability Module \n\n\n## Usage\n```r\ntf_probability() \n```\n\n\n\n\n## Value\n\nReference to [TensorFlow Probability](https://www.tensorflow.org/probability)\n\nfunctions and classes \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \ntfp <- tf_probability() \ntfp$distributions$Normal(loc=0, scale=1) \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/probability.R#L14) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tf_probability\n\n## TensorFlow Probability Module\n\n## Description\nTensorFlow Probability Module \n\n\n## Usage\n```r\ntf_probability() \n```\n\n\n\n\n## Value\n\nReference to [TensorFlow Probability](https://www.tensorflow.org/probability)\n\nfunctions and classes \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \ntfp <- tf_probability() \ntfp$distributions$Normal(loc=0, scale=1) \n```\n:::\n"",
     ""supporting"": [
       ""tf_probability_files""
     ],

---FILE: _freeze/reference/tensorflow/tfe_enable_eager_execution/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""be5a7c637e25779a8c1e510d5c260432"",
+  ""hash"": ""2112189356de775262c7694b9c7af50d"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/deprecated.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/deprecated.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/deprecated.R*\n\n# tfe_enable_eager_execution\n\n## (Deprecated) Enables, for the rest of the lifetime of this program, eager execution.\n\n## Description\nThis function is no longer needed since Tensorflow 2.0, when eager execution became the default. \n\n\n## Usage\n```r\ntfe_enable_eager_execution( \n  config = NULL, \n  device_policy = c(\""explicit\"", \""warn\"", \""silent\"") \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| config | (Optional) A `tf$ConfigProto()` protocol buffer with configuration options for the Context. Note that a lot of these options may be currently unimplemented or irrelevant when eager execution is enabled. |\n| device_policy | (Optional) What policy to use when trying to run an operation on a device with inputs which are not on that device. Valid values: \""explicit\"": raises an error if the placement is not correct. \""warn\"": copies the tensors which are not on the right device but raises a warning. \""silent\"": silently copies the tensors. This might hide performance problems. |\n\n## Details\n\nIf not called immediately on startup risks creating breakage and bugs. \n\nAfter eager execution is enabled, operations are executed as they are defined and tensors hold concrete values, and can be accessed as R matrices or arrays with `as.matrix()`, `as.array()`, `as.double()`, etc. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# load tensorflow and enable eager execution \nlibrary(tensorflow) \ntfe_enable_eager_execution() \n# create a random 10x10 matrix \nx <- tf$random$normal(shape(10, 10)) \n# use it in R via as.matrix() \nheatmap(as.matrix(x)) \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/deprecated.R#L39) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tfe_enable_eager_execution\n\n## (Deprecated) Enables, for the rest of the lifetime of this program, eager execution.\n\n## Description\nThis function is no longer needed since Tensorflow 2.0, when eager execution became the default. \n\n\n## Usage\n```r\ntfe_enable_eager_execution( \n  config = NULL, \n  device_policy = c(\""explicit\"", \""warn\"", \""silent\"") \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| config | (Optional) A `tf$ConfigProto()` protocol buffer with configuration options for the Context. Note that a lot of these options may be currently unimplemented or irrelevant when eager execution is enabled. |\n| device_policy | (Optional) What policy to use when trying to run an operation on a device with inputs which are not on that device. Valid values: \""explicit\"": raises an error if the placement is not correct. \""warn\"": copies the tensors which are not on the right device but raises a warning. \""silent\"": silently copies the tensors. This might hide performance problems. |\n\n## Details\n\nIf not called immediately on startup risks creating breakage and bugs. \n\nAfter eager execution is enabled, operations are executed as they are defined and tensors hold concrete values, and can be accessed as R matrices or arrays with `as.matrix()`, `as.array()`, `as.double()`, etc. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# load tensorflow and enable eager execution \nlibrary(tensorflow) \ntfe_enable_eager_execution() \n# create a random 10x10 matrix \nx <- tf$random$normal(shape(10, 10)) \n# use it in R via as.matrix() \nheatmap(as.matrix(x)) \n```\n:::\n"",
     ""supporting"": [
       ""tfe_enable_eager_execution_files""
     ],

---FILE: _freeze/reference/tensorflow/use_compat/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""582cbcd73ce5d54827f1d31aa65ed737"",
+  ""hash"": ""d5ea2d1c99f07c2e06801b6c3c4d4dea"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/compat.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/compat.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/compat.R*\n\n# use_compat\n\n## Use Compatibility\n\n## Description\nEnables TensorFlow to run under a different API version for compatibility with previous versions. For instance, this is useful to run TensorFlow 1.x code when using TensorFlow 2.x. \n\n\n## Usage\n```r\nuse_compat(version = c(\""v1\"", \""v2\"")) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| version | The version to activate. Must be `\""v1\""` or `\""v2\""` |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \nuse_compat(\""v1\"") \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/compat.R#L17) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# use_compat\n\n## Use Compatibility\n\n## Description\nEnables TensorFlow to run under a different API version for compatibility with previous versions. For instance, this is useful to run TensorFlow 1.x code when using TensorFlow 2.x. \n\n\n## Usage\n```r\nuse_compat(version = c(\""v1\"", \""v2\"")) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| version | The version to activate. Must be `\""v1\""` or `\""v2\""` |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \nuse_compat(\""v1\"") \n```\n:::\n"",
     ""supporting"": [
       ""use_compat_files""
     ],

---FILE: _freeze/reference/tensorflow/use_session_with_seed/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""3318ed17f4d3f39f1c2ed2824c1402ec"",
+  ""hash"": ""5fe95798a4f77534db2bf6d3a56dd0ac"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/seed.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tensorflow//edit/main/R/seed.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/seed.R*\n\n# use_session_with_seed\n\n## Use a session with a random seed\n\n## Description\nSet various random seeds required to ensure reproducible results. The provided `seed` value will establish a new random seed for R, Python, NumPy, and TensorFlow. GPU computations and CPU parallelism will also be disabled by default. \n\n\n## Usage\n```r\nuse_session_with_seed( \n  seed, \n  disable_gpu = TRUE, \n  disable_parallel_cpu = TRUE, \n  quiet = FALSE \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| seed | A single value, interpreted as an integer |\n| disable_gpu | `TRUE` to disable GPU execution (see **Parallelism** below). |\n| disable_parallel_cpu | `TRUE` to disable CPU parallelism (see **Parallelism** below). |\n| quiet | `TRUE` to suppress printing of messages. |\n\n## Details\n\nThis function must be called at the very top of your script (i.e. immediately after `library(tensorflow)`, `library(keras)`, etc.). Any existing TensorFlow session is torn down via `tf$reset_default_graph()`. \n\nThis function takes all measures known to promote reproducible results from TensorFlow sessions, however it's possible that various individual TensorFlow features or dependent libraries escape its effects. If you encounter non-reproducible results please investigate the possible sources of the problem, contributions via pull request are very welcome! \n\nPackages which need to be notified before and after the seed is set can register for the \""tensorflow.on_before_use_session\"" and \""tensorflow.on_use_session\"" hooks (see `setHook()`) for additional details on hooks). \n\n## Section\n\n## Parallelism\n\n By default the `use_session_with_seed()` function disables GPU and CPU parallelism, since both can result in non-deterministic execution patterns (see [https://stackoverflow.com/questions/42022950/](https://stackoverflow.com/questions/42022950/)). You can optionally enable GPU or CPU parallelism by setting the `disable_gpu` and/or `disable_parallel_cpu` parameters to `FALSE`. \n\n## Value\nTensorFlow session object, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \nuse_session_with_seed(42) \n```\n:::\n"",
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tensorflow//blob/main/R/seed.R#L47) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# use_session_with_seed\n\n## Use a session with a random seed\n\n## Description\nSet various random seeds required to ensure reproducible results. The provided `seed` value will establish a new random seed for R, Python, NumPy, and TensorFlow. GPU computations and CPU parallelism will also be disabled by default. \n\n\n## Usage\n```r\nuse_session_with_seed( \n  seed, \n  disable_gpu = TRUE, \n  disable_parallel_cpu = TRUE, \n  quiet = FALSE \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| seed | A single value, interpreted as an integer |\n| disable_gpu | `TRUE` to disable GPU execution (see **Parallelism** below). |\n| disable_parallel_cpu | `TRUE` to disable CPU parallelism (see **Parallelism** below). |\n| quiet | `TRUE` to suppress printing of messages. |\n\n## Details\n\nThis function must be called at the very top of your script (i.e. immediately after `library(tensorflow)`, `library(keras)`, etc.). Any existing TensorFlow session is torn down via `tf$reset_default_graph()`. \n\nThis function takes all measures known to promote reproducible results from TensorFlow sessions, however it's possible that various individual TensorFlow features or dependent libraries escape its effects. If you encounter non-reproducible results please investigate the possible sources of the problem, contributions via pull request are very welcome! \n\nPackages which need to be notified before and after the seed is set can register for the \""tensorflow.on_before_use_session\"" and \""tensorflow.on_use_session\"" hooks (see `setHook()`) for additional details on hooks). \n\n## Section\n\n## Parallelism\n\n By default the `use_session_with_seed()` function disables GPU and CPU parallelism, since both can result in non-deterministic execution patterns (see [https://stackoverflow.com/questions/42022950/](https://stackoverflow.com/questions/42022950/)). You can optionally enable GPU or CPU parallelism by setting the `disable_gpu` and/or `disable_parallel_cpu` parameters to `FALSE`. \n\n## Value\nTensorFlow session object, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow) \nuse_session_with_seed(42) \n```\n:::\n"",
     ""supporting"": [
       ""use_session_with_seed_files""
     ],

---FILE: _freeze/reference/tfautograph/ag_if_vars/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""d56a379f830d0cfdf2ab5b67e1c07491"",
+  ""hash"": ""70fa7b537b7c754848ec27a4af741453"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/hints.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/hints.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/hints.R*\n\n# ag_if_vars\n\n## Specify `tf.cond()` output structure when autographing `if`\n\n## Description\n\nThis function can be used to specify the output structure from `tf.cond()`\n\nwhen autographing an `if` statement. In most use cases, use of this function is purely optional. If not supplied, the `if` output structure is automatically built. \n\n\n## Usage\n```r\nag_if_vars( \n  ..., \n  modified = list(), \n  return = FALSE, \n  undefs = NULL, \n  control_flow = 0 \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Variables modified by the `tf.cond()` node supplied as bare symbols like `foo` or expressions using `$` e.g, `foo$bar`. Symbols do not have to exist before the autographed `if` so long as they are created in both branches. |\n| modified | Variables names supplied as a character vector, or a list of character vectors if specifying nested complex structures. This is an escape hatch for the lazy evaluation semantics of `...` |\n| return | logical, whether to include the return value the evaluated R expression in the `tf.cond()`. if `FALSE` (the default), only the objects assigned in scope are captured. |\n| undefs | A bare character vector or a list of character vectors. Supplied names are exported as undefs in the parent frame. This is used to give a more informative error message when attempting to access a variable that can't be balanced between branches. |\n| control_flow | An integer, the maximum number of control-flow statements (`break` and/or `next`) that will be captured in a single branch as part of the `tf.cond()`. Do not count statements in loops that are dispatching to standard R control flow (e.g., don't count `break` statements in a `for`<br>loop that is iterating over an R vector) |\n\n## Details\n\nIf the output structure is not explicitly supplied via `ag_if_vars()`, then the output structure is automatically composed: The true and false branches of the expression are traced into concrete functions, then the output signature from the two branch functions are balanced. Balancing is performed by either fetching a variable from an outer scope or by reclassifying a symbol as an undef. \n\nWhen dealing with complex composites (that is, nested structures where a modified tensor is part of a named list or dictionary), care is taken to prevent unnecessarily capturing other unmodified tensors in the structure. This is done by pruning unmodified tensors from the returned output structure, and then merging them back with the original object recursively. One limitation of the implementation is that lists must either be fully named with unique names, or not named at all, partially named lists or duplicated names in a list throw an error. This is due to the conversion that happens when going between python and R: named lists get converted to python dictionaries, which require that all keys are unique. Additionally, pruning of unmodified objects from an autographed `if` is currently only supported for named lists (python dictionaries). Unnamed lists or tuples are passed as is (e.g, no pruning and merging done), which may lead to unnecessarily bloat in the constructed graphs. \n\n\n## Value\n`NULL`, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# these examples only have an effect in graph mode \n# to enter graph mode easily we'll create a few helpers \nag <- autograph \n# pass which symbols you expect to be modifed or created liks this: \nag_if_vars(x) \nag(if (y > 0) { \n  x <- y * y \n} else { \n  x <- y \n}) \n# if the return value from the if expression is important, pass `return = TRUE` \nag_if_vars(return = TRUE) \nx <- ag(if(y > 0) y * y else y) \n# pass complex nested structures like this \nx <- list(a = 1, b = 2) \nag_if_vars(x$a) \nag(if(y > 0) { \n  x$a <- y \n}) \n# undefs are for mark branch-local variables \nag_if_vars(y, x$a, undef = \""tmp_local_var\"") \nag(if(y > 0) { \n  y <- y * 100 \n  tmp_local_var <- y + 1 \n  x$a <- tmp_local_var \n}) \n# supplying `undef` is not necessary, it exists purely as a way to supply a \n# guardrail for defensive programming and/or to improve code readability \n## modified vars can be supplied in `...` or as a named arg. \n## these paires of ag_if_vars() calls are equivalent \nag_if_vars(y, x$a) \nag_if_vars(modified = list(\""y\"", c(\""x\"", \""a\""))) \nag_if_vars(x, y, z) \nag_if_vars(modified = c(\""x\"", \""y\"", \""z\"")) \n## control flow \n# count number of odds between 0:10 \nag({ \n  x <- 10 \n  count <- 0 \n  while(x > 0) { \n    ag_if_vars(control_flow = 1) \n    if(x %% 2 == 0) \n      next \n    count <- count + 1 \n  } \n}) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/hints.R#L115) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# ag_if_vars\n\n## Specify `tf.cond()` output structure when autographing `if`\n\n## Description\n\nThis function can be used to specify the output structure from `tf.cond()`\n\nwhen autographing an `if` statement. In most use cases, use of this function is purely optional. If not supplied, the `if` output structure is automatically built. \n\n\n## Usage\n```r\nag_if_vars( \n  ..., \n  modified = list(), \n  return = FALSE, \n  undefs = NULL, \n  control_flow = 0 \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Variables modified by the `tf.cond()` node supplied as bare symbols like `foo` or expressions using `$` e.g, `foo$bar`. Symbols do not have to exist before the autographed `if` so long as they are created in both branches. |\n| modified | Variables names supplied as a character vector, or a list of character vectors if specifying nested complex structures. This is an escape hatch for the lazy evaluation semantics of `...` |\n| return | logical, whether to include the return value the evaluated R expression in the `tf.cond()`. if `FALSE` (the default), only the objects assigned in scope are captured. |\n| undefs | A bare character vector or a list of character vectors. Supplied names are exported as undefs in the parent frame. This is used to give a more informative error message when attempting to access a variable that can't be balanced between branches. |\n| control_flow | An integer, the maximum number of control-flow statements (`break` and/or `next`) that will be captured in a single branch as part of the `tf.cond()`. Do not count statements in loops that are dispatching to standard R control flow (e.g., don't count `break` statements in a `for`<br>loop that is iterating over an R vector) |\n\n## Details\n\nIf the output structure is not explicitly supplied via `ag_if_vars()`, then the output structure is automatically composed: The true and false branches of the expression are traced into concrete functions, then the output signature from the two branch functions are balanced. Balancing is performed by either fetching a variable from an outer scope or by reclassifying a symbol as an undef. \n\nWhen dealing with complex composites (that is, nested structures where a modified tensor is part of a named list or dictionary), care is taken to prevent unnecessarily capturing other unmodified tensors in the structure. This is done by pruning unmodified tensors from the returned output structure, and then merging them back with the original object recursively. One limitation of the implementation is that lists must either be fully named with unique names, or not named at all, partially named lists or duplicated names in a list throw an error. This is due to the conversion that happens when going between python and R: named lists get converted to python dictionaries, which require that all keys are unique. Additionally, pruning of unmodified objects from an autographed `if` is currently only supported for named lists (python dictionaries). Unnamed lists or tuples are passed as is (e.g, no pruning and merging done), which may lead to unnecessarily bloat in the constructed graphs. \n\n\n## Value\n`NULL`, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\n# these examples only have an effect in graph mode \n# to enter graph mode easily we'll create a few helpers \nag <- autograph \n# pass which symbols you expect to be modifed or created liks this: \nag_if_vars(x) \nag(if (y > 0) { \n  x <- y * y \n} else { \n  x <- y \n}) \n# if the return value from the if expression is important, pass `return = TRUE` \nag_if_vars(return = TRUE) \nx <- ag(if(y > 0) y * y else y) \n# pass complex nested structures like this \nx <- list(a = 1, b = 2) \nag_if_vars(x$a) \nag(if(y > 0) { \n  x$a <- y \n}) \n# undefs are for mark branch-local variables \nag_if_vars(y, x$a, undef = \""tmp_local_var\"") \nag(if(y > 0) { \n  y <- y * 100 \n  tmp_local_var <- y + 1 \n  x$a <- tmp_local_var \n}) \n# supplying `undef` is not necessary, it exists purely as a way to supply a \n# guardrail for defensive programming and/or to improve code readability \n## modified vars can be supplied in `...` or as a named arg. \n## these paires of ag_if_vars() calls are equivalent \nag_if_vars(y, x$a) \nag_if_vars(modified = list(\""y\"", c(\""x\"", \""a\""))) \nag_if_vars(x, y, z) \nag_if_vars(modified = c(\""x\"", \""y\"", \""z\"")) \n## control flow \n# count number of odds between 0:10 \nag({ \n  x <- 10 \n  count <- 0 \n  while(x > 0) { \n    ag_if_vars(control_flow = 1) \n    if(x %% 2 == 0) \n      next \n    count <- count + 1 \n  } \n}) \n```\n:::\n"",
+    ""supporting"": [
+      ""ag_if_vars_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/ag_loop_vars/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""e7932050a14fd7322141b9584204db61"",
+  ""hash"": ""cb05ebda2d325af6e2745166850615bc"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/hints.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/hints.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/hints.R*\n\n# ag_loop_vars\n\n## Specify loop variables\n\n## Description\nThis can be used to manually specify which variables are to be included explicitly as `loop_vars` when autographing an expression into a `tf.while_loop()` call, or the `loop_vars` equivalent when building a `dataset.reduce()`. \n\n\n## Usage\n```r\nag_loop_vars( \n  ..., \n  list = character(), \n  include = character(), \n  exclude = character(), \n  undef = character() \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Variables as bare symbol names |\n| list, include, exclude | optionally, the variable names as a character vector (use this as an escape hatch from the `...` lazy evaluation semantics). |\n| undef | character vector of symbols |\n\n## Details\n\nUse of this is usually not required as the loop variables are automatically inferred. Inference is done by statically looking through the loop body and finding the symbols that are the targets of the common assignment operators from base R (`<-`, `->`, `=`), from package:zeallot (`%<-%` and `%->%`) and package:magrittr (`%<>%`). \n\nIn certain circumstances, this approach may capture variables that are intended to be local variables only. In those circumstances it is also possible to specify them preceded with a `-`. \n\nNote, the specified loop vars are expected to exist before the autographed expression, and a warning is issued otherwise (usually immediately preceding an error thrown when attempting to actually autograph the expression) \n\nOnly bare symbol names can be supplied as loop vars. In the future, support may be expanded to allow for nested complex composites (e.g., specifying variables that are nested within a more complex structure--passing `ag_loop_vars(foo$bar$baz)` is currently not supported.) \n\n\n## Value\nthe specified hint invisibly. \n\n## Note\nThe semantics of this function are inspired by base::rm() \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ni <- tf$constant(0L) \nautograph({ \n  ag_loop_vars(x, i) \n  while(x > 0) { \n    if(x %%2 == 0) \n      i <- i + 1L \n    x <- x - 1 \n  } \n}) \n## sometimes, a variable is infered to be a loop_var unnecessarily. For example \nx <- tf$constant(1:10) \n# imagine x is left over in the current scope from some previous calculations \n# It's value is not important, but it exists \nautograph({ \n  for(i in tf$constant(1:6)) { \n    x <- i * i \n    tf$print(x) \n  } \n}) \n# this will throw an error because `x` was infered to be a `loop_var`, \n# but it's shape witin the loop body is different from what it was before. \n# there are two solutions to prevent `x` from being captured as a loop_var \n## 1) remove `x` from the current scope like so: \nrm(x) \n## 2) provide a hint like so: \nag_loop_vars(-x) \n## if your variable names are being dynamically generated, there is an \n## escape hatch for the lazy evaluation semantics of ... \nag_loop_vars(exclude = \""x\"") \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/hints.R#L218) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# ag_loop_vars\n\n## Specify loop variables\n\n## Description\nThis can be used to manually specify which variables are to be included explicitly as `loop_vars` when autographing an expression into a `tf.while_loop()` call, or the `loop_vars` equivalent when building a `dataset.reduce()`. \n\n\n## Usage\n```r\nag_loop_vars( \n  ..., \n  list = character(), \n  include = character(), \n  exclude = character(), \n  undef = character() \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Variables as bare symbol names |\n| list, include, exclude | optionally, the variable names as a character vector (use this as an escape hatch from the `...` lazy evaluation semantics). |\n| undef | character vector of symbols |\n\n## Details\n\nUse of this is usually not required as the loop variables are automatically inferred. Inference is done by statically looking through the loop body and finding the symbols that are the targets of the common assignment operators from base R (`<-`, `->`, `=`), from package:zeallot (`%<-%` and `%->%`) and package:magrittr (`%<>%`). \n\nIn certain circumstances, this approach may capture variables that are intended to be local variables only. In those circumstances it is also possible to specify them preceded with a `-`. \n\nNote, the specified loop vars are expected to exist before the autographed expression, and a warning is issued otherwise (usually immediately preceding an error thrown when attempting to actually autograph the expression) \n\nOnly bare symbol names can be supplied as loop vars. In the future, support may be expanded to allow for nested complex composites (e.g., specifying variables that are nested within a more complex structure--passing `ag_loop_vars(foo$bar$baz)` is currently not supported.) \n\n\n## Value\nthe specified hint invisibly. \n\n## Note\nThe semantics of this function are inspired by base::rm() \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\ni <- tf$constant(0L) \nautograph({ \n  ag_loop_vars(x, i) \n  while(x > 0) { \n    if(x %%2 == 0) \n      i <- i + 1L \n    x <- x - 1 \n  } \n}) \n## sometimes, a variable is infered to be a loop_var unnecessarily. For example \nx <- tf$constant(1:10) \n# imagine x is left over in the current scope from some previous calculations \n# It's value is not important, but it exists \nautograph({ \n  for(i in tf$constant(1:6)) { \n    x <- i * i \n    tf$print(x) \n  } \n}) \n# this will throw an error because `x` was infered to be a `loop_var`, \n# but it's shape witin the loop body is different from what it was before. \n# there are two solutions to prevent `x` from being captured as a loop_var \n## 1) remove `x` from the current scope like so: \nrm(x) \n## 2) provide a hint like so: \nag_loop_vars(-x) \n## if your variable names are being dynamically generated, there is an \n## escape hatch for the lazy evaluation semantics of ... \nag_loop_vars(exclude = \""x\"") \n```\n:::\n"",
+    ""supporting"": [
+      ""ag_loop_vars_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/ag_name/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""b46565fa3dfc794a1dead009a63d2020"",
+  ""hash"": ""ce277e250cc1b0e7ebce8edbe1fc30d4"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/opts.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/opts.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/opts.R*\n\n# ag_name\n\n## Specify a tensor name\n\n## Description\nThis can be used before any autographed expression that results in the creation of a tensor or op graph node. This can be used before `for` (both with tensors and datasets), `while`, and `if` statements. \n\n\n## Usage\n```r\nag_name(x) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A string |\n\n\n\n## Value\n`x`, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n## when you're in graph mode. (e.g, tf$executing_eagerly == FALSE) \nag_name(\""main-training-loop\"") \nfor(elem in dataset) ... \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/opts.R#L20) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# ag_name\n\n## Specify a tensor name\n\n## Description\nThis can be used before any autographed expression that results in the creation of a tensor or op graph node. This can be used before `for` (both with tensors and datasets), `while`, and `if` statements. \n\n\n## Usage\n```r\nag_name(x) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | A string |\n\n\n\n## Value\n`x`, invisibly \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\n## when you're in graph mode. (e.g, tf$executing_eagerly == FALSE) \nag_name(\""main-training-loop\"") \nfor(elem in dataset) ... \n```\n:::\n"",
+    ""supporting"": [
+      ""ag_name_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/ag_while_opts/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""6b455718c58a239f1a2bc0ad8b7b3b20"",
+  ""hash"": ""a9c3c36c01c13dd6de085911bb175552"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/opts.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/opts.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/opts.R*\n\n# ag_while_opts\n\n## specify `tf.while_loop` options\n\n## Description\nSee https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/while_loop for additional details. \n\n\n## Usage\n```r\nag_while_opts( \n  ..., \n  shape_invariants = NULL, \n  parallel_iterations = 10L, \n  back_prop = TRUE, \n  swap_memory = FALSE, \n  maximum_iterations = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Ignored, used to ensure all arguments supplied are named. |\n| shape_invariants | The shape invariants for the loop variables. |\n| parallel_iterations | The number of iterations allowed to run in parallel. It must be a positive integer. |\n| back_prop | Deprecated (optional). `FALSE` disables support for back propagation. Prefer using `tf$stop_gradient` instead. |\n| swap_memory | Whether GPU-CPU memory swap is enabled for this loop. |\n| maximum_iterations | Optional maximum number of iterations of the while loop to run. If provided, the `cond` output is AND-ed with an additional condition ensuring the number of iterations executed is no greater than `maximum_iterations`. |\n\n\n\n## Value\n`NULL`` invisibly, called for it's side effect. \n\n## Note\n\nUse `ag_name()` to supply `name` and `ag_loop_vars()` to supply `loop_vars` directly. \n\nThis is only applicable when autograph in graph mode, otherwise this has no effect. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n## use tf_function() to enter graph mode: \ntf_function(autograph(function(n) { \n  ag_name(\""silly-example\"") \n  ag_while_opts(back_prop = FALSE) \n  while(n > 0) \n    n <- n - 1 \n})) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/opts.R#L60) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# ag_while_opts\n\n## specify `tf.while_loop` options\n\n## Description\nSee https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/while_loop for additional details. \n\n\n## Usage\n```r\nag_while_opts( \n  ..., \n  shape_invariants = NULL, \n  parallel_iterations = 10L, \n  back_prop = TRUE, \n  swap_memory = FALSE, \n  maximum_iterations = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | Ignored, used to ensure all arguments supplied are named. |\n| shape_invariants | The shape invariants for the loop variables. |\n| parallel_iterations | The number of iterations allowed to run in parallel. It must be a positive integer. |\n| back_prop | Deprecated (optional). `FALSE` disables support for back propagation. Prefer using `tf$stop_gradient` instead. |\n| swap_memory | Whether GPU-CPU memory swap is enabled for this loop. |\n| maximum_iterations | Optional maximum number of iterations of the while loop to run. If provided, the `cond` output is AND-ed with an additional condition ensuring the number of iterations executed is no greater than `maximum_iterations`. |\n\n\n\n## Value\n`NULL`` invisibly, called for it's side effect. \n\n## Note\n\nUse `ag_name()` to supply `name` and `ag_loop_vars()` to supply `loop_vars` directly. \n\nThis is only applicable when autograph in graph mode, otherwise this has no effect. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\n## use tf_function() to enter graph mode: \ntf_function(autograph(function(n) { \n  ag_name(\""silly-example\"") \n  ag_while_opts(back_prop = FALSE) \n  while(n > 0) \n    n <- n - 1 \n})) \n```\n:::\n"",
+    ""supporting"": [
+      ""ag_while_opts_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/sub-subset-.tensorflow.python.ops.tensor_array_ops.tensorarray/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""49ac5e2f53f56f60674e2aa0e0b37dd0"",
+  ""hash"": ""243e51e5d458e81b65a772d1ad6eacb5"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/methods.R*\n\n# [[<-.tensorflow.python.ops.tensor_array_ops.TensorArray\n\n## `TensorArray.write()`\n\n## Description\n`TensorArray.write()`\n\n\n## Usage\n```r\n## S3 method for class 'tensorflow.python.ops.tensor_array_ops.TensorArray'\n[[\n(ta, i, ..., name = NULL) <- value \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ta | a tensorflow `TensorArray` |\n| i | something castable to an int32 scalar tensor. 0-based. |\n| ... | Error if anything is passed to `...` |\n| name | A scalar string, name of the op |\n| value | The value to write. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nta <- tf$TensorArray(tf$float32, size = 5L) \nfor(i in 0:4) \n  ta[[i]] <- i \nta$stack() \n# You can use this to grow objects in graph mode \naccuracies_log <- tf$TensorArray(tf$float32, size = 0L, dynamic_size=TRUE) \nfor(epoch in 0:4) \n  accuracies_log[[epoch]] <- runif(1) \nacc <- accuracies_log$stack() \nacc \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/methods.R#L) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# [[<-.tensorflow.python.ops.tensor_array_ops.TensorArray\n\n## `TensorArray.write()`\n\n## Description\n`TensorArray.write()`\n\n\n## Usage\n```r\n## S3 method for class 'tensorflow.python.ops.tensor_array_ops.TensorArray'\n[[(ta, i, ..., name = NULL) <- value \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ta | a tensorflow `TensorArray` |\n| i | something castable to an int32 scalar tensor. 0-based. |\n| ... | Error if anything is passed to `...` |\n| name | A scalar string, name of the op |\n| value | The value to write. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\nta <- tf$TensorArray(tf$float32, size = 5L) \nfor(i in 0:4) \n  ta[[i]] <- i \nta$stack() \n# You can use this to grow objects in graph mode \naccuracies_log <- tf$TensorArray(tf$float32, size = 0L, dynamic_size=TRUE) \nfor(epoch in 0:4) \n  accuracies_log[[epoch]] <- runif(1) \nacc <- accuracies_log$stack() \nacc \n```\n:::\n"",
+    ""supporting"": [
+      ""sub-subset-.tensorflow.python.ops.tensor_array_ops.tensorarray_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/tf_assert/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""cb308c7711a1ef337fade702e3f033f6"",
+  ""hash"": ""e5db7008d5fad48a82cd9afee1c570fb"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf_assert.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/tf_assert.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/tf_assert.R*\n\n# tf_assert\n\n## tf_assert\n\n## Description\n\nA thin wrapper around `tf$Assert()` that automatically constructs an informative error message (passed on to `data` argument), which includes the expression passed to `condition`, the values of the symbols found in the expression, as well as the full R call stack at the time the `tf$Assert()`\n\nnode is created. \n\n\n## Usage\n```r\ntf_assert( \n  condition, \n  ..., \n  expr = substitute(condition), \n  summarize = NULL, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| condition | A boolean tensor |\n| ... | Additional elements passed on to `data`. (e.g, an informative error message as a string, additional tensor values that might be useful to have in the error message, etc.) |\n| expr | A language object, provided in case `condition` is already computed prior to the call |\n| summarize | Print this many entries of each tensor. |\n| name | A name for this operation (optional). |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$constant(-1) \ntry(tf_assert(x > 0, \""oopsies! x must be greater than 0\"")) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf_assert.R#L26) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tf_assert\n\n## tf_assert\n\n## Description\n\nA thin wrapper around `tf$Assert()` that automatically constructs an informative error message (passed on to `data` argument), which includes the expression passed to `condition`, the values of the symbols found in the expression, as well as the full R call stack at the time the `tf$Assert()`\n\nnode is created. \n\n\n## Usage\n```r\ntf_assert( \n  condition, \n  ..., \n  expr = substitute(condition), \n  summarize = NULL, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| condition | A boolean tensor |\n| ... | Additional elements passed on to `data`. (e.g, an informative error message as a string, additional tensor values that might be useful to have in the error message, etc.) |\n| expr | A language object, provided in case `condition` is already computed prior to the call |\n| summarize | Print this many entries of each tensor. |\n| name | A name for this operation (optional). |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\nx <- tf$constant(-1) \ntry(tf_assert(x > 0, \""oopsies! x must be greater than 0\"")) \n```\n:::\n"",
+    ""supporting"": [
+      ""tf_assert_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/tf_case/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""4a1708589df00b878b8f1e2ef1bd65d6"",
+  ""hash"": ""c9933b03554e44b654e0ac93b0a320ea"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf-ctrl-flow-wrappers.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/tf-ctrl-flow-wrappers.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/tf-ctrl-flow-wrappers.R*\n\n# tf_case\n\n## tf.case\n\n## Description\nThis is a minimal wrapper around `tf.case()` that allows you to supply the `pred_fn_pairs` using the `~`. \n\n\n## Usage\n```r\ntf_case( \n  ..., \n  pred_fn_pairs = list(...), \n  default = NULL, \n  exclusive = FALSE, \n  name = \""case\"" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ..., pred_fn_pairs | a list `pred_fn_pairs` supplied with the `~` like so: `pred ~ fn_body` |\n| default | a function, optionally specified with the `~`, (or something coercible to a function via `as.function()`) |\n| exclusive | bool, whether to evaluate all `preds` and ensure only one is true. If `FALSE` (the default), then the `preds` are evaluated in the order supplied until the first `TRUE` value is encountered (effectively, acting as an `if()... else if() ... else if() ...` chain) |\n| name | a string, passed on to `tf.case()` |\n\n\n\n## Value\nThe result from `tf$case()`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nfizz_buzz_one <- function(x) { \n  tf_case( \n    x %% 15 == 0 ~ \""FizzBuzz\"", \n    x %%  5 == 0 ~ \""Buzz\"", \n    x %%  3 == 0 ~ \""Fizz\"", \n    default = ~ tf$as_string(x, precision = 0L) \n  ) \n} \nfn <- tf_function(autograph(function(n) { \n  for(e in tf$range(n)) \n    tf$print(fizz_buzz_one(e)) \n})) \nx <- tf$constant(16) \nfn(x) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf-ctrl-flow-wrappers.R#L94) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tf_case\n\n## tf.case\n\n## Description\nThis is a minimal wrapper around `tf.case()` that allows you to supply the `pred_fn_pairs` using the `~`. \n\n\n## Usage\n```r\ntf_case( \n  ..., \n  pred_fn_pairs = list(...), \n  default = NULL, \n  exclusive = FALSE, \n  name = \""case\"" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ..., pred_fn_pairs | a list `pred_fn_pairs` supplied with the `~` like so: `pred ~ fn_body` |\n| default | a function, optionally specified with the `~`, (or something coercible to a function via `as.function()`) |\n| exclusive | bool, whether to evaluate all `preds` and ensure only one is true. If `FALSE` (the default), then the `preds` are evaluated in the order supplied until the first `TRUE` value is encountered (effectively, acting as an `if()... else if() ... else if() ...` chain) |\n| name | a string, passed on to `tf.case()` |\n\n\n\n## Value\nThe result from `tf$case()`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\nfizz_buzz_one <- function(x) { \n  tf_case( \n    x %% 15 == 0 ~ \""FizzBuzz\"", \n    x %%  5 == 0 ~ \""Buzz\"", \n    x %%  3 == 0 ~ \""Fizz\"", \n    default = ~ tf$as_string(x, precision = 0L) \n  ) \n} \nfn <- tf_function(autograph(function(n) { \n  for(e in tf$range(n)) \n    tf$print(fizz_buzz_one(e)) \n})) \nx <- tf$constant(16) \nfn(x) \n```\n:::\n"",
+    ""supporting"": [
+      ""tf_case_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/tf_cond/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""1ddfabd42cf0c6f92057f62af4aaa898"",
+  ""hash"": ""e42c5780d2966dfebb4efd2a7572b7b8"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf-ctrl-flow-wrappers.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/tf-ctrl-flow-wrappers.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/tf-ctrl-flow-wrappers.R*\n\n# tf_cond\n\n## tf.cond\n\n## Description\nThis is a minimal wrapper around `tf$cond()` that allows you to supply `true_fn` and `false_fn` as lambda functions defined using the tilde `~`. \n\n\n## Usage\n```r\ntf_cond(pred, true_fn, false_fn, name = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| pred | R logical or a tensor. |\n| true_fn, false_fn | a `~` function, a function, or something coercible to a function via `as.function` |\n| name | a string, passed on to `tf.cond()` |\n\n\n\n## Value\nif cond is a tensor, then the result of `tf.cond()`. Otherwise, if `pred` is an `EagerTensor` or an R logical, then the result of either `true_fn()` or `false_fn()`\n\n## Note\nin Tensorflow version 1, the `strict` keyword argument is supplied with a value of `TRUE` (different from the default) \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n## square if positive \n# using tf$cond directly: \nraw <- function(x) tf$cond(x > 0, function() x * x, function() x) \n# using tf_cond() wrapper \ntilde <- function(x) tf_cond(x > 0, ~ x * x, ~ x) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf-ctrl-flow-wrappers.R#L29) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tf_cond\n\n## tf.cond\n\n## Description\nThis is a minimal wrapper around `tf$cond()` that allows you to supply `true_fn` and `false_fn` as lambda functions defined using the tilde `~`. \n\n\n## Usage\n```r\ntf_cond(pred, true_fn, false_fn, name = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| pred | R logical or a tensor. |\n| true_fn, false_fn | a `~` function, a function, or something coercible to a function via `as.function` |\n| name | a string, passed on to `tf.cond()` |\n\n\n\n## Value\nif cond is a tensor, then the result of `tf.cond()`. Otherwise, if `pred` is an `EagerTensor` or an R logical, then the result of either `true_fn()` or `false_fn()`\n\n## Note\nin Tensorflow version 1, the `strict` keyword argument is supplied with a value of `TRUE` (different from the default) \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\n## square if positive \n# using tf$cond directly: \nraw <- function(x) tf$cond(x > 0, function() x * x, function() x) \n# using tf_cond() wrapper \ntilde <- function(x) tf_cond(x > 0, ~ x * x, ~ x) \n```\n:::\n"",
+    ""supporting"": [
+      ""tf_cond_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/tf_switch/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""295687a08380f2613d36eab61c748bc0"",
+  ""hash"": ""4bd603ad68e3da1c9dd94f488cd5543d"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf-ctrl-flow-wrappers.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/tf-ctrl-flow-wrappers.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/tf-ctrl-flow-wrappers.R*\n\n# tf_switch\n\n## tf.switch_case\n\n## Description\ntf.switch_case \n\n\n## Usage\n```r\ntf_switch( \n  branch_index, \n  ..., \n  branch_fns = list(...), \n  default = NULL, \n  name = \""switch_case\"" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| branch_index | an integer tensor |\n| ..., branch_fns | a list of function bodies specified with a `~`, optionally supplied with a branch index on the left hand side. See examples |\n| default | A function defined with a `~`, or something coercible via `as.function()`` |\n| name | a string, passed on to `tf.switch_case()` |\n\n\n\n## Value\nThe result from `tf.switch_case()`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ntf_pow <- tf_function(function(x, pow) { \n   tf_switch(pow, \n   0 ~ 1, \n   1 ~ x, \n   2 ~ x * x, \n   3 ~ x * x * x, \n   default = ~ -1) \n}) \n# can optionally also omit the left hand side int, in which case the order of \n# the functions is used. \ntf_pow <- function(x, pow) { \n  tf_switch(pow, \n            ~ 1, \n            ~ x, \n            ~ x * x, \n            ~ x * x * x, \n            default = ~ -1) \n} \n# supply just some of the ints to override the default order \ntf_pow <- function(x, pow) { \n  tf_switch(pow, \n            3 ~ x * x * x, \n            2 ~ x * x, \n            ~ 1, \n            ~ x, \n            default = ~ -1) \n} \n# A slightly less contrived example: \ntf_norm <- tf_function(function(x, l) { \n  tf_switch(l, \n            0 ~ tf$reduce_sum(tf$cast(x != 0, tf$float32)), # L0 norm \n            1 ~ tf$reduce_sum(tf$abs(x)),                   # L1 norm \n            2 ~ tf$sqrt(tf$reduce_sum(tf$square(x))),       # L2 norm \n            default = ~ tf$reduce_max(tf$abs(x)))         # L-infinity norm \n}) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/tf-ctrl-flow-wrappers.R#L180) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tf_switch\n\n## tf.switch_case\n\n## Description\ntf.switch_case \n\n\n## Usage\n```r\ntf_switch( \n  branch_index, \n  ..., \n  branch_fns = list(...), \n  default = NULL, \n  name = \""switch_case\"" \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| branch_index | an integer tensor |\n| ..., branch_fns | a list of function bodies specified with a `~`, optionally supplied with a branch index on the left hand side. See examples |\n| default | A function defined with a `~`, or something coercible via `as.function()`` |\n| name | a string, passed on to `tf.switch_case()` |\n\n\n\n## Value\nThe result from `tf.switch_case()`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\ntf_pow <- tf_function(function(x, pow) { \n   tf_switch(pow, \n   0 ~ 1, \n   1 ~ x, \n   2 ~ x * x, \n   3 ~ x * x * x, \n   default = ~ -1) \n}) \n# can optionally also omit the left hand side int, in which case the order of \n# the functions is used. \ntf_pow <- function(x, pow) { \n  tf_switch(pow, \n            ~ 1, \n            ~ x, \n            ~ x * x, \n            ~ x * x * x, \n            default = ~ -1) \n} \n# supply just some of the ints to override the default order \ntf_pow <- function(x, pow) { \n  tf_switch(pow, \n            3 ~ x * x * x, \n            2 ~ x * x, \n            ~ 1, \n            ~ x, \n            default = ~ -1) \n} \n# A slightly less contrived example: \ntf_norm <- tf_function(function(x, l) { \n  tf_switch(l, \n            0 ~ tf$reduce_sum(tf$cast(x != 0, tf$float32)), # L0 norm \n            1 ~ tf$reduce_sum(tf$abs(x)),                   # L1 norm \n            2 ~ tf$sqrt(tf$reduce_sum(tf$square(x))),       # L2 norm \n            default = ~ tf$reduce_max(tf$abs(x)))         # L-infinity norm \n}) \n```\n:::\n"",
+    ""supporting"": [
+      ""tf_switch_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfautograph/view_function_graph/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""646b3e6e396e0bcd3dc2c70b7957ee41"",
+  ""hash"": ""62329f29762182c302ddd7df753c21a6"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/view.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/t-kalinowski/tfautograph//edit/main/R/view.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/view.R*\n\n# view_function_graph\n\n## Visualizes the generated graph\n\n## Description\nVisualizes the generated graph \n\n\n## Usage\n```r\nview_function_graph( \n  fn, \n  args, \n  ..., \n  name = deparse(substitute(fn)), \n  profiler = FALSE, \n  concrete_fn = do.call(fn$get_concrete_fn, args), \n  graph = concrete_fn$graph \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| fn | TensorFlow function (returned from `tf.function()`) |\n| args | arguments passed to `fun` |\n| ... | other arguments passed to `tensorflow::tensorboard()` |\n| name | string, provided to tensorboard |\n| profiler | logical, passed on to `tf.compat.v2.summary.trace_on()` (only used in eager mode) |\n| concrete_fn | a `ConcreteFunction` (only used in graph mode, ignored with a warning if executing eagerly) |\n| graph | a tensorflow graph (only used in graph mode, ignored with a warning if executing eagerly) |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nfn <- tf_function(function(x) autograph(if(x > 0) x * x else x)) \nview_function_graph(fn, list(tf$constant(5))) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/t-kalinowski/tfautograph//blob/main/R/view.R#L20) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# view_function_graph\n\n## Visualizes the generated graph\n\n## Description\nVisualizes the generated graph \n\n\n## Usage\n```r\nview_function_graph( \n  fn, \n  args, \n  ..., \n  name = deparse(substitute(fn)), \n  profiler = FALSE, \n  concrete_fn = do.call(fn$get_concrete_fn, args), \n  graph = concrete_fn$graph \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| fn | TensorFlow function (returned from `tf.function()`) |\n| args | arguments passed to `fun` |\n| ... | other arguments passed to `tensorflow::tensorboard()` |\n| name | string, provided to tensorboard |\n| profiler | logical, passed on to `tf.compat.v2.summary.trace_on()` (only used in eager mode) |\n| concrete_fn | a `ConcreteFunction` (only used in graph mode, ignored with a warning if executing eagerly) |\n| graph | a tensorflow graph (only used in graph mode, ignored with a warning if executing eagerly) |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\nfn <- tf_function(function(x) autograph(if(x > 0) x * x else x)) \nview_function_graph(fn, list(tf$constant(5))) \n```\n:::\n"",
+    ""supporting"": [
+      ""view_function_graph_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/choose_from_datasets/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""f6cb296f050ddcda5ee83d628e336655"",
+  ""hash"": ""c6c65f56a29845f032db2f673886078f"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/sample_from_datasets.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/sample_from_datasets.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/sample_from_datasets.R*\n\n# choose_from_datasets\n\n## Creates a dataset that deterministically chooses elements from datasets.\n\n## Description\nCreates a dataset that deterministically chooses elements from datasets. \n\n\n## Usage\n```r\nchoose_from_datasets(datasets, choice_dataset, stop_on_empty_dataset = TRUE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| datasets | A non-empty list of tf.data.Dataset objects with compatible structure. |\n| choice_dataset | A `tf.data.Dataset` of scalar `tf.int64` tensors between `0` and `length(datasets) - 1`. |\n| stop_on_empty_dataset | If `TRUE`, selection stops if it encounters an empty dataset. If `FALSE`, it skips empty datasets. It is recommended to set it to `TRUE`. Otherwise, the selected elements start off as the user intends, but may change as input datasets become empty. This can be difficult to detect since the dataset starts off looking correct. Defaults to `TRUE`. |\n\n\n\n## Value\nReturns a dataset that interleaves elements from datasets according to the values of choice_dataset. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ndatasets <- list(tensors_dataset(\""foo\"") %>% dataset_repeat(), \n                 tensors_dataset(\""bar\"") %>% dataset_repeat(), \n                 tensors_dataset(\""baz\"") %>% dataset_repeat()) \n# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`. \nchoice_dataset <- range_dataset(0, 3) %>% dataset_repeat(3) \nresult <- choose_from_datasets(datasets, choice_dataset) \nresult %>% as_array_iterator() %>% iterate(function(s) s$decode()) %>% print() \n# [1] \""foo\"" \""bar\"" \""baz\"" \""foo\"" \""bar\"" \""baz\"" \""foo\"" \""bar\"" \""baz\"" \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/sample_from_datasets.R#L63) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# choose_from_datasets\n\n## Creates a dataset that deterministically chooses elements from datasets.\n\n## Description\nCreates a dataset that deterministically chooses elements from datasets. \n\n\n## Usage\n```r\nchoose_from_datasets(datasets, choice_dataset, stop_on_empty_dataset = TRUE) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| datasets | A non-empty list of tf.data.Dataset objects with compatible structure. |\n| choice_dataset | A `tf.data.Dataset` of scalar `tf.int64` tensors between `0` and `length(datasets) - 1`. |\n| stop_on_empty_dataset | If `TRUE`, selection stops if it encounters an empty dataset. If `FALSE`, it skips empty datasets. It is recommended to set it to `TRUE`. Otherwise, the selected elements start off as the user intends, but may change as input datasets become empty. This can be difficult to detect since the dataset starts off looking correct. Defaults to `TRUE`. |\n\n\n\n## Value\nReturns a dataset that interleaves elements from datasets according to the values of choice_dataset. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndatasets <- list(tensors_dataset(\""foo\"") %>% dataset_repeat(), \n                 tensors_dataset(\""bar\"") %>% dataset_repeat(), \n                 tensors_dataset(\""baz\"") %>% dataset_repeat()) \n# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`. \nchoice_dataset <- range_dataset(0, 3) %>% dataset_repeat(3) \nresult <- choose_from_datasets(datasets, choice_dataset) \nresult %>% as_array_iterator() %>% iterate(function(s) s$decode()) %>% print() \n# [1] \""foo\"" \""bar\"" \""baz\"" \""foo\"" \""bar\"" \""baz\"" \""foo\"" \""bar\"" \""baz\"" \n```\n:::\n"",
+    ""supporting"": [
+      ""choose_from_datasets_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_bucket_by_sequence_length/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""748416d8cbdf0c7891d0e9445e6005f3"",
+  ""hash"": ""937411d3a339d5a68bfc48943158955d"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_bucket_by_sequence_length\n\n## A transformation that buckets elements in a `Dataset` by length\n\n## Description\nA transformation that buckets elements in a `Dataset` by length \n\n\n## Usage\n```r\ndataset_bucket_by_sequence_length( \n  dataset, \n  element_length_func, \n  bucket_boundaries, \n  bucket_batch_sizes, \n  padded_shapes = NULL, \n  padding_values = NULL, \n  pad_to_bucket_boundary = FALSE, \n  no_padding = FALSE, \n  drop_remainder = FALSE, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A `tf_dataset` |\n| element_length_func | function from element in `Dataset` to `tf$int32`, determines the length of the element, which will determine the bucket it goes into. |\n| bucket_boundaries | integers, upper length boundaries of the buckets. |\n| bucket_batch_sizes | integers, batch size per bucket. Length should be `length(bucket_boundaries) + 1`. |\n| padded_shapes | Nested structure of `tf.TensorShape` (returned by `tensorflow::shape()`) to pass to `tf.data.Dataset.padded_batch`. If not provided, will use `dataset.output_shapes`, which will result in variable length dimensions being padded out to the maximum length in each batch. |\n| padding_values | Values to pad with, passed to `tf.data.Dataset.padded_batch`. Defaults to padding with 0. |\n| pad_to_bucket_boundary | bool, if `FALSE`, will pad dimensions with unknown size to maximum length in batch. If `TRUE`, will pad dimensions with unknown size to bucket boundary minus 1 (i.e., the maximum length in each bucket), and caller must ensure that the source `Dataset` does not contain any elements with length longer than `max(bucket_boundaries)`. |\n| no_padding | boolean, indicates whether to pad the batch features (features need to be either of type `tf.sparse.SparseTensor` or of same shape). |\n| drop_remainder | (Optional.) A logical scalar, representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch. |\n| name | (Optional.) A name for the tf.data operation. |\n\n## Details\n\nElements of the `Dataset` are grouped together by length and then are padded and batched. \n\nThis is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency. \n\nBelow is an example to bucketize the input data to the 3 buckets \""[0, 3), [3, 5), [5, Inf)\"" based on sequence length, with batch size 2. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- list(c(0), \n                c(1, 2, 3, 4), \n                c(5, 6, 7), \n                c(7, 8, 9, 10, 11), \n                c(13, 14, 15, 16, 17, 18, 19, 20), \n                c(21, 22)) %>% \n  lapply(as.array) %>% lapply(as_tensor, \""int32\"") %>% \n  lapply(tensors_dataset) %>% \n  Reduce(dataset_concatenate, .) \ndataset %>% \n  dataset_bucket_by_sequence_length( \n    element_length_func = function(elem) tf$shape(elem)[1], \n    bucket_boundaries = c(3, 5), \n    bucket_batch_sizes = c(2, 2, 2) \n  ) %>% \n  as_array_iterator() %>% \n  iterate(print) \n#      [,1] [,2] [,3] [,4] \n# [1,]    1    2    3    4 \n# [2,]    5    6    7    0 \n#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] \n# [1,]    7    8    9   10   11    0    0    0 \n# [2,]   13   14   15   16   17   18   19   20 \n#      [,1] [,2] \n# [1,]    0    0 \n# [2,]   21   22 \n```\n:::\n\n\n## See Also\n- [https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length)\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L205) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_bucket_by_sequence_length\n\n## A transformation that buckets elements in a `Dataset` by length\n\n## Description\nA transformation that buckets elements in a `Dataset` by length \n\n\n## Usage\n```r\ndataset_bucket_by_sequence_length( \n  dataset, \n  element_length_func, \n  bucket_boundaries, \n  bucket_batch_sizes, \n  padded_shapes = NULL, \n  padding_values = NULL, \n  pad_to_bucket_boundary = FALSE, \n  no_padding = FALSE, \n  drop_remainder = FALSE, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A `tf_dataset` |\n| element_length_func | function from element in `Dataset` to `tf$int32`, determines the length of the element, which will determine the bucket it goes into. |\n| bucket_boundaries | integers, upper length boundaries of the buckets. |\n| bucket_batch_sizes | integers, batch size per bucket. Length should be `length(bucket_boundaries) + 1`. |\n| padded_shapes | Nested structure of `tf.TensorShape` (returned by `tensorflow::shape()`) to pass to `tf.data.Dataset.padded_batch`. If not provided, will use `dataset.output_shapes`, which will result in variable length dimensions being padded out to the maximum length in each batch. |\n| padding_values | Values to pad with, passed to `tf.data.Dataset.padded_batch`. Defaults to padding with 0. |\n| pad_to_bucket_boundary | bool, if `FALSE`, will pad dimensions with unknown size to maximum length in batch. If `TRUE`, will pad dimensions with unknown size to bucket boundary minus 1 (i.e., the maximum length in each bucket), and caller must ensure that the source `Dataset` does not contain any elements with length longer than `max(bucket_boundaries)`. |\n| no_padding | boolean, indicates whether to pad the batch features (features need to be either of type `tf.sparse.SparseTensor` or of same shape). |\n| drop_remainder | (Optional.) A logical scalar, representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch. |\n| name | (Optional.) A name for the tf.data operation. |\n\n## Details\n\nElements of the `Dataset` are grouped together by length and then are padded and batched. \n\nThis is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency. \n\nBelow is an example to bucketize the input data to the 3 buckets \""[0, 3), [3, 5), [5, Inf)\"" based on sequence length, with batch size 2. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndataset <- list(c(0), \n                c(1, 2, 3, 4), \n                c(5, 6, 7), \n                c(7, 8, 9, 10, 11), \n                c(13, 14, 15, 16, 17, 18, 19, 20), \n                c(21, 22)) %>% \n  lapply(as.array) %>% lapply(as_tensor, \""int32\"") %>% \n  lapply(tensors_dataset) %>% \n  Reduce(dataset_concatenate, .) \ndataset %>% \n  dataset_bucket_by_sequence_length( \n    element_length_func = function(elem) tf$shape(elem)[1], \n    bucket_boundaries = c(3, 5), \n    bucket_batch_sizes = c(2, 2, 2) \n  ) %>% \n  as_array_iterator() %>% \n  iterate(print) \n#      [,1] [,2] [,3] [,4] \n# [1,]    1    2    3    4 \n# [2,]    5    6    7    0 \n#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] \n# [1,]    7    8    9   10   11    0    0    0 \n# [2,]   13   14   15   16   17   18   19   20 \n#      [,1] [,2] \n# [1,]    0    0 \n# [2,]   21   22 \n```\n:::\n\n\n## See Also\n- [https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length)\n\n"",
+    ""supporting"": [
+      ""dataset_bucket_by_sequence_length_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_enumerate/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""8e41cef001194df6c55384f7eed07bdb"",
+  ""hash"": ""6d34416f1783761fbe8b24d3488bacf9"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_enumerate\n\n## Enumerates the elements of this dataset\n\n## Description\nEnumerates the elements of this dataset \n\n\n## Usage\n```r\ndataset_enumerate(dataset, start = 0L) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A tensorflow dataset |\n| start | An integer (coerced to a `tf$int64` scalar `tf.Tensor`), representing the start value for enumeration. |\n\n## Details\nIt is similar to python's `enumerate`, this transforms a sequence of elements into a sequence of `list(index, element)`, where index is an integer that indicates the position of the element in the sequence. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- tensor_slices_dataset(100:103) %>% \n  dataset_enumerate() \niterator <- reticulate::as_iterator(dataset) \nreticulate::iter_next(iterator) # list(0, 100) \nreticulate::iter_next(iterator) # list(1, 101) \nreticulate::iter_next(iterator) # list(2, 102) \nreticulate::iter_next(iterator) # list(3, 103) \nreticulate::iter_next(iterator) # NULL (iterator exhausted) \nreticulate::iter_next(iterator) # NULL (iterator exhausted) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L1227) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_enumerate\n\n## Enumerates the elements of this dataset\n\n## Description\nEnumerates the elements of this dataset \n\n\n## Usage\n```r\ndataset_enumerate(dataset, start = 0L) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A tensorflow dataset |\n| start | An integer (coerced to a `tf$int64` scalar `tf.Tensor`), representing the start value for enumeration. |\n\n## Details\nIt is similar to python's `enumerate`, this transforms a sequence of elements into a sequence of `list(index, element)`, where index is an integer that indicates the position of the element in the sequence. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndataset <- tensor_slices_dataset(100:103) %>% \n  dataset_enumerate() \niterator <- reticulate::as_iterator(dataset) \nreticulate::iter_next(iterator) # list(0, 100) \nreticulate::iter_next(iterator) # list(1, 101) \nreticulate::iter_next(iterator) # list(2, 102) \nreticulate::iter_next(iterator) # list(3, 103) \nreticulate::iter_next(iterator) # NULL (iterator exhausted) \nreticulate::iter_next(iterator) # NULL (iterator exhausted) \n```\n:::\n"",
+    ""supporting"": [
+      ""dataset_enumerate_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_filter/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""a0eb781f6e0cce402dc7a5086c370a3b"",
+  ""hash"": ""e353cdf3e9f45854a4358493b173a3a9"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_filter\n\n## Filter a dataset by a predicate\n\n## Description\nFilter a dataset by a predicate \n\n\n## Usage\n```r\ndataset_filter(dataset, predicate) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n| predicate | A function mapping a nested structure of tensors (having shapes and types defined by `output_shapes()` and `output_types()` to a scalar `tf$bool` tensor. |\n\n## Details\nNote that the functions used inside the predicate must be tensor operations (e.g. `tf$not_equal`, `tf$less`, etc.). R generic methods for relational operators (e.g. `<`, `>`, `<=`, etc.) and logical operators (e.g. `!`, `&`, `|`, etc.) are provided so you can use shorthand syntax for most common comparisions (this is illustrated by the example below). \n\n\n## Value\nA dataset composed of records that matched the predicate. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_filter(function(record) { \n    record$mpg >= 20 \n}) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_filter(function(record) { \n    record$mpg >= 20 & record$cyl >= 6L \n  }) \n```\n:::\n\n\n## See Also\nOther dataset methods:  `dataset_batch()`, `dataset_cache()`, `dataset_collect()`, `dataset_concatenate()`, `dataset_decode_delim()`, `dataset_interleave()`, `dataset_map_and_batch()`, `dataset_map()`, `dataset_padded_batch()`, `dataset_prefetch_to_device()`, `dataset_prefetch()`, `dataset_reduce()`, `dataset_repeat()`, `dataset_shuffle_and_repeat()`, `dataset_shuffle()`, `dataset_skip()`, `dataset_take_while()`, `dataset_take()`, `dataset_window()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L477) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_filter\n\n## Filter a dataset by a predicate\n\n## Description\nFilter a dataset by a predicate \n\n\n## Usage\n```r\ndataset_filter(dataset, predicate) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n| predicate | A function mapping a nested structure of tensors (having shapes and types defined by `output_shapes()` and `output_types()` to a scalar `tf$bool` tensor. |\n\n## Details\nNote that the functions used inside the predicate must be tensor operations (e.g. `tf$not_equal`, `tf$less`, etc.). R generic methods for relational operators (e.g. `<`, `>`, `<=`, etc.) and logical operators (e.g. `!`, `&`, `|`, etc.) are provided so you can use shorthand syntax for most common comparisions (this is illustrated by the example below). \n\n\n## Value\nA dataset composed of records that matched the predicate. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_filter(function(record) { \n    record$mpg >= 20 \n}) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_filter(function(record) { \n    record$mpg >= 20 & record$cyl >= 6L \n  }) \n```\n:::\n\n\n## See Also\nOther dataset methods:  `dataset_batch()`, `dataset_cache()`, `dataset_collect()`, `dataset_concatenate()`, `dataset_decode_delim()`, `dataset_interleave()`, `dataset_map_and_batch()`, `dataset_map()`, `dataset_padded_batch()`, `dataset_prefetch_to_device()`, `dataset_prefetch()`, `dataset_reduce()`, `dataset_repeat()`, `dataset_shuffle_and_repeat()`, `dataset_shuffle()`, `dataset_skip()`, `dataset_take_while()`, `dataset_take()`, `dataset_window()`\n\n"",
+    ""supporting"": [
+      ""dataset_filter_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_interleave/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""061250ed90e58eea300e375b726a9083"",
+  ""hash"": ""868ebbdf2aed2f92c855867cc7ccc304"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_interleave\n\n## Maps map_func across this dataset, and interleaves the results\n\n## Description\nMaps map_func across this dataset, and interleaves the results \n\n\n## Usage\n```r\ndataset_interleave(dataset, map_func, cycle_length, block_length = 1) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n| map_func | A function mapping a nested structure of tensors (having shapes and types defined by `output_shapes()` and `output_types()` to a dataset. |\n| cycle_length | The number of elements from this dataset that will be processed concurrently. |\n| block_length | The number of consecutive elements to produce from each input element before cycling to another input element. |\n\n## Details\nThe `cycle_length` and `block_length` arguments control the order in which elements are produced. `cycle_length` controls the number of input elements that are processed concurrently. In general, this transformation will apply `map_func` to `cycle_length` input elements, open iterators on the returned dataset objects, and cycle through them producing `block_length` consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- tensor_slices_dataset(c(1,2,3,4,5)) %>% \n dataset_interleave(cycle_length = 2, block_length = 4, function(x) { \n   tensors_dataset(x) %>% \n     dataset_repeat(6) \n }) \n# resulting dataset (newlines indicate \""block\"" boundaries): \nc(1, 1, 1, 1, \n  2, 2, 2, 2, \n  1, 1, \n  2, 2, \n  3, 3, 3, 3, \n  4, 4, 4, 4, \n  3, 3, \n  4, 4, \n  5, 5, 5, 5, \n  5, 5, \n) \n```\n:::\n\n\n## See Also\nOther dataset methods:  `dataset_batch()`, `dataset_cache()`, `dataset_collect()`, `dataset_concatenate()`, `dataset_decode_delim()`, `dataset_filter()`, `dataset_map_and_batch()`, `dataset_map()`, `dataset_padded_batch()`, `dataset_prefetch_to_device()`, `dataset_prefetch()`, `dataset_reduce()`, `dataset_repeat()`, `dataset_shuffle_and_repeat()`, `dataset_shuffle()`, `dataset_skip()`, `dataset_take_while()`, `dataset_take()`, `dataset_window()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L547) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_interleave\n\n## Maps map_func across this dataset, and interleaves the results\n\n## Description\nMaps map_func across this dataset, and interleaves the results \n\n\n## Usage\n```r\ndataset_interleave(dataset, map_func, cycle_length, block_length = 1) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n| map_func | A function mapping a nested structure of tensors (having shapes and types defined by `output_shapes()` and `output_types()` to a dataset. |\n| cycle_length | The number of elements from this dataset that will be processed concurrently. |\n| block_length | The number of consecutive elements to produce from each input element before cycling to another input element. |\n\n## Details\nThe `cycle_length` and `block_length` arguments control the order in which elements are produced. `cycle_length` controls the number of input elements that are processed concurrently. In general, this transformation will apply `map_func` to `cycle_length` input elements, open iterators on the returned dataset objects, and cycle through them producing `block_length` consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndataset <- tensor_slices_dataset(c(1,2,3,4,5)) %>% \n dataset_interleave(cycle_length = 2, block_length = 4, function(x) { \n   tensors_dataset(x) %>% \n     dataset_repeat(6) \n }) \n# resulting dataset (newlines indicate \""block\"" boundaries): \nc(1, 1, 1, 1, \n  2, 2, 2, 2, \n  1, 1, \n  2, 2, \n  3, 3, 3, 3, \n  4, 4, 4, 4, \n  3, 3, \n  4, 4, \n  5, 5, 5, 5, \n  5, 5, \n) \n```\n:::\n\n\n## See Also\nOther dataset methods:  `dataset_batch()`, `dataset_cache()`, `dataset_collect()`, `dataset_concatenate()`, `dataset_decode_delim()`, `dataset_filter()`, `dataset_map_and_batch()`, `dataset_map()`, `dataset_padded_batch()`, `dataset_prefetch_to_device()`, `dataset_prefetch()`, `dataset_reduce()`, `dataset_repeat()`, `dataset_shuffle_and_repeat()`, `dataset_shuffle()`, `dataset_skip()`, `dataset_take_while()`, `dataset_take()`, `dataset_window()`\n\n"",
+    ""supporting"": [
+      ""dataset_interleave_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_options/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""98fa68a57cdd1a057f53d6009e50b1a9"",
+  ""hash"": ""135ea3ab1fde81382a4730a750b91cb3"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_options\n\n## Get or Set Dataset Options\n\n## Description\nGet or Set Dataset Options \n\n\n## Usage\n```r\ndataset_options(dataset, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | a tensorflow dataset |\n| ... | Valid values include: <br>- A set of named arguments setting options. Names of nested attributes can be separated with a `\"".\""` (see examples). The set of named arguments can be supplied individually to `...`, or as a single named list. <br>- a `tf$data$Options()` instance.  |\n\n## Details\nThe options are \""global\"" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \n\n\n## Value\nIf values are supplied to `...`, returns a `tf.data.Dataset` with the given options set/updated. Otherwise, returns the currently set options for the dataset. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# pass options directly: \nrange_dataset(0, 10) %>% \n  dataset_options( \n    experimental_deterministic = FALSE, \n    threading.private_threadpool_size = 10 \n  ) \n# pass options as a named list: \nopts <- list( \n  experimental_deterministic = FALSE, \n  threading.private_threadpool_size = 10 \n) \nrange_dataset(0, 10) %>% \n  dataset_options(opts) \n# pass a tf.data.Options() instance \nopts <- tf$data$Options() \nopts$experimental_deterministic <- FALSE \nopts$threading$private_threadpool_size <- 10L \nrange_dataset(0, 10) %>% \n  dataset_options(opts) \n# get currently set options \nrange_dataset(0, 10) %>% dataset_options() \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L1105) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_options\n\n## Get or Set Dataset Options\n\n## Description\nGet or Set Dataset Options \n\n\n## Usage\n```r\ndataset_options(dataset, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | a tensorflow dataset |\n| ... | Valid values include: <br>- A set of named arguments setting options. Names of nested attributes can be separated with a `\"".\""` (see examples). The set of named arguments can be supplied individually to `...`, or as a single named list. <br>- a `tf$data$Options()` instance.  |\n\n## Details\nThe options are \""global\"" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values. \n\n\n## Value\nIf values are supplied to `...`, returns a `tf.data.Dataset` with the given options set/updated. Otherwise, returns the currently set options for the dataset. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n# pass options directly: \nrange_dataset(0, 10) %>% \n  dataset_options( \n    experimental_deterministic = FALSE, \n    threading.private_threadpool_size = 10 \n  ) \n# pass options as a named list: \nopts <- list( \n  experimental_deterministic = FALSE, \n  threading.private_threadpool_size = 10 \n) \nrange_dataset(0, 10) %>% \n  dataset_options(opts) \n# pass a tf.data.Options() instance \nopts <- tf$data$Options() \nopts$experimental_deterministic <- FALSE \nopts$threading$private_threadpool_size <- 10L \nrange_dataset(0, 10) %>% \n  dataset_options(opts) \n# get currently set options \nrange_dataset(0, 10) %>% dataset_options() \n```\n:::\n"",
+    ""supporting"": [
+      ""dataset_options_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_padded_batch/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""d0669d4077fe05dd906a4df2df0a2916"",
+  ""hash"": ""5ddea71079839d3fef4912db7ddcd1e7"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_padded_batch\n\n## Combines consecutive elements of this dataset into padded batches.\n\n## Description\nCombines consecutive elements of this dataset into padded batches. \n\n\n## Usage\n```r\ndataset_padded_batch( \n  dataset, \n  batch_size, \n  padded_shapes = NULL, \n  padding_values = NULL, \n  drop_remainder = FALSE, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n| batch_size | An integer, representing the number of consecutive elements of this dataset to combine in a single batch. |\n| padded_shapes | (Optional.) A (nested) structure of `tf.TensorShape` (returned by `tensorflow::shape()`) or `tf$int64` vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. `padded_shapes`<br>must be set if any component has an unknown rank. |\n| padding_values | (Optional.) A (nested) structure of scalar-shaped `tf.Tensor`, representing the padding values to use for the respective components. `NULL` represents that the (nested) structure should be padded with default values.  Defaults are `0` for numeric types and the empty string `\""\""` for string types. The `padding_values` should have the same (nested) structure as the input dataset. If `padding_values` is a single element and the input dataset has multiple components, then the same `padding_values` will be used to pad every component of the dataset. If `padding_values` is a scalar, then its value will be broadcasted to match the shape of each component. |\n| drop_remainder | (Optional.) A boolean scalar, representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch. |\n| name | (Optional.) A name for the tf.data operation. Requires tensorflow version >= 2.7. |\n\n## Details\n\nThis transformation combines multiple consecutive elements of the input dataset into a single element. \n\nLike `dataset_batch()`, the components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N %% batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `FALSE`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `TRUE` to prevent the smaller batch from being produced. \n\nUnlike `dataset_batch()`, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in `padded_shapes`. The `padded_shapes` argument determines the resulting shape for each dimension of each component in an output element: \n\n- If the dimension is a constant, the component will be padded out to that length in that dimension. \n\n- If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension. \n\nSee also `tf$data$experimental$dense_to_sparse_batch`, which combines elements that may have different shapes into a `tf$sparse$SparseTensor`. \n\n\n## Value\nA tf_dataset \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- range_dataset(1, 5, dtype = tf$int32) %>% \n  dataset_map(function(x) tf$fill(list(x), x)) \n# Pad to the smallest per-batch size that fits all elements. \nB <- A %>% dataset_padded_batch(2) \nB %>% as_array_iterator() %>% iterate(print) \n# Pad to a fixed size. \nC <- A %>% dataset_padded_batch(2, padded_shapes=5) \nC %>% as_array_iterator() %>% iterate(print) \n# Pad with a custom value. \nD <- A %>% dataset_padded_batch(2, padded_shapes=5, padding_values = -1L) \nD %>% as_array_iterator() %>% iterate(print) \n# Pad with a single value and multiple components. \nE <- zip_datasets(A, A) %>%  dataset_padded_batch(2, padding_values = -1L) \nE %>% as_array_iterator() %>% iterate(print) \n```\n:::\n\n\n## See Also\n\n- [https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch)\n\nOther dataset methods:  `dataset_batch()`, `dataset_cache()`, `dataset_collect()`, `dataset_concatenate()`, `dataset_decode_delim()`, `dataset_filter()`, `dataset_interleave()`, `dataset_map_and_batch()`, `dataset_map()`, `dataset_prefetch_to_device()`, `dataset_prefetch()`, `dataset_reduce()`, `dataset_repeat()`, `dataset_shuffle_and_repeat()`, `dataset_shuffle()`, `dataset_skip()`, `dataset_take_while()`, `dataset_take()`, `dataset_window()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L660) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_padded_batch\n\n## Combines consecutive elements of this dataset into padded batches.\n\n## Description\nCombines consecutive elements of this dataset into padded batches. \n\n\n## Usage\n```r\ndataset_padded_batch( \n  dataset, \n  batch_size, \n  padded_shapes = NULL, \n  padding_values = NULL, \n  drop_remainder = FALSE, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n| batch_size | An integer, representing the number of consecutive elements of this dataset to combine in a single batch. |\n| padded_shapes | (Optional.) A (nested) structure of `tf.TensorShape` (returned by `tensorflow::shape()`) or `tf$int64` vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. `padded_shapes`<br>must be set if any component has an unknown rank. |\n| padding_values | (Optional.) A (nested) structure of scalar-shaped `tf.Tensor`, representing the padding values to use for the respective components. `NULL` represents that the (nested) structure should be padded with default values.  Defaults are `0` for numeric types and the empty string `\""\""` for string types. The `padding_values` should have the same (nested) structure as the input dataset. If `padding_values` is a single element and the input dataset has multiple components, then the same `padding_values` will be used to pad every component of the dataset. If `padding_values` is a scalar, then its value will be broadcasted to match the shape of each component. |\n| drop_remainder | (Optional.) A boolean scalar, representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch. |\n| name | (Optional.) A name for the tf.data operation. Requires tensorflow version >= 2.7. |\n\n## Details\n\nThis transformation combines multiple consecutive elements of the input dataset into a single element. \n\nLike `dataset_batch()`, the components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N %% batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `FALSE`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `TRUE` to prevent the smaller batch from being produced. \n\nUnlike `dataset_batch()`, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in `padded_shapes`. The `padded_shapes` argument determines the resulting shape for each dimension of each component in an output element: \n\n- If the dimension is a constant, the component will be padded out to that length in that dimension. \n\n- If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension. \n\nSee also `tf$data$experimental$dense_to_sparse_batch`, which combines elements that may have different shapes into a `tf$sparse$SparseTensor`. \n\n\n## Value\nA tf_dataset \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\nA <- range_dataset(1, 5, dtype = tf$int32) %>% \n  dataset_map(function(x) tf$fill(list(x), x)) \n# Pad to the smallest per-batch size that fits all elements. \nB <- A %>% dataset_padded_batch(2) \nB %>% as_array_iterator() %>% iterate(print) \n# Pad to a fixed size. \nC <- A %>% dataset_padded_batch(2, padded_shapes=5) \nC %>% as_array_iterator() %>% iterate(print) \n# Pad with a custom value. \nD <- A %>% dataset_padded_batch(2, padded_shapes=5, padding_values = -1L) \nD %>% as_array_iterator() %>% iterate(print) \n# Pad with a single value and multiple components. \nE <- zip_datasets(A, A) %>%  dataset_padded_batch(2, padding_values = -1L) \nE %>% as_array_iterator() %>% iterate(print) \n```\n:::\n\n\n## See Also\n\n- [https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch)\n\nOther dataset methods:  `dataset_batch()`, `dataset_cache()`, `dataset_collect()`, `dataset_concatenate()`, `dataset_decode_delim()`, `dataset_filter()`, `dataset_interleave()`, `dataset_map_and_batch()`, `dataset_map()`, `dataset_prefetch_to_device()`, `dataset_prefetch()`, `dataset_reduce()`, `dataset_repeat()`, `dataset_shuffle_and_repeat()`, `dataset_shuffle()`, `dataset_skip()`, `dataset_take_while()`, `dataset_take()`, `dataset_window()`\n\n"",
+    ""supporting"": [
+      ""dataset_padded_batch_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_rejection_resample/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""0fb9b0a6e21b720f9aef4a4e093fe386"",
+  ""hash"": ""327b8ecb4d976597f67f25456edcea03"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_rejection_resample\n\n## A transformation that resamples a dataset to a target distribution.\n\n## Description\nA transformation that resamples a dataset to a target distribution. \n\n\n## Usage\n```r\ndataset_rejection_resample( \n  dataset, \n  class_func, \n  target_dist, \n  initial_dist = NULL, \n  seed = NULL, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A `tf.Dataset` |\n| class_func | A function mapping an element of the input dataset to a scalar `tf.int32` tensor. Values should be in `[0, num_classes)`. |\n| target_dist | A floating point type tensor, shaped `[num_classes]`. |\n| initial_dist | (Optional.) A floating point type tensor, shaped `[num_classes]`. If not provided, the true class distribution is estimated live in a streaming fashion. |\n| seed | (Optional.) Integer seed for the resampler. |\n| name | (Optional.) A name for the tf.data operation. |\n\n\n\n## Value\nA `tf.Dataset`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_dist <- c(.5, .5) \ntarget_dist <- c(.6, .4) \nnum_classes <- length(initial_dist) \nnum_samples <- 100000 \ndata <- sample.int(num_classes, num_samples, prob = initial_dist, replace = TRUE) \ndataset <- tensor_slices_dataset(data) \ntally <- c(0, 0) \n`add<-` <- function (x, value) x + value \n# tfautograph::autograph({ \n#   for(i in dataset) \n#     add(tally[as.numeric(i)]) <- 1 \n# }) \ndataset %>% \n  as_array_iterator() %>% \n  iterate(function(i) { \n    add(tally[i]) <<- 1 \n  }, simplify = FALSE) \n# The value of `tally` will be close to c(50000, 50000) as \n# per the `initial_dist` distribution. \ntally # c(50287, 49713) \ntally <- c(0, 0) \ndataset %>% \n  dataset_rejection_resample( \n    class_func = function(x) (x-1) %% 2, \n    target_dist = target_dist, \n    initial_dist = initial_dist \n  ) %>% \n  as_array_iterator() %>% \n  iterate(function(element) { \n    names(element) <- c(\""class_id\"", \""i\"") \n    add(tally[element$i]) <<- 1 \n  }, simplify = FALSE) \n# The value of tally will be now be close to c(75000, 50000) \n# thus satisfying the target_dist distribution. \ntally # c(74822, 49921) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L730) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_rejection_resample\n\n## A transformation that resamples a dataset to a target distribution.\n\n## Description\nA transformation that resamples a dataset to a target distribution. \n\n\n## Usage\n```r\ndataset_rejection_resample( \n  dataset, \n  class_func, \n  target_dist, \n  initial_dist = NULL, \n  seed = NULL, \n  name = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A `tf.Dataset` |\n| class_func | A function mapping an element of the input dataset to a scalar `tf.int32` tensor. Values should be in `[0, num_classes)`. |\n| target_dist | A floating point type tensor, shaped `[num_classes]`. |\n| initial_dist | (Optional.) A floating point type tensor, shaped `[num_classes]`. If not provided, the true class distribution is estimated live in a streaming fashion. |\n| seed | (Optional.) Integer seed for the resampler. |\n| name | (Optional.) A name for the tf.data operation. |\n\n\n\n## Value\nA `tf.Dataset`\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ninitial_dist <- c(.5, .5) \ntarget_dist <- c(.6, .4) \nnum_classes <- length(initial_dist) \nnum_samples <- 100000 \ndata <- sample.int(num_classes, num_samples, prob = initial_dist, replace = TRUE) \ndataset <- tensor_slices_dataset(data) \ntally <- c(0, 0) \n`add<-` <- function (x, value) x + value \n# tfautograph::autograph({ \n#   for(i in dataset) \n#     add(tally[as.numeric(i)]) <- 1 \n# }) \ndataset %>% \n  as_array_iterator() %>% \n  iterate(function(i) { \n    add(tally[i]) <<- 1 \n  }, simplify = FALSE) \n# The value of `tally` will be close to c(50000, 50000) as \n# per the `initial_dist` distribution. \ntally # c(50287, 49713) \ntally <- c(0, 0) \ndataset %>% \n  dataset_rejection_resample( \n    class_func = function(x) (x-1) %% 2, \n    target_dist = target_dist, \n    initial_dist = initial_dist \n  ) %>% \n  as_array_iterator() %>% \n  iterate(function(element) { \n    names(element) <- c(\""class_id\"", \""i\"") \n    add(tally[element$i]) <<- 1 \n  }, simplify = FALSE) \n# The value of tally will be now be close to c(75000, 50000) \n# thus satisfying the target_dist distribution. \ntally # c(74822, 49921) \n```\n:::\n"",
+    ""supporting"": [
+      ""dataset_rejection_resample_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_scan/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""c3bd6cfe254767ac6b675541c2a9f7e9"",
+  ""hash"": ""812ecc7f1d6d4c214499d762fb6142cf"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_scan\n\n## A transformation that scans a function across an input dataset\n\n## Description\nA transformation that scans a function across an input dataset \n\n\n## Usage\n```r\ndataset_scan(dataset, initial_state, scan_func) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A tensorflow dataset |\n| initial_state | A nested structure of tensors, representing the initial state of the accumulator. |\n| scan_func | A function that maps `(old_state, input_element)` to `(new_state, output_element)`. It must take two arguments and return a pair of nested structures of tensors. The `new_state` must match the structure of `initial_state`. |\n\n## Details\nThis transformation is a stateful relative of `dataset_map()`. In addition to mapping `scan_func` across the elements of the input dataset, `scan()` accumulates one or more state tensors, whose initial values are `initial_state`. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_state <- as_tensor(0, dtype=\""int64\"") \nscan_func <- function(state, i) list(state + i, state + i) \ndataset <- range_dataset(0, 10) %>% \n  dataset_scan(initial_state, scan_func) \nreticulate::iterate(dataset, as.array) %>% \n  unlist() \n# 0  1  3  6 10 15 21 28 36 45 \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L1279) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_scan\n\n## A transformation that scans a function across an input dataset\n\n## Description\nA transformation that scans a function across an input dataset \n\n\n## Usage\n```r\ndataset_scan(dataset, initial_state, scan_func) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A tensorflow dataset |\n| initial_state | A nested structure of tensors, representing the initial state of the accumulator. |\n| scan_func | A function that maps `(old_state, input_element)` to `(new_state, output_element)`. It must take two arguments and return a pair of nested structures of tensors. The `new_state` must match the structure of `initial_state`. |\n\n## Details\nThis transformation is a stateful relative of `dataset_map()`. In addition to mapping `scan_func` across the elements of the input dataset, `scan()` accumulates one or more state tensors, whose initial values are `initial_state`. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ninitial_state <- as_tensor(0, dtype=\""int64\"") \nscan_func <- function(state, i) list(state + i, state + i) \ndataset <- range_dataset(0, 10) %>% \n  dataset_scan(initial_state, scan_func) \nreticulate::iterate(dataset, as.array) %>% \n  unlist() \n# 0  1  3  6 10 15 21 28 36 45 \n```\n:::\n"",
+    ""supporting"": [
+      ""dataset_scan_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_unique/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""fa9a65e47e7d7f54a3d65b634926f6f1"",
+  ""hash"": ""3e771223d4673dac28f0ff66fae02906"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# dataset_unique\n\n## A transformation that discards duplicate elements of a Dataset.\n\n## Description\nUse this transformation to produce a dataset that contains one instance of each unique element in the input (See example). \n\n\n## Usage\n```r\ndataset_unique(dataset, name = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A tf.Dataset. |\n| name | (Optional.) A name for the tf.data operation. |\n\n\n\n## Value\nA tf.Dataset \n\n## Note\nThis transformation only supports datasets which fit into memory and have elements of either tf.int32, tf.int64 or tf.string type. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nc(0, 37, 2, 37, 2, 1) %>% as_tensor(\""int32\"") %>% \n  tensor_slices_dataset() %>% \n  dataset_unique() %>% \n  as_array_iterator() %>% iterate() %>% sort() \n# [1]  0  1  2 37 \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L768) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_unique\n\n## A transformation that discards duplicate elements of a Dataset.\n\n## Description\nUse this transformation to produce a dataset that contains one instance of each unique element in the input (See example). \n\n\n## Usage\n```r\ndataset_unique(dataset, name = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A tf.Dataset. |\n| name | (Optional.) A name for the tf.data operation. |\n\n\n\n## Value\nA tf.Dataset \n\n## Note\nThis transformation only supports datasets which fit into memory and have elements of either tf.int32, tf.int64 or tf.string type. \n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\nc(0, 37, 2, 37, 2, 1) %>% as_tensor(\""int32\"") %>% \n  tensor_slices_dataset() %>% \n  dataset_unique() %>% \n  as_array_iterator() %>% iterate() %>% sort() \n# [1]  0  1  2 37 \n```\n:::\n"",
+    ""supporting"": [
+      ""dataset_unique_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/dataset_use_spec/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""371efaafcb5210b8f1a248e2f8b94485"",
+  ""hash"": ""8a1eac72f8d34069d879da306958187c"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# dataset_use_spec\n\n## Transform the dataset using the provided spec.\n\n## Description\nPrepares the dataset to be used directly in a model.The transformed dataset is prepared to return tuples (x,y) that can be used directly in Keras. \n\n\n## Usage\n```r\ndataset_use_spec(dataset, spec) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A TensorFlow dataset. |\n| spec | A feature specification created with `feature_spec()`. |\n\n\n\n## Value\nA TensorFlow dataset. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\n- `feature_spec()` to initialize the feature specification. \n\n- `fit.FeatureSpec()` to create a tensorflow dataset prepared to modeling. \n\n- steps to a list of all implemented steps. \n\nOther Feature Spec Functions:  `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1057) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# dataset_use_spec\n\n## Transform the dataset using the provided spec.\n\n## Description\nPrepares the dataset to be used directly in a model.The transformed dataset is prepared to return tuples (x,y) that can be used directly in Keras. \n\n\n## Usage\n```r\ndataset_use_spec(dataset, spec) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A TensorFlow dataset. |\n| spec | A feature specification created with `feature_spec()`. |\n\n\n\n## Value\nA TensorFlow dataset. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\n- `feature_spec()` to initialize the feature specification. \n\n- `fit.FeatureSpec()` to create a tensorflow dataset prepared to modeling. \n\n- steps to a list of all implemented steps. \n\nOther Feature Spec Functions:  `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""dataset_use_spec_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/feature_spec/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""f4aaffcf896307ef16785fc6d3c3d0a9"",
+  ""hash"": ""81faa3cba6ecc2d94c6b91cd804f7c38"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# feature_spec\n\n## Creates a feature specification.\n\n## Description\nUsed to create initialize a feature columns specification. \n\n\n## Usage\n```r\nfeature_spec(dataset, x, y = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A TensorFlow dataset. |\n| x | Features to include can use `tidyselect::select_helpers()` or a `formula`. |\n| y | (Optional) The response variable. Can also be specified using a `formula` in the `x` argument. |\n\n## Details\nAfter creating the `feature_spec` object you can add steps using the `step` functions. \n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ .) \n# select using `tidyselect` helpers \nspec <- feature_spec(hearts, x = c(thal, age), y = target) \n```\n:::\n\n\n## See Also\n\n- `fit.FeatureSpec()` to fit the FeatureSpec \n\n- `dataset_use_spec()` to create a tensorflow dataset prepared to modeling. \n\n- steps to a list of all implemented steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L972) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# feature_spec\n\n## Creates a feature specification.\n\n## Description\nUsed to create initialize a feature columns specification. \n\n\n## Usage\n```r\nfeature_spec(dataset, x, y = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A TensorFlow dataset. |\n| x | Features to include can use `tidyselect::select_helpers()` or a `formula`. |\n| y | (Optional) The response variable. Can also be specified using a `formula` in the `x` argument. |\n\n## Details\nAfter creating the `feature_spec` object you can add steps using the `step` functions. \n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ .) \n# select using `tidyselect` helpers \nspec <- feature_spec(hearts, x = c(thal, age), y = target) \n```\n:::\n\n\n## See Also\n\n- `fit.FeatureSpec()` to fit the FeatureSpec \n\n- `dataset_use_spec()` to create a tensorflow dataset prepared to modeling. \n\n- steps to a list of all implemented steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""feature_spec_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/fit.featurespec/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""6d533c80736ba5b3a904cebf86d7db7e"",
+  ""hash"": ""8f57008615c3fc187976e12c681b3e9d"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# fit.FeatureSpec\n\n## Fits a feature specification.\n\n## Description\nThis function will `fit` the specification. Depending on the steps added to the specification it will compute for example, the levels of categorical features, normalization constants, etc. \n\n\n## Usage\n```r\n## S3 method for class 'FeatureSpec'\nfit\n(object, dataset = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | A feature specification created with `feature_spec()`. |\n| dataset | (Optional) A TensorFlow dataset. If `NULL` it will use the dataset provided when initilializing the `feature_spec`. |\n| ... | (unused) |\n\n\n\n## Value\na fitted `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) \nspec_fit <- fit(spec) \nspec_fit \n```\n:::\n\n\n## See Also\n\n- `feature_spec()` to initialize the feature specification. \n\n- `dataset_use_spec()` to create a tensorflow dataset prepared to modeling. \n\n- steps to a list of all implemented steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1017) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# fit.FeatureSpec\n\n## Fits a feature specification.\n\n## Description\nThis function will `fit` the specification. Depending on the steps added to the specification it will compute for example, the levels of categorical features, normalization constants, etc. \n\n\n## Usage\n```r\n## S3 method for class 'FeatureSpec'\nfit(object, dataset = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | A feature specification created with `feature_spec()`. |\n| dataset | (Optional) A TensorFlow dataset. If `NULL` it will use the dataset provided when initilializing the `feature_spec`. |\n| ... | (unused) |\n\n\n\n## Value\na fitted `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) \nspec_fit <- fit(spec) \nspec_fit \n```\n:::\n\n\n## See Also\n\n- `feature_spec()` to initialize the feature specification. \n\n- `dataset_use_spec()` to create a tensorflow dataset prepared to modeling. \n\n- steps to a list of all implemented steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""fit.featurespec_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/layer_input_from_dataset/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""9970f2cde0690afd248f64f963878489"",
+  ""hash"": ""1854fa4c5c0db0bfe85c6b0bd8379c34"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# layer_input_from_dataset\n\n## Creates a list of inputs from a dataset\n\n## Description\nCreate a list ok Keras input layers that can be used together with `keras::layer_dense_features()`. \n\n\n## Usage\n```r\nlayer_input_from_dataset(dataset) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | a TensorFlow dataset or a data.frame |\n\n\n\n## Value\na list of Keras input layers \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age + slope) %>% \n  step_numeric_column(age, slope) %>% \n  step_bucketized_column(age, boundaries = c(10, 20, 30)) \nspec <- fit(spec) \ndataset <- hearts %>% dataset_use_spec(spec) \ninput <- layer_input_from_dataset(dataset) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1754) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# layer_input_from_dataset\n\n## Creates a list of inputs from a dataset\n\n## Description\nCreate a list ok Keras input layers that can be used together with `keras::layer_dense_features()`. \n\n\n## Usage\n```r\nlayer_input_from_dataset(dataset) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | a TensorFlow dataset or a data.frame |\n\n\n\n## Value\na list of Keras input layers \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age + slope) %>% \n  step_numeric_column(age, slope) %>% \n  step_bucketized_column(age, boundaries = c(10, 20, 30)) \nspec <- fit(spec) \ndataset <- hearts %>% dataset_use_spec(spec) \ninput <- layer_input_from_dataset(dataset) \n```\n:::\n"",
+    ""supporting"": [
+      ""layer_input_from_dataset_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/length.tf_dataset/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""863540427bb51d7d5e3ba6c0bcc4c01a"",
+  ""hash"": ""f880b3db1acef0dea6811a8e9df6e507"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_methods.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_methods.R*\n\n# length.tf_dataset\n\n## Get Dataset length\n\n## Description\nReturns the length of the dataset. \n\n\n## Usage\n```r\n## S3 method for class 'tf_dataset'\nlength\n(x) \n## S3 method for class 'tensorflow.python.data.ops.dataset_ops.DatasetV2'\nlength\n(x) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | a `tf.data.Dataset` object. |\n\n\n\n## Value\nEither `Inf` if the dataset is infinite, `NA` if the dataset length is unknown, or an R numeric if it is known. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nrange_dataset(0, 42) %>% length() \n# 42 \nrange_dataset(0, 42) %>% dataset_repeat() %>% length() \n# Inf \nrange_dataset(0, 42) %>% dataset_repeat() %>% \n  dataset_filter(function(x) TRUE) %>% length() \n# NA \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_methods.R#L1179) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# length.tf_dataset\n\n## Get Dataset length\n\n## Description\nReturns the length of the dataset. \n\n\n## Usage\n```r\n## S3 method for class 'tf_dataset'\nlength(x) \n\n## S3 method for class 'tensorflow.python.data.ops.dataset_ops.DatasetV2'\nlength(x) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| x | a `tf.data.Dataset` object. |\n\n\n\n## Value\nEither `Inf` if the dataset is infinite, `NA` if the dataset length is unknown, or an R numeric if it is known. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\nrange_dataset(0, 42) %>% length() \n# 42 \nrange_dataset(0, 42) %>% dataset_repeat() %>% length() \n# Inf \nrange_dataset(0, 42) %>% dataset_repeat() %>% \n  dataset_filter(function(x) TRUE) %>% length() \n# NA \n```\n:::\n"",
+    ""supporting"": [
+      ""length.tf_dataset_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/next_batch/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""ab813c1089a3e626b8aa572a1f0cb92b"",
+  ""hash"": ""db2a6bcdab6830acae945bb6e9eae590"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_iterators.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_iterators.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_iterators.R*\n\n# next_batch\n\n## Tensor(s) for retrieving the next batch from a dataset\n\n## Description\nTensor(s) for retrieving the next batch from a dataset \n\n\n## Usage\n```r\nnext_batch(dataset) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n\n## Details\n\nTo access the underlying data within the dataset you iteratively evaluate the tensor(s) to read batches of data. \n\nNote that in many cases you won't need to explicitly evaluate the tensors. Rather, you will pass the tensors to another function that will perform the evaluation (e.g. the Keras layer_input() and compile() functions). \n\nIf you do need to perform iteration manually by evaluating the tensors, there are a couple of possible approaches to controlling/detecting when iteration should end. \n\nOne approach is to create a dataset that yields batches infinitely (traversing the dataset multiple times with different batches randomly drawn). In this case you'd use another mechanism like a global step counter or detecting a learning plateau. \n\nAnother approach is to detect when all batches have been yielded from the dataset. When the tensor reaches the end of iteration a runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in the `with_dataset()` function. \n\nSee the examples below for a demonstration of each of these methods of iteration. \n\n\n## Value\nTensor(s) that can be evaluated to yield the next batch of training data. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# iteration with 'infinite' dataset and explicit step counter \nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_shuffle(5000) %>% \n  dataset_batch(128) %>% \n  dataset_repeat() # repeat infinitely \nbatch <- next_batch(dataset) \nsteps <- 200 \nfor (i in 1:steps) { \n  # use batch$x and batch$y tensors \n} \n# iteration that detects and ignores end of iteration error \nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10) \nbatch <- next_batch(dataset) \nwith_dataset({ \n  while(TRUE) { \n    # use batch$x and batch$y tensors \n  } \n}) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_iterators.R#L67) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# next_batch\n\n## Tensor(s) for retrieving the next batch from a dataset\n\n## Description\nTensor(s) for retrieving the next batch from a dataset \n\n\n## Usage\n```r\nnext_batch(dataset) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| dataset | A dataset |\n\n## Details\n\nTo access the underlying data within the dataset you iteratively evaluate the tensor(s) to read batches of data. \n\nNote that in many cases you won't need to explicitly evaluate the tensors. Rather, you will pass the tensors to another function that will perform the evaluation (e.g. the Keras layer_input() and compile() functions). \n\nIf you do need to perform iteration manually by evaluating the tensors, there are a couple of possible approaches to controlling/detecting when iteration should end. \n\nOne approach is to create a dataset that yields batches infinitely (traversing the dataset multiple times with different batches randomly drawn). In this case you'd use another mechanism like a global step counter or detecting a learning plateau. \n\nAnother approach is to detect when all batches have been yielded from the dataset. When the tensor reaches the end of iteration a runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in the `with_dataset()` function. \n\nSee the examples below for a demonstration of each of these methods of iteration. \n\n\n## Value\nTensor(s) that can be evaluated to yield the next batch of training data. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# iteration with 'infinite' dataset and explicit step counter \nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_shuffle(5000) %>% \n  dataset_batch(128) %>% \n  dataset_repeat() # repeat infinitely \nbatch <- next_batch(dataset) \nsteps <- 200 \nfor (i in 1:steps) { \n  # use batch$x and batch$y tensors \n} \n# iteration that detects and ignores end of iteration error \nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10) \nbatch <- next_batch(dataset) \nwith_dataset({ \n  while(TRUE) { \n    # use batch$x and batch$y tensors \n  } \n}) \n```\n:::\n"",
+    ""supporting"": [
+      ""next_batch_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_bucketized_column/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""6e3ec33394c7f8fb9cb13a2bd0cfae3d"",
+  ""hash"": ""b0030f95ffe8a48712b4baa399c1f69c"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_bucketized_column\n\n## Creates bucketized columns\n\n## Description\nUse this step to create bucketized columns from numeric columns. \n\n\n## Usage\n```r\nstep_bucketized_column(spec, ..., boundaries) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| boundaries | A sorted list or tuple of floats specifying the boundaries. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) %>% \n  step_bucketized_column(age, boundaries = c(10, 20, 30)) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1601) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_bucketized_column\n\n## Creates bucketized columns\n\n## Description\nUse this step to create bucketized columns from numeric columns. \n\n\n## Usage\n```r\nstep_bucketized_column(spec, ..., boundaries) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| boundaries | A sorted list or tuple of floats specifying the boundaries. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) %>% \n  step_bucketized_column(age, boundaries = c(10, 20, 30)) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_bucketized_column_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_categorical_column_with_hash_bucket/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""1dd3b2264e0c99822252d06d5b23b05d"",
+  ""hash"": ""ebdb5d1f28f409e775a87d50231288b4"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_categorical_column_with_hash_bucket\n\n## Creates a categorical column with hash buckets specification\n\n## Description\nRepresents sparse feature where ids are set by hashing. \n\n\n## Usage\n```r\nstep_categorical_column_with_hash_bucket( \n  spec, \n  ..., \n  hash_bucket_size, \n  dtype = tf$string \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| hash_bucket_size | An int > 1. The number of buckets. |\n| dtype | The type of features. Only string and integer types are supported. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_hash_bucket(thal, hash_bucket_size = 3) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1289) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_categorical_column_with_hash_bucket\n\n## Creates a categorical column with hash buckets specification\n\n## Description\nRepresents sparse feature where ids are set by hashing. \n\n\n## Usage\n```r\nstep_categorical_column_with_hash_bucket( \n  spec, \n  ..., \n  hash_bucket_size, \n  dtype = tf$string \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| hash_bucket_size | An int > 1. The number of buckets. |\n| dtype | The type of features. Only string and integer types are supported. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_hash_bucket(thal, hash_bucket_size = 3) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_categorical_column_with_hash_bucket_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_categorical_column_with_identity/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""101fda251363857770165d25b9186ccf"",
+  ""hash"": ""ce6537d579f41f4b7fd2c27f35caee64"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_categorical_column_with_identity\n\n## Create a categorical column with identity\n\n## Description\nUse this when your inputs are integers in the range `[0-num_buckets)`. \n\n\n## Usage\n```r\nstep_categorical_column_with_identity( \n  spec, \n  ..., \n  num_buckets, \n  default_value = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| num_buckets | Range of inputs and outputs is `[0, num_buckets)`. |\n| default_value | If `NULL`, this column's graph operations will fail for out-of-range inputs. Otherwise, this value must be in the range `[0, num_buckets)`, and will replace inputs in that range. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts$thal <- as.integer(as.factor(hearts$thal)) - 1L \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_identity(thal, num_buckets = 5) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1341) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_categorical_column_with_identity\n\n## Create a categorical column with identity\n\n## Description\nUse this when your inputs are integers in the range `[0-num_buckets)`. \n\n\n## Usage\n```r\nstep_categorical_column_with_identity( \n  spec, \n  ..., \n  num_buckets, \n  default_value = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| num_buckets | Range of inputs and outputs is `[0, num_buckets)`. |\n| default_value | If `NULL`, this column's graph operations will fail for out-of-range inputs. Otherwise, this value must be in the range `[0, num_buckets)`, and will replace inputs in that range. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts$thal <- as.integer(as.factor(hearts$thal)) - 1L \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_identity(thal, num_buckets = 5) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_categorical_column_with_identity_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_categorical_column_with_vocabulary_file/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""36293e9f5854c9cc74cf673069363b10"",
+  ""hash"": ""2555d429b6b2ccca7dcbe20c6e9e914e"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_categorical_column_with_vocabulary_file\n\n## Creates a categorical column with vocabulary file\n\n## Description\nUse this function when the vocabulary of a categorical variable is written to a file. \n\n\n## Usage\n```r\nstep_categorical_column_with_vocabulary_file( \n  spec, \n  ..., \n  vocabulary_file, \n  vocabulary_size = NULL, \n  dtype = tf$string, \n  default_value = NULL, \n  num_oov_buckets = 0L \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| vocabulary_file | The vocabulary file name. |\n| vocabulary_size | Number of the elements in the vocabulary. This must be no greater than length of `vocabulary_file`, if less than length, later values are ignored. If None, it is set to the length of `vocabulary_file`. |\n| dtype | The type of features. Only string and integer types are supported. |\n| default_value | The integer ID value to return for out-of-vocabulary feature values, defaults to `-1`. This can not be specified with a positive `num_oov_buckets`. |\n| num_oov_buckets | Non-negative integer, the number of out-of-vocabulary buckets. All out-of-vocabulary inputs will be assigned IDs in the range `[vocabulary_size, vocabulary_size+num_oov_buckets)` based on a hash of the input value. A positive `num_oov_buckets` can not be specified with default_value. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_file(thal, vocabulary_file = file) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1408) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_categorical_column_with_vocabulary_file\n\n## Creates a categorical column with vocabulary file\n\n## Description\nUse this function when the vocabulary of a categorical variable is written to a file. \n\n\n## Usage\n```r\nstep_categorical_column_with_vocabulary_file( \n  spec, \n  ..., \n  vocabulary_file, \n  vocabulary_size = NULL, \n  dtype = tf$string, \n  default_value = NULL, \n  num_oov_buckets = 0L \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| vocabulary_file | The vocabulary file name. |\n| vocabulary_size | Number of the elements in the vocabulary. This must be no greater than length of `vocabulary_file`, if less than length, later values are ignored. If None, it is set to the length of `vocabulary_file`. |\n| dtype | The type of features. Only string and integer types are supported. |\n| default_value | The integer ID value to return for out-of-vocabulary feature values, defaults to `-1`. This can not be specified with a positive `num_oov_buckets`. |\n| num_oov_buckets | Non-negative integer, the number of out-of-vocabulary buckets. All out-of-vocabulary inputs will be assigned IDs in the range `[vocabulary_size, vocabulary_size+num_oov_buckets)` based on a hash of the input value. A positive `num_oov_buckets` can not be specified with default_value. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_file(thal, vocabulary_file = file) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_categorical_column_with_vocabulary_file_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_categorical_column_with_vocabulary_list/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""df6d30e70070abd76211a217a9aff5fe"",
+  ""hash"": ""6579a794c6b3b229047b90c5eca8317f"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_categorical_column_with_vocabulary_list\n\n## Creates a categorical column specification\n\n## Description\nCreates a categorical column specification \n\n\n## Usage\n```r\nstep_categorical_column_with_vocabulary_list( \n  spec, \n  ..., \n  vocabulary_list = NULL, \n  dtype = NULL, \n  default_value = -1L, \n  num_oov_buckets = 0L \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| vocabulary_list | An ordered iterable defining the vocabulary. Each feature is mapped to the index of its value (if present) in vocabulary_list. Must be castable to `dtype`. If `NULL` the vocabulary will be defined as all unique values in the dataset provided when fitting the specification. |\n| dtype | The type of features. Only string and integer types are supported. If `NULL`, it will be inferred from `vocabulary_list`. |\n| default_value | The integer ID value to return for out-of-vocabulary feature values, defaults to `-1`. This can not be specified with a positive num_oov_buckets. |\n| num_oov_buckets | Non-negative integer, the number of out-of-vocabulary buckets. All out-of-vocabulary inputs will be assigned IDs in the range `[lenght(vocabulary_list), length(vocabulary_list)+num_oov_buckets)` based on a hash of the input value. A positive num_oov_buckets can not be specified with default_value. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_list(thal) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1243) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_categorical_column_with_vocabulary_list\n\n## Creates a categorical column specification\n\n## Description\nCreates a categorical column specification \n\n\n## Usage\n```r\nstep_categorical_column_with_vocabulary_list( \n  spec, \n  ..., \n  vocabulary_list = NULL, \n  dtype = NULL, \n  default_value = -1L, \n  num_oov_buckets = 0L \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| vocabulary_list | An ordered iterable defining the vocabulary. Each feature is mapped to the index of its value (if present) in vocabulary_list. Must be castable to `dtype`. If `NULL` the vocabulary will be defined as all unique values in the dataset provided when fitting the specification. |\n| dtype | The type of features. Only string and integer types are supported. If `NULL`, it will be inferred from `vocabulary_list`. |\n| default_value | The integer ID value to return for out-of-vocabulary feature values, defaults to `-1`. This can not be specified with a positive num_oov_buckets. |\n| num_oov_buckets | Non-negative integer, the number of out-of-vocabulary buckets. All out-of-vocabulary inputs will be assigned IDs in the range `[lenght(vocabulary_list), length(vocabulary_list)+num_oov_buckets)` based on a hash of the input value. A positive num_oov_buckets can not be specified with default_value. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_list(thal) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_categorical_column_with_vocabulary_list_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_crossed_column/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""ad727013398593dd211b96a5956e685f"",
+  ""hash"": ""f85c91a10f371114e55e6ce3632c81c3"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_crossed_column\n\n## Creates crosses of categorical columns\n\n## Description\nUse this step to create crosses between categorical columns. \n\n\n## Usage\n```r\nstep_crossed_column(spec, ..., hash_bucket_size, hash_key = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| hash_bucket_size | An int > 1. The number of buckets. |\n| hash_key | (optional) Specify the hash_key that will be used by the FingerprintCat64 function to combine the crosses fingerprints on SparseCrossOp. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) %>% \n  step_bucketized_column(age, boundaries = c(10, 20, 30)) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1675) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_crossed_column\n\n## Creates crosses of categorical columns\n\n## Description\nUse this step to create crosses between categorical columns. \n\n\n## Usage\n```r\nstep_crossed_column(spec, ..., hash_bucket_size, hash_key = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| hash_bucket_size | An int > 1. The number of buckets. |\n| hash_key | (optional) Specify the hash_key that will be used by the FingerprintCat64 function to combine the crosses fingerprints on SparseCrossOp. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age) %>% \n  step_bucketized_column(age, boundaries = c(10, 20, 30)) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_crossed_column_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_embedding_column/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""d44b5929dcebbf2b9145ebb99857934e"",
+  ""hash"": ""4c1e6029064f18f8a35dfef68c4b6655"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_embedding_column\n\n## Creates embeddings columns\n\n## Description\nUse this step to create ambeddings columns from categorical columns. \n\n\n## Usage\n```r\nstep_embedding_column( \n  spec, \n  ..., \n  dimension = function(x) { \n     as.integer(x^0.25) \n }, \n  combiner = \""mean\"", \n  initializer = NULL, \n  ckpt_to_load_from = NULL, \n  tensor_name_in_ckpt = NULL, \n  max_norm = NULL, \n  trainable = TRUE \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| dimension | An integer specifying dimension of the embedding, must be > 0. Can also be a function of the size of the vocabulary. |\n| combiner | A string specifying how to reduce if there are multiple entries in a single row. Currently 'mean', 'sqrtn' and 'sum' are supported, with 'mean' the default. 'sqrtn' often achieves good accuracy, in particular with bag-of-words columns. Each of this can be thought as example level normalizations on the column. For more information, see `tf.embedding_lookup_sparse`. |\n| initializer | A variable initializer function to be used in embedding variable initialization. If not specified, defaults to `tf.truncated_normal_initializer` with mean `0.0` and standard deviation `1/sqrt(dimension)`. |\n| ckpt_to_load_from | String representing checkpoint name/pattern from which to restore column weights. Required if `tensor_name_in_ckpt` is not `NULL`. |\n| tensor_name_in_ckpt | Name of the Tensor in ckpt_to_load_from from which to restore the column weights. Required if `ckpt_to_load_from` is not `NULL`. |\n| max_norm | If not `NULL`, embedding values are l2-normalized to this value. |\n| trainable | Whether or not the embedding is trainable. Default is `TRUE`. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_embedding_column(thal, dimension = 3) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1549) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_embedding_column\n\n## Creates embeddings columns\n\n## Description\nUse this step to create ambeddings columns from categorical columns. \n\n\n## Usage\n```r\nstep_embedding_column( \n  spec, \n  ..., \n  dimension = function(x) { \n     as.integer(x^0.25) \n }, \n  combiner = \""mean\"", \n  initializer = NULL, \n  ckpt_to_load_from = NULL, \n  tensor_name_in_ckpt = NULL, \n  max_norm = NULL, \n  trainable = TRUE \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| dimension | An integer specifying dimension of the embedding, must be > 0. Can also be a function of the size of the vocabulary. |\n| combiner | A string specifying how to reduce if there are multiple entries in a single row. Currently 'mean', 'sqrtn' and 'sum' are supported, with 'mean' the default. 'sqrtn' often achieves good accuracy, in particular with bag-of-words columns. Each of this can be thought as example level normalizations on the column. For more information, see `tf.embedding_lookup_sparse`. |\n| initializer | A variable initializer function to be used in embedding variable initialization. If not specified, defaults to `tf.truncated_normal_initializer` with mean `0.0` and standard deviation `1/sqrt(dimension)`. |\n| ckpt_to_load_from | String representing checkpoint name/pattern from which to restore column weights. Required if `tensor_name_in_ckpt` is not `NULL`. |\n| tensor_name_in_ckpt | Name of the Tensor in ckpt_to_load_from from which to restore the column weights. Required if `ckpt_to_load_from` is not `NULL`. |\n| max_norm | If not `NULL`, embedding values are l2-normalized to this value. |\n| trainable | Whether or not the embedding is trainable. Default is `TRUE`. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_embedding_column(thal, dimension = 3) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_embedding_column_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_indicator_column/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""46db6b180b821d896d3b1ab2acf7c6b9"",
+  ""hash"": ""a75649f7b75eebe01f51d94714953c9f"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_indicator_column\n\n## Creates Indicator Columns\n\n## Description\nUse this step to create indicator columns from categorical columns. \n\n\n## Usage\n```r\nstep_indicator_column(spec, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_indicator_column(thal) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1500) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_indicator_column\n\n## Creates Indicator Columns\n\n## Description\nUse this step to create indicator columns from categorical columns. \n\n\n## Usage\n```r\nstep_indicator_column(spec, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nfile <- tempfile() \nwriteLines(unique(hearts$thal), file) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ thal) %>% \n  step_categorical_column_with_vocabulary_list(thal) %>% \n  step_indicator_column(thal) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_numeric_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_indicator_column_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_numeric_column/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""31e5dcb7159719c223f2cff69696a0b3"",
+  ""hash"": ""86f504e6af1f24847897cda480f18886"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_numeric_column\n\n## Creates a numeric column specification\n\n## Description\n`step_numeric_column` creates a numeric column specification. It can also be used to normalize numeric columns. \n\n\n## Usage\n```r\nstep_numeric_column( \n  spec, \n  ..., \n  shape = 1L, \n  default_value = NULL, \n  dtype = tf$float32, \n  normalizer_fn = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| shape | An iterable of integers specifies the shape of the Tensor. An integer can be given which means a single dimension Tensor with given width. The Tensor representing the column will have the shape of `batch_size` + `shape`. |\n| default_value | A single value compatible with `dtype` or an iterable of values compatible with `dtype` which the column takes on during `tf.Example` parsing if data is missing. A default value of `NULL` will cause `tf.parse_example` to fail if an example does not contain this column. If a single value is provided, the same value will be applied as the default value for every item. If an iterable of values is provided, the shape of the default_value should be equal to the given shape. |\n| dtype | defines the type of values. Default value is `tf$float32`. Must be a non-quantized, real integer or floating point type. |\n| normalizer_fn | If not `NULL`, a function that can be used to normalize the value of the tensor after default_value is applied for parsing. Normalizer function takes the input Tensor as its argument, and returns the output Tensor. (e.g. `function(x) (x - 3.0) / 4.2)`. Please note that even though the most common use case of this function is normalization, it can be used for any kind of Tensorflow transformations. You can also a pre-made scaler, in this case a function will be created after fit.FeatureSpec is called on the feature specification. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age, normalizer_fn = standard_scaler()) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1147) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_numeric_column\n\n## Creates a numeric column specification\n\n## Description\n`step_numeric_column` creates a numeric column specification. It can also be used to normalize numeric columns. \n\n\n## Usage\n```r\nstep_numeric_column( \n  spec, \n  ..., \n  shape = 1L, \n  default_value = NULL, \n  dtype = tf$float32, \n  normalizer_fn = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n| shape | An iterable of integers specifies the shape of the Tensor. An integer can be given which means a single dimension Tensor with given width. The Tensor representing the column will have the shape of `batch_size` + `shape`. |\n| default_value | A single value compatible with `dtype` or an iterable of values compatible with `dtype` which the column takes on during `tf.Example` parsing if data is missing. A default value of `NULL` will cause `tf.parse_example` to fail if an example does not contain this column. If a single value is provided, the same value will be applied as the default value for every item. If an iterable of values is provided, the shape of the default_value should be equal to the given shape. |\n| dtype | defines the type of values. Default value is `tf$float32`. Must be a non-quantized, real integer or floating point type. |\n| normalizer_fn | If not `NULL`, a function that can be used to normalize the value of the tensor after default_value is applied for parsing. Normalizer function takes the input Tensor as its argument, and returns the output Tensor. (e.g. `function(x) (x - 3.0) / 4.2)`. Please note that even though the most common use case of this function is normalization, it can be used for any kind of Tensorflow transformations. You can also a pre-made scaler, in this case a function will be created after fit.FeatureSpec is called on the feature specification. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age, normalizer_fn = standard_scaler()) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_remove_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_numeric_column_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/step_remove_column/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""e493cd5ad7652f5e9799bbcdbc94f0ae"",
+  ""hash"": ""8aaf59a745959f235830199817efc469"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/feature_spec.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/feature_spec.R*\n\n# step_remove_column\n\n## Creates a step that can remove columns\n\n## Description\nRemoves features of the feature specification. \n\n\n## Usage\n```r\nstep_remove_column(spec, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age, normalizer_fn = scaler_standard()) %>% \n  step_bucketized_column(age, boundaries = c(20, 50)) %>% \n  step_remove_column(age) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/feature_spec.R#L1191) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_remove_column\n\n## Creates a step that can remove columns\n\n## Description\nRemoves features of the feature specification. \n\n\n## Usage\n```r\nstep_remove_column(spec, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| spec | A feature specification created with `feature_spec()`. |\n| ... | Comma separated list of variable names to apply the step. selectors can also be used. |\n\n\n\n## Value\na `FeatureSpec` object. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndata(hearts) \nhearts <- tensor_slices_dataset(hearts) %>% dataset_batch(32) \n# use the formula interface \nspec <- feature_spec(hearts, target ~ age) %>% \n  step_numeric_column(age, normalizer_fn = scaler_standard()) %>% \n  step_bucketized_column(age, boundaries = c(20, 50)) %>% \n  step_remove_column(age) \nspec_fit <- fit(spec) \nfinal_dataset <- hearts %>% dataset_use_spec(spec_fit) \n```\n:::\n\n\n## See Also\n\nsteps for a complete list of allowed steps. \n\nOther Feature Spec Functions:  `dataset_use_spec()`, `feature_spec()`, `fit.FeatureSpec()`, `step_bucketized_column()`, `step_categorical_column_with_hash_bucket()`, `step_categorical_column_with_identity()`, `step_categorical_column_with_vocabulary_file()`, `step_categorical_column_with_vocabulary_list()`, `step_crossed_column()`, `step_embedding_column()`, `step_indicator_column()`, `step_numeric_column()`, `step_shared_embeddings_column()`, `steps`\n\n"",
+    ""supporting"": [
+      ""step_remove_column_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/tfrecord_dataset/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""728e7c999ca60c8e7cd44f0d7982735a"",
+  ""hash"": ""8514da1bbc80848b4dea8fc3a419401e"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/tfrecord_dataset.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/tfrecord_dataset.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/tfrecord_dataset.R*\n\n# tfrecord_dataset\n\n## A dataset comprising records from one or more TFRecord files.\n\n## Description\nA dataset comprising records from one or more TFRecord files. \n\n\n## Usage\n```r\ntfrecord_dataset( \n  filenames, \n  compression_type = NULL, \n  buffer_size = NULL, \n  num_parallel_reads = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| filenames | String(s) specifying one or more filenames |\n| compression_type | A string, one of: `NULL` (no compression), `\""ZLIB\""`, or `\""GZIP\""`. |\n| buffer_size | An integer representing the number of bytes in the read buffer. (0 means no buffering). |\n| num_parallel_reads | An integer representing the number of files to read in parallel. Defaults to reading files sequentially. |\n\n## Details\nIf the dataset encodes a set of TFExample instances, then they can be decoded into named records using the `dataset_map()` function (see example below). \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates a dataset that reads all of the examples from two files, and extracts \n# the image and label features. \nfilenames <- c(\""/var/data/file1.tfrecord\"", \""/var/data/file2.tfrecord\"") \ndataset <- tfrecord_dataset(filenames) %>% \n  dataset_map(function(example_proto) { \n    features <- list( \n      image = tf$FixedLenFeature(shape(), tf$string, default_value = \""\""), \n      label = tf$FixedLenFeature(shape(), tf$int32, default_value = 0L) \n    ) \n    tf$parse_single_example(example_proto, features) \n  }) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/tfrecord_dataset.R#L29) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tfrecord_dataset\n\n## A dataset comprising records from one or more TFRecord files.\n\n## Description\nA dataset comprising records from one or more TFRecord files. \n\n\n## Usage\n```r\ntfrecord_dataset( \n  filenames, \n  compression_type = NULL, \n  buffer_size = NULL, \n  num_parallel_reads = NULL \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| filenames | String(s) specifying one or more filenames |\n| compression_type | A string, one of: `NULL` (no compression), `\""ZLIB\""`, or `\""GZIP\""`. |\n| buffer_size | An integer representing the number of bytes in the read buffer. (0 means no buffering). |\n| num_parallel_reads | An integer representing the number of files to read in parallel. Defaults to reading files sequentially. |\n\n## Details\nIf the dataset encodes a set of TFExample instances, then they can be decoded into named records using the `dataset_map()` function (see example below). \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n# Creates a dataset that reads all of the examples from two files, and extracts \n# the image and label features. \nfilenames <- c(\""/var/data/file1.tfrecord\"", \""/var/data/file2.tfrecord\"") \ndataset <- tfrecord_dataset(filenames) %>% \n  dataset_map(function(example_proto) { \n    features <- list( \n      image = tf$FixedLenFeature(shape(), tf$string, default_value = \""\""), \n      label = tf$FixedLenFeature(shape(), tf$int32, default_value = 0L) \n    ) \n    tf$parse_single_example(example_proto, features) \n  }) \n```\n:::\n"",
+    ""supporting"": [
+      ""tfrecord_dataset_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/until_out_of_range/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""52f739f678af290e561904d7cab3a8ce"",
+  ""hash"": ""efffe9f3c070a0800d3c7a406d85cb76"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_iterators.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_iterators.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_iterators.R*\n\n# until_out_of_range\n\n## Execute code that traverses a dataset until an out of range condition occurs\n\n## Description\nExecute code that traverses a dataset until an out of range condition occurs \n\n\n## Usage\n```r\nuntil_out_of_range(expr) \nout_of_range_handler(e) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| expr | Expression to execute (will be executed multiple times until the condition occurs) |\n| e | Error object |\n\n## Details\nWhen a dataset iterator reaches the end, an out of range runtime error will occur. This function will catch and ignore the error when it occurs. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) \niter <- make_iterator_one_shot(dataset) \nnext_batch <- iterator_get_next(iter) \nuntil_out_of_range({ \n  batch <- sess$run(next_batch) \n  # use batch$x and batch$y tensors \n}) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_iterators.R#L146) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# until_out_of_range\n\n## Execute code that traverses a dataset until an out of range condition occurs\n\n## Description\nExecute code that traverses a dataset until an out of range condition occurs \n\n\n## Usage\n```r\nuntil_out_of_range(expr) \n\nout_of_range_handler(e) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| expr | Expression to execute (will be executed multiple times until the condition occurs) |\n| e | Error object |\n\n## Details\nWhen a dataset iterator reaches the end, an out of range runtime error will occur. This function will catch and ignore the error when it occurs. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) \niter <- make_iterator_one_shot(dataset) \nnext_batch <- iterator_get_next(iter) \nuntil_out_of_range({ \n  batch <- sess$run(next_batch) \n  # use batch$x and batch$y tensors \n}) \n```\n:::\n"",
+    ""supporting"": [
+      ""until_out_of_range_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfdatasets/with_dataset/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""299d5cea7738a01cb68c096bdc4b6c9b"",
+  ""hash"": ""a7f754e40c1684f5646ae1ad0bc85359"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_iterators.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfdatasets//edit/main/R/dataset_iterators.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/dataset_iterators.R*\n\n# with_dataset\n\n## Execute code that traverses a dataset\n\n## Description\nExecute code that traverses a dataset \n\n\n## Usage\n```r\nwith_dataset(expr) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| expr | Expression to execute |\n\n## Details\nWhen a dataset iterator reaches the end, an out of range runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in a call to `with_dataset()` (see the example below for an illustration). \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10) \niter <- make_iterator_one_shot(dataset) \nnext_batch <- iterator_get_next(iter) \nwith_dataset({ \n  while(TRUE) { \n    batch <- sess$run(next_batch) \n    # use batch$x and batch$y tensors \n  } \n}) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfdatasets//blob/main/R/dataset_iterators.R#L111) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# with_dataset\n\n## Execute code that traverses a dataset\n\n## Description\nExecute code that traverses a dataset \n\n\n## Usage\n```r\nwith_dataset(expr) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| expr | Expression to execute |\n\n## Details\nWhen a dataset iterator reaches the end, an out of range runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in a call to `with_dataset()` (see the example below for an illustration). \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets) \ndataset <- text_line_dataset(\""mtcars.csv\"", record_spec = mtcars_spec) %>% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %>% \n  dataset_batch(128) %>% \n  dataset_repeat(10) \niter <- make_iterator_one_shot(dataset) \nnext_batch <- iterator_get_next(iter) \nwith_dataset({ \n  while(TRUE) { \n    batch <- sess$run(next_batch) \n    # use batch$x and batch$y tensors \n  } \n}) \n```\n:::\n"",
+    ""supporting"": [
+      ""with_dataset_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfhub/hub_load/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""6fe0d1343e0457379afee3f67cb4470b"",
+  ""hash"": ""d853865a74d01b0f13e558472e94b517"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfhub//blob/main/R/load.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfhub//edit/main/R/load.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/load.R*\n\n# hub_load\n\n## Hub Load\n\n## Description\nLoads a module from a handle. \n\n\n## Usage\n```r\nhub_load(handle, tags = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| handle | (string) the Module handle to resolve. |\n| tags | A set of strings specifying the graph variant to use, if loading from a v1 module. |\n\n## Details\n\nCurrently this method is fully supported only with Tensorflow 2.x and with modules created by calling `export_savedmodel`. The method works in both eager and graph modes. \n\nDepending on the type of handle used, the call may involve downloading a TensorFlow Hub module to a local cache location specified by the `TFHUB_CACHE_DIR` environment variable. If a copy of the module is already present in the TFHUB_CACHE_DIR, the download step is skipped. \n\nCurrently, three types of module handles are supported: 1) Smart URL resolvers such as tfhub.dev, e.g.: https://tfhub.dev/google/nnlm-en-dim128/1. 2) A directory on a file system supported by Tensorflow containing module files. This may include a local directory (e.g. /usr/local/mymodule) or a Google Cloud Storage bucket (gs://mymodule). 3) A URL pointing to a TGZ archive of a module, e.g. https://example.com/mymodule.tar.gz. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- hub_load('https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4') \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfhub//blob/main/R/load.R#L34) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# hub_load\n\n## Hub Load\n\n## Description\nLoads a module from a handle. \n\n\n## Usage\n```r\nhub_load(handle, tags = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| handle | (string) the Module handle to resolve. |\n| tags | A set of strings specifying the graph variant to use, if loading from a v1 module. |\n\n## Details\n\nCurrently this method is fully supported only with Tensorflow 2.x and with modules created by calling `export_savedmodel`. The method works in both eager and graph modes. \n\nDepending on the type of handle used, the call may involve downloading a TensorFlow Hub module to a local cache location specified by the `TFHUB_CACHE_DIR` environment variable. If a copy of the module is already present in the TFHUB_CACHE_DIR, the download step is skipped. \n\nCurrently, three types of module handles are supported: 1) Smart URL resolvers such as tfhub.dev, e.g.: https://tfhub.dev/google/nnlm-en-dim128/1. 2) A directory on a file system supported by Tensorflow containing module files. This may include a local directory (e.g. /usr/local/mymodule) or a Google Cloud Storage bucket (gs://mymodule). 3) A URL pointing to a TGZ archive of a module, e.g. https://example.com/mymodule.tar.gz. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfhub)\nmodel <- hub_load('https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4') \n```\n:::\n"",
+    ""supporting"": [
+      ""hub_load_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfhub/layer_hub/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""a56eaf2ecf6670d3bc96959b411a7df5"",
+  ""hash"": ""0994c256f4101659a94b1a752e4498ba"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfhub//blob/main/R/layer.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfhub//edit/main/R/layer.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/layer.R*\n\n# layer_hub\n\n## Hub Layer\n\n## Description\nWraps a Hub module (or a similar callable) for TF2 as a Keras Layer. \n\n\n## Usage\n```r\nlayer_hub(object, handle, trainable = FALSE, arguments = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Model or layer object |\n| handle | a callable object (subject to the conventions above), or a string for which `hub_load()` returns such a callable. A string is required to save the Keras config of this Layer. |\n| trainable | Boolean controlling whether this layer is trainable. |\n| arguments | optionally, a list with additional keyword arguments passed to the callable. These must be JSON-serializable to save the Keras config of this layer. |\n| ... | Other arguments that are passed to the TensorFlow Hub module. |\n\n## Details\n\nThis layer wraps a callable object for use as a Keras layer. The callable object can be passed directly, or be specified by a string with a handle that gets passed to `hub_load()`. \n\nThe callable object is expected to follow the conventions detailed below. (These are met by TF2-compatible modules loaded from TensorFlow Hub.) \n\nThe callable is invoked with a single positional argument set to one tensor or a list of tensors containing the inputs to the layer. If the callable accepts a training argument, a boolean is passed for it. It is `TRUE` if this layer is marked trainable and called for training. \n\nIf present, the following attributes of callable are understood to have special meanings: variables: a list of all tf.Variable objects that the callable depends on. trainable_variables: those elements of variables that are reported as trainable variables of this Keras Layer when the layer is trainable. regularization_losses: a list of callables to be added as losses of this Keras Layer when the layer is trainable. Each one must accept zero arguments and return a scalar tensor. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) \nmodel <- keras_model_sequential() %>% \n layer_hub( \n   handle = \""https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"", \n   input_shape = c(224, 224, 3) \n ) %>% \n layer_dense(1) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfhub//blob/main/R/layer.R#L50) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# layer_hub\n\n## Hub Layer\n\n## Description\nWraps a Hub module (or a similar callable) for TF2 as a Keras Layer. \n\n\n## Usage\n```r\nlayer_hub(object, handle, trainable = FALSE, arguments = NULL, ...) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| object | Model or layer object |\n| handle | a callable object (subject to the conventions above), or a string for which `hub_load()` returns such a callable. A string is required to save the Keras config of this Layer. |\n| trainable | Boolean controlling whether this layer is trainable. |\n| arguments | optionally, a list with additional keyword arguments passed to the callable. These must be JSON-serializable to save the Keras config of this layer. |\n| ... | Other arguments that are passed to the TensorFlow Hub module. |\n\n## Details\n\nThis layer wraps a callable object for use as a Keras layer. The callable object can be passed directly, or be specified by a string with a handle that gets passed to `hub_load()`. \n\nThe callable object is expected to follow the conventions detailed below. (These are met by TF2-compatible modules loaded from TensorFlow Hub.) \n\nThe callable is invoked with a single positional argument set to one tensor or a list of tensors containing the inputs to the layer. If the callable accepts a training argument, a boolean is passed for it. It is `TRUE` if this layer is marked trainable and called for training. \n\nIf present, the following attributes of callable are understood to have special meanings: variables: a list of all tf.Variable objects that the callable depends on. trainable_variables: those elements of variables that are reported as trainable variables of this Keras Layer when the layer is trainable. regularization_losses: a list of callables to be added as losses of this Keras Layer when the layer is trainable. Each one must accept zero arguments and return a scalar tensor. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfhub)\nlibrary(keras) \nmodel <- keras_model_sequential() %>% \n layer_hub( \n   handle = \""https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"", \n   input_shape = c(224, 224, 3) \n ) %>% \n layer_dense(1) \n```\n:::\n"",
+    ""supporting"": [
+      ""layer_hub_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfhub/step_pretrained_text_embedding/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""afacd97f80a3c9ba4fd1bc478596fe19"",
+  ""hash"": ""2039d218ea303f12b29aaf478406e292"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfhub//blob/main/R/recipe.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfhub//edit/main/R/recipe.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/recipe.R*\n\n# step_pretrained_text_embedding\n\n## Pretrained text-embeddings\n\n## Description\n`step_pretrained_text_embedding` creates a *specification* of a  recipe step that will transform text data into its numerical  transformation based on a pretrained model. \n\n\n## Usage\n```r\nstep_pretrained_text_embedding( \n  recipe, \n  ..., \n  role = \""predictor\"", \n  trained = FALSE, \n  handle, \n  args = NULL, \n  skip = FALSE, \n  id = recipes::rand_id(\""pretrained_text_embedding\"") \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| recipe | A recipe object. The step will be added to the sequence of operations for this recipe. |\n| ... | One or more selector functions to choose variables. |\n| role | Role for the created variables |\n| trained | A logical to indicate if the quantities for preprocessing have been estimated. |\n| handle | the Module handle to resolve. |\n| args | other arguments passed to [hub_load()]. |\n| skip | A logical. Should the step be skipped when the recipe is baked by [recipes::bake.recipe()]? While all operations are baked when [recipes::prep.recipe()] is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using `skip = TRUE` as it may affect the computations for subsequent operations |\n| id | A character string that is unique to this step to identify it. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibble) \nlibrary(recipes) \ndf <- tibble(text = c('hi', \""heello\"", \""goodbye\""), y = 0) \nrec <- recipe(y ~ text, df) \nrec <- rec %>% step_pretrained_text_embedding( \n text, \n handle = \""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1\"" \n) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfhub//blob/main/R/recipe.R#L39) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# step_pretrained_text_embedding\n\n## Pretrained text-embeddings\n\n## Description\n`step_pretrained_text_embedding` creates a *specification* of a  recipe step that will transform text data into its numerical  transformation based on a pretrained model. \n\n\n## Usage\n```r\nstep_pretrained_text_embedding( \n  recipe, \n  ..., \n  role = \""predictor\"", \n  trained = FALSE, \n  handle, \n  args = NULL, \n  skip = FALSE, \n  id = recipes::rand_id(\""pretrained_text_embedding\"") \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| recipe | A recipe object. The step will be added to the sequence of operations for this recipe. |\n| ... | One or more selector functions to choose variables. |\n| role | Role for the created variables |\n| trained | A logical to indicate if the quantities for preprocessing have been estimated. |\n| handle | the Module handle to resolve. |\n| args | other arguments passed to [hub_load()]. |\n| skip | A logical. Should the step be skipped when the recipe is baked by [recipes::bake.recipe()]? While all operations are baked when [recipes::prep.recipe()] is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using `skip = TRUE` as it may affect the computations for subsequent operations |\n| id | A character string that is unique to this step to identify it. |\n\n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfhub)\nlibrary(tibble) \nlibrary(recipes) \ndf <- tibble(text = c('hi', \""heello\"", \""goodbye\""), y = 0) \nrec <- recipe(y ~ text, df) \nrec <- rec %>% step_pretrained_text_embedding( \n text, \n handle = \""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1\"" \n) \n```\n:::\n"",
+    ""supporting"": [
+      ""step_pretrained_text_embedding_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfruns/clean_runs/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""f6fe83d7b991e46b8b5a096726d6138e"",
+  ""hash"": ""c323aff1b4ab85c67f11363a05b057ae"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/clean_runs.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfruns//edit/main/R/clean_runs.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/clean_runs.R*\n\n# clean_runs\n\n## Clean run directories\n\n## Description\nRemove run directories from the filesystem. \n\n\n## Usage\n```r\nclean_runs( \n  runs = ls_runs(runs_dir = runs_dir), \n  runs_dir = getOption(\""tfruns.runs_dir\"", \""runs\""), \n  confirm = interactive() \n) \npurge_runs( \n  runs_dir = getOption(\""tfruns.runs_dir\"", \""runs\""), \n  confirm = interactive() \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| runs | Runs to clean. Can be specified as a data frame (as returned by `ls_runs()`) or as a character vector of run directories. |\n| runs_dir | Directory containing runs. Defaults to \""runs\"" beneath the current working directory (or to the value of the `tfruns.runs_dir` R option if specified). |\n| confirm | `TRUE` to confirm before performing operation |\n\n## Details\n\nThe `clean_runs()` function moves the specified runs (by default, all runs) into an \""archive\"" subdirectory of the \""runs\"" directory. \n\nThe `purge_runs()` function permanently deletes the \""archive\"" subdirectory. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_runs(ls_runs(completed == FALSE)) \n```\n:::\n\n\n## See Also\nOther run management:  `copy_run()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/clean_runs.R#L24) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# clean_runs\n\n## Clean run directories\n\n## Description\nRemove run directories from the filesystem. \n\n\n## Usage\n```r\nclean_runs( \n  runs = ls_runs(runs_dir = runs_dir), \n  runs_dir = getOption(\""tfruns.runs_dir\"", \""runs\""), \n  confirm = interactive() \n) \n\npurge_runs( \n  runs_dir = getOption(\""tfruns.runs_dir\"", \""runs\""), \n  confirm = interactive() \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| runs | Runs to clean. Can be specified as a data frame (as returned by `ls_runs()`) or as a character vector of run directories. |\n| runs_dir | Directory containing runs. Defaults to \""runs\"" beneath the current working directory (or to the value of the `tfruns.runs_dir` R option if specified). |\n| confirm | `TRUE` to confirm before performing operation |\n\n## Details\n\nThe `clean_runs()` function moves the specified runs (by default, all runs) into an \""archive\"" subdirectory of the \""runs\"" directory. \n\nThe `purge_runs()` function permanently deletes the \""archive\"" subdirectory. \n\n\n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfruns)\nclean_runs(ls_runs(completed == FALSE)) \n```\n:::\n\n\n## See Also\nOther run management:  `copy_run()`\n\n"",
+    ""supporting"": [
+      ""clean_runs_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfruns/copy_run/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""0e42b7193f87e1ae36b99335046585c9"",
+  ""hash"": ""eaa1a489c41f2ead030ed0a4dfc0704c"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/copy.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfruns//edit/main/R/copy.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/copy.R*\n\n# copy_run\n\n## Copy run directories\n\n## Description\nFunctions for exporting/copying run directories and run artifact files. \n\n\n## Usage\n```r\ncopy_run(run_dir, to = \"".\"", rename = NULL) \ncopy_run_files(run_dir, to = \"".\"", rename = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| run_dir | Training run directory or data frame returned from `ls_runs()`. |\n| to | Name of parent directory to copy run(s) into. Defaults to the current working directory. |\n| rename | Rename run directory after copying. If not specified this defaults to the basename of the run directory (e.g. \""2017-09-24T10-54-00Z\""). |\n\n## Details\n\nUse `copy_run` to copy one or more run directories. \n\nUse `copy_run_files` to copy only files saved/generated by training run scripts (e.g. saved models, checkpoints, etc.). \n\n\n## Value\nLogical vector indicating which operation succeeded for each of the run directories specified. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\n# export a run directory to the current working directory \ncopy_run(\""runs/2017-09-24T10-54-00Z\"") \n# export to the current working directory then rename \ncopy_run(\""runs/2017-09-24T10-54-00Z\"", rename = \""best-run\"") \n# export artifact files only to the current working directory then rename \ncopy_run_files(\""runs/2017-09-24T10-54-00Z\"", rename = \""best-model\"") \n# export 3 best eval_acc to a \""best-runs\"" directory \ncopy_run(ls_runs(order = eval_acc)[1:3,], to = \""best-runs\"") \n```\n:::\n\n\n## See Also\nOther run management:  `clean_runs()`\n\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/copy.R#L41) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# copy_run\n\n## Copy run directories\n\n## Description\nFunctions for exporting/copying run directories and run artifact files. \n\n\n## Usage\n```r\ncopy_run(run_dir, to = \"".\"", rename = NULL) \n\ncopy_run_files(run_dir, to = \"".\"", rename = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| run_dir | Training run directory or data frame returned from `ls_runs()`. |\n| to | Name of parent directory to copy run(s) into. Defaults to the current working directory. |\n| rename | Rename run directory after copying. If not specified this defaults to the basename of the run directory (e.g. \""2017-09-24T10-54-00Z\""). |\n\n## Details\n\nUse `copy_run` to copy one or more run directories. \n\nUse `copy_run_files` to copy only files saved/generated by training run scripts (e.g. saved models, checkpoints, etc.). \n\n\n## Value\nLogical vector indicating which operation succeeded for each of the run directories specified. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfruns)\n# export a run directory to the current working directory \ncopy_run(\""runs/2017-09-24T10-54-00Z\"") \n# export to the current working directory then rename \ncopy_run(\""runs/2017-09-24T10-54-00Z\"", rename = \""best-run\"") \n# export artifact files only to the current working directory then rename \ncopy_run_files(\""runs/2017-09-24T10-54-00Z\"", rename = \""best-model\"") \n# export 3 best eval_acc to a \""best-runs\"" directory \ncopy_run(ls_runs(order = eval_acc)[1:3,], to = \""best-runs\"") \n```\n:::\n\n\n## See Also\nOther run management:  `clean_runs()`\n\n"",
+    ""supporting"": [
+      ""copy_run_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfruns/flags/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""62256d141a9db6cea6826e5775343f7c"",
+  ""hash"": ""8aad89c84af8fc3e74c5366a8c97a7b6"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/flags.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfruns//edit/main/R/flags.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/flags.R*\n\n# flags\n\n## Flags for a training run\n\n## Description\nDefine the flags (name, type, default value, description) which paramaterize a training run. Optionally read overrides of the default values from a \""flags.yml\"" config file and/or command line arguments. \n\n\n## Usage\n```r\nflags( \n  ..., \n  config = Sys.getenv(\""R_CONFIG_ACTIVE\"", unset = \""default\""), \n  file = \""flags.yml\"", \n  arguments = commandArgs(TRUE) \n) \nflag_numeric(name, default, description = NULL) \nflag_integer(name, default, description = NULL) \nflag_boolean(name, default, description = NULL) \nflag_string(name, default, description = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | One or more flag definitions |\n| config | The configuration to use. Defaults to the active configuration for the current environment (as specified by the `R_CONFIG_ACTIVE`<br>environment variable), or `default` when unset. |\n| file | The flags YAML file to read |\n| arguments | The command line arguments (as a character vector) to be parsed. |\n| name | Flag name |\n| default | Flag default value |\n| description | Flag description |\n\n\n## Section\n\n## Config File Flags\n\nConfig file flags are defined a YAML configuration file (by default named \""flags.yml\""). Flags can either appear at the top-level of the YAML or can be inclued in named configuration sections (see the [config package](https://github.com/rstudio/config) for details). \n\n## Command Line Flags\n\nCommand line flags should be of the form `--key=value` or `--key value`. The values are assumed to be valid `yaml` and will be converted using `yaml.load()`. \n\n## Value\nNamed list of training flags \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfruns) \n# define flags and parse flag values from flags.yml and the command line \nFLAGS <- flags( \n  flag_numeric('learning_rate', 0.01, 'Initial learning rate.'), \n  flag_integer('max_steps', 5000, 'Number of steps to run trainer.'), \n  flag_string('data_dir', 'MNIST-data', 'Directory for training data'), \n  flag_boolean('fake_data', FALSE, 'If true, use fake data for testing') \n) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/flags.R#L49) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# flags\n\n## Flags for a training run\n\n## Description\nDefine the flags (name, type, default value, description) which paramaterize a training run. Optionally read overrides of the default values from a \""flags.yml\"" config file and/or command line arguments. \n\n\n## Usage\n```r\nflags( \n  ..., \n  config = Sys.getenv(\""R_CONFIG_ACTIVE\"", unset = \""default\""), \n  file = \""flags.yml\"", \n  arguments = commandArgs(TRUE) \n) \n\nflag_numeric(name, default, description = NULL) \n\nflag_integer(name, default, description = NULL) \n\nflag_boolean(name, default, description = NULL) \n\nflag_string(name, default, description = NULL) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| ... | One or more flag definitions |\n| config | The configuration to use. Defaults to the active configuration for the current environment (as specified by the `R_CONFIG_ACTIVE`<br>environment variable), or `default` when unset. |\n| file | The flags YAML file to read |\n| arguments | The command line arguments (as a character vector) to be parsed. |\n| name | Flag name |\n| default | Flag default value |\n| description | Flag description |\n\n\n## Section\n\n## Config File Flags\n\nConfig file flags are defined a YAML configuration file (by default named \""flags.yml\""). Flags can either appear at the top-level of the YAML or can be inclued in named configuration sections (see the [config package](https://github.com/rstudio/config) for details). \n\n## Command Line Flags\n\nCommand line flags should be of the form `--key=value` or `--key value`. The values are assumed to be valid `yaml` and will be converted using `yaml.load()`. \n\n## Value\nNamed list of training flags \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfruns) \n# define flags and parse flag values from flags.yml and the command line \nFLAGS <- flags( \n  flag_numeric('learning_rate', 0.01, 'Initial learning rate.'), \n  flag_integer('max_steps', 5000, 'Number of steps to run trainer.'), \n  flag_string('data_dir', 'MNIST-data', 'Directory for training data'), \n  flag_boolean('fake_data', FALSE, 'If true, use fake data for testing') \n) \n```\n:::\n"",
+    ""supporting"": [
+      ""flags_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/reference/tfruns/tuning_run/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
-  ""hash"": ""a77bebc44e5ad14d35c6d1850cbfabdc"",
+  ""hash"": ""3d0379bf957189791304fe87f1350dc1"",
   ""result"": {
-    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/training_run.R) </button> | <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [Suggest edits on GitHub](https://github.com/rstudio/tfruns//edit/main/R/training_run.R) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n*R/training_run.R*\n\n# tuning_run\n\n## Tune hyperparameters using training flags\n\n## Description\nRun all combinations of the specifed training flags. The number of combinations can be reduced by specifying the `sample` parameter, which will result in a random sample of the flag combinations being run. \n\n\n## Usage\n```r\ntuning_run( \n  file = \""train.R\"", \n  context = \""local\"", \n  config = Sys.getenv(\""R_CONFIG_ACTIVE\"", unset = \""default\""), \n  flags = NULL, \n  sample = NULL, \n  properties = NULL, \n  runs_dir = getOption(\""tfruns.runs_dir\"", \""runs\""), \n  artifacts_dir = getwd(), \n  echo = TRUE, \n  confirm = interactive(), \n  envir = parent.frame(), \n  encoding = getOption(\""encoding\"") \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| file | Path to training script (defaults to \""train.R\"") |\n| context | Run context (defaults to \""local\"") |\n| config | The configuration to use. Defaults to the active configuration for the current environment (as specified by the `R_CONFIG_ACTIVE`<br>environment variable), or `default` when unset. |\n| flags | Either a named list with flag values (multiple values can be provided for each flag) or a data frame that contains pre-generated combinations of flags (e.g. via `base::expand.grid()`). The latter can be useful for subsetting combinations. See 'Examples'. |\n| sample | Sampling rate for flag combinations (defaults to running all combinations). |\n| properties | Named character vector with run properties. Properties are additional metadata about the run which will be subsequently available via `ls_runs()`. |\n| runs_dir | Directory containing runs. Defaults to \""runs\"" beneath the current working directory (or to the value of the `tfruns.runs_dir` R option if specified). |\n| artifacts_dir | Directory to capture created and modified files within. Pass `NULL` to not capture any artifcats. |\n| echo | Print expressions within training script |\n| confirm | Confirm before executing tuning run. |\n| envir | The environment in which the script should be evaluated |\n| encoding | The encoding of the training script; see `file()`. |\n\n\n\n## Value\nData frame with summary of all training runs performed during tuning. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfruns) \n# using a list as input to the flags argument \nruns <- tuning_run( \n  system.file(\""examples/mnist_mlp/mnist_mlp.R\"", package = \""tfruns\""), \n  flags = list( \n    dropout1 = c(0.2, 0.3, 0.4), \n    dropout2 = c(0.2, 0.3, 0.4) \n  ) \n) \nruns[order(runs$eval_acc, decreasing = TRUE), ] \n# using a data frame as input to the flags argument \n# resulting in the same combinations above, but remove those \n# where the combined dropout rate exceeds 1 \ngrid <- expand.grid( \n  dropout1 = c(0.2, 0.3, 0.4), \n  dropout2 = c(0.2, 0.3, 0.4) \n) \ngrid$combined_droput <- grid$dropout1 + grid$dropout2 \ngrid <- grid[grid$combined_droput <= 1, ] \nruns <- tuning_run( \n  system.file(\""examples/mnist_mlp/mnist_mlp.R\"", package = \""tfruns\""), \n  flags = grid[, c(\""dropout1\"", \""dropout2\"")] \n) \n```\n:::\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\nformat:\n  html:\n    css: /reference/assets/reference.css\n---\n\n\n| <button class=\""button\""> ![](/reference/assets/GitHub-Mark-32px.png){width=\""20\""} [View source on GitHub](https://github.com/rstudio/tfruns//blob/main/R/training_run.R#L167) </button> |\n|:------------------------------:|:--------------------------------------:|\n\n# tuning_run\n\n## Tune hyperparameters using training flags\n\n## Description\nRun all combinations of the specifed training flags. The number of combinations can be reduced by specifying the `sample` parameter, which will result in a random sample of the flag combinations being run. \n\n\n## Usage\n```r\ntuning_run( \n  file = \""train.R\"", \n  context = \""local\"", \n  config = Sys.getenv(\""R_CONFIG_ACTIVE\"", unset = \""default\""), \n  flags = NULL, \n  sample = NULL, \n  properties = NULL, \n  runs_dir = getOption(\""tfruns.runs_dir\"", \""runs\""), \n  artifacts_dir = getwd(), \n  echo = TRUE, \n  confirm = interactive(), \n  envir = parent.frame(), \n  encoding = getOption(\""encoding\"") \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| file | Path to training script (defaults to \""train.R\"") |\n| context | Run context (defaults to \""local\"") |\n| config | The configuration to use. Defaults to the active configuration for the current environment (as specified by the `R_CONFIG_ACTIVE`<br>environment variable), or `default` when unset. |\n| flags | Either a named list with flag values (multiple values can be provided for each flag) or a data frame that contains pre-generated combinations of flags (e.g. via `base::expand.grid()`). The latter can be useful for subsetting combinations. See 'Examples'. |\n| sample | Sampling rate for flag combinations (defaults to running all combinations). |\n| properties | Named character vector with run properties. Properties are additional metadata about the run which will be subsequently available via `ls_runs()`. |\n| runs_dir | Directory containing runs. Defaults to \""runs\"" beneath the current working directory (or to the value of the `tfruns.runs_dir` R option if specified). |\n| artifacts_dir | Directory to capture created and modified files within. Pass `NULL` to not capture any artifcats. |\n| echo | Print expressions within training script |\n| confirm | Confirm before executing tuning run. |\n| envir | The environment in which the script should be evaluated |\n| encoding | The encoding of the training script; see `file()`. |\n\n\n\n## Value\nData frame with summary of all training runs performed during tuning. \n\n\n## Examples\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfruns) \n# using a list as input to the flags argument \nruns <- tuning_run( \n  system.file(\""examples/mnist_mlp/mnist_mlp.R\"", package = \""tfruns\""), \n  flags = list( \n    dropout1 = c(0.2, 0.3, 0.4), \n    dropout2 = c(0.2, 0.3, 0.4) \n  ) \n) \nruns[order(runs$eval_acc, decreasing = TRUE), ] \n# using a data frame as input to the flags argument \n# resulting in the same combinations above, but remove those \n# where the combined dropout rate exceeds 1 \ngrid <- expand.grid( \n  dropout1 = c(0.2, 0.3, 0.4), \n  dropout2 = c(0.2, 0.3, 0.4) \n) \ngrid$combined_droput <- grid$dropout1 + grid$dropout2 \ngrid <- grid[grid$combined_droput <= 1, ] \nruns <- tuning_run( \n  system.file(\""examples/mnist_mlp/mnist_mlp.R\"", package = \""tfruns\""), \n  flags = grid[, c(\""dropout1\"", \""dropout2\"")] \n) \n```\n:::\n"",
+    ""supporting"": [
+      ""tuning_run_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _tests/tfhub/hub_load.R---
@@ -1 +0,0 @@
-model <- hub_load('https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4') 

---FILE: _tests/tfhub/layer_hub.R---
@@ -1,7 +0,0 @@
-library(keras) 
-model <- keras_model_sequential() %>% 
- layer_hub( 
-   handle = ""https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"", 
-   input_shape = c(224, 224, 3) 
- ) %>% 
- layer_dense(1) 

---FILE: _tests/tfruns/step_pretrained_text_embedding.R---
@@ -1,8 +0,0 @@
-library(tibble) 
-library(recipes) 
-df <- tibble(text = c('hi', ""heello"", ""goodbye""), y = 0) 
-rec <- recipe(y ~ text, df) 
-rec <- rec %>% step_pretrained_text_embedding( 
- text, 
- handle = ""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1"" 
-) 

---FILE: reference/assets/_reference.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub]({{{{notitle.repo}}}}/blob/main/{{{{notitle.source}}}}) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub]({{{{notitle.repo}}}}/edit/main/{{{{notitle.source}}}}) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub]({{{{notitle.repo}}}}/blob/main/{{{{notitle.source}}}}#L{{{{notitle.alias_line}}}}) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*{{{{notitle.source}}}}*
-
 # {{{{notitle.alias}}}}
 
 ## {{{{notitle.title}}}}

---FILE: reference/cloudml/cloudml-package.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/cloudml-package.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/cloudml-package.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/cloudml-package.R#L39) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/cloudml-package.R*
-
 # cloudml-package
 
 ## Interface to the Google Cloud Machine Learning Platform

---FILE: reference/cloudml/cloudml_deploy.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/models.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/models.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/models.R#L34) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/models.R*
-
 # cloudml_deploy
 
 ## Deploy SavedModel to CloudML

---FILE: reference/cloudml/cloudml_predict.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/models.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/models.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/models.R#L95) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/models.R*
-
 # cloudml_predict
 
 ## Perform Prediction over a CloudML Model.

---FILE: reference/cloudml/cloudml_train.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R#L42) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/jobs.R*
-
 # cloudml_train
 
 ## Train a model using Cloud ML

---FILE: reference/cloudml/gcloud_exec.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-exec.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-exec.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-exec.R#L74) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-exec.R*
-
 # gcloud_exec
 
 ## Executes a Google Cloud Command
@@ -36,6 +34,7 @@ gcloud_exec(..., args = NULL, echo = TRUE, dry_run = FALSE)
 
 ## Examples
 ```{r, eval=ecodown::examples_not_run()}
+library(cloudml)
 gcloud_exec(""help"", ""info"") 
 ```
 

---FILE: reference/cloudml/gcloud_init.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/terminal.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/terminal.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/terminal.R#L120) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/terminal.R*
-
 # gcloud_init
 
 ## Initialize the Google Cloud SDK

---FILE: reference/cloudml/gcloud_install.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-install.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-install.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-install.R#L75) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-install.R*
-
 # gcloud_install
 
 ## Install the Google Cloud SDK

---FILE: reference/cloudml/gcloud_terminal.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/terminal.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/terminal.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/terminal.R#L13) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/terminal.R*
-
 # gcloud_terminal
 
 ## Create an RStudio terminal with access to the Google Cloud SDK

---FILE: reference/cloudml/gcloud_version.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-version.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-version.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-version.R#L8) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-version.R*
-
 # gcloud_version
 
 ## Gcloud version

---FILE: reference/cloudml/gs_copy.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-storage.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R#L23) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-storage.R*
-
 # gs_copy
 
 ## Copy files to / from Google Storage

---FILE: reference/cloudml/gs_data_dir.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-storage.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R#L116) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-storage.R*
-
 # gs_data_dir
 
 ## Google storage bucket path that syncs to local storage when not running on CloudML.

---FILE: reference/cloudml/gs_data_dir_local.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-storage.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R#L167) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-storage.R*
-
 # gs_data_dir_local
 
 ## Get a local path to the contents of Google Storage bucket

---FILE: reference/cloudml/gs_local_dir.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-storage.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R#L194) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-storage.R*
-
 # gs_local_dir
 
 ## Alias to gs_data_dir_local() function

---FILE: reference/cloudml/gs_rsync.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gcloud-storage.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gcloud-storage.R#L64) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gcloud-storage.R*
-
 # gs_rsync
 
 ## Synchronize content of two buckets/directories

---FILE: reference/cloudml/gsutil_exec.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gsutil-exec.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/gsutil-exec.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/gsutil-exec.R#L28) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/gsutil-exec.R*
-
 # gsutil_exec
 
 ## Executes a Google Utils Command

---FILE: reference/cloudml/job_cancel.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R#L216) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/jobs.R*
-
 # job_cancel
 
 ## Cancel a job

---FILE: reference/cloudml/job_collect.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R#L473) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/jobs.R*
-
 # job_collect
 
 ## Collect job output

---FILE: reference/cloudml/job_list.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R#L259) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/jobs.R*
-
 # job_list
 
 ## List all jobs

---FILE: reference/cloudml/job_status.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R#L343) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/jobs.R*
-
 # job_status
 
 ## Current status of a job

---FILE: reference/cloudml/job_stream_logs.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R#L310) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/jobs.R*
-
 # job_stream_logs
 
 ## Show job log stream

---FILE: reference/cloudml/job_trials.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](NULL/edit/main/R/jobs.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](NULL/blob/main/R/jobs.R#L394) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/jobs.R*
-
 # job_trials
 
 ## Current trials of a job

---FILE: reference/keras/activation_relu.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/activations.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/activations.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/activations.R#L30) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/activations.R*
-
 # activation_relu
 
 ## Activation functions
@@ -45,17 +43,29 @@ format:
 ## Usage
 ```r
 activation_relu(x, alpha = 0, max_value = NULL, threshold = 0) 
+
 activation_elu(x, alpha = 1) 
+
 activation_selu(x) 
+
 activation_hard_sigmoid(x) 
+
 activation_linear(x) 
+
 activation_sigmoid(x) 
+
 activation_softmax(x, axis = -1) 
+
 activation_softplus(x) 
+
 activation_softsign(x) 
+
 activation_tanh(x) 
+
 activation_exponential(x) 
+
 activation_gelu(x, approximate = FALSE) 
+
 activation_swish(x) 
 ```
 

---FILE: reference/keras/adapt.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layers-preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layers-preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layers-preprocessing.R#L1456) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layers-preprocessing.R*
-
 # adapt
 
 ## Fits the state of the preprocessing layer to the data being passed

---FILE: reference/keras/application_densenet.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L787) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_densenet
 
 ## Instantiates the DenseNet architecture.
@@ -28,6 +26,7 @@ application_densenet(
   pooling = NULL, 
   classes = 1000 
 ) 
+
 application_densenet121( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -36,6 +35,7 @@ application_densenet121(
   pooling = NULL, 
   classes = 1000 
 ) 
+
 application_densenet169( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -44,6 +44,7 @@ application_densenet169(
   pooling = NULL, 
   classes = 1000 
 ) 
+
 application_densenet201( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -52,6 +53,7 @@ application_densenet201(
   pooling = NULL, 
   classes = 1000 
 ) 
+
 densenet_preprocess_input(x, data_format = NULL) 
 ```
 

---FILE: reference/keras/application_efficientnet.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L987) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_efficientnet
 
 ## Instantiates the EfficientNetB0 architecture
@@ -29,6 +27,7 @@ application_efficientnet_b0(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_efficientnet_b1( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -39,6 +38,7 @@ application_efficientnet_b1(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_efficientnet_b2( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -49,6 +49,7 @@ application_efficientnet_b2(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_efficientnet_b3( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -59,6 +60,7 @@ application_efficientnet_b3(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_efficientnet_b4( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -69,6 +71,7 @@ application_efficientnet_b4(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_efficientnet_b5( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -79,6 +82,7 @@ application_efficientnet_b5(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_efficientnet_b6( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -89,6 +93,7 @@ application_efficientnet_b6(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_efficientnet_b7( 
   include_top = TRUE, 
   weights = ""imagenet"", 

---FILE: reference/keras/application_inception_resnet_v2.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L346) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_inception_resnet_v2
 
 ## Inception-ResNet v2 model, with weights trained on ImageNet
@@ -29,6 +27,7 @@ application_inception_resnet_v2(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 inception_resnet_v2_preprocess_input(x) 
 ```
 

---FILE: reference/keras/application_inception_v3.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L312) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_inception_v3
 
 ## Inception V3 model, with weights pre-trained on ImageNet.
@@ -29,6 +27,7 @@ application_inception_v3(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 inception_v3_preprocess_input(x) 
 ```
 

---FILE: reference/keras/application_mobilenet.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L479) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_mobilenet
 
 ## MobileNet model architecture.
@@ -32,8 +30,11 @@ application_mobilenet(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 mobilenet_preprocess_input(x) 
+
 mobilenet_decode_predictions(preds, top = 5) 
+
 mobilenet_load_model_hdf5(filepath) 
 ```
 

---FILE: reference/keras/application_mobilenet_v2.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L544) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_mobilenet_v2
 
 ## MobileNetV2 model architecture
@@ -30,8 +28,11 @@ application_mobilenet_v2(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 mobilenet_v2_preprocess_input(x) 
+
 mobilenet_v2_decode_predictions(preds, top = 5) 
+
 mobilenet_v2_load_model_hdf5(filepath) 
 ```
 

---FILE: reference/keras/application_mobilenet_v3.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L696) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_mobilenet_v3
 
 ## Instantiates the MobileNetV3Large architecture
@@ -32,6 +30,7 @@ application_mobilenet_v3_large(
   classifier_activation = ""softmax"", 
   include_preprocessing = TRUE 
 ) 
+
 application_mobilenet_v3_small( 
   input_shape = NULL, 
   alpha = 1, 

---FILE: reference/keras/application_nasnet.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L899) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_nasnet
 
 ## Instantiates a NASNet model.
@@ -33,6 +31,7 @@ application_nasnet(
   classes = 1000, 
   default_size = NULL 
 ) 
+
 application_nasnetlarge( 
   input_shape = NULL, 
   include_top = TRUE, 
@@ -41,6 +40,7 @@ application_nasnetlarge(
   pooling = NULL, 
   classes = 1000 
 ) 
+
 application_nasnetmobile( 
   input_shape = NULL, 
   include_top = TRUE, 
@@ -49,6 +49,7 @@ application_nasnetmobile(
   pooling = NULL, 
   classes = 1000 
 ) 
+
 nasnet_preprocess_input(x) 
 ```
 

---FILE: reference/keras/application_resnet.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L200) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_resnet
 
 ## Instantiates the ResNet architecture
@@ -28,6 +26,7 @@ application_resnet50(
   classes = 1000, 
   ... 
 ) 
+
 application_resnet101( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -37,6 +36,7 @@ application_resnet101(
   classes = 1000, 
   ... 
 ) 
+
 application_resnet152( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -46,6 +46,7 @@ application_resnet152(
   classes = 1000, 
   ... 
 ) 
+
 application_resnet50_v2( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -56,6 +57,7 @@ application_resnet50_v2(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_resnet101_v2( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -66,6 +68,7 @@ application_resnet101_v2(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 application_resnet152_v2( 
   include_top = TRUE, 
   weights = ""imagenet"", 
@@ -76,7 +79,9 @@ application_resnet152_v2(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 resnet_preprocess_input(x) 
+
 resnet_v2_preprocess_input(x) 
 ```
 

---FILE: reference/keras/application_vgg.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L119) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_vgg
 
 ## VGG16 and VGG19 models for Keras.
@@ -28,6 +26,7 @@ application_vgg16(
   classes = 1000, 
   classifier_activation = ""softmax"" 
 ) 
+
 application_vgg19( 
   include_top = TRUE, 
   weights = ""imagenet"", 

---FILE: reference/keras/application_xception.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L79) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # application_xception
 
 ## Instantiates the Xception architecture
@@ -29,6 +27,7 @@ application_xception(
   classifier_activation = ""softmax"", 
   ... 
 ) 
+
 xception_preprocess_input(x) 
 ```
 

---FILE: reference/keras/backend.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L16) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # backend
 
 ## Keras backend tensor engine

---FILE: reference/keras/bidirectional.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-wrappers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layer-wrappers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-wrappers.R#L100) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layer-wrappers.R*
-
 # bidirectional
 
 ## Bidirectional wrapper for RNNs

---FILE: reference/keras/callback_backup_and_restore.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L171) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_backup_and_restore
 
 ## Callback to back up and restore the training state

---FILE: reference/keras/callback_csv_logger.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L471) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_csv_logger
 
 ## Callback that streams epoch results to a csv file

---FILE: reference/keras/callback_early_stopping.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L206) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_early_stopping
 
 ## Stop training when a monitored quantity has stopped improving.

---FILE: reference/keras/callback_lambda.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L516) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_lambda
 
 ## Create a custom callback

---FILE: reference/keras/callback_learning_rate_scheduler.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L277) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_learning_rate_scheduler
 
 ## Learning rate scheduler.

---FILE: reference/keras/callback_model_checkpoint.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L63) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_model_checkpoint
 
 ## Save the model after every epoch.

---FILE: reference/keras/callback_progbar_logger.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L15) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_progbar_logger
 
 ## Callback that prints metrics to stdout.

---FILE: reference/keras/callback_reduce_lr_on_plateau.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L437) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_reduce_lr_on_plateau
 
 ## Reduce learning rate when a metric has stopped improving.

---FILE: reference/keras/callback_remote_monitor.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L246) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_remote_monitor
 
 ## Callback used to stream events to a server.

---FILE: reference/keras/callback_tensorboard.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L348) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_tensorboard
 
 ## TensorBoard basic visualizations

---FILE: reference/keras/callback_terminate_on_naan.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/callbacks.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/callbacks.R#L289) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/callbacks.R*
-
 # callback_terminate_on_naan
 
 ## Callback that terminates training when a NaN loss is encountered.

---FILE: reference/keras/clone_model.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L345) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model.R*
-
 # clone_model
 
 ## Clone a model instance.

---FILE: reference/keras/compile.keras.engine.training.model.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L427) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model.R*
-
 # compile.keras.engine.training.Model
 
 ## Configure a Keras model for training
@@ -20,8 +18,7 @@ Configure a Keras model for training
 ## Usage
 ```r
 ## S3 method for class 'keras.engine.training.Model'
-compile
-( 
+compile( 
   object, 
   optimizer = NULL, 
   loss = NULL, 

---FILE: reference/keras/constraints.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/constraints.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/constraints.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/constraints.R#L55) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/constraints.R*
-
 # constraints
 
 ## Weight constraints
@@ -20,8 +18,11 @@ Functions that impose constraints on weight values.
 ## Usage
 ```r
 constraint_maxnorm(max_value = 2, axis = 0) 
+
 constraint_nonneg() 
+
 constraint_unitnorm(axis = 0) 
+
 constraint_minmaxnorm(min_value = 0, max_value = 1, rate = 1, axis = 0) 
 ```
 

---FILE: reference/keras/count_params.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layer-methods.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R#L110) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layer-methods.R*
-
 # count_params
 
 ## Count the total number of scalars composing the weights.

---FILE: reference/keras/create_layer.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layers-core.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layers-core.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layers-core.R#L504) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layers-core.R*
-
 # create_layer
 
 ## Create a Keras Layer

---FILE: reference/keras/create_layer_wrapper.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-custom.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layer-custom.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-custom.R#L196) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layer-custom.R*
-
 # create_layer_wrapper
 
 ## Create a Keras Layer wrapper

---FILE: reference/keras/create_wrapper.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/wrapper_custom.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/wrapper_custom.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/wrapper_custom.R#L113) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/wrapper_custom.R*
-
 # create_wrapper
 
 ## (Deprecated) Create a Keras Wrapper

---FILE: reference/keras/custom_metric.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/metrics.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/metrics.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/metrics.R#L1319) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/metrics.R*
-
 # custom_metric
 
 ## Custom metric function

---FILE: reference/keras/dataset_boston_housing.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/datasets.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R#L249) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/datasets.R*
-
 # dataset_boston_housing
 
 ## Boston housing price regression dataset

---FILE: reference/keras/dataset_cifar10.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/datasets.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R#L20) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/datasets.R*
-
 # dataset_cifar10
 
 ## CIFAR10 small image classification

---FILE: reference/keras/dataset_cifar100.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/datasets.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R#L43) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/datasets.R*
-
 # dataset_cifar100
 
 ## CIFAR100 small image classification

---FILE: reference/keras/dataset_fashion_mnist.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/datasets.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R#L223) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/datasets.R*
-
 # dataset_fashion_mnist
 
 ## Fashion-MNIST database of fashion articles

---FILE: reference/keras/dataset_imdb.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/datasets.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R#L94) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/datasets.R*
-
 # dataset_imdb
 
 ## IMDB Movie reviews sentiment classification
@@ -29,6 +27,7 @@ dataset_imdb(
   oov_char = 2L, 
   index_from = 3L 
 ) 
+
 dataset_imdb_word_index(path = ""imdb_word_index.json"") 
 ```
 

---FILE: reference/keras/dataset_mnist.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/datasets.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R#L187) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/datasets.R*
-
 # dataset_mnist
 
 ## MNIST database of handwritten digits

---FILE: reference/keras/dataset_reuters.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/datasets.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/datasets.R#L148) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/datasets.R*
-
 # dataset_reuters
 
 ## Reuters newswire topics classification
@@ -30,6 +28,7 @@ dataset_reuters(
   oov_char = 2L, 
   index_from = 3L 
 ) 
+
 dataset_reuters_word_index(path = ""reuters_word_index.pkl"") 
 ```
 

---FILE: reference/keras/evaluate.keras.engine.training.model.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L791) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model.R*
-
 # evaluate.keras.engine.training.Model
 
 ## Evaluate a Keras model
@@ -20,8 +18,7 @@ Evaluate a Keras model
 ## Usage
 ```r
 ## S3 method for class 'keras.engine.training.Model'
-evaluate
-( 
+evaluate( 
   object, 
   x = NULL, 
   y = NULL, 

---FILE: reference/keras/evaluate_generator.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L1198) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model.R*
-
 # evaluate_generator
 
 ## (Deprecated) Evaluates the model on a data generator.

---FILE: reference/keras/export_savedmodel.keras.engine.training.model.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model-persistence.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model-persistence.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model-persistence.R#L398) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model-persistence.R*
-
 # export_savedmodel.keras.engine.training.Model
 
 ## Export a Saved Model
@@ -20,8 +18,7 @@ Serialize a model to disk.
 ## Usage
 ```r
 ## S3 method for class 'keras.engine.training.Model'
-export_savedmodel
-( 
+export_savedmodel( 
   object, 
   export_dir_base, 
   overwrite = TRUE, 

---FILE: reference/keras/fit.keras.engine.training.model.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L695) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model.R*
-
 # fit.keras.engine.training.Model
 
 ## Train a Keras model
@@ -20,8 +18,7 @@ Trains the model for a fixed number of epochs (iterations on a dataset).
 ## Usage
 ```r
 ## S3 method for class 'keras.engine.training.Model'
-fit
-( 
+fit( 
   object, 
   x = NULL, 
   y = NULL, 

---FILE: reference/keras/fit_generator.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L1135) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model.R*
-
 # fit_generator
 
 ## (Deprecated) Fits the model on data yielded batch-by-batch by a generator.

---FILE: reference/keras/fit_image_data_generator.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L730) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # fit_image_data_generator
 
 ## Fit image data generator internal statistics to some sample data.

---FILE: reference/keras/fit_text_tokenizer.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L324) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # fit_text_tokenizer
 
 ## Update tokenizer internal vocabulary based on a list of texts or list of sequences.

---FILE: reference/keras/flow_images_from_data.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L770) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # flow_images_from_data
 
 ## Generates batches of augmented/normalized data from image data and labels

---FILE: reference/keras/flow_images_from_dataframe.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L926) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # flow_images_from_dataframe
 
 ## Takes the dataframe and the path to a directory and generates batches of augmented/normalized data.

---FILE: reference/keras/flow_images_from_directory.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L835) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # flow_images_from_directory
 
 ## Generates batches of data from images in a directory (with optional augmented/normalized data)

---FILE: reference/keras/freeze_weights.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/freeze.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/freeze.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/freeze.R#L89) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/freeze.R*
-
 # freeze_weights
 
 ## Freeze and unfreeze weights
@@ -20,6 +18,7 @@ Freeze weights in a model or layer so that they are no longer trainable.
 ## Usage
 ```r
 freeze_weights(object, from = NULL, to = NULL, which = NULL) 
+
 unfreeze_weights(object, from = NULL, to = NULL, which = NULL) 
 ```
 
@@ -44,6 +43,7 @@ Models must be compiled again after weights are frozen or unfrozen.
 
 ## Examples
 ```{r, eval=ecodown::examples_not_run()}
+library(keras)
 conv_base <- application_vgg16( 
   weights = ""imagenet"", 
   include_top = FALSE, 

---FILE: reference/keras/generator_next.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L710) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # generator_next
 
 ## Retrieve the next item from a generator

---FILE: reference/keras/get_config.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layer-methods.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R#L31) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layer-methods.R*
-
 # get_config
 
 ## Layer/Model configuration
@@ -20,6 +18,7 @@ A layer config is an object returned from `get_config()` that contains the confi
 ## Usage
 ```r
 get_config(object) 
+
 from_config(config, custom_objects = NULL) 
 ```
 

---FILE: reference/keras/get_file.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/utils.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R#L62) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/utils.R*
-
 # get_file
 
 ## Downloads a file from a URL if it not already in the cache.

---FILE: reference/keras/get_input_at.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layer-methods.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R#L135) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layer-methods.R*
-
 # get_input_at
 
 ## Retrieve tensors for layers with multiple nodes
@@ -20,10 +18,15 @@ Whenever you are calling a layer on some input, you are creating a new tensor (t
 ## Usage
 ```r
 get_input_at(object, node_index) 
+
 get_output_at(object, node_index) 
+
 get_input_shape_at(object, node_index) 
+
 get_output_shape_at(object, node_index) 
+
 get_input_mask_at(object, node_index) 
+
 get_output_mask_at(object, node_index) 
 ```
 

---FILE: reference/keras/get_layer.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/model.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/model.R#L1413) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/model.R*
-
 # get_layer
 
 ## Retrieves a layer based on either its name (unique) or index.

---FILE: reference/keras/get_weights.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/layer-methods.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/layer-methods.R#L80) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/layer-methods.R*
-
 # get_weights
 
 ## Layer/Model weights as R arrays
@@ -20,6 +18,7 @@ Layer/Model weights as R arrays
 ## Usage
 ```r
 get_weights(object, trainable = NA) 
+
 set_weights(object, weights) 
 ```
 

---FILE: reference/keras/grapes-py_class-grapes.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/py-classes.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R#L) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/py-classes.R*
-
 # %py_class%
 
 ## Make a python class constructor
@@ -36,6 +34,7 @@ The python class constructor, invisibly. Note, the same constructor is also assi
 
 ## Examples
 ```{r, eval=ecodown::examples_not_run()}
+library(keras)
 MyClass %py_class% { 
   initialize <- function(x) { 
     print(""Hi from MyClass$initialize()!"") 

---FILE: reference/keras/grapes-set-active-grapes.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/py-classes.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/py-classes.R#L) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/py-classes.R*
-
 # %<-active%
 
 ## Make an Active Binding
@@ -38,6 +36,7 @@ Active bindings defined in a `%py_class%` are converted to `@property` decorated
 
 ## Examples
 ```{r, eval=ecodown::examples_run()}
+library(keras)
 set.seed(1234) 
 x %<-active% function(value) { 
   message(""Evaluating function of active binding"") 

---FILE: reference/keras/hdf5_matrix.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/utils.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R#L97) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/utils.R*
-
 # hdf5_matrix
 
 ## Representation of HDF5 dataset to be used instead of an R array

---FILE: reference/keras/image_data_generator.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L662) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # image_data_generator
 
 ## Generate batches of image data with real-time data augmentation. The data will be looped over (in batches).

---FILE: reference/keras/image_dataset_from_directory.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L1079) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # image_dataset_from_directory
 
 ## Create a dataset from a directory

---FILE: reference/keras/image_load.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L496) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # image_load
 
 ## Loads an image into PIL format.

---FILE: reference/keras/image_to_array.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/preprocessing.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/preprocessing.R#L544) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/preprocessing.R*
-
 # image_to_array
 
 ## 3D array representation of images
@@ -20,12 +18,14 @@ format:
 ## Usage
 ```r
 image_to_array(img, data_format = c(""channels_last"", ""channels_first"")) 
+
 image_array_resize( 
   img, 
   height, 
   width, 
   data_format = c(""channels_last"", ""channels_first"") 
 ) 
+
 image_array_save( 
   img, 
   path, 

---FILE: reference/keras/imagenet_decode_predictions.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L370) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # imagenet_decode_predictions
 
 ## Decodes the prediction of an ImageNet model.

---FILE: reference/keras/imagenet_preprocess_input.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/applications.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/applications.R#L407) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/applications.R*
-
 # imagenet_preprocess_input
 
 ## Preprocesses a tensor or array encoding a batch of images.

---FILE: reference/keras/implementation.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/utils.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R#L373) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/utils.R*
-
 # implementation
 
 ## Keras implementation

---FILE: reference/keras/initializer_constant.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L28) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_constant
 
 ## Initializer that generates tensors initialized to a constant value.

---FILE: reference/keras/initializer_glorot_normal.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L214) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_glorot_normal
 
 ## Glorot normal initializer, also called Xavier normal initializer.

---FILE: reference/keras/initializer_glorot_uniform.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L236) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_glorot_uniform
 
 ## Glorot uniform initializer, also called Xavier uniform initializer.

---FILE: reference/keras/initializer_he_normal.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L256) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_he_normal
 
 ## He normal initializer.

---FILE: reference/keras/initializer_he_uniform.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L275) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_he_uniform
 
 ## He uniform variance scaling initializer.

---FILE: reference/keras/initializer_identity.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L170) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_identity
 
 ## Initializer that generates the identity matrix.

---FILE: reference/keras/initializer_lecun_normal.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L191) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_lecun_normal
 
 ## LeCun normal initializer.

---FILE: reference/keras/initializer_lecun_uniform.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L294) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_lecun_uniform
 
 ## LeCun uniform initializer.

---FILE: reference/keras/initializer_ones.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L17) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_ones
 
 ## Initializer that generates tensors initialized to 1.

---FILE: reference/keras/initializer_orthogonal.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L153) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_orthogonal
 
 ## Initializer that generates a random orthogonal matrix.

---FILE: reference/keras/initializer_random_normal.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L44) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_random_normal
 
 ## Initializer that generates tensors with a normal distribution.

---FILE: reference/keras/initializer_random_uniform.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L62) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_random_uniform
 
 ## Initializer that generates tensors with a uniform distribution.

---FILE: reference/keras/initializer_truncated_normal.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L83) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_truncated_normal
 
 ## Initializer that generates a truncated normal distribution.

---FILE: reference/keras/initializer_variance_scaling.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L113) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_variance_scaling
 
 ## Initializer capable of adapting its scale to the shape of weights.

---FILE: reference/keras/initializer_zeros.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/initializers.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/initializers.R#L8) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/initializers.R*
-
 # initializer_zeros
 
 ## Initializer that generates tensors initialized to 0.

---FILE: reference/keras/install_keras.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/install.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/install.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/install.R#L22) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/install.R*
-
 # install_keras
 
 ## Install TensorFlow and Keras, including all Python dependencies

---FILE: reference/keras/is_keras_available.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/utils.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/utils.R#L343) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/utils.R*
-
 # is_keras_available
 
 ## Check if Keras is Available
@@ -35,6 +33,7 @@ Logical indicating whether Keras (or the specified minimum version of Keras) is
 
 ## Examples
 ```{r, eval=ecodown::examples_not_run()}
+library(keras)
 # testthat utilty for skipping tests when Keras isn't available 
 skip_if_no_keras <- function(version = NULL) { 
   if (!is_keras_available(version)) 

---FILE: reference/keras/k_abs.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L33) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_abs
 
 ## Element-wise absolute value.

---FILE: reference/keras/k_all.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L52) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_all
 
 ## Bitwise reduction (logical AND).

---FILE: reference/keras/k_any.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L73) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_any
 
 ## Bitwise reduction (logical OR).

---FILE: reference/keras/k_arange.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L98) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_arange
 
 ## Creates a 1D tensor containing a sequence of integers.

---FILE: reference/keras/k_argmax.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L119) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_argmax
 
 ## Returns the index of the maximum value along an axis.

---FILE: reference/keras/k_argmin.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L138) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_argmin
 
 ## Returns the index of the minimum value along an axis.

---FILE: reference/keras/k_backend.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L153) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_backend
 
 ## Active Keras backend

---FILE: reference/keras/k_batch_dot.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L181) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_batch_dot
 
 ## Batchwise dot product.

---FILE: reference/keras/k_batch_flatten.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L201) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_batch_flatten
 
 ## Turn a nD tensor into a 2D tensor with same 1st dimension.

---FILE: reference/keras/k_batch_get_value.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L219) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_batch_get_value
 
 ## Returns the value of more than one tensor variable.

---FILE: reference/keras/k_batch_normalization.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L243) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_batch_normalization
 
 ## Applies batch normalization on x given mean, var, beta and gamma.

---FILE: reference/keras/k_batch_set_value.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L270) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_batch_set_value
 
 ## Sets the values of many tensor variables at once.

---FILE: reference/keras/k_bias_add.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L288) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_bias_add
 
 ## Adds a bias vector to a tensor.

---FILE: reference/keras/k_binary_crossentropy.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L309) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_binary_crossentropy
 
 ## Binary crossentropy between an output tensor and a target tensor.

---FILE: reference/keras/k_cast.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L330) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_cast
 
 ## Casts a tensor to a different dtype and returns it.

---FILE: reference/keras/k_cast_to_floatx.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L347) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_cast_to_floatx
 
 ## Cast an array to the default Keras float type.

---FILE: reference/keras/k_categorical_crossentropy.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L369) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_categorical_crossentropy
 
 ## Categorical crossentropy between an output tensor and a target tensor.

---FILE: reference/keras/k_clear_session.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L391) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_clear_session
 
 ## Destroys the current TF graph and creates a new one.

---FILE: reference/keras/k_clip.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L407) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_clip
 
 ## Element-wise value clipping.

---FILE: reference/keras/k_concatenate.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L427) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_concatenate
 
 ## Concatenates a list of tensors alongside the specified axis.

---FILE: reference/keras/k_constant.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L447) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_constant
 
 ## Creates a constant tensor.

---FILE: reference/keras/k_conv1d.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L472) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_conv1d
 
 ## 1D convolution.

---FILE: reference/keras/k_conv2d.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L499) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_conv2d
 
 ## 2D convolution.

---FILE: reference/keras/k_conv2d_transpose.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L528) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_conv2d_transpose
 
 ## 2D deconvolution (i.e. transposed convolution).

---FILE: reference/keras/k_conv3d.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L556) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_conv3d
 
 ## 3D convolution.

---FILE: reference/keras/k_conv3d_transpose.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L584) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_conv3d_transpose
 
 ## 3D deconvolution (i.e. transposed convolution).

---FILE: reference/keras/k_cos.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L606) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_cos
 
 ## Computes cos of x element-wise.

---FILE: reference/keras/k_count_params.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L622) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_count_params
 
 ## Returns the static number of elements in a Keras variable or tensor.

---FILE: reference/keras/k_ctc_batch_cost.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L646) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_ctc_batch_cost
 
 ## Runs CTC loss algorithm on each batch element.

---FILE: reference/keras/k_ctc_decode.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L681) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_ctc_decode
 
 ## Decodes the output of a softmax.

---FILE: reference/keras/k_ctc_label_dense_to_sparse.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L702) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_ctc_label_dense_to_sparse
 
 ## Converts CTC labels from dense to sparse.

---FILE: reference/keras/k_cumprod.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L721) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_cumprod
 
 ## Cumulative product of the values in a tensor, alongside the specified axis.

---FILE: reference/keras/k_cumsum.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L740) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_cumsum
 
 ## Cumulative sum of the values in a tensor, alongside the specified axis.

---FILE: reference/keras/k_depthwise_conv2d.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L763) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_depthwise_conv2d
 
 ## Depthwise 2D convolution with separable filters.

---FILE: reference/keras/k_dot.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L790) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_dot
 
 ## Multiplies 2 tensors (and/or variables) and returns a **tensor**.

---FILE: reference/keras/k_dropout.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L811) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_dropout
 
 ## Sets entries in `x` to zero at random, while scaling the entire tensor.

---FILE: reference/keras/k_dtype.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L830) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_dtype
 
 ## Returns the dtype of a Keras tensor or variable, as a string.

---FILE: reference/keras/k_elu.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L847) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_elu
 
 ## Exponential linear unit.

---FILE: reference/keras/k_epsilon.qmd---
@@ -4,11 +4,9 @@ format:
     css: /reference/assets/reference.css
 ---
 
-| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R) </button> | <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [Suggest edits on GitHub](https://github.com/rstudio/keras//edit/main/R/backend.R) </button> |
+| <button class=""button""> ![](/reference/assets/GitHub-Mark-32px.png){width=""20""} [View source on GitHub](https://github.com/rstudio/keras//blob/main/R/backend.R#L862) </button> |
 |:------------------------------:|:--------------------------------------:|
 
-*R/backend.R*
-
 # k_epsilon
 
 ## Fuzz factor used in numeric expressions.
@@ -20,6 +18,7 @@ Fuzz factor used in numeric expressions.
 ## Usage
 ```r
 k_epsilon() 
+
 k_set_epsilon(e) 
 ```
 "
rstudio,tensorflow.rstudio.com,d2ebbb2afb936ed45b6c1f17810d2640e756ec99,Edgar Ruiz,edgararuiz@gmail.com,2022-08-25T17:51:29Z,Edgar Ruiz,edgararuiz@gmail.com,2022-08-25T17:51:29Z,"Adds title and description to index pages, fixes missing topic names",_ecodown.yml;reference/cloudml/index.qmd;reference/keras/index.qmd;reference/tensorflow/index.qmd;reference/tfautograph/index.qmd;reference/tfdatasets/index.qmd;reference/tfhub/index.qmd;reference/tfruns/index.qmd,True,False,True,False,35,20,55,"---FILE: _ecodown.yml---
@@ -5,22 +5,33 @@ site:
     convert_news: false
     convert_articles: false
     reference_folder: """"
+    reference_examples: false
+    reference_examples_not_run: false
     reference_template: ""reference/assets/_reference.qmd""
-    reference_qmd_options: ""execute:\n  freeze: true""
     packages:
       - repo_url: https://github.com/rstudio/tensorflow
         quarto_sub_folder: ""reference/tensorflow""
+        package_description: ""Tensors, Graphs and other primatives""
       - repo_url: https://github.com/rstudio/keras
         quarto_sub_folder: ""reference/keras""
+        package_description: ""High level API for deep learning""
       - repo_url: https://github.com/rstudio/tfdatasets
         quarto_sub_folder: ""reference/tfdatasets""
+        package_description: ""Create efficient and fast data loading pipelines""
       - repo_url: https://github.com/t-kalinowski/tfautograph
         quarto_sub_folder: ""reference/tfautograph""
+        package_description: ""Tools for translating R code to Tensorflow Graphs""
       - repo_url: https://github.com/rstudio/tfhub
         quarto_sub_folder: ""reference/tfhub""
+        package_description: ""R interface to TensorFlow Hub, a library for reusable machine learning modules""
       - repo_url: https://github.com/rstudio/tfruns
         quarto_sub_folder: ""reference/tfruns""
+        package_description: ""Track and visualize training runs and experiments""
       - repo_url: https://github.com/rstudio/cloudml
         quarto_sub_folder: ""reference/cloudml""
+        package_description: ""R interface to Google CloudML""
   autolink:
     run: FALSE
+  quarto:
+    run: FALSE
+

---FILE: reference/cloudml/index.qmd---
@@ -1,5 +1,6 @@
 ---
- 
+title: cloudml
+description: R interface to Google CloudML
 ---
  
 ## Training

---FILE: reference/keras/index.qmd---
@@ -1,5 +1,6 @@
 ---
- 
+title: keras
+description: High level API for deep learning
 ---
  
 ## Keras Models
@@ -376,7 +377,7 @@ Function(s) | Description
 |[callback_terminate_on_naan()](/reference/keras/callback_terminate_on_naan.html)|Callback that terminates training when a NaN loss is encountered.|
 |[callback_csv_logger()](/reference/keras/callback_csv_logger.html)|Callback that streams epoch results to a csv file|
 |[callback_lambda()](/reference/keras/callback_lambda.html)|Create a custom callback|
-|[](/reference/keras/KerasCallback.html)|(Deprecated) Base R6 class for Keras callbacks|
+|[KerasCallback](/reference/keras/KerasCallback.html)|(Deprecated) Base R6 class for Keras callbacks|
  
 ## Initializers
  
@@ -403,7 +404,7 @@ Function(s) | Description
 Function(s) | Description
 |---|---|
 |[constraint_maxnorm() constraint_nonneg() constraint_unitnorm() constraint_minmaxnorm()](/reference/keras/constraints.html)|Weight constraints|
-|[](/reference/keras/KerasConstraint.html)|(Deprecated) Base R6 class for Keras constraints|
+|[KerasConstraint](/reference/keras/KerasConstraint.html)|(Deprecated) Base R6 class for Keras constraints|
  
 ## Utils
  
@@ -420,7 +421,7 @@ Function(s) | Description
 |[keras_array()](/reference/keras/keras_array.html)|Keras array object|
 |[hdf5_matrix()](/reference/keras/hdf5_matrix.html)|Representation of HDF5 dataset to be used instead of an R array|
 |[get_file()](/reference/keras/get_file.html)|Downloads a file from a URL if it not already in the cache.|
-|[](/reference/keras/reexports.html)|Objects exported from other packages|
+|[reexports %<>% use_python use_virtualenv use_condaenv array_reshape tuple use_session_with_seed tensorboard evaluate export_savedmodel shape as_tensor flags flag_numeric flag_integer flag_string flag_boolean run_dir fit compile](/reference/keras/reexports.html)|Objects exported from other packages|
 |[install_keras()](/reference/keras/install_keras.html)|Install TensorFlow and Keras, including all Python dependencies|
 |[is_keras_available()](/reference/keras/is_keras_available.html)|Check if Keras is Available|
 |[backend()](/reference/keras/backend.html)|Keras backend tensor engine|
@@ -439,7 +440,7 @@ Function(s) | Description
  
 Function(s) | Description
 |---|---|
-|[](/reference/keras/Metric.html)|Metric|
+|[Metric](/reference/keras/Metric.html)|Metric|
 |[metric_accuracy()](/reference/keras/metric_accuracy.html)|Calculates how often predictions equal labels|
 |[metric_auc()](/reference/keras/metric_auc.html)|Approximates the AUC (Area under the curve) of the ROC or PR curves|
 |[metric_binary_accuracy()](/reference/keras/metric_binary_accuracy.html)|Calculates how often predictions match binary labels|
@@ -655,8 +656,8 @@ Function(s) | Description
  
 Function(s) | Description
 |---|---|
-|[](/reference/keras/KerasLayer.html)|(Deprecated) Base R6 class for Keras layers|
-|[](/reference/keras/KerasWrapper.html)|(Deprecated) Base R6 class for Keras wrappers|
+|[KerasLayer](/reference/keras/KerasLayer.html)|(Deprecated) Base R6 class for Keras layers|
+|[KerasWrapper](/reference/keras/KerasWrapper.html)|(Deprecated) Base R6 class for Keras wrappers|
 |[create_wrapper()](/reference/keras/create_wrapper.html)|(Deprecated) Create a Keras Wrapper|
 |[loss_cosine_proximity()](/reference/keras/loss_cosine_proximity.html)|(Deprecated) loss_cosine_proximity|
 |[layer_cudnn_gru()](/reference/keras/layer_cudnn_gru.html)|(Deprecated) Fast GRU implementation backed by <a href='https://developer.nvidia.com/cudnn'>CuDNN</a>.|

---FILE: reference/tensorflow/index.qmd---
@@ -1,5 +1,6 @@
 ---
- 
+title: tensorflow
+description: Tensors, Graphs and other primatives
 ---
  
 ## Installation

---FILE: reference/tfautograph/index.qmd---
@@ -1,5 +1,6 @@
 ---
- 
+title: tfautograph
+description: Tools for translating R code to Tensorflow Graphs
 ---
  
 ## Autograph

---FILE: reference/tfdatasets/index.qmd---
@@ -1,5 +1,6 @@
 ---
- 
+title: tfdatasets
+description: Create efficient and fast data loading pipelines
 ---
  
 ## Creating Datasets
@@ -77,7 +78,7 @@ Function(s) | Description
 |[dense_features()](/reference/tfdatasets/dense_features.html)|Dense Features|
 |[dataset_use_spec()](/reference/tfdatasets/dataset_use_spec.html)|Transform the dataset using the provided spec.|
 |[fit(<i><FeatureSpec></i>)](/reference/tfdatasets/fit.FeatureSpec.html)|Fits a feature specification.|
-|[](/reference/tfdatasets/scaler.html)|List of pre-made scalers|
+|[scaler](/reference/tfdatasets/scaler.html)|List of pre-made scalers|
 |[scaler_min_max()](/reference/tfdatasets/scaler_min_max.html)|Creates an instance of a min max scaler|
 |[scaler_standard()](/reference/tfdatasets/scaler_standard.html)|Creates an instance of a standard scaler|
 |[step_bucketized_column()](/reference/tfdatasets/step_bucketized_column.html)|Creates bucketized columns|
@@ -91,7 +92,7 @@ Function(s) | Description
 |[step_numeric_column()](/reference/tfdatasets/step_numeric_column.html)|Creates a numeric column specification|
 |[step_remove_column()](/reference/tfdatasets/step_remove_column.html)|Creates a step that can remove columns|
 |[step_shared_embeddings_column()](/reference/tfdatasets/step_shared_embeddings_column.html)|Creates shared embeddings for categorical columns|
-|[](/reference/tfdatasets/steps.html)|Steps for feature columns specification.|
+|[steps](/reference/tfdatasets/steps.html)|Steps for feature columns specification.|
 |[all_nominal()](/reference/tfdatasets/all_nominal.html)|Find all nominal variables.|
 |[all_numeric()](/reference/tfdatasets/all_numeric.html)|Speciy all numeric variables.|
 |[has_type()](/reference/tfdatasets/has_type.html)|Identify the type of the variable.|

---FILE: reference/tfhub/index.qmd---
@@ -1,9 +1,7 @@
 ---
- 
+title: tfhub
+description: R interface to TensorFlow Hub, a library for reusable machine learning modules
 ---
- 
-## 1
- 
 Function(s) | Description
 |---|---|
 |[bake.step_pretrained_text_embedding()](/reference/tfhub/bake.step_pretrained_text_embedding.html)|Bake method for step_pretrained_text_embedding|
@@ -15,5 +13,5 @@ Function(s) | Description
 |[layer_hub()](/reference/tfhub/layer_hub.html)|Hub Layer|
 |[`%>%`](/reference/tfhub/pipe.html)|Pipe operator|
 |[prep.step_pretrained_text_embedding()](/reference/tfhub/prep.step_pretrained_text_embedding.html)|Prep method for step_pretrained_text_embedding|
-|[](/reference/tfhub/reexports.html)|Objects exported from other packages|
+|[reexports install_tensorflow tf shape](/reference/tfhub/reexports.html)|Objects exported from other packages|
 |[step_pretrained_text_embedding()](/reference/tfhub/step_pretrained_text_embedding.html)|Pretrained text-embeddings|

---FILE: reference/tfruns/index.qmd---
@@ -1,5 +1,6 @@
 ---
- 
+title: tfruns
+description: Track and visualize training runs and experiments
 ---
  
 ## Training"
rstudio,tensorflow.rstudio.com,f72b3e76aea36ec8ca966c4b345c2c4a6fad3188,Tomasz Kalinowski,kalinowskit@gmail.com,2022-08-02T14:48:40Z,Tomasz Kalinowski,kalinowskit@gmail.com,2022-08-02T14:48:40Z,fix publish.yml,.github/workflows/publish.yml,False,False,False,False,3,3,6,"---FILE: .github/workflows/publish.yml---
@@ -2,9 +2,9 @@ on:
   workflow_dispatch:
   push:
     branches: main
-  paths:
-    - '**.qmd'
-    - '_freeze'
+    paths:
+      - '**.qmd'
+      - '_freeze'
 
 name: Quarto Publish
 "
rstudio,tensorflow.rstudio.com,1194caafa84436c967b519b86b6fe476c102eec7,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-21T19:16:33Z,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-21T19:16:33Z,fix duplicate redirect,_freeze/guides/keras/basics/execute-results/html.json;_freeze/guides/keras/sequential_model/execute-results/html.json;guides/keras/basics.qmd;guides/keras/sequential_model.qmd;index.qmd;reference/index.qmd,True,False,True,False,45,46,91,"---FILE: _freeze/guides/keras/basics/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""a213953229ba021e2f2f6381b91dd197"",
+  ""hash"": ""d043c2a5f51f6c1f179a83705d381540"",
   ""result"": {
-    ""markdown"": ""---\ntitle: \""Guide to Keras Basics\""\naliases:\n  - /keras/articles/guide_keras.html\n---\n\n\nKeras is a high-level API to build and train deep learning models. It's\nused for fast prototyping, advanced research, and production, with three\nkey advantages:\n\n-   *User friendly*<br> Keras has a simple, consistent interface\n    optimized for common use cases. It provides clear and actionable\n    feedback for user errors.\n-   *Modular and composable*<br> Keras models are made by connecting\n    configurable building blocks together, with few restrictions.\n-   *Easy to extend*<br> Write custom building blocks to express new\n    ideas for research. Create new layers, loss functions, and develop\n    state-of-the-art models.\n\n## Import keras\n\nTo get started, load the `keras` library:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n```\n:::\n\n\n## Build a simple model\n\n### Sequential model\n\nIn Keras, you assemble *layers* to build *models*. A model is (usually)\na graph of layers. The most common type of model is a stack of layers:\nthe `sequential` model.\n\nTo build a simple, fully-connected network (i.e., a multi-layer\nperceptron):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nmodel %>% \n  \n  # Adds a densely-connected layer with 64 units to the model:\n  layer_dense(units = 64, activation = 'relu') %>%\n  \n  # Add another:\n  layer_dense(units = 64, activation = 'relu') %>%\n  \n  # Add a softmax layer with 10 output units:\n  layer_dense(units = 10, activation = 'softmax')\n```\n:::\n\n\n### Configure the layers\n\nThere are many `layers` available with some common constructor\nparameters:\n\n-   `activation`: Set the [activation\n    function](https://tensorflow.rstudio.com/reference/keras/#section-activation-layers)\n    for the layer. By default, no activation is applied.\n-   `kernel_initializer` and `bias_initializer`: The initialization\n    schemes that create the layer's weights (kernel and bias). This\n    defaults to the\n    [`Glorot uniform`](https://tensorflow.rstudio.com/keras/reference/initializer_glorot_uniform.html)\n    initializer.\n-   `kernel_regularizer` and `bias_regularizer`: The regularization\n    schemes that apply to the layer's weights (kernel and bias), such as\n    L1 or L2 regularization. By default, no regularization is applied.\n\nThe following instantiates `dense` layers using constructor arguments:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sigmoid layer:\nlayer_dense(units = 64, activation ='sigmoid')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7f60ce963e20>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\nlayer_dense(units = 64, kernel_regularizer = regularizer_l1(0.01))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7f606002a7c0>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\nlayer_dense(units = 64, bias_regularizer = regularizer_l2(0.01))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7f606002ab20>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with a kernel initialized to a random orthogonal matrix:\nlayer_dense(units = 64, kernel_initializer = 'orthogonal')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7f606002ac70>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with a bias vector initialized to 2.0:\nlayer_dense(units = 64, bias_initializer = initializer_constant(2.0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7f606004a220>\n```\n:::\n:::\n\n\n## Train and evaluate\n\n### Set up training\n\nAfter the model is constructed, configure its learning process by\ncalling the `compile` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n```\n:::\n\n\n`compile` takes three important arguments:\n\n-   `optimizer`: This object specifies the training procedure. Commonly\n    used optimizers are e.g.\\\n    [`adam`](https://tensorflow.rstudio.com/keras/reference/optimizer_adam.html),\n    [`rmsprop`](https://tensorflow.rstudio.com/keras/reference/optimizer_rmsprop.html),\n    or\n    [`sgd`](https://tensorflow.rstudio.com/keras/reference/optimizer_sgd.html).\n-   `loss`: The function to minimize during optimization. Common choices\n    include mean square error (`mse`), `categorical_crossentropy`, and\n    `binary_crossentropy`.\n-   `metrics`: Used to monitor training. In classification, this usually\n    is accuracy.\n\nThe following shows a few examples of configuring a model for training:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Configure a model for mean-squared error regression.\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'mse',           # mean squared error\n  metrics = list('mae')   # mean absolute error\n)\n\n# Configure a model for categorical classification.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(learning_rate = 0.01),\n  loss = \""categorical_crossentropy\"",\n  metrics = list(\""categorical_accuracy\"")\n)\n```\n:::\n\n\n### Input data\n\nYou can train keras models directly on R matrices and arrays (possibly\ncreated from R `data.frames`). A model is fit to the training data using\nthe `fit` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32\n)\n```\n:::\n\n\n`fit` takes three important arguments:\n\n-   `epochs`: Training is structured into *epochs*. An epoch is one\n    iteration over the entire input data (this is done in smaller\n    batches).\n-   `batch_size`: When passed matrix or array data, the model slices the\n    data into smaller batches and iterates over these batches during\n    training. This integer specifies the size of each batch. Be aware\n    that the last batch may be smaller if the total number of samples is\n    not divisible by the batch size.\n-   `validation_data`: When prototyping a model, you want to easily\n    monitor its performance on some validation data. Passing this\n    argument --- a list of inputs and labels --- allows the model to\n    display the loss and metrics in inference mode for the passed data,\n    at the end of each epoch.\n\nHere's an example using `validation_data`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nval_data <- matrix(rnorm(1000 * 32), nrow = 100, ncol = 32)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in matrix(rnorm(1000 * 32), nrow = 100, ncol = 32): data length\ndiffers from size of matrix: [32000 != 100 x 32]\n```\n:::\n\n```{.r .cell-code}\nval_labels <- matrix(rnorm(100 * 10), nrow = 100, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(val_data, val_labels)\n)\n```\n:::\n\n\n### Evaluate and predict\n\nSame as `fit`, the `evaluate` and `predict` methods can use raw R data\nas well as a `dataset`.\n\nTo *evaluate* the inference-mode loss and metrics for the data provided:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(test_data, test_labels, batch_size = 32)\n\nmodel %>% evaluate(test_dataset, steps = 30)\n```\n:::\n\n\nAnd to *predict* the output of the last layer in inference for the data\nprovided, again as R data as well as a `dataset`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% predict(test_data, batch_size = 32)\n    \nmodel %>% predict(test_dataset, steps = 30)\n```\n:::\n\n\n## Build advanced models\n\n### Functional API\n\nThe `sequential` model is a simple stack of layers that cannot represent\narbitrary models. Use the [Keras functional API](functional_api.html) to\nbuild complex model topologies such as:\n\n-   multi-input models,\n-   multi-output models,\n-   models with shared layers (the same layer called several times),\n-   models with non-sequential data flows (e.g., residual connections).\n\nBuilding a model with the functional API works like this:\n\n1.  A layer instance is callable and returns a tensor.\n2.  Input tensors and output tensors are used to define a `keras_model`\n    instance.\n3.  This model is trained just like the `sequential` model.\n\nThe following example uses the functional API to build a simple,\nfully-connected network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = (32))  # Returns a placeholder tensor\n\npredictions <- inputs %>% \n  layer_dense(units = 64, activation = 'relu') %>%\n  layer_dense(units = 64, activation = 'relu') %>% \n  layer_dense(units = 10, activation = 'softmax')\n\n# Instantiate the model given inputs and outputs.\nmodel <- keras_model(inputs = inputs, outputs = predictions)\n\n# The compile step specifies the training configuration.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in backcompat_fix_rename_lr_to_learning_rate(...): the `lr` argument\nhas been renamed to `learning_rate`.\n```\n:::\n\n```{.r .cell-code}\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n```\n:::\n\n\n### Custom layers\n\nTo create a custom Keras layer, you create an R6 class derived from\n`KerasLayer`. There are three methods to implement (only one of which,\n`call()`, is required for all types of layer):\n\n-   `build(input_shape)`: This is where you will define your weights.\n    Note that if your layer doesn't define trainable weights then you\n    need not implement this method.\n-   `call(x)`: This is where the layer's logic lives. Unless you want\n    your layer to support masking, you only have to care about the first\n    argument passed to call: the input tensor.\n-   `compute_output_shape(input_shape)`: In case your layer modifies the\n    shape of its input, you should specify here the shape transformation\n    logic. This allows Keras to do automatic shape inference. If you\n    don't modify the shape of the input then you need not implement this\n    method.\n\nHere is an example custom layer that performs a matrix multiplication:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\nCustomLayer <- R6::R6Class(\""CustomLayer\"",\n                                  \n  inherit = KerasLayer,\n  \n  public = list(\n    \n    output_dim = NULL,\n    \n    kernel = NULL,\n    \n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n    \n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel', \n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n    \n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n    \n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n)\n```\n:::\n\n\nIn order to use the custom layer within a Keras model you also need to\ncreate a wrapper function which instantiates the layer using the\n`create_layer()` function. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define layer wrapper function\nlayer_custom <- function(object, output_dim, name = NULL, trainable = TRUE) {\n  create_layer(CustomLayer, object, list(\n    output_dim = as.integer(output_dim),\n    name = name,\n    trainable = trainable\n  ))\n}\n```\n:::\n\n\nYou can now use the layer in a model as usual:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 32, input_shape = c(32,32)) %>% \n  layer_custom(output_dim = 32)\n```\n:::\n\n\n### Custom models\n\nIn addition to creating custom layers, you can also create a custom\nmodel. This might be necessary if you wanted to use TensorFlow eager\nexecution in combination with an imperatively written forward pass.\n\nIn cases where this is not needed, but flexibility in building the\narchitecture is required, it is recommended to just stick with the\nfunctional API.\n\nA custom model is defined by calling `keras_model_custom()` passing a\nfunction that specifies the layers to be created and the operations to\nbe executed on forward pass.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define a custom model type\nmy_model_constructor <- new_model_class(\n  \""MyModel\"",\n  \n  initialize = function(output_dim, ...) {\n    super$initialize(...)\n    # store our output dim in self until build() is called\n    self$output_dim <- output_dim\n  },\n  \n  build = function(input_shape) {\n    # create layers we'll need for the call (this code executes once)\n    # note: the layers have to be created on the self object!\n    self$dense1 <- layer_dense(units = 64,\n                               activation = 'relu',\n                               input_shape = input_shape)\n    self$dense2 <- layer_dense(units = 64, activation = 'relu')\n    self$dense3 <- layer_dense(units = self$output_dim, activation = 'softmax')\n  },\n  \n  # implement call (this code executes during training & inference)\n  call = function(inputs) {\n    x <- inputs %>%\n      self$dense1() %>%\n      self$dense2() %>%\n      self$dense3()\n    x\n  },\n  \n  # define a `get_config()` method in custom objects \n  # to enable model saving and restoring\n  get_config = function() {\n    list(output_dim = self$output_dim)\n  }\n)\n\n\n\nmodel <- my_model_constructor(output_dim = 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(learning_rate = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n```\n:::\n\n\n## Callbacks\n\nA callback is an object passed to a model to customize and extend its\nbehavior during training. You can write your own custom callback, or use\nthe built-in `callbacks` that include:\n\n-   `callback_model_checkpoint`: Save checkpoints of your model at\n    regular intervals.\n-   `callback_learning_rate_scheduler`: Dynamically change the learning\n    rate.\n-   `callback_early_stopping`: Interrupt training when validation\n    performance has stopped improving.\n-   `callbacks_tensorboard`: Monitor the model's behavior using\n    [TensorBoard](training_visualization.html#tensorboard).\n\nTo use a `callback`, pass it to the model's `fit` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncallbacks <- list(\n  callback_early_stopping(patience = 2, monitor = 'val_loss'),\n  callback_tensorboard(log_dir = './logs')\n)\n\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5,\n  callbacks = callbacks,\n  validation_data = list(val_data, val_labels)\n)\n```\n:::\n\n\n## Save and restore\n\n### Weights only\n\nSave and load the weights of a model using `save_model_weights_hdf5` and\n`load_model_weights_hdf5`, respectively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save in SavedModel format\nmodel %>% save_model_weights_tf('my_model/')\n\n# Restore the model's state,\n# this requires a model with the same architecture.\nmodel %>% load_model_weights_tf('my_model/')\n```\n:::\n\n\n### Configuration only\n\nA model's configuration can be saved - this serializes the model\narchitecture without any weights. A saved configuration can recreate and\ninitialize the same model, even without the code that defined the\noriginal model. Keras supports JSON and YAML serialization formats:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Serialize a model to JSON format\njson_string <- model %>% model_to_json()\n\n# Recreate the model (freshly initialized)\nfresh_model <- model_from_json(json_string, \n                               custom_objects = list('MyModel' = my_model_constructor))\n```\n:::\n\n\n### Entire model\n\nThe entire model can be saved to a file that contains the weight values,\nthe model's configuration, and even the optimizer's configuration. This\nallows you to checkpoint a model and resume training later ---from the\nexact same state ---without access to the original code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save entire model to the SavedModel format\nmodel %>% save_model_tf('my_model/')\n\n# Recreate the exact same model, including weights and optimizer.\nmodel <- load_model_tf('my_model/')\n```\n:::\n"",
+    ""markdown"": ""---\ntitle: \""Guide to Keras Basics\""\naliases:\n  - /articles/guide_keras.html\n---\n\n\nKeras is a high-level API to build and train deep learning models. It's\nused for fast prototyping, advanced research, and production, with three\nkey advantages:\n\n-   *User friendly* -- Keras has a simple, consistent interface\n    optimized for common use cases. It provides clear and actionable\n    feedback for user errors.\n-   *Modular and composable* -- Keras models are made by connecting\n    configurable building blocks together, with few restrictions.\n-   *Easy to extend* -- Write custom building blocks to express new\n    ideas for research. Create new layers, loss functions, and develop\n    state-of-the-art models.\n\n## Import keras\n\nTo get started, load the `keras` library:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n```\n:::\n\n\n## Build a simple model\n\n### Sequential model\n\nIn Keras, you assemble *layers* to build *models*. A model is (usually)\na graph of layers. The most common type of model is a stack of layers:\nthe `sequential` model.\n\nTo build a simple, fully-connected network (i.e., a multi-layer\nperceptron):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n\n  # Adds a densely-connected layer with 64 units to the model:\n  layer_dense(units = 64, activation = 'relu') %>%\n\n  # Add another:\n  layer_dense(units = 64, activation = 'relu') %>%\n\n  # Add a softmax layer with 10 output units:\n  layer_dense(units = 10, activation = 'softmax')\n```\n:::\n\n\n### Configure the layers\n\nThere are many `layers` available with some common constructor\nparameters:\n\n-   `activation`: Set the [activation\n    function](https://tensorflow.rstudio.com/reference/keras/#section-activation-layers)\n    for the layer. By default, no activation is applied.\n-   `kernel_initializer` and `bias_initializer`: The initialization\n    schemes that create the layer's weights (kernel and bias). This\n    defaults to the\n    [`Glorot uniform`](https://tensorflow.rstudio.com/keras/reference/initializer_glorot_uniform.html)\n    initializer.\n-   `kernel_regularizer` and `bias_regularizer`: The regularization\n    schemes that apply to the layer's weights (kernel and bias), such as\n    L1 or L2 regularization. By default, no regularization is applied.\n\nThe following instantiates `dense` layers using constructor arguments:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sigmoid layer:\nlayer_dense(units = 64, activation ='sigmoid')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7fd57fb73dc0>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\nlayer_dense(units = 64, kernel_regularizer = regularizer_l1(0.01))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7fd510235760>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\nlayer_dense(units = 64, bias_regularizer = regularizer_l2(0.01))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7fd510235ac0>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with a kernel initialized to a random orthogonal matrix:\nlayer_dense(units = 64, kernel_initializer = 'orthogonal')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7fd510235c10>\n```\n:::\n\n```{.r .cell-code}\n# A linear layer with a bias vector initialized to 2.0:\nlayer_dense(units = 64, bias_initializer = initializer_constant(2.0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.layers.core.dense.Dense object at 0x7fd5102561c0>\n```\n:::\n:::\n\n\n## Train and evaluate\n\n### Set up training\n\nAfter the model is constructed, configure its learning process by\ncalling the `compile` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n```\n:::\n\n\n`compile` takes three important arguments:\n\n-   `optimizer`: This object specifies the training procedure. Commonly\n    used optimizers are e.g.\\\n    [`adam`](https://tensorflow.rstudio.com/keras/reference/optimizer_adam.html),\n    [`rmsprop`](https://tensorflow.rstudio.com/keras/reference/optimizer_rmsprop.html),\n    or\n    [`sgd`](https://tensorflow.rstudio.com/keras/reference/optimizer_sgd.html).\n-   `loss`: The function to minimize during optimization. Common choices\n    include mean square error (`mse`), `categorical_crossentropy`, and\n    `binary_crossentropy`.\n-   `metrics`: Used to monitor training. In classification, this usually\n    is accuracy.\n\nThe following shows a few examples of configuring a model for training:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Configure a model for mean-squared error regression.\nmodel %>% compile(\n  optimizer = 'adam',\n  loss = 'mse',           # mean squared error\n  metrics = list('mae')   # mean absolute error\n)\n\n# Configure a model for categorical classification.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(learning_rate = 0.01),\n  loss = \""categorical_crossentropy\"",\n  metrics = list(\""categorical_accuracy\"")\n)\n```\n:::\n\n\n### Input data\n\nYou can train keras models directly on R matrices and arrays (possibly\ncreated from R `data.frames`). A model is fit to the training data using\nthe `fit` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32\n)\n```\n:::\n\n\n`fit` takes three important arguments:\n\n-   `epochs`: Training is structured into *epochs*. An epoch is one\n    iteration over the entire input data (this is done in smaller\n    batches).\n-   `batch_size`: When passed matrix or array data, the model slices the\n    data into smaller batches and iterates over these batches during\n    training. This integer specifies the size of each batch. Be aware\n    that the last batch may be smaller if the total number of samples is\n    not divisible by the batch size.\n-   `validation_data`: When prototyping a model, you want to easily\n    monitor its performance on some validation data. Passing this\n    argument --- a list of inputs and labels --- allows the model to\n    display the loss and metrics in inference mode for the passed data,\n    at the end of each epoch.\n\nHere's an example using `validation_data`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nval_data <- matrix(rnorm(1000 * 32), nrow = 100, ncol = 32)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in matrix(rnorm(1000 * 32), nrow = 100, ncol = 32): data length\ndiffers from size of matrix: [32000 != 100 x 32]\n```\n:::\n\n```{.r .cell-code}\nval_labels <- matrix(rnorm(100 * 10), nrow = 100, ncol = 10)\n\nmodel %>% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32,\n  validation_data = list(val_data, val_labels)\n)\n```\n:::\n\n\n### Evaluate and predict\n\nSame as `fit`, the `evaluate` and `predict` methods can use raw R data\nas well as a `dataset`.\n\nTo *evaluate* the inference-mode loss and metrics for the data provided:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% evaluate(test_data, test_labels, batch_size = 32)\n\nmodel %>% evaluate(test_dataset, steps = 30)\n```\n:::\n\n\nAnd to *predict* the output of the last layer in inference for the data\nprovided, again as R data as well as a `dataset`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% predict(test_data, batch_size = 32)\n\nmodel %>% predict(test_dataset, steps = 30)\n```\n:::\n\n\n## Build advanced models\n\n### Functional API\n\nThe `sequential` model is a simple stack of layers that cannot represent\narbitrary models. Use the [Keras functional API](functional_api.html) to\nbuild complex model topologies such as:\n\n-   multi-input models,\n-   multi-output models,\n-   models with shared layers (the same layer called several times),\n-   models with non-sequential data flows (e.g., residual connections).\n\nBuilding a model with the functional API works like this:\n\n1.  A layer instance is callable and returns a tensor.\n2.  Input tensors and output tensors are used to define a `keras_model`\n    instance.\n3.  This model is trained just like the `sequential` model.\n\nThe following example uses the functional API to build a simple,\nfully-connected network:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = (32))  # Returns a placeholder tensor\n\npredictions <- inputs %>%\n  layer_dense(units = 64, activation = 'relu') %>%\n  layer_dense(units = 64, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\n# Instantiate the model given inputs and outputs.\nmodel <- keras_model(inputs = inputs, outputs = predictions)\n\n# The compile step specifies the training configuration.\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in backcompat_fix_rename_lr_to_learning_rate(...): the `lr` argument\nhas been renamed to `learning_rate`.\n```\n:::\n\n```{.r .cell-code}\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n```\n:::\n\n\n### Custom layers\n\nTo create a custom Keras layer, you create an R6 class derived from\n`KerasLayer`. There are three methods to implement (only one of which,\n`call()`, is required for all types of layer):\n\n-   `build(input_shape)`: This is where you will define your weights.\n    Note that if your layer doesn't define trainable weights then you\n    need not implement this method.\n-   `call(x)`: This is where the layer's logic lives. Unless you want\n    your layer to support masking, you only have to care about the first\n    argument passed to call: the input tensor.\n-   `compute_output_shape(input_shape)`: In case your layer modifies the\n    shape of its input, you should specify here the shape transformation\n    logic. This allows Keras to do automatic shape inference. If you\n    don't modify the shape of the input then you need not implement this\n    method.\n\nHere is an example custom layer that performs a matrix multiplication:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\nCustomLayer <- R6::R6Class(\""CustomLayer\"",\n\n  inherit = KerasLayer,\n\n  public = list(\n\n    output_dim = NULL,\n\n    kernel = NULL,\n\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n)\n```\n:::\n\n\nIn order to use the custom layer within a Keras model you also need to\ncreate a wrapper function which instantiates the layer using the\n`create_layer()` function. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define layer wrapper function\nlayer_custom <- function(object, output_dim, name = NULL, trainable = TRUE) {\n  create_layer(CustomLayer, object, list(\n    output_dim = as.integer(output_dim),\n    name = name,\n    trainable = trainable\n  ))\n}\n```\n:::\n\n\nYou can now use the layer in a model as usual:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 32, input_shape = c(32,32)) %>%\n  layer_custom(output_dim = 32)\n```\n:::\n\n\n### Custom models\n\nIn addition to creating custom layers, you can also create a custom\nmodel. This might be necessary if you wanted to use TensorFlow eager\nexecution in combination with an imperatively written forward pass.\n\nIn cases where this is not needed, but flexibility in building the\narchitecture is required, it is recommended to just stick with the\nfunctional API.\n\nA custom model is defined by calling `keras_model_custom()` passing a\nfunction that specifies the layers to be created and the operations to\nbe executed on forward pass.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define a custom model type\nmy_model_constructor <- new_model_class(\n  \""MyModel\"",\n\n  initialize = function(output_dim, ...) {\n    super$initialize(...)\n    # store our output dim in self until build() is called\n    self$output_dim <- output_dim\n  },\n\n  build = function(input_shape) {\n    # create layers we'll need for the call (this code executes once)\n    # note: the layers have to be created on the self object!\n    self$dense1 <- layer_dense(units = 64,\n                               activation = 'relu',\n                               input_shape = input_shape)\n    self$dense2 <- layer_dense(units = 64, activation = 'relu')\n    self$dense3 <- layer_dense(units = self$output_dim, activation = 'softmax')\n  },\n\n  # implement call (this code executes during training & inference)\n  call = function(inputs) {\n    x <- inputs %>%\n      self$dense1() %>%\n      self$dense2() %>%\n      self$dense3()\n    x\n  },\n\n  # define a `get_config()` method in custom objects\n  # to enable model saving and restoring\n  get_config = function() {\n    list(output_dim = self$output_dim)\n  }\n)\n\n\n\nmodel <- my_model_constructor(output_dim = 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(learning_rate = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\n# Trains for 5 epochs\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n```\n:::\n\n\n## Callbacks\n\nA callback is an object passed to a model to customize and extend its\nbehavior during training. You can write your own custom callback, or use\nthe built-in `callbacks` that include:\n\n-   `callback_model_checkpoint`: Save checkpoints of your model at\n    regular intervals.\n-   `callback_learning_rate_scheduler`: Dynamically change the learning\n    rate.\n-   `callback_early_stopping`: Interrupt training when validation\n    performance has stopped improving.\n-   `callbacks_tensorboard`: Monitor the model's behavior using\n    [TensorBoard](training_visualization.html#tensorboard).\n\nTo use a `callback`, pass it to the model's `fit` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncallbacks <- list(\n  callback_early_stopping(patience = 2, monitor = 'val_loss'),\n  callback_tensorboard(log_dir = './logs')\n)\n\nmodel %>% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5,\n  callbacks = callbacks,\n  validation_data = list(val_data, val_labels)\n)\n```\n:::\n\n\n## Save and restore\n\n### Weights only\n\nSave and load the weights of a model using `save_model_weights_hdf5` and\n`load_model_weights_hdf5`, respectively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save in SavedModel format\nmodel %>% save_model_weights_tf('my_model/')\n\n# Restore the model's state,\n# this requires a model with the same architecture.\nmodel %>% load_model_weights_tf('my_model/')\n```\n:::\n\n\n### Configuration only\n\nA model's configuration can be saved - this serializes the model\narchitecture without any weights. A saved configuration can recreate and\ninitialize the same model, even without the code that defined the\noriginal model. Keras supports JSON and YAML serialization formats:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Serialize a model to JSON format\njson_string <- model %>% model_to_json()\n\n# Recreate the model (freshly initialized)\nfresh_model <- model_from_json(json_string,\n                               custom_objects = list('MyModel' = my_model_constructor))\n```\n:::\n\n\n### Entire model\n\nThe entire model can be saved to a file that contains the weight values,\nthe model's configuration, and even the optimizer's configuration. This\nallows you to checkpoint a model and resume training later ---from the\nexact same state ---without access to the original code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save entire model to the SavedModel format\nmodel %>% save_model_tf('my_model/')\n\n# Recreate the exact same model, including weights and optimizer.\nmodel <- load_model_tf('my_model/')\n```\n:::\n"",
     ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""

---FILE: _freeze/guides/keras/sequential_model/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""daa85aea7eb14113ca52df98116b9380"",
+  ""hash"": ""243945fa3479aa7316053245452314fa"",
   ""result"": {
-    ""markdown"": ""---\ntitle: The Sequential model\nAuthor: \""[fchollet](https://twitter.com/fchollet), Tomasz Kalinowski\""\ndate-created: 2020/04/12\ndate-last-modified: 2020/04/12\ndescription: Complete guide to the Sequential model.\naliases:\n  - ../../articles/guide_keras.html\n  - ../../articles/sequential_model.html\n  - ../../guide/keras/sequential_model/index.html\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## When to use a Sequential model\n\nA `Sequential` model is appropriate for **a plain stack of layers**\nwhere each layer has **exactly one input tensor and one output tensor**.\n\nSchematically, the following `Sequential` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define Sequential model with 3 layers\nmodel <- keras_model_sequential() %>% \n  layer_dense(2, activation = \""relu\"", name = \""layer1\"") %>% \n  layer_dense(3, activation = \""relu\"", name = \""layer2\"") %>% \n  layer_dense(4, name = \""layer3\"")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n# Call model on a test input\nx <- tf$ones(shape(3, 3))\ny <- model(x)\n```\n:::\n\n\nis equivalent to this function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 3 layers\nlayer1 <- layer_dense(units = 2, activation = \""relu\"", name = \""layer1\"")\nlayer2 <- layer_dense(units = 3, activation = \""relu\"", name = \""layer2\"")\nlayer3 <- layer_dense(units = 4, name = \""layer3\"")\n\n# Call layers on a test input\nx <- tf$ones(shape(3, 3))\ny <- layer3(layer2(layer1(x)))\n```\n:::\n\n\nA Sequential model is **not appropriate** when:\n\n-   Your model has multiple inputs or multiple outputs\n-   Any of your layers has multiple inputs or multiple outputs\n-   You need to do layer sharing\n-   You want non-linear topology (e.g. a residual connection, a\n    multi-branch model)\n\n## Creating a Sequential model\n\nYou can create a Sequential model by piping a model through a series\nlayers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_dense(2, activation = \""relu\"") %>%\n  layer_dense(3, activation = \""relu\"") %>%\n  layer_dense(4)\n```\n:::\n\n\nIts layers are accessible via the `layers` attribute:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$layers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<keras.layers.core.dense.Dense object at 0x7fbbd02d7d00>\n\n[[2]]\n<keras.layers.core.dense.Dense object at 0x7fbbd4479ee0>\n\n[[3]]\n<keras.layers.core.dense.Dense object at 0x7fbbd4411a60>\n```\n:::\n:::\n\n\nYou can also create a Sequential model incrementally:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(2, activation = \""relu\"")\nmodel %>% layer_dense(3, activation = \""relu\"")\nmodel %>% layer_dense(4)\n```\n:::\n\n\nNote that there's also a corresponding `pop()` method to remove layers:\na Sequential model behaves very much like a stack of layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% pop_layer()\nlength(model$layers)  # 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nAlso note that the Sequential constructor accepts a `name` argument,\njust like any layer or model in Keras. This is useful to annotate\nTensorBoard graphs with semantically meaningful names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(name = \""my_sequential\"")\nmodel %>% layer_dense(2, activation = \""relu\"", name = \""layer1\"")\nmodel %>% layer_dense(3, activation = \""relu\"", name = \""layer2\"")\nmodel %>% layer_dense(4, name = \""layer3\"")\n```\n:::\n\n\n## Specifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in\norder to be able to create their weights. So when you create a layer\nlike this, initially, it has no weights:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 3)\nlayer$weights  # Empty\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlist()\n```\n:::\n:::\n\n\nIt creates its weights the first time it is called on an input, since\nthe shape of the weights depends on the shape of the inputs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Call layer on a test input\nx <- tf$ones(shape(1, 4))\ny <- layer(x)\nlayer$weights  # Now it has weights, of shape (4, 3) and (3,)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=\narray([[ 0.06395423, -0.14237106, -0.62638205],\n       [-0.5884756 ,  0.46201825,  0.75721586],\n       [ 0.91452694,  0.8030274 , -0.8158957 ],\n       [ 0.20228338, -0.33104193, -0.1134184 ]], dtype=float32)>\n\n[[2]]\n<tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n```\n:::\n:::\n\n\nNaturally, this also applies to Sequential models. When you instantiate\na Sequential model without an input shape, it isn't \""built\"": it has no\nweights (and calling `model$weights` results in an error stating just\nthis). The weights are created when the model first sees some input\ndata:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n        layer_dense(2, activation = \""relu\"") %>% \n        layer_dense(3, activation = \""relu\"") %>% \n        layer_dense(4)\n\n# No weights at this stage!\n# At this point, you can't do this:\n\ntry(model$weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_get_attr_impl(x, name, silent) : \n  ValueError: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n```\n:::\n\n```{.r .cell-code}\n# The model summary is also not available:\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: <no summary available, model was not built>\n```\n:::\n\n```{.r .cell-code}\n# Call the model on a test input\nx <- tf$ones(shape(1, 4))\ny <- model(x)\ncat(\""Number of weights after calling the model:\"", length(model$weights), \""\\n\"")  # 6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of weights after calling the model: 6 \n```\n:::\n:::\n\n\nOnce a model is \""built\"", you can call its `summary()` method to display\nits contents (the `summary()` method is also called by the default\n`print()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_3\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_9 (Dense)                  (1, 2)                        10          \n dense_8 (Dense)                  (1, 3)                        9           \n dense_7 (Dense)                  (1, 4)                        16          \n============================================================================\nTotal params: 35\nTrainable params: 35\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nHowever, it can be very useful when building a Sequential model\nincrementally to be able to display the summary of the model so far,\nincluding the current output shape. In this case, you should start your\nmodel by passing an `input_shape` argument to your model, so that it\nknows its input shape from the start:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(4))\nmodel %>% layer_dense(2, activation = \""relu\"")\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_4\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_10 (Dense)                 (None, 2)                     10          \n============================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nModels built with a predefined input shape like this always have weights\n(even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input\nshape of a Sequential model in advance if you know what it is.\n\n## A common debugging workflow: `%>%` + `summary()`\n\nWhen building a new Sequential architecture, it's useful to\nincrementally stack layers and print model summaries. For instance, this\nenables you to monitor how a stack of `Conv2D` and `MaxPooling2D` layers\nis downsampling image feature maps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(250, 250, 3)) # 250x250 RGB images\n  \nmodel %>% \n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3) \n\n# Can you guess what the current output shape is at this point? Probably not.\n# Let's just print it:\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_5\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n============================================================================\nTotal params: 11,680\nTrainable params: 11,680\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# The answer was: (40, 40, 32), so we can keep downsampling...\nmodel %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(2) \n\n# And now?\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_5\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n conv2d_5 (Conv2D)                (None, 38, 38, 32)            9248        \n conv2d_4 (Conv2D)                (None, 36, 36, 32)            9248        \n max_pooling2d_2 (MaxPooling2D)   (None, 12, 12, 32)            0           \n conv2d_3 (Conv2D)                (None, 10, 10, 32)            9248        \n conv2d_2 (Conv2D)                (None, 8, 8, 32)              9248        \n max_pooling2d_1 (MaxPooling2D)   (None, 4, 4, 32)              0           \n============================================================================\nTotal params: 48,672\nTrainable params: 48,672\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# Now that we have 4x4 feature maps, time to apply global max pooling.\nmodel %>% layer_global_max_pooling_2d()\n\n# Finally, we add a classification layer.\nmodel %>% layer_dense(10)\n```\n:::\n\n\nVery practical, right?\n\n## What to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n\n-   Train your model, evaluate it, and run inference. See our [guide to\n    training & evaluation with the built-in\n    loops](/guides/training_with_built_in_methods/)\n-   Save your model to disk and restore it. See our [guide to\n    serialization & saving](/guides/serialization_and_saving/).\n-   Speed up model training by leveraging multiple GPUs. See our [guide\n    to multi-GPU and distributed\n    training](https://keras.io/guides/distributed_training/).\n\n## Feature extraction with a Sequential model\n\nOnce a Sequential model has been built, it behaves like a [Functional\nAPI model](/guides/functional_api/). This means that every layer has an\n`input` and `output` attribute. These attributes can be used to do neat\nthings, like quickly creating a model that extracts the outputs of all\nintermediate layers in a Sequential model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs = lapply(initial_model$layers, \\(layer) layer$output)\n)\n\n# Call feature extractor on test input.\n\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n```\n:::\n\n\nHere's a similar example that only extract features from one layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"", name = \""my_intermediate_layer\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs =  get_layer(initial_model, name = \""my_intermediate_layer\"")$output\n)\n\n# Call feature extractor on test input.\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n```\n:::\n\n\n## Transfer learning with a Sequential model\n\nTransfer learning consists of freezing the bottom layers in a model and\nonly training the top layers. If you aren't familiar with it, make sure\nto read our [guide to transfer learning](/guides/transfer_learning/).\n\nHere are two common transfer learning blueprint involving Sequential\nmodels.\n\nFirst, let's say that you have a Sequential model, and you want to\nfreeze all layers except the last one. In this case, you would simply\niterate over `model$layers` and set `layer$trainable = FALSE` on each\nlayer, except the last one. Like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(784)) %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(10)\n\n\n# Presumably you would want to first load pre-trained weights.\nmodel$load_weights(...)\n\n# Freeze all layers except the last one.\nfor (layer in head(model$layers, -1))\n  layer$trainable <- FALSE\n\n# can also just call: freeze_weights(model, to = -2)\n\n# Recompile and train (this will only update the weights of the last layer).\nmodel %>% compile(...)\nmodel %>% fit(...)\n```\n:::\n\n\nAnother common blueprint is to use a Sequential model to stack a\npre-trained model and some freshly initialized classification layers.\nLike this:\n\n# Load a convolutional base with pre-trained weights\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model <- application_xception(\n    weights = 'imagenet',\n    include_top = FALSE,\n    pooling = 'avg')\n\n# Freeze the base model\nbase_model$trainable <- FALSE\n\n# Use a Sequential model to add a trainable classifier on top\nmodel <- keras_model_sequential() %>%\n  base_model() %>%\n  layer_dense(1000)\n\n# Compile & train\nmodel %>% compile(...)\nmodel %>% fit(...)\n```\n:::\n\n\nIf you do transfer learning, you will probably find yourself frequently\nusing these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n\n-   [Guide to the Functional API](/guides/functional_api/)\n-   [Guide to making new Layers & Models via\n    subclassing](/guides/making_new_layers_and_models_via_subclassing/)\n\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.9.1 (~/.virtualenvs/r-tensorflow-site/lib/python3.9/site-packages/tensorflow)\nPython v3.9 (~/.virtualenvs/r-tensorflow-site/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.4 LTS\n\nMatrix products: default\nBLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libmkl_rt.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.9.0.9000      tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           knitr_1.39           whisker_0.4         \n [4] magrittr_2.0.3       here_1.0.1           lattice_0.20-45     \n [7] R6_2.5.1             rlang_1.0.4          fastmap_1.1.0       \n[10] stringr_1.4.0        tools_4.2.1          grid_4.2.1          \n[13] xfun_0.31            png_0.1-7            cli_3.3.0           \n[16] htmltools_0.5.2      tfruns_1.5.0         rprojroot_2.0.3     \n[19] yaml_2.3.5           digest_0.6.29        Matrix_1.4-1        \n[22] base64enc_0.1-3      htmlwidgets_1.5.4    zeallot_0.1.0       \n[25] evaluate_0.15        rmarkdown_2.14       stringi_1.7.8       \n[28] compiler_4.2.1       generics_0.1.3       reticulate_1.25-9000\n[31] jsonlite_1.8.0      \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\""-m pip freeze\""), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.1.0\nasttokens==2.0.5\nastunparse==1.6.3\nbackcall==0.2.0\ncachetools==5.2.0\ncertifi==2022.6.15\ncharset-normalizer==2.1.0\ndecorator==5.1.1\ndill==0.3.5.1\netils==0.6.0\nexecuting==0.8.3\nflatbuffers==1.12\ngast==0.4.0\ngoogle-auth==2.9.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.4\ngrpcio==1.47.0\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.12.0\nimportlib-resources==5.8.0\nipython==8.4.0\njedi==0.18.1\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.2\nkt-legacy==1.0.4\nlibclang==14.0.1\nMarkdown==3.3.7\nmatplotlib-inline==0.1.3\nnumpy==1.23.1\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.2.0\npromise==2.3\nprompt-toolkit==3.0.30\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.12.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npytz==2022.1\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscipy==1.8.1\nsix==1.16.0\nstack-data==0.3.0\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-datasets==4.6.0\ntensorflow-estimator==2.9.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.26.0\ntensorflow-metadata==1.9.0\ntermcolor==1.1.0\ntoml==0.10.2\ntqdm==4.64.0\ntraitlets==5.3.0\ntyping_extensions==4.3.0\nurllib3==1.26.10\nwcwidth==0.2.5\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.1\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-07-14 \nPage render time: 5 seconds\n```\n:::\n:::\n:::\n\n"",
+    ""markdown"": ""---\ntitle: The Sequential model\nAuthor: \""[fchollet](https://twitter.com/fchollet), Tomasz Kalinowski\""\ndate-created: 2020/04/12\ndate-last-modified: 2020/04/12\ndescription: Complete guide to the Sequential model.\naliases:\n  - ../../articles/sequential_model.html\n  - ../../guide/keras/sequential_model/index.html\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## When to use a Sequential model\n\nA `Sequential` model is appropriate for **a plain stack of layers**\nwhere each layer has **exactly one input tensor and one output tensor**.\n\nSchematically, the following `Sequential` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define Sequential model with 3 layers\nmodel <- keras_model_sequential() %>%\n  layer_dense(2, activation = \""relu\"", name = \""layer1\"") %>%\n  layer_dense(3, activation = \""relu\"", name = \""layer2\"") %>%\n  layer_dense(4, name = \""layer3\"")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n# Call model on a test input\nx <- tf$ones(shape(3, 3))\ny <- model(x)\n```\n:::\n\n\nis equivalent to this function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 3 layers\nlayer1 <- layer_dense(units = 2, activation = \""relu\"", name = \""layer1\"")\nlayer2 <- layer_dense(units = 3, activation = \""relu\"", name = \""layer2\"")\nlayer3 <- layer_dense(units = 4, name = \""layer3\"")\n\n# Call layers on a test input\nx <- tf$ones(shape(3, 3))\ny <- layer3(layer2(layer1(x)))\n```\n:::\n\n\nA Sequential model is **not appropriate** when:\n\n-   Your model has multiple inputs or multiple outputs\n-   Any of your layers has multiple inputs or multiple outputs\n-   You need to do layer sharing\n-   You want non-linear topology (e.g. a residual connection, a\n    multi-branch model)\n\n## Creating a Sequential model\n\nYou can create a Sequential model by piping a model through a series\nlayers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_dense(2, activation = \""relu\"") %>%\n  layer_dense(3, activation = \""relu\"") %>%\n  layer_dense(4)\n```\n:::\n\n\nIts layers are accessible via the `layers` attribute:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$layers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<keras.layers.core.dense.Dense object at 0x7fc370086b50>\n\n[[2]]\n<keras.layers.core.dense.Dense object at 0x7fc3700bbe20>\n\n[[3]]\n<keras.layers.core.dense.Dense object at 0x7fc37111e520>\n```\n:::\n:::\n\n\nYou can also create a Sequential model incrementally:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(2, activation = \""relu\"")\nmodel %>% layer_dense(3, activation = \""relu\"")\nmodel %>% layer_dense(4)\n```\n:::\n\n\nNote that there's also a corresponding `pop()` method to remove layers:\na Sequential model behaves very much like a stack of layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% pop_layer()\nlength(model$layers)  # 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nAlso note that the Sequential constructor accepts a `name` argument,\njust like any layer or model in Keras. This is useful to annotate\nTensorBoard graphs with semantically meaningful names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(name = \""my_sequential\"")\nmodel %>% layer_dense(2, activation = \""relu\"", name = \""layer1\"")\nmodel %>% layer_dense(3, activation = \""relu\"", name = \""layer2\"")\nmodel %>% layer_dense(4, name = \""layer3\"")\n```\n:::\n\n\n## Specifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in\norder to be able to create their weights. So when you create a layer\nlike this, initially, it has no weights:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 3)\nlayer$weights  # Empty\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlist()\n```\n:::\n:::\n\n\nIt creates its weights the first time it is called on an input, since\nthe shape of the weights depends on the shape of the inputs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Call layer on a test input\nx <- tf$ones(shape(1, 4))\ny <- layer(x)\nlayer$weights  # Now it has weights, of shape (4, 3) and (3,)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=\narray([[ 0.04102254,  0.4609574 ,  0.2932011 ],\n       [-0.8724068 ,  0.4684844 ,  0.3217095 ],\n       [-0.3895173 , -0.25451916, -0.8876987 ],\n       [-0.22169489, -0.0800401 ,  0.8842974 ]], dtype=float32)>\n\n[[2]]\n<tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n```\n:::\n:::\n\n\nNaturally, this also applies to Sequential models. When you instantiate\na Sequential model without an input shape, it isn't \""built\"": it has no\nweights (and calling `model$weights` results in an error stating just\nthis). The weights are created when the model first sees some input\ndata:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n        layer_dense(2, activation = \""relu\"") %>%\n        layer_dense(3, activation = \""relu\"") %>%\n        layer_dense(4)\n\n# No weights at this stage!\n# At this point, you can't do this:\n\ntry(model$weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_get_attr_impl(x, name, silent) : \n  ValueError: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n```\n:::\n\n```{.r .cell-code}\n# The model summary is also not available:\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: <no summary available, model was not built>\n```\n:::\n\n```{.r .cell-code}\n# Call the model on a test input\nx <- tf$ones(shape(1, 4))\ny <- model(x)\ncat(\""Number of weights after calling the model:\"", length(model$weights), \""\\n\"")  # 6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of weights after calling the model: 6 \n```\n:::\n:::\n\n\nOnce a model is \""built\"", you can call its `summary()` method to display\nits contents (the `summary()` method is also called by the default\n`print()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_3\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_9 (Dense)                  (1, 2)                        10          \n dense_8 (Dense)                  (1, 3)                        9           \n dense_7 (Dense)                  (1, 4)                        16          \n============================================================================\nTotal params: 35\nTrainable params: 35\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nHowever, it can be very useful when building a Sequential model\nincrementally to be able to display the summary of the model so far,\nincluding the current output shape. In this case, you should start your\nmodel by passing an `input_shape` argument to your model, so that it\nknows its input shape from the start:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(4))\nmodel %>% layer_dense(2, activation = \""relu\"")\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_4\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_10 (Dense)                 (None, 2)                     10          \n============================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nModels built with a predefined input shape like this always have weights\n(even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input\nshape of a Sequential model in advance if you know what it is.\n\n## A common debugging workflow: `%>%` + `summary()`\n\nWhen building a new Sequential architecture, it's useful to\nincrementally stack layers and print model summaries. For instance, this\nenables you to monitor how a stack of `Conv2D` and `MaxPooling2D` layers\nis downsampling image feature maps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(250, 250, 3)) # 250x250 RGB images\n\nmodel %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3)\n\n# Can you guess what the current output shape is at this point? Probably not.\n# Let's just print it:\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_5\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n============================================================================\nTotal params: 11,680\nTrainable params: 11,680\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# The answer was: (40, 40, 32), so we can keep downsampling...\nmodel %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(2)\n\n# And now?\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_5\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n conv2d_5 (Conv2D)                (None, 38, 38, 32)            9248        \n conv2d_4 (Conv2D)                (None, 36, 36, 32)            9248        \n max_pooling2d_2 (MaxPooling2D)   (None, 12, 12, 32)            0           \n conv2d_3 (Conv2D)                (None, 10, 10, 32)            9248        \n conv2d_2 (Conv2D)                (None, 8, 8, 32)              9248        \n max_pooling2d_1 (MaxPooling2D)   (None, 4, 4, 32)              0           \n============================================================================\nTotal params: 48,672\nTrainable params: 48,672\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# Now that we have 4x4 feature maps, time to apply global max pooling.\nmodel %>% layer_global_max_pooling_2d()\n\n# Finally, we add a classification layer.\nmodel %>% layer_dense(10)\n```\n:::\n\n\nVery practical, right?\n\n## What to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n\n-   Train your model, evaluate it, and run inference. See our [guide to\n    training & evaluation with the built-in\n    loops](/guides/training_with_built_in_methods/)\n-   Save your model to disk and restore it. See our [guide to\n    serialization & saving](/guides/serialization_and_saving/).\n-   Speed up model training by leveraging multiple GPUs. See our [guide\n    to multi-GPU and distributed\n    training](https://keras.io/guides/distributed_training/).\n\n## Feature extraction with a Sequential model\n\nOnce a Sequential model has been built, it behaves like a [Functional\nAPI model](/guides/functional_api/). This means that every layer has an\n`input` and `output` attribute. These attributes can be used to do neat\nthings, like quickly creating a model that extracts the outputs of all\nintermediate layers in a Sequential model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs = lapply(initial_model$layers, \\(layer) layer$output)\n)\n\n# Call feature extractor on test input.\n\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n```\n:::\n\n\nHere's a similar example that only extract features from one layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"", name = \""my_intermediate_layer\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs =  get_layer(initial_model, name = \""my_intermediate_layer\"")$output\n)\n\n# Call feature extractor on test input.\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n```\n:::\n\n\n## Transfer learning with a Sequential model\n\nTransfer learning consists of freezing the bottom layers in a model and\nonly training the top layers. If you aren't familiar with it, make sure\nto read our [guide to transfer learning](/guides/transfer_learning/).\n\nHere are two common transfer learning blueprint involving Sequential\nmodels.\n\nFirst, let's say that you have a Sequential model, and you want to\nfreeze all layers except the last one. In this case, you would simply\niterate over `model$layers` and set `layer$trainable = FALSE` on each\nlayer, except the last one. Like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(784)) %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(10)\n\n\n# Presumably you would want to first load pre-trained weights.\nmodel$load_weights(...)\n\n# Freeze all layers except the last one.\nfor (layer in head(model$layers, -1))\n  layer$trainable <- FALSE\n\n# can also just call: freeze_weights(model, to = -2)\n\n# Recompile and train (this will only update the weights of the last layer).\nmodel %>% compile(...)\nmodel %>% fit(...)\n```\n:::\n\n\nAnother common blueprint is to use a Sequential model to stack a\npre-trained model and some freshly initialized classification layers.\nLike this:\n\n# Load a convolutional base with pre-trained weights\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model <- application_xception(\n    weights = 'imagenet',\n    include_top = FALSE,\n    pooling = 'avg')\n\n# Freeze the base model\nbase_model$trainable <- FALSE\n\n# Use a Sequential model to add a trainable classifier on top\nmodel <- keras_model_sequential() %>%\n  base_model() %>%\n  layer_dense(1000)\n\n# Compile & train\nmodel %>% compile(...)\nmodel %>% fit(...)\n```\n:::\n\n\nIf you do transfer learning, you will probably find yourself frequently\nusing these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n\n-   [Guide to the Functional API](/guides/functional_api/)\n-   [Guide to making new Layers & Models via\n    subclassing](/guides/making_new_layers_and_models_via_subclassing/)\n\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.9.1 (~/.virtualenvs/r-tensorflow-site/lib/python3.9/site-packages/tensorflow)\nPython v3.9 (~/.virtualenvs/r-tensorflow-site/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.4 LTS\n\nMatrix products: default\nBLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libmkl_rt.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.9.0.9000      tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           knitr_1.39           whisker_0.4         \n [4] magrittr_2.0.3       here_1.0.1           lattice_0.20-45     \n [7] R6_2.5.1             rlang_1.0.4          fastmap_1.1.0       \n[10] stringr_1.4.0        tools_4.2.1          grid_4.2.1          \n[13] xfun_0.31            png_0.1-7            cli_3.3.0           \n[16] htmltools_0.5.2      tfruns_1.5.0         rprojroot_2.0.3     \n[19] yaml_2.3.5           digest_0.6.29        Matrix_1.4-1        \n[22] base64enc_0.1-3      htmlwidgets_1.5.4    zeallot_0.1.0       \n[25] evaluate_0.15        rmarkdown_2.14       stringi_1.7.8       \n[28] compiler_4.2.1       generics_0.1.3       reticulate_1.25-9000\n[31] jsonlite_1.8.0      \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\""-m pip freeze\""), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.1.0\nasttokens==2.0.5\nastunparse==1.6.3\nbackcall==0.2.0\nbeautifulsoup4==4.11.1\ncachetools==5.2.0\ncertifi==2022.6.15\ncffi==1.15.1\ncharset-normalizer==2.1.0\ndecorator==5.1.1\ndill==0.3.5.1\netils==0.6.0\nexecuting==0.8.3\nfilelock==3.7.1\nflatbuffers==1.12\ngast==0.4.0\ngdown==4.5.1\ngoogle-auth==2.9.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.56.4\ngrpcio==1.47.0\nh5py==3.7.0\nidna==3.3\nimportlib-metadata==4.12.0\nimportlib-resources==5.8.0\nipython==8.4.0\njedi==0.18.1\nJinja2==3.1.2\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.2\nkt-legacy==1.0.4\nlibclang==14.0.1\nMarkdown==3.3.7\nMarkupSafe==2.1.1\nmatplotlib-inline==0.1.3\nnumpy==1.23.1\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.3\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.2.0\npromise==2.3\nprompt-toolkit==3.0.30\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npycparser==2.21\npydot==1.4.2\nPygments==2.12.0\npyparsing==3.0.9\nPySocks==1.7.1\npython-dateutil==2.8.2\npytz==2022.1\npytz-deprecation-shim==0.1.0.post0\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrpy2==3.5.2\nrsa==4.8\nscipy==1.8.1\nsix==1.16.0\nsoupsieve==2.3.2.post1\nstack-data==0.3.0\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-datasets==4.6.0\ntensorflow-estimator==2.9.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.26.0\ntensorflow-metadata==1.9.0\ntermcolor==1.1.0\ntoml==0.10.2\ntqdm==4.64.0\ntraitlets==5.3.0\ntyping_extensions==4.3.0\ntzdata==2022.1\ntzlocal==4.2\nurllib3==1.26.10\nwcwidth==0.2.5\nWerkzeug==2.1.2\nwrapt==1.14.1\nzipp==3.8.1\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\""simple\"" collapse=\""true\""}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-07-21 \nPage render time: 5 seconds\n```\n:::\n:::\n:::\n\n"",
     ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""

---FILE: guides/keras/basics.qmd---
@@ -1,19 +1,19 @@
 ---
 title: ""Guide to Keras Basics""
 aliases:
-  - /keras/articles/guide_keras.html
+  - /articles/guide_keras.html
 ---
 
 Keras is a high-level API to build and train deep learning models. It's
 used for fast prototyping, advanced research, and production, with three
 key advantages:
 
--   *User friendly*<br> Keras has a simple, consistent interface
+-   *User friendly* -- Keras has a simple, consistent interface
     optimized for common use cases. It provides clear and actionable
     feedback for user errors.
--   *Modular and composable*<br> Keras models are made by connecting
+-   *Modular and composable* -- Keras models are made by connecting
     configurable building blocks together, with few restrictions.
--   *Easy to extend*<br> Write custom building blocks to express new
+-   *Easy to extend* -- Write custom building blocks to express new
     ideas for research. Create new layers, loss functions, and develop
     state-of-the-art models.
 
@@ -39,14 +39,14 @@ perceptron):
 ```{r}
 model <- keras_model_sequential()
 
-model %>% 
-  
+model %>%
+
   # Adds a densely-connected layer with 64 units to the model:
   layer_dense(units = 64, activation = 'relu') %>%
-  
+
   # Add another:
   layer_dense(units = 64, activation = 'relu') %>%
-  
+
   # Add a softmax layer with 10 output units:
   layer_dense(units = 10, activation = 'softmax')
 ```
@@ -205,7 +205,7 @@ provided, again as R data as well as a `dataset`:
 
 ```{r, eval = FALSE}
 model %>% predict(test_data, batch_size = 32)
-    
+
 model %>% predict(test_dataset, steps = 30)
 ```
 
@@ -235,9 +235,9 @@ fully-connected network:
 ```{r}
 inputs <- layer_input(shape = (32))  # Returns a placeholder tensor
 
-predictions <- inputs %>% 
+predictions <- inputs %>%
+  layer_dense(units = 64, activation = 'relu') %>%
   layer_dense(units = 64, activation = 'relu') %>%
-  layer_dense(units = 64, activation = 'relu') %>% 
   layer_dense(units = 10, activation = 'softmax')
 
 # Instantiate the model given inputs and outputs.
@@ -283,32 +283,32 @@ Here is an example custom layer that performs a matrix multiplication:
 library(keras)
 
 CustomLayer <- R6::R6Class(""CustomLayer"",
-                                  
+
   inherit = KerasLayer,
-  
+
   public = list(
-    
+
     output_dim = NULL,
-    
+
     kernel = NULL,
-    
+
     initialize = function(output_dim) {
       self$output_dim <- output_dim
     },
-    
+
     build = function(input_shape) {
       self$kernel <- self$add_weight(
-        name = 'kernel', 
+        name = 'kernel',
         shape = list(input_shape[[2]], self$output_dim),
         initializer = initializer_random_normal(),
         trainable = TRUE
       )
     },
-    
+
     call = function(x, mask = NULL) {
       k_dot(x, self$kernel)
     },
-    
+
     compute_output_shape = function(input_shape) {
       list(input_shape[[1]], self$output_dim)
     }
@@ -336,8 +336,8 @@ You can now use the layer in a model as usual:
 
 ```{r}
 model <- keras_model_sequential()
-model %>% 
-  layer_dense(units = 32, input_shape = c(32,32)) %>% 
+model %>%
+  layer_dense(units = 32, input_shape = c(32,32)) %>%
   layer_custom(output_dim = 32)
 ```
 
@@ -359,13 +359,13 @@ be executed on forward pass.
 # define a custom model type
 my_model_constructor <- new_model_class(
   ""MyModel"",
-  
+
   initialize = function(output_dim, ...) {
     super$initialize(...)
     # store our output dim in self until build() is called
     self$output_dim <- output_dim
   },
-  
+
   build = function(input_shape) {
     # create layers we'll need for the call (this code executes once)
     # note: the layers have to be created on the self object!
@@ -375,7 +375,7 @@ my_model_constructor <- new_model_class(
     self$dense2 <- layer_dense(units = 64, activation = 'relu')
     self$dense3 <- layer_dense(units = self$output_dim, activation = 'softmax')
   },
-  
+
   # implement call (this code executes during training & inference)
   call = function(inputs) {
     x <- inputs %>%
@@ -384,8 +384,8 @@ my_model_constructor <- new_model_class(
       self$dense3()
     x
   },
-  
-  # define a `get_config()` method in custom objects 
+
+  # define a `get_config()` method in custom objects
   # to enable model saving and restoring
   get_config = function() {
     list(output_dim = self$output_dim)
@@ -480,7 +480,7 @@ fresh_model <- model_from_json(json_string,
 json_string <- model %>% model_to_json()
 
 # Recreate the model (freshly initialized)
-fresh_model <- model_from_json(json_string, 
+fresh_model <- model_from_json(json_string,
                                custom_objects = list('MyModel' = my_model_constructor))
 ```
 

---FILE: guides/keras/sequential_model.qmd---
@@ -5,7 +5,6 @@ date-created: 2020/04/12
 date-last-modified: 2020/04/12
 description: Complete guide to the Sequential model.
 aliases:
-  - ../../articles/guide_keras.html
   - ../../articles/sequential_model.html
   - ../../guide/keras/sequential_model/index.html
 ---
@@ -26,9 +25,9 @@ Schematically, the following `Sequential` model:
 
 ```{r}
 # Define Sequential model with 3 layers
-model <- keras_model_sequential() %>% 
-  layer_dense(2, activation = ""relu"", name = ""layer1"") %>% 
-  layer_dense(3, activation = ""relu"", name = ""layer2"") %>% 
+model <- keras_model_sequential() %>%
+  layer_dense(2, activation = ""relu"", name = ""layer1"") %>%
+  layer_dense(3, activation = ""relu"", name = ""layer2"") %>%
   layer_dense(4, name = ""layer3"")
 
 # Call model on a test input
@@ -131,9 +130,9 @@ this). The weights are created when the model first sees some input
 data:
 
 ```{r}
-model <- keras_model_sequential() %>% 
-        layer_dense(2, activation = ""relu"") %>% 
-        layer_dense(3, activation = ""relu"") %>% 
+model <- keras_model_sequential() %>%
+        layer_dense(2, activation = ""relu"") %>%
+        layer_dense(3, activation = ""relu"") %>%
         layer_dense(4)
 
 # No weights at this stage!
@@ -188,11 +187,11 @@ is downsampling image feature maps:
 
 ```{r}
 model <- keras_model_sequential(input_shape = c(250, 250, 3)) # 250x250 RGB images
-  
-model %>% 
+
+model %>%
   layer_conv_2d(32, 5, strides = 2, activation = ""relu"") %>%
   layer_conv_2d(32, 3, activation = ""relu"") %>%
-  layer_max_pooling_2d(3) 
+  layer_max_pooling_2d(3)
 
 # Can you guess what the current output shape is at this point? Probably not.
 # Let's just print it:
@@ -205,7 +204,7 @@ model %>%
   layer_max_pooling_2d(3) %>%
   layer_conv_2d(32, 3, activation = ""relu"") %>%
   layer_conv_2d(32, 3, activation = ""relu"") %>%
-  layer_max_pooling_2d(2) 
+  layer_max_pooling_2d(2)
 
 # And now?
 model
@@ -351,4 +350,3 @@ To find out more about building models in Keras, see:
 
 
 {{< include ../../_environment.qmd >}}
-

---FILE: index.qmd---
@@ -42,8 +42,7 @@ css: _css/index.css
 ::: feature
 ### [Tutorials](tutorials/)
 
-Tutorials teach how to use Tensorflow with complete, end-to-end examples.
-<!-- feature -->
+Tutorials help you get started with deep learning using end-to-end examples.
 :::
 
 ::: feature

---FILE: reference/index.qmd---
@@ -20,3 +20,5 @@ listing:
 ---
 
 # R packages
+
+"
rstudio,tensorflow.rstudio.com,b0ad2d7ee480bd8ccee7c895db4512464b42b4cc,Daniel Falbel,dfalbel@gmail.com,2022-07-15T20:17:26Z,Daniel Falbel,dfalbel@gmail.com,2022-07-15T20:17:26Z,fix toggling and added a small footer.,_quarto.yml;light.scss,False,False,False,False,12,2,14,"---FILE: _quarto.yml---
@@ -27,10 +27,17 @@ website:
   page-navigation: true
   reader-mode: true
   favicon: ""images/favicon/icon.png""
+  page-footer:
+    left: |
+      Proudly supported by
+      [![](https://www.rstudio.com/wp-content/uploads/2018/10/RStudio-Logo-flat.svg){fig-alt=""RStudio"" width=65px}](https://www.rstudio.com)
+    right:
+      - icon: github
+        href: https://github.com/rstudio/tensorflow
+        aria-label: GitHub repository
   navbar:
     search: true
     logo: ""images/favicon/icon.png""
-    background: ""#E83E18""
     file: index.qmd
     left:
       - { text: Install, href: install/ }
@@ -136,5 +143,5 @@ format:
     toc: true
     code-overflow: wrap
     theme:
-      light: [custom.scss]
+      light: [flatly, light.scss, custom.scss]
       dark: [darkly, dark.scss, custom.scss]

---FILE: light.scss---
@@ -0,0 +1,3 @@
+/*-- scss:defaults --*/
+
+$navbar-bg: #e83e18;"
rstudio,tensorflow.rstudio.com,af1a810ff7198035e498c3c8cc31a5c4f7be6c9a,Daniel Falbel,dfalbel@gmail.com,2022-07-15T17:53:06Z,Daniel Falbel,dfalbel@gmail.com,2022-07-15T17:53:06Z,fix light theme.,_quarto.yml,False,False,False,False,1,0,1,"---FILE: _quarto.yml---
@@ -30,6 +30,7 @@ website:
   navbar:
     search: true
     logo: ""images/favicon/icon.png""
+    background: ""#E83E18""
     file: index.qmd
     left:
       - { text: Install, href: install/ }"
rstudio,tensorflow.rstudio.com,c8328814df38daf6c1859cac25c71c415e67e295,Daniel Falbel,dfalbel@gmail.com,2022-07-15T15:02:21Z,GitHub,noreply@github.com,2022-07-15T15:02:21Z,Fix identation,.github/workflows/publish.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/publish.yml---
@@ -8,7 +8,7 @@ name: Quarto Publish
 jobs:
   build-deploy:
     env:
-          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
     runs-on: ubuntu-latest
     steps:
       - name: Check out repository"
rstudio,tensorflow.rstudio.com,d94251de6e64e076e749d5687d10e9c76d2f203a,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-15T14:55:51Z,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-15T14:55:51Z,publish.yml fix?,.github/workflows/publish.yml,False,False,False,False,2,2,4,"---FILE: .github/workflows/publish.yml---
@@ -17,7 +17,7 @@ jobs:
 
       - name: Render and Publish
         uses: quarto-dev/quarto-actions/publish@v2
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
         with:
           target: gh-pages
-          env:
-            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}"
rstudio,tensorflow.rstudio.com,ed40664b5d186db35ac086a25dc5705f52cf2708,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-15T12:24:57Z,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-15T12:24:57Z,fix reference links,_quarto.yml;_utils/build-reference.R;reference/keras/index.md;reference/tensorflow/index.md;reference/tfautograph/index.md;reference/tfdatasets/index.md;reference/tfhub/index.md,False,True,True,False,302,299,601,"---FILE: _quarto.yml---
@@ -33,22 +33,17 @@ website:
     background: ""#E83E18""
     file: index.qmd
     left:
-      - { text: Install    ,href: install/ }
+      - { text: Install    ,href: install/   }
       - { text: Tutorials  ,href: tutorials/ }
-      - { text: Guides     ,href: guides/ }
-      - { text: Examples   ,href: examples/ }
+      - { text: Guides     ,href: guides/    }
+      - { text: Examples   ,href: examples/  }
       - text: Reference
         menu:
-          - text: ""tensorlow""
-            href: ""reference/tensorflow/index.html""
-          - text: ""keras""
-            href: ""reference/keras/index.html""
-          - text: ""tfdatasets""
-            href: ""reference/tfdatasets/index.html""
-          - text: ""tfautograph""
-            href: ""reference/tfautograph/index.html""
-          - text: ""tfhub""
-            href: ""reference/tfhub/index.html""
+          - { text: ""tensorlow""   ,href: ""reference/tensorflow/index.html""  }
+          - { text: ""keras""       ,href: ""reference/keras/index.html""       }
+          - { text: ""tfdatasets""  ,href: ""reference/tfdatasets/index.html""  }
+          - { text: ""tfautograph"" ,href: ""reference/tfautograph/index.html"" }
+          - { text: ""tfhub""       ,href: ""reference/tfhub/index.html""       }
 
   sidebar:
     - title: ""Install""
@@ -90,8 +85,8 @@ website:
             - { text: Overview                   ,href: guides/tensorflow/basics.qmd }
             - { text: Tensors                    ,href: guides/tensorflow/tensor.qmd }
             - { text: Variables                  ,href: guides/tensorflow/variable.qmd }
-            - { text: Automatic differentiation  ,href: guides/tensorflow/autodiff.qmd}
-            - { text: Graphs and functions       ,href: guides/tensorflow/intro_to_graphs.qmd}
+            - { text: Automatic differentiation  ,href: guides/tensorflow/autodiff.qmd }
+            - { text: Graphs and functions       ,href: guides/tensorflow/intro_to_graphs.qmd }
 
         - section: Keras
           contents:

---FILE: _utils/build-reference.R---
@@ -12,11 +12,19 @@ dir_create(""reference"")
 
 try(ecodown::ecodown_build(verbosity = ""verbose""))
 
+
 dir_ls(""reference/"") |>
   fs::path_file() |>
   lapply(function(pkg) {
     file_move(glue(""reference/{pkg}/latest/reference""),
               glue(""reference/_{pkg}""))
     dir_delete(glue(""reference/{pkg}""))
     file_move(glue(""reference/_{pkg}""), glue(""reference/{pkg}""))
-  })
+    index_pth <- glue(""reference/{pkg}/index.md"")
+    index <- readLines(index_pth)
+
+    index <- sub(glue(""/reference/{pkg}/latest/reference/""),
+                 glue(""/reference/{pkg}/""),
+                 index)
+    writeLines(index, index_pth)
+  }) | invisible()

---FILE: reference/keras/index.md---
@@ -2,44 +2,44 @@
 
 Function(s) | Description
 ------------- |----------------
-[keras_model()](/reference/keras/latest/reference/keras_model.html) | Keras Model
-[keras_model_sequential()](/reference/keras/latest/reference/keras_model_sequential.html) | Keras Model composed of a linear stack of layers
-[keras_model_custom()](/reference/keras/latest/reference/keras_model_custom.html) | (Deprecated) Create a Keras custom model
-[multi_gpu_model()](/reference/keras/latest/reference/multi_gpu_model.html) | (Deprecated) Replicates a model on different GPUs.
-[summary(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/summary.keras.engine.training.Model.html) [format(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/summary.keras.engine.training.Model.html) [print(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/summary.keras.engine.training.Model.html) | Print a summary of a Keras model
-[compile(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/compile.keras.engine.training.Model.html) | Configure a Keras model for training
-[evaluate(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/evaluate.keras.engine.training.Model.html) | Evaluate a Keras model
-[export_savedmodel(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/export_savedmodel.keras.engine.training.Model.html) | Export a Saved Model
-[fit(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/fit.keras.engine.training.Model.html) | Train a Keras model
-[fit_generator()](/reference/keras/latest/reference/fit_generator.html) | (Deprecated) Fits the model on data yielded batch-by-batch by a generator.
-[evaluate_generator()](/reference/keras/latest/reference/evaluate_generator.html) | (Deprecated) Evaluates the model on a data generator.
-[predict(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/predict.keras.engine.training.Model.html) | Generate predictions from a Keras model
-[predict_proba()](/reference/keras/latest/reference/predict_proba.html) [predict_classes()](/reference/keras/latest/reference/predict_proba.html) | (Deprecated) Generates probability or class probability predictions for the input samples.
-[predict_on_batch()](/reference/keras/latest/reference/predict_on_batch.html) | Returns predictions for a single batch of samples.
-[predict_generator()](/reference/keras/latest/reference/predict_generator.html) | (Deprecated) Generates predictions for the input samples from a data generator.
-[train_on_batch()](/reference/keras/latest/reference/train_on_batch.html) [test_on_batch()](/reference/keras/latest/reference/train_on_batch.html) | Single gradient update or model evaluation over one batch of samples.
-[get_layer()](/reference/keras/latest/reference/get_layer.html) | Retrieves a layer based on either its name (unique) or index.
-[pop_layer()](/reference/keras/latest/reference/pop_layer.html) | Remove the last layer in a model
-[save_model_hdf5()](/reference/keras/latest/reference/save_model_hdf5.html) [load_model_hdf5()](/reference/keras/latest/reference/save_model_hdf5.html) | Save/Load models using HDF5 files
-[serialize_model()](/reference/keras/latest/reference/serialize_model.html) [unserialize_model()](/reference/keras/latest/reference/serialize_model.html) | Serialize a model to an R object
-[clone_model()](/reference/keras/latest/reference/clone_model.html) | Clone a model instance.
-[freeze_weights()](/reference/keras/latest/reference/freeze_weights.html) [unfreeze_weights()](/reference/keras/latest/reference/freeze_weights.html) | Freeze and unfreeze weights
+[keras_model()](/reference/keras/keras_model.html) | Keras Model
+[keras_model_sequential()](/reference/keras/keras_model_sequential.html) | Keras Model composed of a linear stack of layers
+[keras_model_custom()](/reference/keras/keras_model_custom.html) | (Deprecated) Create a Keras custom model
+[multi_gpu_model()](/reference/keras/multi_gpu_model.html) | (Deprecated) Replicates a model on different GPUs.
+[summary(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/summary.keras.engine.training.Model.html) [format(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/summary.keras.engine.training.Model.html) [print(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/summary.keras.engine.training.Model.html) | Print a summary of a Keras model
+[compile(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/compile.keras.engine.training.Model.html) | Configure a Keras model for training
+[evaluate(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/evaluate.keras.engine.training.Model.html) | Evaluate a Keras model
+[export_savedmodel(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/export_savedmodel.keras.engine.training.Model.html) | Export a Saved Model
+[fit(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/fit.keras.engine.training.Model.html) | Train a Keras model
+[fit_generator()](/reference/keras/fit_generator.html) | (Deprecated) Fits the model on data yielded batch-by-batch by a generator.
+[evaluate_generator()](/reference/keras/evaluate_generator.html) | (Deprecated) Evaluates the model on a data generator.
+[predict(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/predict.keras.engine.training.Model.html) | Generate predictions from a Keras model
+[predict_proba()](/reference/keras/predict_proba.html) [predict_classes()](/reference/keras/latest/reference/predict_proba.html) | (Deprecated) Generates probability or class probability predictions for the input samples.
+[predict_on_batch()](/reference/keras/predict_on_batch.html) | Returns predictions for a single batch of samples.
+[predict_generator()](/reference/keras/predict_generator.html) | (Deprecated) Generates predictions for the input samples from a data generator.
+[train_on_batch()](/reference/keras/train_on_batch.html) [test_on_batch()](/reference/keras/latest/reference/train_on_batch.html) | Single gradient update or model evaluation over one batch of samples.
+[get_layer()](/reference/keras/get_layer.html) | Retrieves a layer based on either its name (unique) or index.
+[pop_layer()](/reference/keras/pop_layer.html) | Remove the last layer in a model
+[save_model_hdf5()](/reference/keras/save_model_hdf5.html) [load_model_hdf5()](/reference/keras/latest/reference/save_model_hdf5.html) | Save/Load models using HDF5 files
+[serialize_model()](/reference/keras/serialize_model.html) [unserialize_model()](/reference/keras/latest/reference/serialize_model.html) | Serialize a model to an R object
+[clone_model()](/reference/keras/clone_model.html) | Clone a model instance.
+[freeze_weights()](/reference/keras/freeze_weights.html) [unfreeze_weights()](/reference/keras/latest/reference/freeze_weights.html) | Freeze and unfreeze weights
 
 ## Core Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_input()](/reference/keras/latest/reference/layer_input.html) | Input layer
-[layer_dense()](/reference/keras/latest/reference/layer_dense.html) | Add a densely-connected NN layer to an output
-[layer_activation()](/reference/keras/latest/reference/layer_activation.html) | Apply an activation function to an output.
-[layer_dropout()](/reference/keras/latest/reference/layer_dropout.html) | Applies Dropout to the input.
-[layer_reshape()](/reference/keras/latest/reference/layer_reshape.html) | Reshapes an output to a certain shape.
-[layer_permute()](/reference/keras/latest/reference/layer_permute.html) | Permute the dimensions of an input according to a given pattern
-[layer_repeat_vector()](/reference/keras/latest/reference/layer_repeat_vector.html) | Repeats the input n times.
-[layer_lambda()](/reference/keras/latest/reference/layer_lambda.html) | Wraps arbitrary expression as a layer
-[layer_activity_regularization()](/reference/keras/latest/reference/layer_activity_regularization.html) | Layer that applies an update to the cost function based input activity.
-[layer_masking()](/reference/keras/latest/reference/layer_masking.html) | Masks a sequence by using a mask value to skip timesteps.
-[layer_flatten()](/reference/keras/latest/reference/layer_flatten.html) | Flattens an input
+[layer_input()](/reference/keras/layer_input.html) | Input layer
+[layer_dense()](/reference/keras/layer_dense.html) | Add a densely-connected NN layer to an output
+[layer_activation()](/reference/keras/layer_activation.html) | Apply an activation function to an output.
+[layer_dropout()](/reference/keras/layer_dropout.html) | Applies Dropout to the input.
+[layer_reshape()](/reference/keras/layer_reshape.html) | Reshapes an output to a certain shape.
+[layer_permute()](/reference/keras/layer_permute.html) | Permute the dimensions of an input according to a given pattern
+[layer_repeat_vector()](/reference/keras/layer_repeat_vector.html) | Repeats the input n times.
+[layer_lambda()](/reference/keras/layer_lambda.html) | Wraps arbitrary expression as a layer
+[layer_activity_regularization()](/reference/keras/layer_activity_regularization.html) | Layer that applies an update to the cost function based input activity.
+[layer_masking()](/reference/keras/layer_masking.html) | Masks a sequence by using a mask value to skip timesteps.
+[layer_flatten()](/reference/keras/layer_flatten.html) | Flattens an input
 
 ## Convolutional Layers
 
@@ -50,177 +50,177 @@ Function(s) | Description
 
 Function(s) | Description
 ------------- |----------------
-[layer_global_max_pooling_1d()](/reference/keras/latest/reference/layer_global_max_pooling_1d.html) | Global max pooling operation for temporal data.
-[layer_global_average_pooling_1d()](/reference/keras/latest/reference/layer_global_average_pooling_1d.html) | Global average pooling operation for temporal data.
-[layer_global_max_pooling_2d()](/reference/keras/latest/reference/layer_global_max_pooling_2d.html) | Global max pooling operation for spatial data.
-[layer_global_average_pooling_2d()](/reference/keras/latest/reference/layer_global_average_pooling_2d.html) | Global average pooling operation for spatial data.
-[layer_global_max_pooling_3d()](/reference/keras/latest/reference/layer_global_max_pooling_3d.html) | Global Max pooling operation for 3D data.
-[layer_global_average_pooling_3d()](/reference/keras/latest/reference/layer_global_average_pooling_3d.html) | Global Average pooling operation for 3D data.
+[layer_global_max_pooling_1d()](/reference/keras/layer_global_max_pooling_1d.html) | Global max pooling operation for temporal data.
+[layer_global_average_pooling_1d()](/reference/keras/layer_global_average_pooling_1d.html) | Global average pooling operation for temporal data.
+[layer_global_max_pooling_2d()](/reference/keras/layer_global_max_pooling_2d.html) | Global max pooling operation for spatial data.
+[layer_global_average_pooling_2d()](/reference/keras/layer_global_average_pooling_2d.html) | Global average pooling operation for spatial data.
+[layer_global_max_pooling_3d()](/reference/keras/layer_global_max_pooling_3d.html) | Global Max pooling operation for 3D data.
+[layer_global_average_pooling_3d()](/reference/keras/layer_global_average_pooling_3d.html) | Global Average pooling operation for 3D data.
 
 ## Activation Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_activation()](/reference/keras/latest/reference/layer_activation.html) | Apply an activation function to an output.
-[layer_activation_relu()](/reference/keras/latest/reference/layer_activation_relu.html) | Rectified Linear Unit activation function
-[layer_activation_leaky_relu()](/reference/keras/latest/reference/layer_activation_leaky_relu.html) | Leaky version of a Rectified Linear Unit.
-[layer_activation_parametric_relu()](/reference/keras/latest/reference/layer_activation_parametric_relu.html) | Parametric Rectified Linear Unit.
-[layer_activation_thresholded_relu()](/reference/keras/latest/reference/layer_activation_thresholded_relu.html) | Thresholded Rectified Linear Unit.
-[layer_activation_elu()](/reference/keras/latest/reference/layer_activation_elu.html) | Exponential Linear Unit.
-[layer_activation_softmax()](/reference/keras/latest/reference/layer_activation_softmax.html) | Softmax activation function.
-[layer_activation_selu()](/reference/keras/latest/reference/layer_activation_selu.html) | Scaled Exponential Linear Unit.
+[layer_activation()](/reference/keras/layer_activation.html) | Apply an activation function to an output.
+[layer_activation_relu()](/reference/keras/layer_activation_relu.html) | Rectified Linear Unit activation function
+[layer_activation_leaky_relu()](/reference/keras/layer_activation_leaky_relu.html) | Leaky version of a Rectified Linear Unit.
+[layer_activation_parametric_relu()](/reference/keras/layer_activation_parametric_relu.html) | Parametric Rectified Linear Unit.
+[layer_activation_thresholded_relu()](/reference/keras/layer_activation_thresholded_relu.html) | Thresholded Rectified Linear Unit.
+[layer_activation_elu()](/reference/keras/layer_activation_elu.html) | Exponential Linear Unit.
+[layer_activation_softmax()](/reference/keras/layer_activation_softmax.html) | Softmax activation function.
+[layer_activation_selu()](/reference/keras/layer_activation_selu.html) | Scaled Exponential Linear Unit.
 
 ## Dropout Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_dropout()](/reference/keras/latest/reference/layer_dropout.html) | Applies Dropout to the input.
+[layer_dropout()](/reference/keras/layer_dropout.html) | Applies Dropout to the input.
 
 ## Locally-connected Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_locally_connected_1d()](/reference/keras/latest/reference/layer_locally_connected_1d.html) | Locally-connected layer for 1D inputs.
-[layer_locally_connected_2d()](/reference/keras/latest/reference/layer_locally_connected_2d.html) | Locally-connected layer for 2D inputs.
+[layer_locally_connected_1d()](/reference/keras/layer_locally_connected_1d.html) | Locally-connected layer for 1D inputs.
+[layer_locally_connected_2d()](/reference/keras/layer_locally_connected_2d.html) | Locally-connected layer for 2D inputs.
 
 ## Recurrent Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_simple_rnn()](/reference/keras/latest/reference/layer_simple_rnn.html) | Fully-connected RNN where the output is to be fed back to input.
-[layer_gru()](/reference/keras/latest/reference/layer_gru.html) | Gated Recurrent Unit - Cho et al.
-[layer_lstm()](/reference/keras/latest/reference/layer_lstm.html) | Long Short-Term Memory unit - Hochreiter 1997.
+[layer_simple_rnn()](/reference/keras/layer_simple_rnn.html) | Fully-connected RNN where the output is to be fed back to input.
+[layer_gru()](/reference/keras/layer_gru.html) | Gated Recurrent Unit - Cho et al.
+[layer_lstm()](/reference/keras/layer_lstm.html) | Long Short-Term Memory unit - Hochreiter 1997.
 
 ## Customize Recurrent Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_rnn()](/reference/keras/latest/reference/layer_rnn.html) | Base class for recurrent layers
-[layer_simple_rnn_cell()](/reference/keras/latest/reference/layer_simple_rnn_cell.html) | Cell class for SimpleRNN
-[layer_gru_cell()](/reference/keras/latest/reference/layer_gru_cell.html) | Cell class for the GRU layer
-[layer_lstm_cell()](/reference/keras/latest/reference/layer_lstm_cell.html) | Cell class for the LSTM layer
-[layer_stacked_rnn_cells()](/reference/keras/latest/reference/layer_stacked_rnn_cells.html) | Wrapper allowing a stack of RNN cells to behave as a single cell
+[layer_rnn()](/reference/keras/layer_rnn.html) | Base class for recurrent layers
+[layer_simple_rnn_cell()](/reference/keras/layer_simple_rnn_cell.html) | Cell class for SimpleRNN
+[layer_gru_cell()](/reference/keras/layer_gru_cell.html) | Cell class for the GRU layer
+[layer_lstm_cell()](/reference/keras/layer_lstm_cell.html) | Cell class for the LSTM layer
+[layer_stacked_rnn_cells()](/reference/keras/layer_stacked_rnn_cells.html) | Wrapper allowing a stack of RNN cells to behave as a single cell
 
 ## Embedding Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_embedding()](/reference/keras/latest/reference/layer_embedding.html) | Turns positive integers (indexes) into dense vectors of fixed size.
+[layer_embedding()](/reference/keras/layer_embedding.html) | Turns positive integers (indexes) into dense vectors of fixed size.
 
 ## Normalization Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_batch_normalization()](/reference/keras/latest/reference/layer_batch_normalization.html) | Batch normalization layer (Ioffe and Szegedy, 2014).
-[layer_layer_normalization()](/reference/keras/latest/reference/layer_layer_normalization.html) | Layer normalization layer (Ba et al., 2016).
+[layer_batch_normalization()](/reference/keras/layer_batch_normalization.html) | Batch normalization layer (Ioffe and Szegedy, 2014).
+[layer_layer_normalization()](/reference/keras/layer_layer_normalization.html) | Layer normalization layer (Ba et al., 2016).
 
 ## Noise Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_gaussian_noise()](/reference/keras/latest/reference/layer_gaussian_noise.html) | Apply additive zero-centered Gaussian noise.
-[layer_gaussian_dropout()](/reference/keras/latest/reference/layer_gaussian_dropout.html) | Apply multiplicative 1-centered Gaussian noise.
-[layer_alpha_dropout()](/reference/keras/latest/reference/layer_alpha_dropout.html) | Applies Alpha Dropout to the input.
+[layer_gaussian_noise()](/reference/keras/layer_gaussian_noise.html) | Apply additive zero-centered Gaussian noise.
+[layer_gaussian_dropout()](/reference/keras/layer_gaussian_dropout.html) | Apply multiplicative 1-centered Gaussian noise.
+[layer_alpha_dropout()](/reference/keras/layer_alpha_dropout.html) | Applies Alpha Dropout to the input.
 
 ## Merge Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_add()](/reference/keras/latest/reference/layer_add.html) | Layer that adds a list of inputs.
-[layer_subtract()](/reference/keras/latest/reference/layer_subtract.html) | Layer that subtracts two inputs.
-[layer_multiply()](/reference/keras/latest/reference/layer_multiply.html) | Layer that multiplies (element-wise) a list of inputs.
-[layer_average()](/reference/keras/latest/reference/layer_average.html) | Layer that averages a list of inputs.
-[layer_maximum()](/reference/keras/latest/reference/layer_maximum.html) | Layer that computes the maximum (element-wise) a list of inputs.
-[layer_minimum()](/reference/keras/latest/reference/layer_minimum.html) | Layer that computes the minimum (element-wise) a list of inputs.
-[layer_concatenate()](/reference/keras/latest/reference/layer_concatenate.html) | Layer that concatenates a list of inputs.
-[layer_dot()](/reference/keras/latest/reference/layer_dot.html) | Layer that computes a dot product between samples in two tensors.
+[layer_add()](/reference/keras/layer_add.html) | Layer that adds a list of inputs.
+[layer_subtract()](/reference/keras/layer_subtract.html) | Layer that subtracts two inputs.
+[layer_multiply()](/reference/keras/layer_multiply.html) | Layer that multiplies (element-wise) a list of inputs.
+[layer_average()](/reference/keras/layer_average.html) | Layer that averages a list of inputs.
+[layer_maximum()](/reference/keras/layer_maximum.html) | Layer that computes the maximum (element-wise) a list of inputs.
+[layer_minimum()](/reference/keras/layer_minimum.html) | Layer that computes the minimum (element-wise) a list of inputs.
+[layer_concatenate()](/reference/keras/layer_concatenate.html) | Layer that concatenates a list of inputs.
+[layer_dot()](/reference/keras/layer_dot.html) | Layer that computes a dot product between samples in two tensors.
 
 ## Image Preprocessing Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_resizing()](/reference/keras/latest/reference/layer_resizing.html) | Image resizing layer
-[layer_rescaling()](/reference/keras/latest/reference/layer_rescaling.html) | Multiply inputs by <code>scale</code> and adds <code>offset</code>
-[layer_center_crop()](/reference/keras/latest/reference/layer_center_crop.html) | Crop the central portion of the images to target height and width
+[layer_resizing()](/reference/keras/layer_resizing.html) | Image resizing layer
+[layer_rescaling()](/reference/keras/layer_rescaling.html) | Multiply inputs by <code>scale</code> and adds <code>offset</code>
+[layer_center_crop()](/reference/keras/layer_center_crop.html) | Crop the central portion of the images to target height and width
 
 ## Image Augmentation Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_random_contrast()](/reference/keras/latest/reference/layer_random_contrast.html) | Adjust the contrast of an image or images by a random factor
-[layer_random_crop()](/reference/keras/latest/reference/layer_random_crop.html) | Randomly crop the images to target height and width
-[layer_random_flip()](/reference/keras/latest/reference/layer_random_flip.html) | Randomly flip each image horizontally and vertically
-[layer_random_height()](/reference/keras/latest/reference/layer_random_height.html) | Randomly vary the height of a batch of images during training
-[layer_random_rotation()](/reference/keras/latest/reference/layer_random_rotation.html) | Randomly rotate each image
-[layer_random_translation()](/reference/keras/latest/reference/layer_random_translation.html) | Randomly translate each image during training
-[layer_random_width()](/reference/keras/latest/reference/layer_random_width.html) | Randomly vary the width of a batch of images during training
-[layer_random_zoom()](/reference/keras/latest/reference/layer_random_zoom.html) | A preprocessing layer which randomly zooms images during training.
+[layer_random_contrast()](/reference/keras/layer_random_contrast.html) | Adjust the contrast of an image or images by a random factor
+[layer_random_crop()](/reference/keras/layer_random_crop.html) | Randomly crop the images to target height and width
+[layer_random_flip()](/reference/keras/layer_random_flip.html) | Randomly flip each image horizontally and vertically
+[layer_random_height()](/reference/keras/layer_random_height.html) | Randomly vary the height of a batch of images during training
+[layer_random_rotation()](/reference/keras/layer_random_rotation.html) | Randomly rotate each image
+[layer_random_translation()](/reference/keras/layer_random_translation.html) | Randomly translate each image during training
+[layer_random_width()](/reference/keras/layer_random_width.html) | Randomly vary the width of a batch of images during training
+[layer_random_zoom()](/reference/keras/layer_random_zoom.html) | A preprocessing layer which randomly zooms images during training.
 
 ## Categorical Features Preprocessing
 
 Function(s) | Description
 ------------- |----------------
-[layer_category_encoding()](/reference/keras/latest/reference/layer_category_encoding.html) | A preprocessing layer which encodes integer features.
-[layer_hashing()](/reference/keras/latest/reference/layer_hashing.html) | A preprocessing layer which hashes and bins categorical features.
-[layer_integer_lookup()](/reference/keras/latest/reference/layer_integer_lookup.html) | A preprocessing layer which maps integer features to contiguous ranges.
-[layer_string_lookup()](/reference/keras/latest/reference/layer_string_lookup.html) | A preprocessing layer which maps string features to integer indices.
+[layer_category_encoding()](/reference/keras/layer_category_encoding.html) | A preprocessing layer which encodes integer features.
+[layer_hashing()](/reference/keras/layer_hashing.html) | A preprocessing layer which hashes and bins categorical features.
+[layer_integer_lookup()](/reference/keras/layer_integer_lookup.html) | A preprocessing layer which maps integer features to contiguous ranges.
+[layer_string_lookup()](/reference/keras/layer_string_lookup.html) | A preprocessing layer which maps string features to integer indices.
 
 ## Numerical Features Preprocessing
 
 Function(s) | Description
 ------------- |----------------
-[layer_normalization()](/reference/keras/latest/reference/layer_normalization.html) | A preprocessing layer which normalizes continuous features.
-[layer_discretization()](/reference/keras/latest/reference/layer_discretization.html) | A preprocessing layer which buckets continuous features by ranges.
+[layer_normalization()](/reference/keras/layer_normalization.html) | A preprocessing layer which normalizes continuous features.
+[layer_discretization()](/reference/keras/layer_discretization.html) | A preprocessing layer which buckets continuous features by ranges.
 
 ## Attention Layers
 
 Function(s) | Description
 ------------- |----------------
-[layer_attention()](/reference/keras/latest/reference/layer_attention.html) | Creates attention layer
-[layer_multi_head_attention()](/reference/keras/latest/reference/layer_multi_head_attention.html) | MultiHeadAttention layer
-[layer_additive_attention()](/reference/keras/latest/reference/layer_additive_attention.html) | Additive attention layer, a.k.a. Bahdanau-style attention
+[layer_attention()](/reference/keras/layer_attention.html) | Creates attention layer
+[layer_multi_head_attention()](/reference/keras/layer_multi_head_attention.html) | MultiHeadAttention layer
+[layer_additive_attention()](/reference/keras/layer_additive_attention.html) | Additive attention layer, a.k.a. Bahdanau-style attention
 
 ## Layer Wrappers
 
 Function(s) | Description
 ------------- |----------------
-[time_distributed()](/reference/keras/latest/reference/time_distributed.html) | This layer wrapper allows to apply a layer to every temporal slice of an input
-[bidirectional()](/reference/keras/latest/reference/bidirectional.html) | Bidirectional wrapper for RNNs
+[time_distributed()](/reference/keras/time_distributed.html) | This layer wrapper allows to apply a layer to every temporal slice of an input
+[bidirectional()](/reference/keras/bidirectional.html) | Bidirectional wrapper for RNNs
 
 ## Layer Methods
 
 Function(s) | Description
 ------------- |----------------
-[get_config()](/reference/keras/latest/reference/get_config.html) [from_config()](/reference/keras/latest/reference/get_config.html) | Layer/Model configuration
-[get_weights()](/reference/keras/latest/reference/get_weights.html) [set_weights()](/reference/keras/latest/reference/get_weights.html) | Layer/Model weights as R arrays
-[get_input_at()](/reference/keras/latest/reference/get_input_at.html) [get_output_at()](/reference/keras/latest/reference/get_input_at.html) [get_input_shape_at()](/reference/keras/latest/reference/get_input_at.html) [get_output_shape_at()](/reference/keras/latest/reference/get_input_at.html) [get_input_mask_at()](/reference/keras/latest/reference/get_input_at.html) [get_output_mask_at()](/reference/keras/latest/reference/get_input_at.html) | Retrieve tensors for layers with multiple nodes
-[count_params()](/reference/keras/latest/reference/count_params.html) | Count the total number of scalars composing the weights.
-[reset_states()](/reference/keras/latest/reference/reset_states.html) | Reset the states for a layer
+[get_config()](/reference/keras/get_config.html) [from_config()](/reference/keras/latest/reference/get_config.html) | Layer/Model configuration
+[get_weights()](/reference/keras/get_weights.html) [set_weights()](/reference/keras/latest/reference/get_weights.html) | Layer/Model weights as R arrays
+[get_input_at()](/reference/keras/get_input_at.html) [get_output_at()](/reference/keras/latest/reference/get_input_at.html) [get_input_shape_at()](/reference/keras/latest/reference/get_input_at.html) [get_output_shape_at()](/reference/keras/latest/reference/get_input_at.html) [get_input_mask_at()](/reference/keras/latest/reference/get_input_at.html) [get_output_mask_at()](/reference/keras/latest/reference/get_input_at.html) | Retrieve tensors for layers with multiple nodes
+[count_params()](/reference/keras/count_params.html) | Count the total number of scalars composing the weights.
+[reset_states()](/reference/keras/reset_states.html) | Reset the states for a layer
 
 ## Custom Layers
 
 Function(s) | Description
 ------------- |----------------
-[`%py_class%`](/reference/keras/latest/reference/grapes-py_class-grapes.html) | Make a python class constructor
-[Layer()](/reference/keras/latest/reference/Layer.html) | (Deprecated) Create a custom Layer
-[create_layer_wrapper()](/reference/keras/latest/reference/create_layer_wrapper.html) | Create a Keras Layer wrapper
-[create_layer()](/reference/keras/latest/reference/create_layer.html) | Create a Keras Layer
+[`%py_class%`](/reference/keras/grapes-py_class-grapes.html) | Make a python class constructor
+[Layer()](/reference/keras/Layer.html) | (Deprecated) Create a custom Layer
+[create_layer_wrapper()](/reference/keras/create_layer_wrapper.html) | Create a Keras Layer wrapper
+[create_layer()](/reference/keras/create_layer.html) | Create a Keras Layer
 
 ## Model Persistence
 
 Function(s) | Description
 ------------- |----------------
-[save_model_hdf5()](/reference/keras/latest/reference/save_model_hdf5.html) [load_model_hdf5()](/reference/keras/latest/reference/save_model_hdf5.html) | Save/Load models using HDF5 files
-[save_model_weights_hdf5()](/reference/keras/latest/reference/save_model_weights_hdf5.html) [load_model_weights_hdf5()](/reference/keras/latest/reference/save_model_weights_hdf5.html) | Save/Load model weights using HDF5 files
-[serialize_model()](/reference/keras/latest/reference/serialize_model.html) [unserialize_model()](/reference/keras/latest/reference/serialize_model.html) | Serialize a model to an R object
-[get_weights()](/reference/keras/latest/reference/get_weights.html) [set_weights()](/reference/keras/latest/reference/get_weights.html) | Layer/Model weights as R arrays
-[get_config()](/reference/keras/latest/reference/get_config.html) [from_config()](/reference/keras/latest/reference/get_config.html) | Layer/Model configuration
-[model_to_saved_model()](/reference/keras/latest/reference/model_to_saved_model.html) | (Deprecated) Export to Saved Model format
-[model_from_saved_model()](/reference/keras/latest/reference/model_from_saved_model.html) | Load a Keras model from the Saved Model format
-[save_model_tf()](/reference/keras/latest/reference/save_model_tf.html) [load_model_tf()](/reference/keras/latest/reference/save_model_tf.html) | Save/Load models using SavedModel format
-[save_model_weights_tf()](/reference/keras/latest/reference/save_model_weights_tf.html) [load_model_weights_tf()](/reference/keras/latest/reference/save_model_weights_tf.html) | Save model weights in the SavedModel format
-[model_to_json()](/reference/keras/latest/reference/model_to_json.html) [model_from_json()](/reference/keras/latest/reference/model_to_json.html) | Model configuration as JSON
-[model_to_yaml()](/reference/keras/latest/reference/model_to_yaml.html) [model_from_yaml()](/reference/keras/latest/reference/model_to_yaml.html) | Model configuration as YAML
+[save_model_hdf5()](/reference/keras/save_model_hdf5.html) [load_model_hdf5()](/reference/keras/latest/reference/save_model_hdf5.html) | Save/Load models using HDF5 files
+[save_model_weights_hdf5()](/reference/keras/save_model_weights_hdf5.html) [load_model_weights_hdf5()](/reference/keras/latest/reference/save_model_weights_hdf5.html) | Save/Load model weights using HDF5 files
+[serialize_model()](/reference/keras/serialize_model.html) [unserialize_model()](/reference/keras/latest/reference/serialize_model.html) | Serialize a model to an R object
+[get_weights()](/reference/keras/get_weights.html) [set_weights()](/reference/keras/latest/reference/get_weights.html) | Layer/Model weights as R arrays
+[get_config()](/reference/keras/get_config.html) [from_config()](/reference/keras/latest/reference/get_config.html) | Layer/Model configuration
+[model_to_saved_model()](/reference/keras/model_to_saved_model.html) | (Deprecated) Export to Saved Model format
+[model_from_saved_model()](/reference/keras/model_from_saved_model.html) | Load a Keras model from the Saved Model format
+[save_model_tf()](/reference/keras/save_model_tf.html) [load_model_tf()](/reference/keras/latest/reference/save_model_tf.html) | Save/Load models using SavedModel format
+[save_model_weights_tf()](/reference/keras/save_model_weights_tf.html) [load_model_weights_tf()](/reference/keras/latest/reference/save_model_weights_tf.html) | Save model weights in the SavedModel format
+[model_to_json()](/reference/keras/model_to_json.html) [model_from_json()](/reference/keras/latest/reference/model_to_json.html) | Model configuration as JSON
+[model_to_yaml()](/reference/keras/model_to_yaml.html) [model_from_yaml()](/reference/keras/latest/reference/model_to_yaml.html) | Model configuration as YAML
 
 ## Datasets
 
@@ -231,158 +231,158 @@ Function(s) | Description
 
 Function(s) | Description
 ------------- |----------------
-[imagenet_preprocess_input()](/reference/keras/latest/reference/imagenet_preprocess_input.html) | Preprocesses a tensor or array encoding a batch of images.
-[imagenet_decode_predictions()](/reference/keras/latest/reference/imagenet_decode_predictions.html) | Decodes the prediction of an ImageNet model.
-[application_mobilenet()](/reference/keras/latest/reference/application_mobilenet.html) [mobilenet_preprocess_input()](/reference/keras/latest/reference/application_mobilenet.html) [mobilenet_decode_predictions()](/reference/keras/latest/reference/application_mobilenet.html) [mobilenet_load_model_hdf5()](/reference/keras/latest/reference/application_mobilenet.html) | MobileNet model architecture.
+[imagenet_preprocess_input()](/reference/keras/imagenet_preprocess_input.html) | Preprocesses a tensor or array encoding a batch of images.
+[imagenet_decode_predictions()](/reference/keras/imagenet_decode_predictions.html) | Decodes the prediction of an ImageNet model.
+[application_mobilenet()](/reference/keras/application_mobilenet.html) [mobilenet_preprocess_input()](/reference/keras/latest/reference/application_mobilenet.html) [mobilenet_decode_predictions()](/reference/keras/latest/reference/application_mobilenet.html) [mobilenet_load_model_hdf5()](/reference/keras/latest/reference/application_mobilenet.html) | MobileNet model architecture.
 
 ## Sequence Preprocessing
 
 Function(s) | Description
 ------------- |----------------
-[pad_sequences()](/reference/keras/latest/reference/pad_sequences.html) | Pads sequences to the same length
-[skipgrams()](/reference/keras/latest/reference/skipgrams.html) | Generates skipgram word pairs.
-[make_sampling_table()](/reference/keras/latest/reference/make_sampling_table.html) | Generates a word rank-based probabilistic sampling table.
-[timeseries_dataset_from_array()](/reference/keras/latest/reference/timeseries_dataset_from_array.html) | Creates a dataset of sliding windows over a timeseries provided as array
+[pad_sequences()](/reference/keras/pad_sequences.html) | Pads sequences to the same length
+[skipgrams()](/reference/keras/skipgrams.html) | Generates skipgram word pairs.
+[make_sampling_table()](/reference/keras/make_sampling_table.html) | Generates a word rank-based probabilistic sampling table.
+[timeseries_dataset_from_array()](/reference/keras/timeseries_dataset_from_array.html) | Creates a dataset of sliding windows over a timeseries provided as array
 
 ## Text Preprocessing
 
 Function(s) | Description
 ------------- |----------------
-[text_dataset_from_directory()](/reference/keras/latest/reference/text_dataset_from_directory.html) | Generate a <code>tf.data.Dataset</code> from text files in a directory
-[text_tokenizer()](/reference/keras/latest/reference/text_tokenizer.html) | Text tokenization utility
-[fit_text_tokenizer()](/reference/keras/latest/reference/fit_text_tokenizer.html) | Update tokenizer internal vocabulary based on a list of texts or list of
+[text_dataset_from_directory()](/reference/keras/text_dataset_from_directory.html) | Generate a <code>tf.data.Dataset</code> from text files in a directory
+[text_tokenizer()](/reference/keras/text_tokenizer.html) | Text tokenization utility
+[fit_text_tokenizer()](/reference/keras/fit_text_tokenizer.html) | Update tokenizer internal vocabulary based on a list of texts or list of
 sequences.
-[save_text_tokenizer()](/reference/keras/latest/reference/save_text_tokenizer.html) [load_text_tokenizer()](/reference/keras/latest/reference/save_text_tokenizer.html) | Save a text tokenizer to an external file
-[texts_to_sequences()](/reference/keras/latest/reference/texts_to_sequences.html) | Transform each text in texts in a sequence of integers.
-[texts_to_sequences_generator()](/reference/keras/latest/reference/texts_to_sequences_generator.html) | Transforms each text in texts in a sequence of integers.
-[texts_to_matrix()](/reference/keras/latest/reference/texts_to_matrix.html) | Convert a list of texts to a matrix.
-[sequences_to_matrix()](/reference/keras/latest/reference/sequences_to_matrix.html) | Convert a list of sequences into a matrix.
-[text_one_hot()](/reference/keras/latest/reference/text_one_hot.html) | One-hot encode a text into a list of word indexes in a vocabulary of size n.
-[text_hashing_trick()](/reference/keras/latest/reference/text_hashing_trick.html) | Converts a text to a sequence of indexes in a fixed-size hashing space.
-[text_to_word_sequence()](/reference/keras/latest/reference/text_to_word_sequence.html) | Convert text to a sequence of words (or tokens).
-[layer_text_vectorization()](/reference/keras/latest/reference/layer_text_vectorization.html) [get_vocabulary()](/reference/keras/latest/reference/layer_text_vectorization.html) [set_vocabulary()](/reference/keras/latest/reference/layer_text_vectorization.html) | A preprocessing layer which maps text features to integer sequences.
-[adapt()](/reference/keras/latest/reference/adapt.html) | Fits the state of the preprocessing layer to the data being passed
+[save_text_tokenizer()](/reference/keras/save_text_tokenizer.html) [load_text_tokenizer()](/reference/keras/latest/reference/save_text_tokenizer.html) | Save a text tokenizer to an external file
+[texts_to_sequences()](/reference/keras/texts_to_sequences.html) | Transform each text in texts in a sequence of integers.
+[texts_to_sequences_generator()](/reference/keras/texts_to_sequences_generator.html) | Transforms each text in texts in a sequence of integers.
+[texts_to_matrix()](/reference/keras/texts_to_matrix.html) | Convert a list of texts to a matrix.
+[sequences_to_matrix()](/reference/keras/sequences_to_matrix.html) | Convert a list of sequences into a matrix.
+[text_one_hot()](/reference/keras/text_one_hot.html) | One-hot encode a text into a list of word indexes in a vocabulary of size n.
+[text_hashing_trick()](/reference/keras/text_hashing_trick.html) | Converts a text to a sequence of indexes in a fixed-size hashing space.
+[text_to_word_sequence()](/reference/keras/text_to_word_sequence.html) | Convert text to a sequence of words (or tokens).
+[layer_text_vectorization()](/reference/keras/layer_text_vectorization.html) [get_vocabulary()](/reference/keras/latest/reference/layer_text_vectorization.html) [set_vocabulary()](/reference/keras/latest/reference/layer_text_vectorization.html) | A preprocessing layer which maps text features to integer sequences.
+[adapt()](/reference/keras/adapt.html) | Fits the state of the preprocessing layer to the data being passed
 
 ## Image Preprocessing
 
 Function(s) | Description
 ------------- |----------------
-[image_load()](/reference/keras/latest/reference/image_load.html) | Loads an image into PIL format.
-[image_to_array()](/reference/keras/latest/reference/image_to_array.html) [image_array_resize()](/reference/keras/latest/reference/image_to_array.html) [image_array_save()](/reference/keras/latest/reference/image_to_array.html) | 3D array representation of images
-[image_data_generator()](/reference/keras/latest/reference/image_data_generator.html) | Generate batches of image data with real-time data augmentation. The data will be
+[image_load()](/reference/keras/image_load.html) | Loads an image into PIL format.
+[image_to_array()](/reference/keras/image_to_array.html) [image_array_resize()](/reference/keras/latest/reference/image_to_array.html) [image_array_save()](/reference/keras/latest/reference/image_to_array.html) | 3D array representation of images
+[image_data_generator()](/reference/keras/image_data_generator.html) | Generate batches of image data with real-time data augmentation. The data will be
 looped over (in batches).
-[fit_image_data_generator()](/reference/keras/latest/reference/fit_image_data_generator.html) | Fit image data generator internal statistics to some sample data.
-[image_dataset_from_directory()](/reference/keras/latest/reference/image_dataset_from_directory.html) | Create a dataset from a directory
-[flow_images_from_data()](/reference/keras/latest/reference/flow_images_from_data.html) | Generates batches of augmented/normalized data from image data and labels
-[flow_images_from_directory()](/reference/keras/latest/reference/flow_images_from_directory.html) | Generates batches of data from images in a directory (with optional
+[fit_image_data_generator()](/reference/keras/fit_image_data_generator.html) | Fit image data generator internal statistics to some sample data.
+[image_dataset_from_directory()](/reference/keras/image_dataset_from_directory.html) | Create a dataset from a directory
+[flow_images_from_data()](/reference/keras/flow_images_from_data.html) | Generates batches of augmented/normalized data from image data and labels
+[flow_images_from_directory()](/reference/keras/flow_images_from_directory.html) | Generates batches of data from images in a directory (with optional
 augmented/normalized data)
-[flow_images_from_dataframe()](/reference/keras/latest/reference/flow_images_from_dataframe.html) | Takes the dataframe and the path to a directory and generates batches of
+[flow_images_from_dataframe()](/reference/keras/flow_images_from_dataframe.html) | Takes the dataframe and the path to a directory and generates batches of
 augmented/normalized data.
-[generator_next()](/reference/keras/latest/reference/generator_next.html) | Retrieve the next item from a generator
+[generator_next()](/reference/keras/generator_next.html) | Retrieve the next item from a generator
 
 ## Optimizers
 
 Function(s) | Description
 ------------- |----------------
-[optimizer_sgd()](/reference/keras/latest/reference/optimizer_sgd.html) | Stochastic gradient descent optimizer
-[optimizer_rmsprop()](/reference/keras/latest/reference/optimizer_rmsprop.html) | RMSProp optimizer
-[optimizer_adagrad()](/reference/keras/latest/reference/optimizer_adagrad.html) | Adagrad optimizer.
-[optimizer_adadelta()](/reference/keras/latest/reference/optimizer_adadelta.html) | Adadelta optimizer.
-[optimizer_adam()](/reference/keras/latest/reference/optimizer_adam.html) | Adam optimizer
-[optimizer_adamax()](/reference/keras/latest/reference/optimizer_adamax.html) | Adamax optimizer
-[optimizer_nadam()](/reference/keras/latest/reference/optimizer_nadam.html) | Nesterov Adam optimizer
+[optimizer_sgd()](/reference/keras/optimizer_sgd.html) | Stochastic gradient descent optimizer
+[optimizer_rmsprop()](/reference/keras/optimizer_rmsprop.html) | RMSProp optimizer
+[optimizer_adagrad()](/reference/keras/optimizer_adagrad.html) | Adagrad optimizer.
+[optimizer_adadelta()](/reference/keras/optimizer_adadelta.html) | Adadelta optimizer.
+[optimizer_adam()](/reference/keras/optimizer_adam.html) | Adam optimizer
+[optimizer_adamax()](/reference/keras/optimizer_adamax.html) | Adamax optimizer
+[optimizer_nadam()](/reference/keras/optimizer_nadam.html) | Nesterov Adam optimizer
 
 ## Learning Rate Schedules
 
 Function(s) | Description
 ------------- |----------------
-[new_learning_rate_schedule_class()](/reference/keras/latest/reference/new_learning_rate_schedule_class.html) | Create a new learning rate schedule type
+[new_learning_rate_schedule_class()](/reference/keras/new_learning_rate_schedule_class.html) | Create a new learning rate schedule type
 
 ## Callbacks
 
 Function(s) | Description
 ------------- |----------------
-[callback_progbar_logger()](/reference/keras/latest/reference/callback_progbar_logger.html) | Callback that prints metrics to stdout.
-[callback_model_checkpoint()](/reference/keras/latest/reference/callback_model_checkpoint.html) | Save the model after every epoch.
-[callback_early_stopping()](/reference/keras/latest/reference/callback_early_stopping.html) | Stop training when a monitored quantity has stopped improving.
-[callback_remote_monitor()](/reference/keras/latest/reference/callback_remote_monitor.html) | Callback used to stream events to a server.
-[callback_learning_rate_scheduler()](/reference/keras/latest/reference/callback_learning_rate_scheduler.html) | Learning rate scheduler.
-[callback_tensorboard()](/reference/keras/latest/reference/callback_tensorboard.html) | TensorBoard basic visualizations
-[callback_reduce_lr_on_plateau()](/reference/keras/latest/reference/callback_reduce_lr_on_plateau.html) | Reduce learning rate when a metric has stopped improving.
-[callback_terminate_on_naan()](/reference/keras/latest/reference/callback_terminate_on_naan.html) | Callback that terminates training when a NaN loss is encountered.
-[callback_csv_logger()](/reference/keras/latest/reference/callback_csv_logger.html) | Callback that streams epoch results to a csv file
-[callback_lambda()](/reference/keras/latest/reference/callback_lambda.html) | Create a custom callback
+[callback_progbar_logger()](/reference/keras/callback_progbar_logger.html) | Callback that prints metrics to stdout.
+[callback_model_checkpoint()](/reference/keras/callback_model_checkpoint.html) | Save the model after every epoch.
+[callback_early_stopping()](/reference/keras/callback_early_stopping.html) | Stop training when a monitored quantity has stopped improving.
+[callback_remote_monitor()](/reference/keras/callback_remote_monitor.html) | Callback used to stream events to a server.
+[callback_learning_rate_scheduler()](/reference/keras/callback_learning_rate_scheduler.html) | Learning rate scheduler.
+[callback_tensorboard()](/reference/keras/callback_tensorboard.html) | TensorBoard basic visualizations
+[callback_reduce_lr_on_plateau()](/reference/keras/callback_reduce_lr_on_plateau.html) | Reduce learning rate when a metric has stopped improving.
+[callback_terminate_on_naan()](/reference/keras/callback_terminate_on_naan.html) | Callback that terminates training when a NaN loss is encountered.
+[callback_csv_logger()](/reference/keras/callback_csv_logger.html) | Callback that streams epoch results to a csv file
+[callback_lambda()](/reference/keras/callback_lambda.html) | Create a custom callback
 
 ## Initializers
 
 Function(s) | Description
 ------------- |----------------
-[initializer_zeros()](/reference/keras/latest/reference/initializer_zeros.html) | Initializer that generates tensors initialized to 0.
-[initializer_ones()](/reference/keras/latest/reference/initializer_ones.html) | Initializer that generates tensors initialized to 1.
-[initializer_constant()](/reference/keras/latest/reference/initializer_constant.html) | Initializer that generates tensors initialized to a constant value.
-[initializer_random_normal()](/reference/keras/latest/reference/initializer_random_normal.html) | Initializer that generates tensors with a normal distribution.
-[initializer_random_uniform()](/reference/keras/latest/reference/initializer_random_uniform.html) | Initializer that generates tensors with a uniform distribution.
-[initializer_truncated_normal()](/reference/keras/latest/reference/initializer_truncated_normal.html) | Initializer that generates a truncated normal distribution.
-[initializer_variance_scaling()](/reference/keras/latest/reference/initializer_variance_scaling.html) | Initializer capable of adapting its scale to the shape of weights.
-[initializer_orthogonal()](/reference/keras/latest/reference/initializer_orthogonal.html) | Initializer that generates a random orthogonal matrix.
-[initializer_identity()](/reference/keras/latest/reference/initializer_identity.html) | Initializer that generates the identity matrix.
-[initializer_glorot_normal()](/reference/keras/latest/reference/initializer_glorot_normal.html) | Glorot normal initializer, also called Xavier normal initializer.
-[initializer_glorot_uniform()](/reference/keras/latest/reference/initializer_glorot_uniform.html) | Glorot uniform initializer, also called Xavier uniform initializer.
-[initializer_he_normal()](/reference/keras/latest/reference/initializer_he_normal.html) | He normal initializer.
-[initializer_he_uniform()](/reference/keras/latest/reference/initializer_he_uniform.html) | He uniform variance scaling initializer.
-[initializer_lecun_uniform()](/reference/keras/latest/reference/initializer_lecun_uniform.html) | LeCun uniform initializer.
-[initializer_lecun_normal()](/reference/keras/latest/reference/initializer_lecun_normal.html) | LeCun normal initializer.
+[initializer_zeros()](/reference/keras/initializer_zeros.html) | Initializer that generates tensors initialized to 0.
+[initializer_ones()](/reference/keras/initializer_ones.html) | Initializer that generates tensors initialized to 1.
+[initializer_constant()](/reference/keras/initializer_constant.html) | Initializer that generates tensors initialized to a constant value.
+[initializer_random_normal()](/reference/keras/initializer_random_normal.html) | Initializer that generates tensors with a normal distribution.
+[initializer_random_uniform()](/reference/keras/initializer_random_uniform.html) | Initializer that generates tensors with a uniform distribution.
+[initializer_truncated_normal()](/reference/keras/initializer_truncated_normal.html) | Initializer that generates a truncated normal distribution.
+[initializer_variance_scaling()](/reference/keras/initializer_variance_scaling.html) | Initializer capable of adapting its scale to the shape of weights.
+[initializer_orthogonal()](/reference/keras/initializer_orthogonal.html) | Initializer that generates a random orthogonal matrix.
+[initializer_identity()](/reference/keras/initializer_identity.html) | Initializer that generates the identity matrix.
+[initializer_glorot_normal()](/reference/keras/initializer_glorot_normal.html) | Glorot normal initializer, also called Xavier normal initializer.
+[initializer_glorot_uniform()](/reference/keras/initializer_glorot_uniform.html) | Glorot uniform initializer, also called Xavier uniform initializer.
+[initializer_he_normal()](/reference/keras/initializer_he_normal.html) | He normal initializer.
+[initializer_he_uniform()](/reference/keras/initializer_he_uniform.html) | He uniform variance scaling initializer.
+[initializer_lecun_uniform()](/reference/keras/initializer_lecun_uniform.html) | LeCun uniform initializer.
+[initializer_lecun_normal()](/reference/keras/initializer_lecun_normal.html) | LeCun normal initializer.
 
 ## Constraints
 
 Function(s) | Description
 ------------- |----------------
-[constraint_maxnorm()](/reference/keras/latest/reference/constraints.html) [constraint_nonneg()](/reference/keras/latest/reference/constraints.html) [constraint_unitnorm()](/reference/keras/latest/reference/constraints.html) [constraint_minmaxnorm()](/reference/keras/latest/reference/constraints.html) | Weight constraints
+[constraint_maxnorm()](/reference/keras/constraints.html) [constraint_nonneg()](/reference/keras/latest/reference/constraints.html) [constraint_unitnorm()](/reference/keras/latest/reference/constraints.html) [constraint_minmaxnorm()](/reference/keras/latest/reference/constraints.html) | Weight constraints
 
 ## Utils
 
 Function(s) | Description
 ------------- |----------------
-[plot(<i>&lt;keras_training_history&gt;</i>)](/reference/keras/latest/reference/plot.keras_training_history.html) | Plot training history
-[plot(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/latest/reference/plot.keras.engine.training.Model.html) | Plot a Keras model
-[zip_lists()](/reference/keras/latest/reference/zip_lists.html) | zip lists
-[mark_active()](/reference/keras/latest/reference/new-classes.html) [new_metric_class()](/reference/keras/latest/reference/new-classes.html) [new_loss_class()](/reference/keras/latest/reference/new-classes.html) [new_callback_class()](/reference/keras/latest/reference/new-classes.html) [new_model_class()](/reference/keras/latest/reference/new-classes.html) [new_layer_class()](/reference/keras/latest/reference/new-classes.html) | Define new keras types
-[timeseries_generator()](/reference/keras/latest/reference/timeseries_generator.html) | Utility function for generating batches of temporal data.
-[to_categorical()](/reference/keras/latest/reference/to_categorical.html) | Converts a class vector (integers) to binary class matrix.
-[normalize()](/reference/keras/latest/reference/normalize.html) | Normalize a matrix or nd-array
-[with_custom_object_scope()](/reference/keras/latest/reference/with_custom_object_scope.html) | Provide a scope with mappings of names to custom objects
-[keras_array()](/reference/keras/latest/reference/keras_array.html) | Keras array object
-[hdf5_matrix()](/reference/keras/latest/reference/hdf5_matrix.html) | Representation of HDF5 dataset to be used instead of an R array
-[get_file()](/reference/keras/latest/reference/get_file.html) | Downloads a file from a URL if it not already in the cache.
-[install_keras()](/reference/keras/latest/reference/install_keras.html) | Install TensorFlow and Keras, including all Python dependencies
-[is_keras_available()](/reference/keras/latest/reference/is_keras_available.html) | Check if Keras is Available
-[backend()](/reference/keras/latest/reference/backend.html) | Keras backend tensor engine
-[implementation()](/reference/keras/latest/reference/implementation.html) | Keras implementation
-[use_implementation()](/reference/keras/latest/reference/use_implementation.html) [use_backend()](/reference/keras/latest/reference/use_implementation.html) | Select a Keras implementation and backend
+[plot(<i>&lt;keras_training_history&gt;</i>)](/reference/keras/plot.keras_training_history.html) | Plot training history
+[plot(<i>&lt;keras.engine.training.Model&gt;</i>)](/reference/keras/plot.keras.engine.training.Model.html) | Plot a Keras model
+[zip_lists()](/reference/keras/zip_lists.html) | zip lists
+[mark_active()](/reference/keras/new-classes.html) [new_metric_class()](/reference/keras/latest/reference/new-classes.html) [new_loss_class()](/reference/keras/latest/reference/new-classes.html) [new_callback_class()](/reference/keras/latest/reference/new-classes.html) [new_model_class()](/reference/keras/latest/reference/new-classes.html) [new_layer_class()](/reference/keras/latest/reference/new-classes.html) | Define new keras types
+[timeseries_generator()](/reference/keras/timeseries_generator.html) | Utility function for generating batches of temporal data.
+[to_categorical()](/reference/keras/to_categorical.html) | Converts a class vector (integers) to binary class matrix.
+[normalize()](/reference/keras/normalize.html) | Normalize a matrix or nd-array
+[with_custom_object_scope()](/reference/keras/with_custom_object_scope.html) | Provide a scope with mappings of names to custom objects
+[keras_array()](/reference/keras/keras_array.html) | Keras array object
+[hdf5_matrix()](/reference/keras/hdf5_matrix.html) | Representation of HDF5 dataset to be used instead of an R array
+[get_file()](/reference/keras/get_file.html) | Downloads a file from a URL if it not already in the cache.
+[install_keras()](/reference/keras/install_keras.html) | Install TensorFlow and Keras, including all Python dependencies
+[is_keras_available()](/reference/keras/is_keras_available.html) | Check if Keras is Available
+[backend()](/reference/keras/backend.html) | Keras backend tensor engine
+[implementation()](/reference/keras/implementation.html) | Keras implementation
+[use_implementation()](/reference/keras/use_implementation.html) [use_backend()](/reference/keras/latest/reference/use_implementation.html) | Select a Keras implementation and backend
 
 ## Losses
 
 Function(s) | Description
 ------------- |----------------
-[loss_binary_crossentropy()](/reference/keras/latest/reference/loss-functions.html) [loss_categorical_crossentropy()](/reference/keras/latest/reference/loss-functions.html) [loss_categorical_hinge()](/reference/keras/latest/reference/loss-functions.html) [loss_cosine_similarity()](/reference/keras/latest/reference/loss-functions.html) [loss_hinge()](/reference/keras/latest/reference/loss-functions.html) [loss_huber()](/reference/keras/latest/reference/loss-functions.html) [loss_kullback_leibler_divergence()](/reference/keras/latest/reference/loss-functions.html) [loss_kl_divergence()](/reference/keras/latest/reference/loss-functions.html) [loss_logcosh()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_absolute_error()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_absolute_percentage_error()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_squared_error()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_squared_logarithmic_error()](/reference/keras/latest/reference/loss-functions.html) [loss_poisson()](/reference/keras/latest/reference/loss-functions.html) [loss_sparse_categorical_crossentropy()](/reference/keras/latest/reference/loss-functions.html) [loss_squared_hinge()](/reference/keras/latest/reference/loss-functions.html) | Loss functions
+[loss_binary_crossentropy()](/reference/keras/loss-functions.html) [loss_categorical_crossentropy()](/reference/keras/latest/reference/loss-functions.html) [loss_categorical_hinge()](/reference/keras/latest/reference/loss-functions.html) [loss_cosine_similarity()](/reference/keras/latest/reference/loss-functions.html) [loss_hinge()](/reference/keras/latest/reference/loss-functions.html) [loss_huber()](/reference/keras/latest/reference/loss-functions.html) [loss_kullback_leibler_divergence()](/reference/keras/latest/reference/loss-functions.html) [loss_kl_divergence()](/reference/keras/latest/reference/loss-functions.html) [loss_logcosh()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_absolute_error()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_absolute_percentage_error()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_squared_error()](/reference/keras/latest/reference/loss-functions.html) [loss_mean_squared_logarithmic_error()](/reference/keras/latest/reference/loss-functions.html) [loss_poisson()](/reference/keras/latest/reference/loss-functions.html) [loss_sparse_categorical_crossentropy()](/reference/keras/latest/reference/loss-functions.html) [loss_squared_hinge()](/reference/keras/latest/reference/loss-functions.html) | Loss functions
 
 ## Metrics
 
 Function(s) | Description
 ------------- |----------------
-[custom_metric()](/reference/keras/latest/reference/custom_metric.html) | Custom metric function
+[custom_metric()](/reference/keras/custom_metric.html) | Custom metric function
 
 ## Regularizers
 
 Function(s) | Description
 ------------- |----------------
-[regularizer_l1()](/reference/keras/latest/reference/regularizer_l1.html) [regularizer_l2()](/reference/keras/latest/reference/regularizer_l1.html) [regularizer_l1_l2()](/reference/keras/latest/reference/regularizer_l1.html) | L1 and L2 regularization
+[regularizer_l1()](/reference/keras/regularizer_l1.html) [regularizer_l2()](/reference/keras/latest/reference/regularizer_l1.html) [regularizer_l1_l2()](/reference/keras/latest/reference/regularizer_l1.html) | L1 and L2 regularization
 
 ## Activations
 
 Function(s) | Description
 ------------- |----------------
-[activation_relu()](/reference/keras/latest/reference/activation_relu.html) [activation_elu()](/reference/keras/latest/reference/activation_relu.html) [activation_selu()](/reference/keras/latest/reference/activation_relu.html) [activation_hard_sigmoid()](/reference/keras/latest/reference/activation_relu.html) [activation_linear()](/reference/keras/latest/reference/activation_relu.html) [activation_sigmoid()](/reference/keras/latest/reference/activation_relu.html) [activation_softmax()](/reference/keras/latest/reference/activation_relu.html) [activation_softplus()](/reference/keras/latest/reference/activation_relu.html) [activation_softsign()](/reference/keras/latest/reference/activation_relu.html) [activation_tanh()](/reference/keras/latest/reference/activation_relu.html) [activation_exponential()](/reference/keras/latest/reference/activation_relu.html) [activation_gelu()](/reference/keras/latest/reference/activation_relu.html) [activation_swish()](/reference/keras/latest/reference/activation_relu.html) | Activation functions
+[activation_relu()](/reference/keras/activation_relu.html) [activation_elu()](/reference/keras/latest/reference/activation_relu.html) [activation_selu()](/reference/keras/latest/reference/activation_relu.html) [activation_hard_sigmoid()](/reference/keras/latest/reference/activation_relu.html) [activation_linear()](/reference/keras/latest/reference/activation_relu.html) [activation_sigmoid()](/reference/keras/latest/reference/activation_relu.html) [activation_softmax()](/reference/keras/latest/reference/activation_relu.html) [activation_softplus()](/reference/keras/latest/reference/activation_relu.html) [activation_softsign()](/reference/keras/latest/reference/activation_relu.html) [activation_tanh()](/reference/keras/latest/reference/activation_relu.html) [activation_exponential()](/reference/keras/latest/reference/activation_relu.html) [activation_gelu()](/reference/keras/latest/reference/activation_relu.html) [activation_swish()](/reference/keras/latest/reference/activation_relu.html) | Activation functions
 
 ## Backend
 
@@ -393,17 +393,17 @@ Function(s) | Description
 
 Function(s) | Description
 ------------- |----------------
-[keras](/reference/keras/latest/reference/keras.html) | Main Keras module
-[`%py_class%`](/reference/keras/latest/reference/grapes-py_class-grapes.html) | Make a python class constructor
-[`%&lt;-active%`](/reference/keras/latest/reference/grapes-set-active-grapes.html) | Make an Active Binding
+[keras](/reference/keras/keras.html) | Main Keras module
+[`%py_class%`](/reference/keras/grapes-py_class-grapes.html) | Make a python class constructor
+[`%&lt;-active%`](/reference/keras/grapes-set-active-grapes.html) | Make an Active Binding
 
 ## Deprecated
 
 Function(s) | Description
 ------------- |----------------
-[create_wrapper()](/reference/keras/latest/reference/create_wrapper.html) | (Deprecated) Create a Keras Wrapper
-[loss_cosine_proximity()](/reference/keras/latest/reference/loss_cosine_proximity.html) | (Deprecated) loss_cosine_proximity
-[layer_cudnn_gru()](/reference/keras/latest/reference/layer_cudnn_gru.html) | (Deprecated) Fast GRU implementation backed by <a href='https://developer.nvidia.com/cudnn'>CuDNN</a>.
-[layer_cudnn_lstm()](/reference/keras/latest/reference/layer_cudnn_lstm.html) | (Deprecated) Fast LSTM implementation backed by <a href='https://developer.nvidia.com/cudnn'>CuDNN</a>.
-[layer_dense_features()](/reference/keras/latest/reference/layer_dense_features.html) | Constructs a DenseFeatures.
+[create_wrapper()](/reference/keras/create_wrapper.html) | (Deprecated) Create a Keras Wrapper
+[loss_cosine_proximity()](/reference/keras/loss_cosine_proximity.html) | (Deprecated) loss_cosine_proximity
+[layer_cudnn_gru()](/reference/keras/layer_cudnn_gru.html) | (Deprecated) Fast GRU implementation backed by <a href='https://developer.nvidia.com/cudnn'>CuDNN</a>.
+[layer_cudnn_lstm()](/reference/keras/layer_cudnn_lstm.html) | (Deprecated) Fast LSTM implementation backed by <a href='https://developer.nvidia.com/cudnn'>CuDNN</a>.
+[layer_dense_features()](/reference/keras/layer_dense_features.html) | Constructs a DenseFeatures.
 

---FILE: reference/tensorflow/index.md---
@@ -2,27 +2,27 @@
 
 Function(s) | Description
 ------------- |----------------
-[install_tensorflow()](/reference/tensorflow/latest/reference/install_tensorflow.html) | Install TensorFlow and its dependencies
-[tf_config()](/reference/tensorflow/latest/reference/tf_config.html) [tf_version()](/reference/tensorflow/latest/reference/tf_config.html) | TensorFlow configuration information
+[install_tensorflow()](/reference/tensorflow/install_tensorflow.html) | Install TensorFlow and its dependencies
+[tf_config()](/reference/tensorflow/tf_config.html) [tf_version()](/reference/tensorflow/latest/reference/tf_config.html) | TensorFlow configuration information
 
 ## Configuration
 
 Function(s) | Description
 ------------- |----------------
-[parse_flags()](/reference/tensorflow/latest/reference/parse_flags.html) | Parse Configuration Flags for a TensorFlow Application
-[parse_arguments()](/reference/tensorflow/latest/reference/parse_arguments.html) | Parse Command Line Arguments
+[parse_flags()](/reference/tensorflow/parse_flags.html) | Parse Configuration Flags for a TensorFlow Application
+[parse_arguments()](/reference/tensorflow/parse_arguments.html) | Parse Command Line Arguments
 
 ## TensorFlow API
 
 Function(s) | Description
 ------------- |----------------
-[tf](/reference/tensorflow/latest/reference/tf.html) | Main TensorFlow module
-[shape()](/reference/tensorflow/latest/reference/shape.html) | Create a <code>tf.TensorShape</code> object
+[tf](/reference/tensorflow/tf.html) | Main TensorFlow module
+[shape()](/reference/tensorflow/shape.html) | Create a <code>tf.TensorShape</code> object
 
 ## Utilities
 
 Function(s) | Description
 ------------- |----------------
-[tensorboard()](/reference/tensorflow/latest/reference/tensorboard.html) | TensorBoard Visualization Tool
-[use_session_with_seed()](/reference/tensorflow/latest/reference/use_session_with_seed.html) | Use a session with a random seed
+[tensorboard()](/reference/tensorflow/tensorboard.html) | TensorBoard Visualization Tool
+[use_session_with_seed()](/reference/tensorflow/use_session_with_seed.html) | Use a session with a random seed
 

---FILE: reference/tfautograph/index.md---
@@ -2,21 +2,21 @@
 
 Function(s) | Description
 ------------- |----------------
-[autograph()](/reference/tfautograph/latest/reference/autograph.html) | Autograph R code
+[autograph()](/reference/tfautograph/autograph.html) | Autograph R code
 
 ## Options
 
 Function(s) | Description
 ------------- |----------------
-[ag_name()](/reference/tfautograph/latest/reference/ag_name.html) | Specify a tensor name
-[ag_while_opts()](/reference/tfautograph/latest/reference/ag_while_opts.html) | specify <code>tf.while_loop</code> options
+[ag_name()](/reference/tfautograph/ag_name.html) | Specify a tensor name
+[ag_while_opts()](/reference/tfautograph/ag_while_opts.html) | specify <code>tf.while_loop</code> options
 
 ## Hints
 
 Function(s) | Description
 ------------- |----------------
-[ag_if_vars()](/reference/tfautograph/latest/reference/ag_if_vars.html) | Specify <code>tf.cond()</code> output structure when autographing <code>if</code>
-[ag_loop_vars()](/reference/tfautograph/latest/reference/ag_loop_vars.html) | Specify loop variables
+[ag_if_vars()](/reference/tfautograph/ag_if_vars.html) | Specify <code>tf.cond()</code> output structure when autographing <code>if</code>
+[ag_loop_vars()](/reference/tfautograph/ag_loop_vars.html) | Specify loop variables
 
 ## Thin Convenience Wrappers
 
@@ -27,5 +27,5 @@ Function(s) | Description
 
 Function(s) | Description
 ------------- |----------------
-[view_function_graph()](/reference/tfautograph/latest/reference/view_function_graph.html) | Visualizes the generated graph
+[view_function_graph()](/reference/tfautograph/view_function_graph.html) | Visualizes the generated graph
 

---FILE: reference/tfdatasets/index.md---
@@ -2,84 +2,84 @@
 
 Function(s) | Description
 ------------- |----------------
-[text_line_dataset()](/reference/tfdatasets/latest/reference/text_line_dataset.html) | A dataset comprising lines from one or more text files.
-[tfrecord_dataset()](/reference/tfdatasets/latest/reference/tfrecord_dataset.html) | A dataset comprising records from one or more TFRecord files.
-[sql_record_spec()](/reference/tfdatasets/latest/reference/sql_dataset.html) [sql_dataset()](/reference/tfdatasets/latest/reference/sql_dataset.html) [sqlite_dataset()](/reference/tfdatasets/latest/reference/sql_dataset.html) | A dataset consisting of the results from a SQL query
-[tensors_dataset()](/reference/tfdatasets/latest/reference/tensors_dataset.html) | Creates a dataset with a single element, comprising the given tensors.
-[tensor_slices_dataset()](/reference/tfdatasets/latest/reference/tensor_slices_dataset.html) | Creates a dataset whose elements are slices of the given tensors.
-[sparse_tensor_slices_dataset()](/reference/tfdatasets/latest/reference/sparse_tensor_slices_dataset.html) | Splits each rank-N <code>tf$SparseTensor</code> in this dataset row-wise.
-[fixed_length_record_dataset()](/reference/tfdatasets/latest/reference/fixed_length_record_dataset.html) | A dataset of fixed-length records from one or more binary files.
-[file_list_dataset()](/reference/tfdatasets/latest/reference/file_list_dataset.html) | A dataset of all files matching a pattern
-[range_dataset()](/reference/tfdatasets/latest/reference/range_dataset.html) | Creates a dataset of a step-separated range of values.
-[read_files()](/reference/tfdatasets/latest/reference/read_files.html) | Read a dataset from a set of files
-[delim_record_spec()](/reference/tfdatasets/latest/reference/delim_record_spec.html) [csv_record_spec()](/reference/tfdatasets/latest/reference/delim_record_spec.html) [tsv_record_spec()](/reference/tfdatasets/latest/reference/delim_record_spec.html) | Specification for reading a record from a text file with delimited values
-[make_csv_dataset()](/reference/tfdatasets/latest/reference/make_csv_dataset.html) | Reads CSV files into a batched dataset
+[text_line_dataset()](/reference/tfdatasets/text_line_dataset.html) | A dataset comprising lines from one or more text files.
+[tfrecord_dataset()](/reference/tfdatasets/tfrecord_dataset.html) | A dataset comprising records from one or more TFRecord files.
+[sql_record_spec()](/reference/tfdatasets/sql_dataset.html) [sql_dataset()](/reference/tfdatasets/latest/reference/sql_dataset.html) [sqlite_dataset()](/reference/tfdatasets/latest/reference/sql_dataset.html) | A dataset consisting of the results from a SQL query
+[tensors_dataset()](/reference/tfdatasets/tensors_dataset.html) | Creates a dataset with a single element, comprising the given tensors.
+[tensor_slices_dataset()](/reference/tfdatasets/tensor_slices_dataset.html) | Creates a dataset whose elements are slices of the given tensors.
+[sparse_tensor_slices_dataset()](/reference/tfdatasets/sparse_tensor_slices_dataset.html) | Splits each rank-N <code>tf$SparseTensor</code> in this dataset row-wise.
+[fixed_length_record_dataset()](/reference/tfdatasets/fixed_length_record_dataset.html) | A dataset of fixed-length records from one or more binary files.
+[file_list_dataset()](/reference/tfdatasets/file_list_dataset.html) | A dataset of all files matching a pattern
+[range_dataset()](/reference/tfdatasets/range_dataset.html) | Creates a dataset of a step-separated range of values.
+[read_files()](/reference/tfdatasets/read_files.html) | Read a dataset from a set of files
+[delim_record_spec()](/reference/tfdatasets/delim_record_spec.html) [csv_record_spec()](/reference/tfdatasets/latest/reference/delim_record_spec.html) [tsv_record_spec()](/reference/tfdatasets/latest/reference/delim_record_spec.html) | Specification for reading a record from a text file with delimited values
+[make_csv_dataset()](/reference/tfdatasets/make_csv_dataset.html) | Reads CSV files into a batched dataset
 
 ## Transforming Datasets
 
 Function(s) | Description
 ------------- |----------------
-[dataset_map()](/reference/tfdatasets/latest/reference/dataset_map.html) | Map a function across a dataset.
-[dataset_map_and_batch()](/reference/tfdatasets/latest/reference/dataset_map_and_batch.html) | Fused implementation of dataset_map() and dataset_batch()
-[dataset_prepare()](/reference/tfdatasets/latest/reference/dataset_prepare.html) | Prepare a dataset for analysis
-[dataset_skip()](/reference/tfdatasets/latest/reference/dataset_skip.html) | Creates a dataset that skips count elements from this dataset
-[dataset_filter()](/reference/tfdatasets/latest/reference/dataset_filter.html) | Filter a dataset by a predicate
-[dataset_shard()](/reference/tfdatasets/latest/reference/dataset_shard.html) | Creates a dataset that includes only 1 / num_shards of this dataset.
-[dataset_shuffle()](/reference/tfdatasets/latest/reference/dataset_shuffle.html) | Randomly shuffles the elements of this dataset.
-[dataset_shuffle_and_repeat()](/reference/tfdatasets/latest/reference/dataset_shuffle_and_repeat.html) | Shuffles and repeats a dataset returning a new permutation for each epoch.
-[dataset_prefetch()](/reference/tfdatasets/latest/reference/dataset_prefetch.html) | Creates a Dataset that prefetches elements from this dataset.
-[dataset_batch()](/reference/tfdatasets/latest/reference/dataset_batch.html) | Combines consecutive elements of this dataset into batches.
-[dataset_repeat()](/reference/tfdatasets/latest/reference/dataset_repeat.html) | Repeats a dataset count times.
-[dataset_cache()](/reference/tfdatasets/latest/reference/dataset_cache.html) | Caches the elements in this dataset.
-[dataset_take()](/reference/tfdatasets/latest/reference/dataset_take.html) | Creates a dataset with at most count elements from this dataset
-[dataset_flat_map()](/reference/tfdatasets/latest/reference/dataset_flat_map.html) | Maps map_func across this dataset and flattens the result.
-[dataset_padded_batch()](/reference/tfdatasets/latest/reference/dataset_padded_batch.html) | Combines consecutive elements of this dataset into padded batches.
-[dataset_decode_delim()](/reference/tfdatasets/latest/reference/dataset_decode_delim.html) | Transform a dataset with delimted text lines into a dataset with named
+[dataset_map()](/reference/tfdatasets/dataset_map.html) | Map a function across a dataset.
+[dataset_map_and_batch()](/reference/tfdatasets/dataset_map_and_batch.html) | Fused implementation of dataset_map() and dataset_batch()
+[dataset_prepare()](/reference/tfdatasets/dataset_prepare.html) | Prepare a dataset for analysis
+[dataset_skip()](/reference/tfdatasets/dataset_skip.html) | Creates a dataset that skips count elements from this dataset
+[dataset_filter()](/reference/tfdatasets/dataset_filter.html) | Filter a dataset by a predicate
+[dataset_shard()](/reference/tfdatasets/dataset_shard.html) | Creates a dataset that includes only 1 / num_shards of this dataset.
+[dataset_shuffle()](/reference/tfdatasets/dataset_shuffle.html) | Randomly shuffles the elements of this dataset.
+[dataset_shuffle_and_repeat()](/reference/tfdatasets/dataset_shuffle_and_repeat.html) | Shuffles and repeats a dataset returning a new permutation for each epoch.
+[dataset_prefetch()](/reference/tfdatasets/dataset_prefetch.html) | Creates a Dataset that prefetches elements from this dataset.
+[dataset_batch()](/reference/tfdatasets/dataset_batch.html) | Combines consecutive elements of this dataset into batches.
+[dataset_repeat()](/reference/tfdatasets/dataset_repeat.html) | Repeats a dataset count times.
+[dataset_cache()](/reference/tfdatasets/dataset_cache.html) | Caches the elements in this dataset.
+[dataset_take()](/reference/tfdatasets/dataset_take.html) | Creates a dataset with at most count elements from this dataset
+[dataset_flat_map()](/reference/tfdatasets/dataset_flat_map.html) | Maps map_func across this dataset and flattens the result.
+[dataset_padded_batch()](/reference/tfdatasets/dataset_padded_batch.html) | Combines consecutive elements of this dataset into padded batches.
+[dataset_decode_delim()](/reference/tfdatasets/dataset_decode_delim.html) | Transform a dataset with delimted text lines into a dataset with named
 columns
-[dataset_concatenate()](/reference/tfdatasets/latest/reference/dataset_concatenate.html) | Creates a dataset by concatenating given dataset with this dataset.
-[dataset_interleave()](/reference/tfdatasets/latest/reference/dataset_interleave.html) | Maps map_func across this dataset, and interleaves the results
-[dataset_prefetch_to_device()](/reference/tfdatasets/latest/reference/dataset_prefetch_to_device.html) | A transformation that prefetches dataset values to the given <code>device</code>
-[dataset_window()](/reference/tfdatasets/latest/reference/dataset_window.html) | Combines input elements into a dataset of windows.
-[dataset_collect()](/reference/tfdatasets/latest/reference/dataset_collect.html) | Collects a dataset
-[zip_datasets()](/reference/tfdatasets/latest/reference/zip_datasets.html) | Creates a dataset by zipping together the given datasets.
-[sample_from_datasets()](/reference/tfdatasets/latest/reference/sample_from_datasets.html) | Samples elements at random from the datasets in <code>datasets</code>.
-[with_dataset()](/reference/tfdatasets/latest/reference/with_dataset.html) | Execute code that traverses a dataset
+[dataset_concatenate()](/reference/tfdatasets/dataset_concatenate.html) | Creates a dataset by concatenating given dataset with this dataset.
+[dataset_interleave()](/reference/tfdatasets/dataset_interleave.html) | Maps map_func across this dataset, and interleaves the results
+[dataset_prefetch_to_device()](/reference/tfdatasets/dataset_prefetch_to_device.html) | A transformation that prefetches dataset values to the given <code>device</code>
+[dataset_window()](/reference/tfdatasets/dataset_window.html) | Combines input elements into a dataset of windows.
+[dataset_collect()](/reference/tfdatasets/dataset_collect.html) | Collects a dataset
+[zip_datasets()](/reference/tfdatasets/zip_datasets.html) | Creates a dataset by zipping together the given datasets.
+[sample_from_datasets()](/reference/tfdatasets/sample_from_datasets.html) | Samples elements at random from the datasets in <code>datasets</code>.
+[with_dataset()](/reference/tfdatasets/with_dataset.html) | Execute code that traverses a dataset
 
 ## Dataset Properites
 
 Function(s) | Description
 ------------- |----------------
-[output_types()](/reference/tfdatasets/latest/reference/output_types.html) [output_shapes()](/reference/tfdatasets/latest/reference/output_types.html) | Output types and shapes
+[output_types()](/reference/tfdatasets/output_types.html) [output_shapes()](/reference/tfdatasets/latest/reference/output_types.html) | Output types and shapes
 
 ## Dataset Iterators
 
 Function(s) | Description
 ------------- |----------------
-[input_fn.tf_dataset()](/reference/tfdatasets/latest/reference/input_fn.html) | Construct a tfestimators input function from a dataset
-[make_iterator_one_shot()](/reference/tfdatasets/latest/reference/make-iterator.html) [make_iterator_initializable()](/reference/tfdatasets/latest/reference/make-iterator.html) [make_iterator_from_structure()](/reference/tfdatasets/latest/reference/make-iterator.html) [make_iterator_from_string_handle()](/reference/tfdatasets/latest/reference/make-iterator.html) | Creates an iterator for enumerating the elements of this dataset.
-[iterator_get_next()](/reference/tfdatasets/latest/reference/iterator_get_next.html) | Get next element from iterator
-[iterator_initializer()](/reference/tfdatasets/latest/reference/iterator_initializer.html) | An operation that should be run to initialize this iterator.
-[iterator_string_handle()](/reference/tfdatasets/latest/reference/iterator_string_handle.html) | String-valued tensor that represents this iterator
-[iterator_make_initializer()](/reference/tfdatasets/latest/reference/iterator_make_initializer.html) | Create an operation that can be run to initialize this iterator
-[until_out_of_range()](/reference/tfdatasets/latest/reference/until_out_of_range.html) [out_of_range_handler()](/reference/tfdatasets/latest/reference/until_out_of_range.html) | Execute code that traverses a dataset until an out of range condition occurs
-[next_batch()](/reference/tfdatasets/latest/reference/next_batch.html) | Tensor(s) for retrieving the next batch from a dataset
+[input_fn.tf_dataset()](/reference/tfdatasets/input_fn.html) | Construct a tfestimators input function from a dataset
+[make_iterator_one_shot()](/reference/tfdatasets/make-iterator.html) [make_iterator_initializable()](/reference/tfdatasets/latest/reference/make-iterator.html) [make_iterator_from_structure()](/reference/tfdatasets/latest/reference/make-iterator.html) [make_iterator_from_string_handle()](/reference/tfdatasets/latest/reference/make-iterator.html) | Creates an iterator for enumerating the elements of this dataset.
+[iterator_get_next()](/reference/tfdatasets/iterator_get_next.html) | Get next element from iterator
+[iterator_initializer()](/reference/tfdatasets/iterator_initializer.html) | An operation that should be run to initialize this iterator.
+[iterator_string_handle()](/reference/tfdatasets/iterator_string_handle.html) | String-valued tensor that represents this iterator
+[iterator_make_initializer()](/reference/tfdatasets/iterator_make_initializer.html) | Create an operation that can be run to initialize this iterator
+[until_out_of_range()](/reference/tfdatasets/until_out_of_range.html) [out_of_range_handler()](/reference/tfdatasets/latest/reference/until_out_of_range.html) | Execute code that traverses a dataset until an out of range condition occurs
+[next_batch()](/reference/tfdatasets/next_batch.html) | Tensor(s) for retrieving the next batch from a dataset
 
 ## Feature Spec API
 
 Function(s) | Description
 ------------- |----------------
-[feature_spec()](/reference/tfdatasets/latest/reference/feature_spec.html) | Creates a feature specification.
-[dense_features()](/reference/tfdatasets/latest/reference/dense_features.html) | Dense Features
-[dataset_use_spec()](/reference/tfdatasets/latest/reference/dataset_use_spec.html) | Transform the dataset using the provided spec.
-[fit(<i>&lt;FeatureSpec&gt;</i>)](/reference/tfdatasets/latest/reference/fit.FeatureSpec.html) | Fits a feature specification.
-[scaler_min_max()](/reference/tfdatasets/latest/reference/scaler_min_max.html) | Creates an instance of a min max scaler
-[scaler_standard()](/reference/tfdatasets/latest/reference/scaler_standard.html) | Creates an instance of a standard scaler
-[cur_info_env](/reference/tfdatasets/latest/reference/selectors.html) | Selectors
-[layer_input_from_dataset()](/reference/tfdatasets/latest/reference/layer_input_from_dataset.html) | Creates a list of inputs from a dataset
+[feature_spec()](/reference/tfdatasets/feature_spec.html) | Creates a feature specification.
+[dense_features()](/reference/tfdatasets/dense_features.html) | Dense Features
+[dataset_use_spec()](/reference/tfdatasets/dataset_use_spec.html) | Transform the dataset using the provided spec.
+[fit(<i>&lt;FeatureSpec&gt;</i>)](/reference/tfdatasets/fit.FeatureSpec.html) | Fits a feature specification.
+[scaler_min_max()](/reference/tfdatasets/scaler_min_max.html) | Creates an instance of a min max scaler
+[scaler_standard()](/reference/tfdatasets/scaler_standard.html) | Creates an instance of a standard scaler
+[cur_info_env](/reference/tfdatasets/selectors.html) | Selectors
+[layer_input_from_dataset()](/reference/tfdatasets/layer_input_from_dataset.html) | Creates a list of inputs from a dataset
 
 ## Data
 
 Function(s) | Description
 ------------- |----------------
-[hearts](/reference/tfdatasets/latest/reference/hearts.html) | Heart Disease Data Set
+[hearts](/reference/tfdatasets/hearts.html) | Heart Disease Data Set
 

---FILE: reference/tfhub/index.md---
@@ -2,14 +2,14 @@
 
 Function(s) | Description
 ------------- |----------------
-[bake.step_pretrained_text_embedding()](/reference/tfhub/latest/reference/bake.step_pretrained_text_embedding.html) | Bake method for step_pretrained_text_embedding
-[hub_image_embedding_column()](/reference/tfhub/latest/reference/hub_image_embedding_column.html) | Module to construct a dense 1-D representation from the pixels of images.
-[hub_load()](/reference/tfhub/latest/reference/hub_load.html) | Hub Load
-[hub_sparse_text_embedding_column()](/reference/tfhub/latest/reference/hub_sparse_text_embedding_column.html) | Module to construct dense representations from sparse text features.
-[hub_text_embedding_column()](/reference/tfhub/latest/reference/hub_text_embedding_column.html) | Module to construct a dense representation from a text feature.
-[install_tfhub()](/reference/tfhub/latest/reference/install_tfhub.html) | Install TensorFlow Hub
-[layer_hub()](/reference/tfhub/latest/reference/layer_hub.html) | Hub Layer
-[`%&gt;%`](/reference/tfhub/latest/reference/pipe.html) | Pipe operator
-[prep.step_pretrained_text_embedding()](/reference/tfhub/latest/reference/prep.step_pretrained_text_embedding.html) | Prep method for step_pretrained_text_embedding
-[step_pretrained_text_embedding()](/reference/tfhub/latest/reference/step_pretrained_text_embedding.html) | Pretrained text-embeddings
+[bake.step_pretrained_text_embedding()](/reference/tfhub/bake.step_pretrained_text_embedding.html) | Bake method for step_pretrained_text_embedding
+[hub_image_embedding_column()](/reference/tfhub/hub_image_embedding_column.html) | Module to construct a dense 1-D representation from the pixels of images.
+[hub_load()](/reference/tfhub/hub_load.html) | Hub Load
+[hub_sparse_text_embedding_column()](/reference/tfhub/hub_sparse_text_embedding_column.html) | Module to construct dense representations from sparse text features.
+[hub_text_embedding_column()](/reference/tfhub/hub_text_embedding_column.html) | Module to construct a dense representation from a text feature.
+[install_tfhub()](/reference/tfhub/install_tfhub.html) | Install TensorFlow Hub
+[layer_hub()](/reference/tfhub/layer_hub.html) | Hub Layer
+[`%&gt;%`](/reference/tfhub/pipe.html) | Pipe operator
+[prep.step_pretrained_text_embedding()](/reference/tfhub/prep.step_pretrained_text_embedding.html) | Prep method for step_pretrained_text_embedding
+[step_pretrained_text_embedding()](/reference/tfhub/step_pretrained_text_embedding.html) | Pretrained text-embeddings
 "
rstudio,tensorflow.rstudio.com,079fdeefa2a9175cf14c16f440d63520ddd6af4e,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-15T09:16:41Z,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-15T09:16:41Z,fix reference builds,_quarto.yml;_utils/build-refs.R,False,True,True,False,28,6,34,"---FILE: _quarto.yml---
@@ -6,7 +6,7 @@ project:
     - ""!v1/**""
     - ""examples/**""
     - ""!examples/_check/**""
-    - ""packages/**/**.md""
+    - ""reference/**/**.md""
 
   # pre-render: spin-examples.R
   preview:
@@ -40,15 +40,15 @@ website:
       - text: Reference
         menu:
           - text: ""tensorlow""
-            href: ""packages/tensorflow/latest/reference/index.html""
+            href: ""reference/tensorflow/index.html""
           - text: ""keras""
-            href: ""packages/keras/latest/reference/index.html""
+            href: ""reference/keras/index.html""
           - text: ""tfdatasets""
-            href: ""packages/tfdatasets/latest/reference/index.html""
+            href: ""reference/tfdatasets/index.html""
           - text: ""tfautograph""
-            href: ""packages/tfautograph/latest/reference/index.html""
+            href: ""reference/tfautograph/index.html""
           - text: ""tfhub""
-            href: ""packages/tfhub/latest/reference/index.html""
+            href: ""reference/tfhub/index.html""
 
   sidebar:
     - title: ""Install""

---FILE: _utils/build-refs.R---
@@ -0,0 +1,22 @@
+#!/usr/bin/env Rscript
+
+if (!requireNamespace(""ecodown"", quietly = TRUE))
+  remotes::install_github(""edgararuiz/ecodown"")
+
+
+library(glue)
+library(fs)
+
+dir_delete(""reference"")
+dir_create(""reference"")
+
+try(ecodown::ecodown_build(verbosity = ""verbose""))
+
+dir_ls(""reference/"") %>%
+  fs::path_file() %>%
+  lapply(function(pkg) {
+    file_move(glue(""reference/{pkg}/latest/reference""),
+              glue(""reference/_{pkg}""))
+    dir_delete(glue(""reference/{pkg}""))
+    file_move(glue(""reference/_{pkg}""), glue(""reference/{pkg}""))
+  })"
rstudio,tensorflow.rstudio.com,b10321bc71f2f396808a342c24fd16ef284e8d68,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-14T14:58:28Z,Tomasz Kalinowski,kalinowskit@gmail.com,2022-07-14T14:58:28Z,fix render,_quarto.yml;_utils/convert.R;_utils/install-dev-r-tensorflow.R,False,True,True,False,4,4,8,"---FILE: _quarto.yml---
@@ -60,11 +60,12 @@ website:
       style: ""docked""
       contents:
         - install/index.qmd
+        - install/custom.qmd
         - section: GPU
+          href: install/gpu/index.qmd
           contents:
-            - install/gpu/local_gpu/index.qmd
+            - install/gpu/local_gpu.qmd
             - install/gpu/cloud_server_gpu/index.qmd
-            - install/gpu/cloud_desktop_gpu/index.qmd
 
     - title: ""Tutorials""
       contents:

---FILE: _utils/install-dev-r-tensorflow.R---
@@ -12,9 +12,8 @@ envname <- ""r-tensorflow-site""
 
 if(""--fresh"" %in% commandArgs(TRUE)) {
   reticulate:::rm_all_reticulate_state()
-  # reticulate::miniconda_uninstall()
   # unlink(""~/.pyenv"", recursive = TRUE)
-  # unlink(file.path(reticulate::virtualenv_root(), envname), recursive = TRUE)
+  unlink(file.path(reticulate::virtualenv_root(), envname), recursive = TRUE)
 }
 
 python <- reticulate::install_python(force = ""--fresh"" %in% commandArgs(TRUE))"
rstudio,tensorflow.rstudio.com,100a9ccafcb1c4b159b211985ba3708dd1fa36c7,Daniel Falbel,dfalbel@gmail.com,2022-06-24T16:47:44Z,Daniel Falbel,dfalbel@gmail.com,2022-06-24T16:47:44Z,add regression ttutorial,_freeze/tutorials/keras/regression/execute-results/html.json;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-16-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-42-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-48-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-60-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-72-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-76-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-84-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-90-1.png;_freeze/tutorials/keras/regression/figure-html/unnamed-chunk-92-1.png;_quarto.yml;_site/search.json;_site/sitemap.xml;_site/tutorials/index.html;_site/tutorials/keras/classification.html;_site/tutorials/keras/regression.html;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-16-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-42-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-48-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-60-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-72-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-76-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-84-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-90-1.png;_site/tutorials/keras/regression_files/figure-html/unnamed-chunk-92-1.png;_site/tutorials/keras/text_classification.html;_site/tutorials/keras/text_classification_with_hub.html;_site/tutorials/quickstart/advanced.html;_site/tutorials/quickstart/beginner.html;tutorials/keras/.gitignore;tutorials/keras/img/.gitignore;tutorials/keras/regression.qmd,True,False,True,False,2121,123,2244,"---FILE: _freeze/tutorials/keras/regression/execute-results/html.json---
@@ -0,0 +1,16 @@
+{
+  ""hash"": ""b161ca261b570fafaca45bae4525c0f1"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: Regression\n---\n\n\nIn a *regression* problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a *classification* problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n\nThis tutorial uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.\n\nThis example uses the Keras API. (Visit the Keras [tutorials](../keras) and [guides](../../guides/keras) to learn more.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tibble' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────── tidymodels 0.1.4 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        0.7.12         ✔ rsample      0.1.0     \n✔ dials        0.0.10         ✔ tune         0.1.6.9000\n✔ infer        1.0.0          ✔ workflows    0.2.3     \n✔ modeldata    0.1.1          ✔ workflowsets 0.1.0     \n✔ parsnip      0.2.1          ✔ yardstick    0.0.8     \n✔ recipes      0.2.0          \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'broom' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'scales' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'parsnip' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'recipes' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ yardstick::get_weights() masks keras::get_weights()\n✖ dplyr::lag()             masks stats::lag()\n✖ yardstick::spec()        masks readr::spec()\n✖ recipes::step()          masks stats::step()\n✖ tune::tune()             masks parsnip::tune()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n:::\n:::\n\n\n## The Auto MPG dataset\n\nThe dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n\n### Get the data\n\nFirst download and import the dataset using pandas:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\""\ncol_names <- c(\""mpg\"",\""cylinders\"",\""displacement\"",\""horsepower\"",\""weight\"",\""acceleration\"",\""model_year\"", \""origin\"",\""car_name\"")\n\nraw_dataset <- read.table(\n  url, \n  header = T, \n  col.names = col_names, \n  na.strings = \""?\""\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- raw_dataset %>% select(-car_name)\ntail(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    mpg cylinders displacement horsepower weight acceleration model_year\n392  27         4          151         90   2950         17.3         82\n393  27         4          140         86   2790         15.6         82\n394  44         4           97         52   2130         24.6         82\n395  32         4          135         84   2295         11.6         82\n396  28         4          120         79   2625         18.6         82\n397  31         4          119         82   2720         19.4         82\n    origin\n392      1\n393      1\n394      2\n395      1\n396      1\n397      1\n```\n:::\n:::\n\n\n### Clean the data\n\nThe dataset contains a few unknown values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlapply(dataset, function(x) sum(is.na(x))) %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 8\n $ mpg         : int 0\n $ cylinders   : int 0\n $ displacement: int 0\n $ horsepower  : int 6\n $ weight      : int 0\n $ acceleration: int 0\n $ model_year  : int 0\n $ origin      : int 0\n```\n:::\n:::\n\n\nDrop those rows to keep this initial tutorial simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- na.omit(dataset)\n```\n:::\n\n\nThe `\""origin\""` column is categorical, not numeric. So the next step is to one-hot encode the values in the column with the `recipes` package. \n\nNote: You can set up the `keras_model()` to do this kind of transformation for you but that's beyond the scope of this tutorial. Check out the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) or [Load CSV data](../load_data/csv.qmd) tutorials for examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\ndataset <- recipe(mpg ~ ., dataset) %>% \n  step_num2factor(origin, levels = c(\""USA\"", \""Europe\"", \""Japan\"")) %>% \n  step_dummy(origin, one_hot = TRUE) %>% \n  prep() %>% \n  bake(new_data = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 391\nColumns: 10\n$ cylinders     <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4…\n$ displacement  <dbl> 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…\n$ horsepower    <dbl> 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…\n$ weight        <dbl> 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 3850…\n$ acceleration  <dbl> 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, 1…\n$ model_year    <int> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ mpg           <dbl> 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 14, …\n$ origin_USA    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0…\n$ origin_Europe <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ origin_Japan  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…\n```\n:::\n:::\n\n\n### Split the data into training and test sets\n\nNow, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- initial_split(dataset, 0.8)\ntrain_dataset <- training(split)\ntest_dataset <- testing(split)\n```\n:::\n\n\n### Inspect the data\n\nReview the joint distribution of a few pairs of columns from the training set.\n\nThe top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_dataset %>% \n  select(mpg, cylinders, displacement, weight) %>% \n  GGally::ggpairs()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nLet's also check the overall statistics. Note how each feature covers a very different range:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(train_dataset)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |train_dataset |\n|Number of rows           |312           |\n|Number of columns        |10            |\n|_______________________  |              |\n|Column type frequency:   |              |\n|numeric                  |10            |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|     sd|   p0|     p25|     p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|-------:|------:|----:|-------:|-------:|------:|------:|:-----|\n|cylinders     |         0|             1|    5.50|   1.72|    3|    4.00|    4.00|    8.0|    8.0|▇▁▃▁▅ |\n|displacement  |         0|             1|  196.99| 105.81|   68|  105.00|  151.00|  302.0|  455.0|▇▂▂▃▁ |\n|horsepower    |         0|             1|  104.93|  38.85|   46|   76.75|   92.00|  126.0|  230.0|▆▇▃▂▁ |\n|weight        |         0|             1| 2995.04| 844.26| 1613| 2232.50| 2849.00| 3622.5| 4997.0|▇▇▆▅▂ |\n|acceleration  |         0|             1|   15.57|   2.83|    8|   13.88|   15.50|   17.2|   24.8|▁▆▇▂▁ |\n|model_year    |         0|             1|   76.06|   3.71|   70|   73.00|   76.00|   79.0|   82.0|▇▆▇▆▇ |\n|mpg           |         0|             1|   23.48|   7.91|    9|   17.00|   22.15|   29.0|   46.6|▆▇▆▃▁ |\n|origin_USA    |         0|             1|    0.62|   0.48|    0|    0.00|    1.00|    1.0|    1.0|▅▁▁▁▇ |\n|origin_Europe |         0|             1|    0.17|   0.38|    0|    0.00|    0.00|    0.0|    1.0|▇▁▁▁▂ |\n|origin_Japan  |         0|             1|    0.21|   0.40|    0|    0.00|    0.00|    0.0|    1.0|▇▁▁▁▂ |\n:::\n:::\n\n\n### Split features from labels\n\nSeparate the target value—the \""label\""—from the features. This label is the value that you will train the model to predict.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_features <- train_dataset %>% select(-mpg)\ntest_features <- test_dataset %>% select(-mpg)\n\ntrain_labels <- train_dataset %>% select(mpg)\ntest_labels <- test_dataset %>% select(mpg)\n```\n:::\n\n\n## Normalization\n\nIn the table of statistics it's easy to see how different the ranges of each feature are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_skim <- skimr::skim_with(numeric = skimr::sfl(mean, sd))\ntrain_dataset %>% \n  select(where(~is.numeric(.x))) %>% \n  pivot_longer(\n    cols = everything(), names_to = \""variable\"", values_to = \""values\"") %>% \n  group_by(variable) %>% \n  summarise(mean = mean(values), sd = sd(values))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n   variable          mean      sd\n   <chr>            <dbl>   <dbl>\n 1 acceleration    15.6     2.83 \n 2 cylinders        5.5     1.72 \n 3 displacement   197.    106.   \n 4 horsepower     105.     38.9  \n 5 model_year      76.1     3.71 \n 6 mpg             23.5     7.91 \n 7 origin_Europe    0.170   0.376\n 8 origin_Japan     0.205   0.404\n 9 origin_USA       0.625   0.485\n10 weight        2995.    844.   \n```\n:::\n:::\n\n\nIt is good practice to normalize features that use different scales and ranges.\n\nOne reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n\nAlthough a model *might* converge without feature normalization, normalization makes training much more stable.\n\nNote: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) tutorial.\n\n### The Normalization layer\n\nThe `layer_normalization()` is a clean and simple way to add feature normalization into your model.\n\nThe first step is to create the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer <- layer_normalization(axis = -1L)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n:::\n\n\nThen, fit the state of the preprocessing layer to the data by calling `adapt()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer %>% adapt(as.matrix(train_features))\n```\n:::\n\n\nCalculate the mean and variance, and store them in the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(normalizer$mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[5.5000000e+00 1.9699361e+02 1.0493270e+02 2.9950388e+03 1.5573077e+01\n  7.6060890e+01 6.2500000e-01 1.6987181e-01 2.0512822e-01]], shape=(1, 9), dtype=float32)\n```\n:::\n:::\n\n\nWhen the layer is called, it returns the input data, with each feature independently normalized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst <- as.matrix(train_features[1,])\n\ncat('First example:', first)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst example: 8 318 150 4077 14 72 1 0 0\n```\n:::\n\n```{.r .cell-code}\ncat('Normalized:', as.matrix(normalizer(first)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNormalized: 1.452718 1.145492 1.161751 1.283603 -0.5567052 -1.095416 0.7745966 -0.452364 -0.5080005\n```\n:::\n:::\n\n\n## Linear regression\n\nBefore building a deep neural network model, start with linear regression using one and several variables.\n\n### Linear regression with one variable\n\nBegin with a single-variable linear regression to predict `'mpg'` from `'horsepower'`.\n\nTraining a model with Keras typically starts by defining the model architecture. Use a Sequential model, which [represents a sequence of steps](https://www.tensorflow.org/guide/keras/sequential_model).\n\nThere are two steps in your single-variable linear regression model:\n\n- Normalize the `'horsepower'` input features using the `normalization` preprocessing layer.\n- Apply a linear transformation ($y = mx+b$) to produce 1 output using a linear layer (`dense`).\n\nThe number of _inputs_ can either be set by the `input_shape` argument, or automatically when the model is run for the first time.\n\nFirst, create a matrix made of the `'horsepower'` features. Then, instantiate the `layer_normalization` and fit its state to the `horsepower` data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower <- matrix(train_features$horsepower)\nhorsepower_normalizer <- layer_normalization(input_shape = shape(1), axis = NULL)\nhorsepower_normalizer %>% adapt(horsepower)\n```\n:::\n\n\nBuild the Keras Sequential model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model <- keras_model_sequential() %>% \n  horsepower_normalizer() %>% \n  layer_dense(units = 1)\n\nsummary(horsepower_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense (Dense)               (None, 1)                 2         Y          \n============================================================================\nTotal params: 5\nTrainable params: 2\nNon-trainable params: 3\n____________________________________________________________________________\n```\n:::\n:::\n\n\nThis model will predict `'mpg'` from `'horsepower'`.\n\nRun the untrained model on the first 10 'horsepower' values. The output won't be good, but notice that it has the expected shape of `(10, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(horsepower_model, horsepower[1:10,])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1]\n [1,]  0.918656170\n [2,]  0.918656170\n [3,] -0.182085037\n [4,] -0.222853214\n [5,] -0.610151052\n [6,] -0.161700934\n [7,]  0.001371827\n [8,]  0.103292309\n [9,]  0.103292309\n[10,] -0.447078258\n```\n:::\n:::\n\n\nOnce the model is built, configure the training procedure using the Keras `compile()` method. The most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized (`mean_absolute_error`) and how (using the `optimizer_adam`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n\nUse Keras `fit()` to execute the training for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nVisualize the model's training progress using the stats stored in the `history` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results <- list()\ntest_results[[\""horsepower_model\""]] <- horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels), \n  verbose = 0\n)\n```\n:::\n\n\nSince this is a single variable regression, it's easy to view the model's predictions as a function of the input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 250, length.out = 251)\ny <- predict(horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\n### Linear regression with multiple inputs\n\n\nYou can use an almost identical setup to make predictions based on multiple inputs. This model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.\n\nCreate a two-step Keras Sequential model again with the first layer being `normalizer` (`layer_normalization(axis = -1)`) you defined earlier and adapted to the whole dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- keras_model_sequential() %>% \n  normalizer() %>% \n  layer_dense(units = 1)\n```\n:::\n\n\nWhen you call `predict()` on a batch of inputs, it produces `units = 1` outputs for each example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(linear_model, as.matrix(train_features[1:10, ]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n [1,] -2.9093571\n [2,] -3.0792081\n [3,]  1.2285657\n [4,]  1.3383932\n [5,]  2.0067687\n [6,]  0.9256226\n [7,] -0.9580150\n [8,] -0.7418302\n [9,] -0.5034288\n[10,]  0.2313302\n```\n:::\n:::\n\n\nWhen you call the model, its weight matrices will be built—check that the `kernel` weights (the $m$ in $y = mx+b$) have a shape of `(9, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model$layers[[2]]$kernel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'dense_1/kernel:0' shape=(9, 1) dtype=float32, numpy=\narray([[ 0.09463781],\n       [-0.04923761],\n       [-0.73521256],\n       [-0.61125565],\n       [ 0.66817605],\n       [ 0.3159597 ],\n       [-0.6834484 ],\n       [ 0.23534286],\n       [-0.00442344]], dtype=float32)>\n```\n:::\n:::\n\n\nConfigure the model with Keras `compile()` and train with `fit()` for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- linear_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nUsing all the inputs in this regression model achieves a much lower training and validation error than the `horsepower_model`, which had one input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['linear_model']] <- linear_model %>% \n  evaluate(\n    as.matrix(test_features), \n    as.matrix(test_labels), \n    verbose = 0\n  )\n```\n:::\n\n\n## Regression with a deep neural network (DNN)\n\n\nIn the previous section, you implemented two linear models for single and multiple inputs.\n\nHere, you will implement single-input and multiple-input DNN models.\n\nThe code is basically the same except the model is expanded to include some \""hidden\"" non-linear layers. The name \""hidden\"" here just means not directly connected to the inputs or outputs.\n\nThese models will contain a few more layers than the linear model:\n\n* The normalization layer, as before (with `horsepower_normalizer` for a single-input model and `normalizer` for a multiple-input model).\n* Two hidden, non-linear, `Dense` layers with the ReLU (`relu`) activation function nonlinearity.\n* A linear `Dense` single-output layer.\n\nBoth models will use the same training procedure so the `compile` method is included in the `build_and_compile_model` function below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuild_and_compile_model <- function(norm) {\n  model <- keras_model_sequential() %>% \n    norm() %>% \n    layer_dense(64, activation = 'relu') %>% \n    layer_dense(64, activation = 'relu') %>% \n    layer_dense(1)\n\n  model %>% compile(\n    loss = 'mean_absolute_error',\n    optimizer = optimizer_adam(0.001)\n  )\n  \n  model\n}\n```\n:::\n\n\n### Regression using a DNN and a single input\n\n\nCreate a DNN model with only `'Horsepower'` as input and `horsepower_normalizer` (defined earlier) as the normalization layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_horsepower_model <- build_and_compile_model(horsepower_normalizer)\n```\n:::\n\n\nThis model has quite a few more trainable parameters than the linear models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dnn_horsepower_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_2\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense_4 (Dense)             (None, 64)                128       Y          \n dense_3 (Dense)             (None, 64)                4160      Y          \n dense_2 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,356\nTrainable params: 4,353\nNon-trainable params: 3\n____________________________________________________________________________\n```\n:::\n:::\n\n\nTrain the model with Keras `Model$fit`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0, \n  epochs = 100\n)\n```\n:::\n\n\nThis model does slightly better than the linear single-input `horsepower_model`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n:::\n\n\nIf you plot the predictions as a function of `'horsepower'`, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0.0, 250, length.out = 251)\ny <- predict(dnn_horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-76-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_horsepower_model']] <- dnn_horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower), \n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\n### Regression using a DNN and multiple inputs\n\n\nRepeat the previous process using all the inputs. The model's performance slightly improves on the validation dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_model <- build_and_compile_model(normalizer)\nsummary(dnn_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_3\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization (Normalizatio  (None, 9)                19        Y          \n n)                                                                         \n dense_7 (Dense)             (None, 64)                640       Y          \n dense_6 (Dense)             (None, 64)                4160      Y          \n dense_5 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,884\nTrainable params: 4,865\nNon-trainable params: 19\n____________________________________________________________________________\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0, \n  epochs = 100\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-84-1.png){width=672}\n:::\n:::\n\n\nCollect the results on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_model']] <- dnn_model %>% evaluate(\n  as.matrix(test_features), \n  as.matrix(test_labels), \n  verbose = 0\n)\n```\n:::\n\n\n## Performance\n\n\nSince all models have been trained, you can review their test set performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    horsepower_model.loss         linear_model.loss \n                 3.575989                  2.465263 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.201861                  1.801497 \n```\n:::\n:::\n\n\nThese results match the validation error observed during training.\n\n### Make predictions\n\nYou can now make predictions with the `dnn_model` on the test set using Keras `predict()` and review the loss:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(dnn_model, as.matrix(test_features))\nggplot(data.frame(pred = as.numeric(test_predictions), mpg = test_labels$mpg)) +\n  geom_point(aes(x = pred, y = mpg)) +\n  geom_abline(intercept = 0, slope = 1, color = \""blue\"")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-90-1.png){width=672}\n:::\n:::\n\n\nIt appears that the model predicts reasonably well.\n\nNow, check the error distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(test_predictions - test_labels$mpg, geom = \""density\"")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-92-1.png){width=672}\n:::\n\n```{.r .cell-code}\nerror <- test_predictions - test_labels\n```\n:::\n\n\nIf you're happy with the model, save it for later use with `Model$save`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_tf(dnn_model, 'dnn_model')\n```\n:::\n\n\nIf you reload the model, it gives identical output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- load_model_tf('dnn_model')\n\ntest_results[['reloaded']] <- reloaded %>% evaluate(\n  as.matrix(test_features), \n  as.matrix(test_labels), \n  verbose = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    horsepower_model.loss         linear_model.loss \n                 3.575989                  2.465263 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.201861                  1.801497 \n            reloaded.loss \n                 1.801497 \n```\n:::\n:::\n\n\n## Conclusion\n\nThis notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:\n\n- Mean squared error (MSE) (`loss_mean_squared_error()`) and mean absolute error (MAE) (`loss_mean_absolute_error()`) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.\n- Similarly, evaluation metrics used for regression differ from classification.\n- When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n- Overfitting is a common problem for DNN models, though it wasn't a problem for this tutorial. Visit the [Overfit and underfit](overfit_and_underfit.qmd) tutorial for more help with this.\n"",
+    ""supporting"": [
+      ""regression_files""
+    ],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: _quarto.yml---
@@ -66,6 +66,7 @@ website:
             - tutorials/keras/classification.qmd
             - tutorials/keras/text_classification.qmd
             - tutorials/keras/text_classification_with_hub.qmd
+            - tutorials/keras/regression.qmd
 
     - title: ""Guides""
       contents:

---FILE: _site/search.json---
@@ -1805,6 +1805,90 @@
     ""section"": ""Alternate Versions"",
     ""text"": ""Alternate Versions\nBy default, install_tensorflow() install the latest release version of TensorFlow. You can override this behavior by specifying the version parameter. For example:\n\ninstall_tensorflow(version = \""2.7\"")\n\nNote that you can provide a full major.minor.patch version specification, or just a major.minor specification, in which case the latest patch is automatically selected.\nYou can install the nightly build of TensorFlow (CPU or GPU version) with:\n\ninstall_tensorflow(version = \""nightly\"")      # cpu+gpu version\ninstall_tensorflow(version = \""nightly-cpu\"")  # cpu version\n\nYou can install any other build of TensorFlow by specifying a URL to a TensorFlow binary. For example:\n\ninstall_tensorflow(version = \""https://files.pythonhosted.org/packages/c2/c1/a035e377cf5a5b90eff27f096448fa5c5a90cbcf13b7eb0673df888f2c2d/tf_nightly-1.12.0.dev20180918-cp36-cp36m-manylinux1_x86_64.whl\"")""
   },
+  {
+    ""objectID"": ""tutorials/keras/classification.html"",
+    ""href"": ""tutorials/keras/classification.html"",
+    ""title"": ""Basic Image Classification"",
+    ""section"": """",
+    ""text"": ""In this guide, we will train a neural network model to classify images of clothing, like sneakers and shirts. It’s fine if you don’t understand all the details, this is a fast-paced overview of a complete Keras program with the details explained as we go.""
+  },
+  {
+    ""objectID"": ""tutorials/keras/classification.html#import-the-fashion-mnist-dataset"",
+    ""href"": ""tutorials/keras/classification.html#import-the-fashion-mnist-dataset"",
+    ""title"": ""Basic Image Classification"",
+    ""section"": ""Import the Fashion MNIST dataset"",
+    ""text"": ""Import the Fashion MNIST dataset\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\n\n\nFigure 1. Fashion-MNIST samples (by Zalando, MIT License).\n\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the “Hello, World” of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we’ll use here.\nThis guide uses Fashion MNIST for variety, and because it’s a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They’re good starting points to test and debug code.\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from Keras.\n\nfashion_mnist <- dataset_fashion_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nc(train_images, train_labels) %<-% fashion_mnist$train\nc(test_images, test_labels) %<-% fashion_mnist$test\n\nAt this point we have four arrays: The train_images and train_labels arrays are the training set — the data the model uses to learn. The model is tested against the test set: the test_images, and test_labels arrays.\nThe images each are 28 x 28 arrays, with pixel values ranging between 0 and 255. The labels are arrays of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\n\n\nDigit\nClass\n\n\n\n\n0\nT-shirt/top\n\n\n1\nTrouser\n\n\n2\nPullover\n\n\n3\nDress\n\n\n4\nCoat\n\n\n5\nSandal\n\n\n6\nShirt\n\n\n7\nSneaker\n\n\n8\nBag\n\n\n9\nAnkle boot\n\n\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, we’ll store them in a vector to use later when plotting the images.\n\nclass_names = c('T-shirt/top',\n                'Trouser',\n                'Pullover',\n                'Dress',\n                'Coat', \n                'Sandal',\n                'Shirt',\n                'Sneaker',\n                'Bag',\n                'Ankle boot')""
+  },
+  {
+    ""objectID"": ""tutorials/keras/classification.html#explore-the-data"",
+    ""href"": ""tutorials/keras/classification.html#explore-the-data"",
+    ""title"": ""Basic Image Classification"",
+    ""section"": ""Explore the data"",
+    ""text"": ""Explore the data\nLet’s explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\ndim(train_images)\n\n[1] 60000    28    28\n\n\n[1] 60000    28    28\nLikewise, there are 60,000 labels in the training set:\n\ndim(train_labels)\n\n[1] 60000\n\n\n[1] 60000\nEach label is an integer between 0 and 9:\n\ntrain_labels[1:20]\n\n [1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\n\n\n[1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\ndim(test_images)\n\n[1] 10000    28    28\n\n\n[1] 10000    28    28\nAnd the test set contains 10,000 images labels:\n\ndim(test_labels)\n\n[1] 10000\n\n\n[1] 10000""
+  },
+  {
+    ""objectID"": ""tutorials/keras/classification.html#preprocess-the-data"",
+    ""href"": ""tutorials/keras/classification.html#preprocess-the-data"",
+    ""title"": ""Basic Image Classification"",
+    ""section"": ""Preprocess the data"",
+    ""text"": ""Preprocess the data\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.1.2\n\nlibrary(ggplot2)\n\nimage_1 <- as.data.frame(train_images[1, , ])\ncolnames(image_1) <- seq_len(ncol(image_1))\nimage_1$y <- seq_len(nrow(image_1))\nimage_1 <- gather(image_1, \""x\"", \""value\"", -y)\nimage_1$x <- as.integer(image_1$x)\n\nggplot(image_1, aes(x = x, y = y, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \""white\"", high = \""black\"", na.value = NA) +\n  scale_y_reverse() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())   +\n  theme(aspect.ratio = 1) +\n  xlab(\""\"") +\n  ylab(\""\"")\n\n\n\n\nWe scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we simply divide by 255.\nIt’s important that the training set and the testing set are preprocessed in the same way:\n\ntrain_images <- train_images / 255\ntest_images <- test_images / 255\n\nDisplay the first 25 images from the training set and display the class name below each image. Verify that the data is in the correct format and we’re ready to build and train the network.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- train_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste(class_names[train_labels[i] + 1]))\n}""
+  },
+  {
+    ""objectID"": ""tutorials/keras/classification.html#build-the-model"",
+    ""href"": ""tutorials/keras/classification.html#build-the-model"",
+    ""title"": ""Basic Image Classification"",
+    ""section"": ""Build the model"",
+    ""text"": ""Build the model\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nSetup the layers\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\nMost of deep learning consists of chaining together simple layers. Most layers, like layer_dense, have parameters that are learned during training.\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_flatten(input_shape = c(28, 28)) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nThe first layer in this network, layer_flatten, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\nAfter the pixels are flattened, the network consists of a sequence of two dense layers. These are densely-connected, or fully-connected, neural layers. The first dense layer has 128 nodes (or neurons). The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes.\n\n\nCompile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model’s compile step:\n\nLoss function — This measures how accurate the model is during training. We want to minimize this function to “steer” the model in the right direction.\nOptimizer — This is how the model is updated based on the data it sees and its loss function.\nMetrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\n\nmodel %>% compile(\n  optimizer = 'adam', \n  loss = 'sparse_categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\n\n\nTrain the model\nTraining the neural network model requires the following steps:\n\nFeed the training data to the model — in this example, the train_images and train_labels arrays.\nThe model learns to associate images and labels.\nWe ask the model to make predictions about a test set — in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\n\nTo start training, call the fit method — the model is “fit” to the training data:\n\nmodel %>% fit(train_images, train_labels, epochs = 5, verbose = 2)\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data.\n\n\nEvaluate accuracy\nNext, compare how the model performs on the test dataset:\n\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\n\ncat('Test loss:', score[\""loss\""], \""\\n\"")\n\nTest loss: 0.356607 \n\ncat('Test accuracy:', score[\""acc\""], \""\\n\"")\n\nTest accuracy: NA \n\n\nIt turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data.\n\n\nMake predictions\nWith the model trained, we can use it to make predictions about some images.\n\npredictions <- model %>% predict(test_images)\n\nHere, the model has predicted the label for each image in the testing set. Let’s take a look at the first prediction:\n\npredictions[1, ]\n\n [1] 0.00000610363713 0.00000013651153 0.00000006553638 0.00000079706132\n [5] 0.00000156569411 0.01494382042438 0.00000785118391 0.03553177043796\n [9] 0.00002116547330 0.94948673248291\n\n\nA prediction is an array of 10 numbers. These describe the “confidence” of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\nwhich.max(predictions[1, ])\n\n[1] 10\n\n\nAs the labels are 0-based, this actually means a predicted label of 9 (to be found in class_names[9]). So the model is most confident that this image is an ankle boot. And we can check the test label to see this is correct:\n\ntest_labels[1]\n\n[1] 9\n\n\nLet’s plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- test_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  # subtract 1 as labels go from 0 to 9\n  predicted_label <- which.max(predictions[i, ]) - 1\n  true_label <- test_labels[i]\n  if (predicted_label == true_label) {\n    color <- '#008800' \n  } else {\n    color <- '#bb0000'\n  }\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste0(class_names[predicted_label + 1], \"" (\"",\n                      class_names[true_label + 1], \"")\""),\n        col.main = color)\n}\n\n\n\n\nFinally, use the trained model to make a prediction about a single image.\n\n# Grab an image from the test dataset\n# take care to keep the batch dimension, as this is expected by the model\nimg <- test_images[1, , , drop = FALSE]\ndim(img)\n\n[1]  1 28 28\n\n\nNow predict the image:\n\npredictions <- model %>% predict(img)\npredictions\n\n               [,1]            [,2]             [,3]           [,4]\n[1,] 0.000006103648 0.0000001365119 0.00000006553637 0.000000797062\n               [,5]       [,6]           [,7]       [,8]          [,9]\n[1,] 0.000001565694 0.01494383 0.000007851183 0.03553182 0.00002116551\n         [,10]\n[1,] 0.9494866\n\n\npredict returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n\n# subtract 1 as labels are 0-based\nprediction <- predictions[1, ] - 1\nwhich.max(prediction)\n\n[1] 10\n\n\nAnd, as before, the model predicts a label of 9.""
+  },
+  {
+    ""objectID"": ""tutorials/keras/regression.html"",
+    ""href"": ""tutorials/keras/regression.html"",
+    ""title"": ""Regression"",
+    ""section"": """",
+    ""text"": ""In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\nThis tutorial uses the classic Auto MPG dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.\nThis example uses the Keras API. (Visit the Keras tutorials and guides to learn more.)""
+  },
+  {
+    ""objectID"": ""tutorials/keras/regression.html#the-auto-mpg-dataset"",
+    ""href"": ""tutorials/keras/regression.html#the-auto-mpg-dataset"",
+    ""title"": ""Regression"",
+    ""section"": ""The Auto MPG dataset"",
+    ""text"": ""The Auto MPG dataset\nThe dataset is available from the UCI Machine Learning Repository.\n\nGet the data\nFirst download and import the dataset using pandas:\n\nurl <- \""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\""\ncol_names <- c(\""mpg\"",\""cylinders\"",\""displacement\"",\""horsepower\"",\""weight\"",\""acceleration\"",\""model_year\"", \""origin\"",\""car_name\"")\n\nraw_dataset <- read.table(\n  url, \n  header = T, \n  col.names = col_names, \n  na.strings = \""?\""\n)\n\n\ndataset <- raw_dataset %>% select(-car_name)\ntail(dataset)\n\n    mpg cylinders displacement horsepower weight acceleration model_year\n392  27         4          151         90   2950         17.3         82\n393  27         4          140         86   2790         15.6         82\n394  44         4           97         52   2130         24.6         82\n395  32         4          135         84   2295         11.6         82\n396  28         4          120         79   2625         18.6         82\n397  31         4          119         82   2720         19.4         82\n    origin\n392      1\n393      1\n394      2\n395      1\n396      1\n397      1\n\n\n\n\nClean the data\nThe dataset contains a few unknown values:\n\nlapply(dataset, function(x) sum(is.na(x))) %>% str()\n\nList of 8\n $ mpg         : int 0\n $ cylinders   : int 0\n $ displacement: int 0\n $ horsepower  : int 6\n $ weight      : int 0\n $ acceleration: int 0\n $ model_year  : int 0\n $ origin      : int 0\n\n\nDrop those rows to keep this initial tutorial simple:\n\ndataset <- na.omit(dataset)\n\nThe \""origin\"" column is categorical, not numeric. So the next step is to one-hot encode the values in the column with the recipes package.\nNote: You can set up the keras_model() to do this kind of transformation for you but that’s beyond the scope of this tutorial. Check out the Classify structured data using Keras preprocessing layers or Load CSV data tutorials for examples.\n\nlibrary(recipes)\ndataset <- recipe(mpg ~ ., dataset) %>% \n  step_num2factor(origin, levels = c(\""USA\"", \""Europe\"", \""Japan\"")) %>% \n  step_dummy(origin, one_hot = TRUE) %>% \n  prep() %>% \n  bake(new_data = NULL)\n\n\nglimpse(dataset)\n\nRows: 391\nColumns: 10\n$ cylinders     <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4…\n$ displacement  <dbl> 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…\n$ horsepower    <dbl> 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…\n$ weight        <dbl> 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 3850…\n$ acceleration  <dbl> 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, 1…\n$ model_year    <int> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …\n$ mpg           <dbl> 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 14, …\n$ origin_USA    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0…\n$ origin_Europe <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ origin_Japan  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…\n\n\n\n\nSplit the data into training and test sets\nNow, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.\n\nsplit <- initial_split(dataset, 0.8)\ntrain_dataset <- training(split)\ntest_dataset <- testing(split)\n\n\n\nInspect the data\nReview the joint distribution of a few pairs of columns from the training set.\nThe top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other.\n\ntrain_dataset %>% \n  select(mpg, cylinders, displacement, weight) %>% \n  GGally::ggpairs()\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\n\nLet’s also check the overall statistics. Note how each feature covers a very different range:\n\nskimr::skim(train_dataset)\n\n\nData summary\n\n\nName\ntrain_dataset\n\n\nNumber of rows\n312\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncylinders\n0\n1\n5.50\n1.72\n3\n4.00\n4.00\n8.0\n8.0\n▇▁▃▁▅\n\n\ndisplacement\n0\n1\n196.99\n105.81\n68\n105.00\n151.00\n302.0\n455.0\n▇▂▂▃▁\n\n\nhorsepower\n0\n1\n104.93\n38.85\n46\n76.75\n92.00\n126.0\n230.0\n▆▇▃▂▁\n\n\nweight\n0\n1\n2995.04\n844.26\n1613\n2232.50\n2849.00\n3622.5\n4997.0\n▇▇▆▅▂\n\n\nacceleration\n0\n1\n15.57\n2.83\n8\n13.88\n15.50\n17.2\n24.8\n▁▆▇▂▁\n\n\nmodel_year\n0\n1\n76.06\n3.71\n70\n73.00\n76.00\n79.0\n82.0\n▇▆▇▆▇\n\n\nmpg\n0\n1\n23.48\n7.91\n9\n17.00\n22.15\n29.0\n46.6\n▆▇▆▃▁\n\n\norigin_USA\n0\n1\n0.62\n0.48\n0\n0.00\n1.00\n1.0\n1.0\n▅▁▁▁▇\n\n\norigin_Europe\n0\n1\n0.17\n0.38\n0\n0.00\n0.00\n0.0\n1.0\n▇▁▁▁▂\n\n\norigin_Japan\n0\n1\n0.21\n0.40\n0\n0.00\n0.00\n0.0\n1.0\n▇▁▁▁▂\n\n\n\n\n\n\n\nSplit features from labels\nSeparate the target value—the “label”—from the features. This label is the value that you will train the model to predict.\n\ntrain_features <- train_dataset %>% select(-mpg)\ntest_features <- test_dataset %>% select(-mpg)\n\ntrain_labels <- train_dataset %>% select(mpg)\ntest_labels <- test_dataset %>% select(mpg)""
+  },
+  {
+    ""objectID"": ""tutorials/keras/regression.html#normalization"",
+    ""href"": ""tutorials/keras/regression.html#normalization"",
+    ""title"": ""Regression"",
+    ""section"": ""Normalization"",
+    ""text"": ""Normalization\nIn the table of statistics it’s easy to see how different the ranges of each feature are:\n\nmy_skim <- skimr::skim_with(numeric = skimr::sfl(mean, sd))\ntrain_dataset %>% \n  select(where(~is.numeric(.x))) %>% \n  pivot_longer(\n    cols = everything(), names_to = \""variable\"", values_to = \""values\"") %>% \n  group_by(variable) %>% \n  summarise(mean = mean(values), sd = sd(values))\n\n# A tibble: 10 × 3\n   variable          mean      sd\n   <chr>            <dbl>   <dbl>\n 1 acceleration    15.6     2.83 \n 2 cylinders        5.5     1.72 \n 3 displacement   197.    106.   \n 4 horsepower     105.     38.9  \n 5 model_year      76.1     3.71 \n 6 mpg             23.5     7.91 \n 7 origin_Europe    0.170   0.376\n 8 origin_Japan     0.205   0.404\n 9 origin_USA       0.625   0.485\n10 weight        2995.    844.   \n\n\nIt is good practice to normalize features that use different scales and ranges.\nOne reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\nAlthough a model might converge without feature normalization, normalization makes training much more stable.\nNote: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the Working with preprocessing layers guide and the Classify structured data using Keras preprocessing layers tutorial.\n\nThe Normalization layer\nThe layer_normalization() is a clean and simple way to add feature normalization into your model.\nThe first step is to create the layer:\n\nnormalizer <- layer_normalization(axis = -1L)\n\nLoaded Tensorflow version 2.9.1\n\n\nThen, fit the state of the preprocessing layer to the data by calling adapt():\n\nnormalizer %>% adapt(as.matrix(train_features))\n\nCalculate the mean and variance, and store them in the layer:\n\nprint(normalizer$mean)\n\ntf.Tensor(\n[[5.5000000e+00 1.9699361e+02 1.0493270e+02 2.9950388e+03 1.5573077e+01\n  7.6060890e+01 6.2500000e-01 1.6987181e-01 2.0512822e-01]], shape=(1, 9), dtype=float32)\n\n\nWhen the layer is called, it returns the input data, with each feature independently normalized.\n\nfirst <- as.matrix(train_features[1,])\n\ncat('First example:', first)\n\nFirst example: 8 318 150 4077 14 72 1 0 0\n\ncat('Normalized:', as.matrix(normalizer(first)))\n\nNormalized: 1.452718 1.145492 1.161751 1.283603 -0.5567052 -1.095416 0.7745966 -0.452364 -0.5080005""
+  },
+  {
+    ""objectID"": ""tutorials/keras/regression.html#linear-regression"",
+    ""href"": ""tutorials/keras/regression.html#linear-regression"",
+    ""title"": ""Regression"",
+    ""section"": ""Linear regression"",
+    ""text"": ""Linear regression\nBefore building a deep neural network model, start with linear regression using one and several variables.\n\nLinear regression with one variable\nBegin with a single-variable linear regression to predict 'mpg' from 'horsepower'.\nTraining a model with Keras typically starts by defining the model architecture. Use a Sequential model, which represents a sequence of steps.\nThere are two steps in your single-variable linear regression model:\n\nNormalize the 'horsepower' input features using the normalization preprocessing layer.\nApply a linear transformation (\\(y = mx+b\\)) to produce 1 output using a linear layer (dense).\n\nThe number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time.\nFirst, create a matrix made of the 'horsepower' features. Then, instantiate the layer_normalization and fit its state to the horsepower data:\n\nhorsepower <- matrix(train_features$horsepower)\nhorsepower_normalizer <- layer_normalization(input_shape = shape(1), axis = NULL)\nhorsepower_normalizer %>% adapt(horsepower)\n\nBuild the Keras Sequential model:\n\nhorsepower_model <- keras_model_sequential() %>% \n  horsepower_normalizer() %>% \n  layer_dense(units = 1)\n\nsummary(horsepower_model)\n\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense (Dense)               (None, 1)                 2         Y          \n============================================================================\nTotal params: 5\nTrainable params: 2\nNon-trainable params: 3\n____________________________________________________________________________\n\n\nThis model will predict 'mpg' from 'horsepower'.\nRun the untrained model on the first 10 ‘horsepower’ values. The output won’t be good, but notice that it has the expected shape of (10, 1):\n\npredict(horsepower_model, horsepower[1:10,])\n\n              [,1]\n [1,]  0.918656170\n [2,]  0.918656170\n [3,] -0.182085037\n [4,] -0.222853214\n [5,] -0.610151052\n [6,] -0.161700934\n [7,]  0.001371827\n [8,]  0.103292309\n [9,]  0.103292309\n[10,] -0.447078258\n\n\nOnce the model is built, configure the training procedure using the Keras compile() method. The most important arguments to compile are the loss and the optimizer, since these define what will be optimized (mean_absolute_error) and how (using the optimizer_adam).\n\nhorsepower_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n\nUse Keras fit() to execute the training for 100 epochs:\n\nhistory <- horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n\nVisualize the model’s training progress using the stats stored in the history object:\n\nplot(history)\n\n\n\n\nCollect the results on the test set for later:\n\ntest_results <- list()\ntest_results[[\""horsepower_model\""]] <- horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels), \n  verbose = 0\n)\n\nSince this is a single variable regression, it’s easy to view the model’s predictions as a function of the input:\n\nx <- seq(0, 250, length.out = 251)\ny <- predict(horsepower_model, x)\n\n\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n\n\n\n\n\n\nLinear regression with multiple inputs\nYou can use an almost identical setup to make predictions based on multiple inputs. This model still does the same \\(y = mx+b\\) except that \\(m\\) is a matrix and \\(b\\) is a vector.\nCreate a two-step Keras Sequential model again with the first layer being normalizer (layer_normalization(axis = -1)) you defined earlier and adapted to the whole dataset:\n\nlinear_model <- keras_model_sequential() %>% \n  normalizer() %>% \n  layer_dense(units = 1)\n\nWhen you call predict() on a batch of inputs, it produces units = 1 outputs for each example:\n\npredict(linear_model, as.matrix(train_features[1:10, ]))\n\n            [,1]\n [1,] -2.9093571\n [2,] -3.0792081\n [3,]  1.2285657\n [4,]  1.3383932\n [5,]  2.0067687\n [6,]  0.9256226\n [7,] -0.9580150\n [8,] -0.7418302\n [9,] -0.5034288\n[10,]  0.2313302\n\n\nWhen you call the model, its weight matrices will be built—check that the kernel weights (the \\(m\\) in \\(y = mx+b\\)) have a shape of (9, 1):\n\nlinear_model$layers[[2]]$kernel\n\n<tf.Variable 'dense_1/kernel:0' shape=(9, 1) dtype=float32, numpy=\narray([[ 0.09463781],\n       [-0.04923761],\n       [-0.73521256],\n       [-0.61125565],\n       [ 0.66817605],\n       [ 0.3159597 ],\n       [-0.6834484 ],\n       [ 0.23534286],\n       [-0.00442344]], dtype=float32)>\n\n\nConfigure the model with Keras compile() and train with fit() for 100 epochs:\n\nlinear_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n\n\nhistory <- linear_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n\nUsing all the inputs in this regression model achieves a much lower training and validation error than the horsepower_model, which had one input:\n\nplot(history)\n\n\n\n\nCollect the results on the test set for later:\n\ntest_results[['linear_model']] <- linear_model %>% \n  evaluate(\n    as.matrix(test_features), \n    as.matrix(test_labels), \n    verbose = 0\n  )""
+  },
+  {
+    ""objectID"": ""tutorials/keras/regression.html#regression-with-a-deep-neural-network-dnn"",
+    ""href"": ""tutorials/keras/regression.html#regression-with-a-deep-neural-network-dnn"",
+    ""title"": ""Regression"",
+    ""section"": ""Regression with a deep neural network (DNN)"",
+    ""text"": ""Regression with a deep neural network (DNN)\nIn the previous section, you implemented two linear models for single and multiple inputs.\nHere, you will implement single-input and multiple-input DNN models.\nThe code is basically the same except the model is expanded to include some “hidden” non-linear layers. The name “hidden” here just means not directly connected to the inputs or outputs.\nThese models will contain a few more layers than the linear model:\n\nThe normalization layer, as before (with horsepower_normalizer for a single-input model and normalizer for a multiple-input model).\nTwo hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity.\nA linear Dense single-output layer.\n\nBoth models will use the same training procedure so the compile method is included in the build_and_compile_model function below.\n\nbuild_and_compile_model <- function(norm) {\n  model <- keras_model_sequential() %>% \n    norm() %>% \n    layer_dense(64, activation = 'relu') %>% \n    layer_dense(64, activation = 'relu') %>% \n    layer_dense(1)\n\n  model %>% compile(\n    loss = 'mean_absolute_error',\n    optimizer = optimizer_adam(0.001)\n  )\n  \n  model\n}\n\n\nRegression using a DNN and a single input\nCreate a DNN model with only 'Horsepower' as input and horsepower_normalizer (defined earlier) as the normalization layer:\n\ndnn_horsepower_model <- build_and_compile_model(horsepower_normalizer)\n\nThis model has quite a few more trainable parameters than the linear models:\n\nsummary(dnn_horsepower_model)\n\nModel: \""sequential_2\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization_1 (Normalizat  (None, 1)                3         Y          \n ion)                                                                       \n dense_4 (Dense)             (None, 64)                128       Y          \n dense_3 (Dense)             (None, 64)                4160      Y          \n dense_2 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,356\nTrainable params: 4,353\nNon-trainable params: 3\n____________________________________________________________________________\n\n\nTrain the model with Keras Model$fit:\n\nhistory <- dnn_horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0, \n  epochs = 100\n)\n\nThis model does slightly better than the linear single-input horsepower_model:\n\nplot(history)\n\n\n\n\nIf you plot the predictions as a function of 'horsepower', you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:\n\nx <- seq(0.0, 250, length.out = 251)\ny <- predict(dnn_horsepower_model, x)\n\n\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \""data\"")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \""prediction\""))\n\n\n\n\nCollect the results on the test set for later:\n\ntest_results[['dnn_horsepower_model']] <- dnn_horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower), \n  as.matrix(test_labels),\n  verbose = 0\n)\n\n\n\nRegression using a DNN and multiple inputs\nRepeat the previous process using all the inputs. The model’s performance slightly improves on the validation dataset.\n\ndnn_model <- build_and_compile_model(normalizer)\nsummary(dnn_model)\n\nModel: \""sequential_3\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n normalization (Normalizatio  (None, 9)                19        Y          \n n)                                                                         \n dense_7 (Dense)             (None, 64)                640       Y          \n dense_6 (Dense)             (None, 64)                4160      Y          \n dense_5 (Dense)             (None, 1)                 65        Y          \n============================================================================\nTotal params: 4,884\nTrainable params: 4,865\nNon-trainable params: 19\n____________________________________________________________________________\n\n\n\nhistory <- dnn_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0, \n  epochs = 100\n)\n\n\nplot(history)\n\n\n\n\nCollect the results on the test set:\n\ntest_results[['dnn_model']] <- dnn_model %>% evaluate(\n  as.matrix(test_features), \n  as.matrix(test_labels), \n  verbose = 0\n)""
+  },
+  {
+    ""objectID"": ""tutorials/keras/regression.html#performance"",
+    ""href"": ""tutorials/keras/regression.html#performance"",
+    ""title"": ""Regression"",
+    ""section"": ""Performance"",
+    ""text"": ""Performance\nSince all models have been trained, you can review their test set performance:\n\nsapply(test_results, function(x) x)\n\n    horsepower_model.loss         linear_model.loss \n                 3.575989                  2.465263 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.201861                  1.801497 \n\n\nThese results match the validation error observed during training.\n\nMake predictions\nYou can now make predictions with the dnn_model on the test set using Keras predict() and review the loss:\n\ntest_predictions <- predict(dnn_model, as.matrix(test_features))\nggplot(data.frame(pred = as.numeric(test_predictions), mpg = test_labels$mpg)) +\n  geom_point(aes(x = pred, y = mpg)) +\n  geom_abline(intercept = 0, slope = 1, color = \""blue\"")\n\n\n\n\nIt appears that the model predicts reasonably well.\nNow, check the error distribution:\n\nqplot(test_predictions - test_labels$mpg, geom = \""density\"")\n\n\n\nerror <- test_predictions - test_labels\n\nIf you’re happy with the model, save it for later use with Model$save:\n\nsave_model_tf(dnn_model, 'dnn_model')\n\nIf you reload the model, it gives identical output:\n\nreloaded <- load_model_tf('dnn_model')\n\ntest_results[['reloaded']] <- reloaded %>% evaluate(\n  as.matrix(test_features), \n  as.matrix(test_labels), \n  verbose = 0\n)\n\n\nsapply(test_results, function(x) x)\n\n    horsepower_model.loss         linear_model.loss \n                 3.575989                  2.465263 \ndnn_horsepower_model.loss            dnn_model.loss \n                 3.201861                  1.801497 \n            reloaded.loss \n                 1.801497""
+  },
+  {
+    ""objectID"": ""tutorials/keras/regression.html#conclusion"",
+    ""href"": ""tutorials/keras/regression.html#conclusion"",
+    ""title"": ""Regression"",
+    ""section"": ""Conclusion"",
+    ""text"": ""Conclusion\nThis notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:\n\nMean squared error (MSE) (loss_mean_squared_error()) and mean absolute error (MAE) (loss_mean_absolute_error()) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.\nSimilarly, evaluation metrics used for regression differ from classification.\nWhen numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\nOverfitting is a common problem for DNN models, though it wasn’t a problem for this tutorial. Visit the Overfit and underfit tutorial for more help with this.""
+  },
   {
     ""objectID"": ""tutorials/keras/text_classification.html"",
     ""href"": ""tutorials/keras/text_classification.html"",
@@ -1937,40 +2021,5 @@
     ""title"": ""Beginner"",
     ""section"": ""Conclusion"",
     ""text"": ""Conclusion\nCongratulations! You have trained a machine learning model using a prebuilt dataset using the Keras API.\nFor more examples of using Keras, check out the tutorials. To learn more about building models with Keras, read the guides. If you want learn more about loading and preparing data, see the tutorials on image data loading or CSV data loading.""
-  },
-  {
-    ""objectID"": ""tutorials/keras/classification.html"",
-    ""href"": ""tutorials/keras/classification.html"",
-    ""title"": ""Basic Image Classification"",
-    ""section"": """",
-    ""text"": ""In this guide, we will train a neural network model to classify images of clothing, like sneakers and shirts. It’s fine if you don’t understand all the details, this is a fast-paced overview of a complete Keras program with the details explained as we go.""
-  },
-  {
-    ""objectID"": ""tutorials/keras/classification.html#import-the-fashion-mnist-dataset"",
-    ""href"": ""tutorials/keras/classification.html#import-the-fashion-mnist-dataset"",
-    ""title"": ""Basic Image Classification"",
-    ""section"": ""Import the Fashion MNIST dataset"",
-    ""text"": ""Import the Fashion MNIST dataset\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\n\n\nFigure 1. Fashion-MNIST samples (by Zalando, MIT License).\n\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the “Hello, World” of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we’ll use here.\nThis guide uses Fashion MNIST for variety, and because it’s a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They’re good starting points to test and debug code.\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from Keras.\n\nfashion_mnist <- dataset_fashion_mnist()\n\nLoaded Tensorflow version 2.9.1\n\nc(train_images, train_labels) %<-% fashion_mnist$train\nc(test_images, test_labels) %<-% fashion_mnist$test\n\nAt this point we have four arrays: The train_images and train_labels arrays are the training set — the data the model uses to learn. The model is tested against the test set: the test_images, and test_labels arrays.\nThe images each are 28 x 28 arrays, with pixel values ranging between 0 and 255. The labels are arrays of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\n\n\nDigit\nClass\n\n\n\n\n0\nT-shirt/top\n\n\n1\nTrouser\n\n\n2\nPullover\n\n\n3\nDress\n\n\n4\nCoat\n\n\n5\nSandal\n\n\n6\nShirt\n\n\n7\nSneaker\n\n\n8\nBag\n\n\n9\nAnkle boot\n\n\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, we’ll store them in a vector to use later when plotting the images.\n\nclass_names = c('T-shirt/top',\n                'Trouser',\n                'Pullover',\n                'Dress',\n                'Coat', \n                'Sandal',\n                'Shirt',\n                'Sneaker',\n                'Bag',\n                'Ankle boot')""
-  },
-  {
-    ""objectID"": ""tutorials/keras/classification.html#explore-the-data"",
-    ""href"": ""tutorials/keras/classification.html#explore-the-data"",
-    ""title"": ""Basic Image Classification"",
-    ""section"": ""Explore the data"",
-    ""text"": ""Explore the data\nLet’s explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\ndim(train_images)\n\n[1] 60000    28    28\n\n\n[1] 60000    28    28\nLikewise, there are 60,000 labels in the training set:\n\ndim(train_labels)\n\n[1] 60000\n\n\n[1] 60000\nEach label is an integer between 0 and 9:\n\ntrain_labels[1:20]\n\n [1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\n\n\n[1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\ndim(test_images)\n\n[1] 10000    28    28\n\n\n[1] 10000    28    28\nAnd the test set contains 10,000 images labels:\n\ndim(test_labels)\n\n[1] 10000\n\n\n[1] 10000""
-  },
-  {
-    ""objectID"": ""tutorials/keras/classification.html#preprocess-the-data"",
-    ""href"": ""tutorials/keras/classification.html#preprocess-the-data"",
-    ""title"": ""Basic Image Classification"",
-    ""section"": ""Preprocess the data"",
-    ""text"": ""Preprocess the data\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.1.2\n\nlibrary(ggplot2)\n\nimage_1 <- as.data.frame(train_images[1, , ])\ncolnames(image_1) <- seq_len(ncol(image_1))\nimage_1$y <- seq_len(nrow(image_1))\nimage_1 <- gather(image_1, \""x\"", \""value\"", -y)\nimage_1$x <- as.integer(image_1$x)\n\nggplot(image_1, aes(x = x, y = y, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \""white\"", high = \""black\"", na.value = NA) +\n  scale_y_reverse() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())   +\n  theme(aspect.ratio = 1) +\n  xlab(\""\"") +\n  ylab(\""\"")\n\n\n\n\nWe scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we simply divide by 255.\nIt’s important that the training set and the testing set are preprocessed in the same way:\n\ntrain_images <- train_images / 255\ntest_images <- test_images / 255\n\nDisplay the first 25 images from the training set and display the class name below each image. Verify that the data is in the correct format and we’re ready to build and train the network.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- train_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste(class_names[train_labels[i] + 1]))\n}""
-  },
-  {
-    ""objectID"": ""tutorials/keras/classification.html#build-the-model"",
-    ""href"": ""tutorials/keras/classification.html#build-the-model"",
-    ""title"": ""Basic Image Classification"",
-    ""section"": ""Build the model"",
-    ""text"": ""Build the model\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nSetup the layers\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\nMost of deep learning consists of chaining together simple layers. Most layers, like layer_dense, have parameters that are learned during training.\n\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_flatten(input_shape = c(28, 28)) %>%\n  layer_dense(units = 128, activation = 'relu') %>%\n  layer_dense(units = 10, activation = 'softmax')\n\nThe first layer in this network, layer_flatten, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\nAfter the pixels are flattened, the network consists of a sequence of two dense layers. These are densely-connected, or fully-connected, neural layers. The first dense layer has 128 nodes (or neurons). The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes.\n\n\nCompile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model’s compile step:\n\nLoss function — This measures how accurate the model is during training. We want to minimize this function to “steer” the model in the right direction.\nOptimizer — This is how the model is updated based on the data it sees and its loss function.\nMetrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\n\nmodel %>% compile(\n  optimizer = 'adam', \n  loss = 'sparse_categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\n\n\nTrain the model\nTraining the neural network model requires the following steps:\n\nFeed the training data to the model — in this example, the train_images and train_labels arrays.\nThe model learns to associate images and labels.\nWe ask the model to make predictions about a test set — in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\n\nTo start training, call the fit method — the model is “fit” to the training data:\n\nmodel %>% fit(train_images, train_labels, epochs = 5, verbose = 2)\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data.\n\n\nEvaluate accuracy\nNext, compare how the model performs on the test dataset:\n\nscore <- model %>% evaluate(test_images, test_labels, verbose = 0)\n\ncat('Test loss:', score[\""loss\""], \""\\n\"")\n\nTest loss: 0.356607 \n\ncat('Test accuracy:', score[\""acc\""], \""\\n\"")\n\nTest accuracy: NA \n\n\nIt turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data.\n\n\nMake predictions\nWith the model trained, we can use it to make predictions about some images.\n\npredictions <- model %>% predict(test_images)\n\nHere, the model has predicted the label for each image in the testing set. Let’s take a look at the first prediction:\n\npredictions[1, ]\n\n [1] 0.00000610363713 0.00000013651153 0.00000006553638 0.00000079706132\n [5] 0.00000156569411 0.01494382042438 0.00000785118391 0.03553177043796\n [9] 0.00002116547330 0.94948673248291\n\n\nA prediction is an array of 10 numbers. These describe the “confidence” of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\nwhich.max(predictions[1, ])\n\n[1] 10\n\n\nAs the labels are 0-based, this actually means a predicted label of 9 (to be found in class_names[9]). So the model is most confident that this image is an ankle boot. And we can check the test label to see this is correct:\n\ntest_labels[1]\n\n[1] 9\n\n\nLet’s plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25) { \n  img <- test_images[i, , ]\n  img <- t(apply(img, 2, rev)) \n  # subtract 1 as labels go from 0 to 9\n  predicted_label <- which.max(predictions[i, ]) - 1\n  true_label <- test_labels[i]\n  if (predicted_label == true_label) {\n    color <- '#008800' \n  } else {\n    color <- '#bb0000'\n  }\n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste0(class_names[predicted_label + 1], \"" (\"",\n                      class_names[true_label + 1], \"")\""),\n        col.main = color)\n}\n\n\n\n\nFinally, use the trained model to make a prediction about a single image.\n\n# Grab an image from the test dataset\n# take care to keep the batch dimension, as this is expected by the model\nimg <- test_images[1, , , drop = FALSE]\ndim(img)\n\n[1]  1 28 28\n\n\nNow predict the image:\n\npredictions <- model %>% predict(img)\npredictions\n\n               [,1]            [,2]             [,3]           [,4]\n[1,] 0.000006103648 0.0000001365119 0.00000006553637 0.000000797062\n               [,5]       [,6]           [,7]       [,8]          [,9]\n[1,] 0.000001565694 0.01494383 0.000007851183 0.03553182 0.00002116551\n         [,10]\n[1,] 0.9494866\n\n\npredict returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n\n# subtract 1 as labels are 0-based\nprediction <- predictions[1, ] - 1\nwhich.max(prediction)\n\n[1] 10\n\n\nAnd, as before, the model predicts a label of 9.""
   }
 ]
\ No newline at end of file

---FILE: _site/sitemap.xml---
@@ -2,310 +2,314 @@
 <urlset xmlns=""http://www.sitemaps.org/schemas/sitemap/0.9"">
   <url>
     <loc>https://tensorflow.rstudio.com/deploy/docker.html</loc>
-    <lastmod>2022-06-24T00:22:04.484Z</lastmod>
+    <lastmod>2022-06-24T16:46:05.472Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/deploy/index.html</loc>
-    <lastmod>2022-06-24T00:22:04.870Z</lastmod>
+    <lastmod>2022-06-24T16:43:25.532Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/deploy/plumber.html</loc>
-    <lastmod>2022-06-24T00:22:05.539Z</lastmod>
+    <lastmod>2022-06-24T16:43:26.249Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/deploy/rsconnect.html</loc>
-    <lastmod>2022-06-24T00:22:06.117Z</lastmod>
+    <lastmod>2022-06-24T16:43:26.960Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/deploy/shiny.html</loc>
-    <lastmod>2022-06-24T00:22:06.723Z</lastmod>
+    <lastmod>2022-06-24T16:43:27.636Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/addition_rnn.html</loc>
-    <lastmod>2022-06-24T00:22:07.287Z</lastmod>
+    <lastmod>2022-06-24T16:43:28.214Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/babi_memnn.html</loc>
-    <lastmod>2022-06-24T00:22:07.830Z</lastmod>
+    <lastmod>2022-06-24T16:43:28.792Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/babi_rnn.html</loc>
-    <lastmod>2022-06-24T00:22:08.570Z</lastmod>
+    <lastmod>2022-06-24T16:43:29.506Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/cifar10_cnn.html</loc>
-    <lastmod>2022-06-24T00:22:09.146Z</lastmod>
+    <lastmod>2022-06-24T16:43:30.072Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/cifar10_densenet.html</loc>
-    <lastmod>2022-06-24T00:22:09.655Z</lastmod>
+    <lastmod>2022-06-24T16:43:30.544Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/conv_lstm.html</loc>
-    <lastmod>2022-06-24T00:22:10.146Z</lastmod>
+    <lastmod>2022-06-24T16:43:31.115Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/deep_dream.html</loc>
-    <lastmod>2022-06-24T00:22:10.669Z</lastmod>
+    <lastmod>2022-06-24T16:43:31.631Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/eager_dcgan.html</loc>
-    <lastmod>2022-06-24T00:22:11.207Z</lastmod>
+    <lastmod>2022-06-24T16:43:32.211Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/eager_image_captioning.html</loc>
-    <lastmod>2022-06-24T00:22:11.857Z</lastmod>
+    <lastmod>2022-06-24T16:43:32.944Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/eager_pix2pix.html</loc>
-    <lastmod>2022-06-24T00:22:12.489Z</lastmod>
+    <lastmod>2022-06-24T16:43:33.830Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/eager_styletransfer.html</loc>
-    <lastmod>2022-06-24T00:22:13.051Z</lastmod>
+    <lastmod>2022-06-24T16:43:34.635Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/fine_tuning.html</loc>
-    <lastmod>2022-06-24T00:22:13.562Z</lastmod>
+    <lastmod>2022-06-24T16:43:35.497Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/imdb_bidirectional_lstm.html</loc>
-    <lastmod>2022-06-24T00:22:14.076Z</lastmod>
+    <lastmod>2022-06-24T16:43:36.141Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/imdb_cnn.html</loc>
-    <lastmod>2022-06-24T00:22:14.551Z</lastmod>
+    <lastmod>2022-06-24T16:43:36.709Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/imdb_cnn_lstm.html</loc>
-    <lastmod>2022-06-24T00:22:15.089Z</lastmod>
+    <lastmod>2022-06-24T16:43:37.241Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/imdb_fasttext.html</loc>
-    <lastmod>2022-06-24T00:22:15.613Z</lastmod>
+    <lastmod>2022-06-24T16:43:37.845Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/imdb_lstm.html</loc>
-    <lastmod>2022-06-24T00:22:16.133Z</lastmod>
+    <lastmod>2022-06-24T16:43:38.326Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/index.html</loc>
-    <lastmod>2022-06-24T00:22:16.714Z</lastmod>
+    <lastmod>2022-06-24T16:43:38.947Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/lstm_seq2seq.html</loc>
-    <lastmod>2022-06-24T00:22:17.290Z</lastmod>
+    <lastmod>2022-06-24T16:43:39.502Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/lstm_text_generation.html</loc>
-    <lastmod>2022-06-24T00:22:17.781Z</lastmod>
+    <lastmod>2022-06-24T16:43:40.006Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_acgan.html</loc>
-    <lastmod>2022-06-24T00:22:18.373Z</lastmod>
+    <lastmod>2022-06-24T16:43:40.554Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_antirectifier.html</loc>
-    <lastmod>2022-06-24T00:22:18.871Z</lastmod>
+    <lastmod>2022-06-24T16:43:41.029Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_cnn.html</loc>
-    <lastmod>2022-06-24T00:22:19.440Z</lastmod>
+    <lastmod>2022-06-24T16:43:41.526Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_cnn_embeddings.html</loc>
-    <lastmod>2022-06-24T00:22:19.962Z</lastmod>
+    <lastmod>2022-06-24T16:43:42.244Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_hierarchical_rnn.html</loc>
-    <lastmod>2022-06-24T00:22:20.454Z</lastmod>
+    <lastmod>2022-06-24T16:43:42.971Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_irnn.html</loc>
-    <lastmod>2022-06-24T00:22:20.966Z</lastmod>
+    <lastmod>2022-06-24T16:43:43.470Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_mlp.html</loc>
-    <lastmod>2022-06-24T00:22:21.481Z</lastmod>
+    <lastmod>2022-06-24T16:43:44.006Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_tfrecord.html</loc>
-    <lastmod>2022-06-24T00:22:21.997Z</lastmod>
+    <lastmod>2022-06-24T16:43:44.571Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/mnist_transfer_cnn.html</loc>
-    <lastmod>2022-06-24T00:22:22.486Z</lastmod>
+    <lastmod>2022-06-24T16:43:45.097Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/neural_style_transfer.html</loc>
-    <lastmod>2022-06-24T00:22:23.031Z</lastmod>
+    <lastmod>2022-06-24T16:43:45.635Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/nmt_attention.html</loc>
-    <lastmod>2022-06-24T00:22:23.621Z</lastmod>
+    <lastmod>2022-06-24T16:43:46.248Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/quora_siamese_lstm.html</loc>
-    <lastmod>2022-06-24T00:22:24.143Z</lastmod>
+    <lastmod>2022-06-24T16:43:46.801Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/reuters_mlp.html</loc>
-    <lastmod>2022-06-24T00:22:24.644Z</lastmod>
+    <lastmod>2022-06-24T16:43:47.323Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/stateful_lstm.html</loc>
-    <lastmod>2022-06-24T00:22:25.153Z</lastmod>
+    <lastmod>2022-06-24T16:43:47.837Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/text_explanation_lime.html</loc>
-    <lastmod>2022-06-24T00:22:25.691Z</lastmod>
+    <lastmod>2022-06-24T16:43:48.346Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/tfprob_vae.html</loc>
-    <lastmod>2022-06-24T00:22:26.305Z</lastmod>
+    <lastmod>2022-06-24T16:43:48.903Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/variational_autoencoder.html</loc>
-    <lastmod>2022-06-24T00:22:26.780Z</lastmod>
+    <lastmod>2022-06-24T16:43:49.400Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/variational_autoencoder_deconv.html</loc>
-    <lastmod>2022-06-24T00:22:27.445Z</lastmod>
+    <lastmod>2022-06-24T16:43:50.013Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/examples/vq_vae.html</loc>
-    <lastmod>2022-06-24T00:22:28.087Z</lastmod>
+    <lastmod>2022-06-24T16:43:50.640Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/index.html</loc>
-    <lastmod>2022-06-24T00:22:28.563Z</lastmod>
+    <lastmod>2022-06-24T16:43:51.211Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/customizing_what_happens_in_fit.html</loc>
-    <lastmod>2022-06-24T00:22:29.292Z</lastmod>
+    <lastmod>2022-06-24T16:43:51.906Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/functional_api.html</loc>
-    <lastmod>2022-06-24T00:22:30.329Z</lastmod>
+    <lastmod>2022-06-24T16:43:53.084Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/making_new_layers_and_models_via_subclassing.html</loc>
-    <lastmod>2022-06-24T00:22:31.441Z</lastmod>
+    <lastmod>2022-06-24T16:43:54.138Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/preprocessing_layers.html</loc>
-    <lastmod>2022-06-24T00:22:32.254Z</lastmod>
+    <lastmod>2022-06-24T16:43:54.970Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/python_subclasses.html</loc>
-    <lastmod>2022-06-24T00:22:32.830Z</lastmod>
+    <lastmod>2022-06-24T16:43:55.563Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/sequential_model.html</loc>
-    <lastmod>2022-06-24T00:22:33.509Z</lastmod>
+    <lastmod>2022-06-24T16:43:56.364Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/serialization_and_saving.html</loc>
-    <lastmod>2022-06-24T00:22:34.482Z</lastmod>
+    <lastmod>2022-06-24T16:43:57.496Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/training_with_built_in_methods.html</loc>
-    <lastmod>2022-06-24T00:22:35.432Z</lastmod>
+    <lastmod>2022-06-24T16:43:58.517Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/transfer_learning.html</loc>
-    <lastmod>2022-06-24T00:22:36.344Z</lastmod>
+    <lastmod>2022-06-24T16:43:59.349Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/understanding_masking_and_padding.html</loc>
-    <lastmod>2022-06-24T00:22:37.145Z</lastmod>
+    <lastmod>2022-06-24T16:44:00.205Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/working_with_rnns.html</loc>
-    <lastmod>2022-06-24T00:22:37.931Z</lastmod>
+    <lastmod>2022-06-24T16:44:01.211Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/writing_a_training_loop_from_scratch.html</loc>
-    <lastmod>2022-06-24T00:22:38.863Z</lastmod>
+    <lastmod>2022-06-24T16:44:02.197Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/keras/writing_your_own_callbacks.html</loc>
-    <lastmod>2022-06-24T00:22:39.613Z</lastmod>
+    <lastmod>2022-06-24T16:44:03.223Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/tensorflow/autodiff.html</loc>
-    <lastmod>2022-06-24T00:22:40.491Z</lastmod>
+    <lastmod>2022-06-24T16:44:04.206Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/tensorflow/basics.html</loc>
-    <lastmod>2022-06-24T00:34:04.203Z</lastmod>
+    <lastmod>2022-06-24T16:44:05.312Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/tensorflow/intro_to_graphs.html</loc>
-    <lastmod>2022-06-24T00:22:42.334Z</lastmod>
+    <lastmod>2022-06-24T16:44:06.471Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/tensorflow/tensor.html</loc>
-    <lastmod>2022-06-24T00:22:43.586Z</lastmod>
+    <lastmod>2022-06-24T16:44:08.376Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/tensorflow/tensor_slicing.html</loc>
-    <lastmod>2022-06-24T00:22:44.739Z</lastmod>
+    <lastmod>2022-06-24T16:44:10.066Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/guides/tensorflow/variable.html</loc>
-    <lastmod>2022-06-24T00:22:45.735Z</lastmod>
+    <lastmod>2022-06-24T16:44:11.970Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/install/custom.html</loc>
-    <lastmod>2022-06-24T00:22:46.558Z</lastmod>
+    <lastmod>2022-06-24T16:44:13.329Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/install/gpu/cloud_desktop_gpu/index.html</loc>
-    <lastmod>2022-06-24T00:22:47.450Z</lastmod>
+    <lastmod>2022-06-24T16:44:15.519Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/install/gpu/cloud_server_gpu/index.html</loc>
-    <lastmod>2022-06-24T00:22:48.255Z</lastmod>
+    <lastmod>2022-06-24T16:44:19.141Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/install/gpu/index.html</loc>
-    <lastmod>2022-06-24T00:22:49.052Z</lastmod>
+    <lastmod>2022-06-24T16:44:23.445Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/install/gpu/local_gpu/index.html</loc>
-    <lastmod>2022-06-24T00:22:50.234Z</lastmod>
+    <lastmod>2022-06-24T16:44:27.289Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/install/index.html</loc>
-    <lastmod>2022-06-24T00:22:51.262Z</lastmod>
+    <lastmod>2022-06-24T16:44:28.972Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/tutorials-new/quickstart/index.html</loc>
-    <lastmod>2022-06-24T00:22:51.985Z</lastmod>
+    <lastmod>2022-06-24T16:44:29.751Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/tutorials/index.html</loc>
-    <lastmod>2022-06-24T01:05:48.625Z</lastmod>
+    <lastmod>2022-06-24T16:46:10.178Z</lastmod>
+  </url>
+  <url>
+    <loc>https://tensorflow.rstudio.com/tutorials/keras/classification.html</loc>
+    <lastmod>2022-06-24T16:44:35.946Z</lastmod>
+  </url>
+  <url>
+    <loc>https://tensorflow.rstudio.com/tutorials/keras/regression.html</loc>
+    <lastmod>2022-06-24T16:46:15.629Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/tutorials/keras/text_classification.html</loc>
-    <lastmod>2022-06-24T00:35:23.334Z</lastmod>
+    <lastmod>2022-06-24T16:45:11.678Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/tutorials/keras/text_classification_with_hub.html</loc>
-    <lastmod>2022-06-24T00:50:32.103Z</lastmod>
+    <lastmod>2022-06-24T16:45:12.811Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/tutorials/quickstart/advanced.html</loc>
-    <lastmod>2022-06-24T00:30:54.279Z</lastmod>
+    <lastmod>2022-06-24T16:45:13.973Z</lastmod>
   </url>
   <url>
     <loc>https://tensorflow.rstudio.com/tutorials/quickstart/beginner.html</loc>
-    <lastmod>2022-06-24T00:30:55.054Z</lastmod>
-  </url>
-  <url>
-    <loc>https://tensorflow.rstudio.com/tutorials/keras/classification.html</loc>
-    <lastmod>2022-06-24T00:35:12.627Z</lastmod>
+    <lastmod>2022-06-24T16:45:15.231Z</lastmod>
   </url>
 </urlset>

---FILE: _site/tutorials/index.html---
@@ -219,6 +219,11 @@ <h1 class=""quarto-secondary-nav-title"">Tutorials</h1>
   <div class=""sidebar-item-container""> 
   <a href=""../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with Hub</a>
   </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../tutorials/keras/regression.html"" class=""sidebar-item-text sidebar-link"">Regression</a>
+  </div>
 </li>
     </ul>
   </li>

---FILE: _site/tutorials/keras/classification.html---
@@ -234,7 +234,12 @@ <h1 class=""quarto-secondary-nav-title"">Basic Image Classification</h1>
 </li>
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
-  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with TensorFlow Hub</a>
+  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with Hub</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/regression.html"" class=""sidebar-item-text sidebar-link"">Regression</a>
   </div>
 </li>
     </ul>

---FILE: _site/tutorials/keras/regression.html---
@@ -0,0 +1,1372 @@
+<!DOCTYPE html>
+<html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
+
+<meta charset=""utf-8"">
+<meta name=""generator"" content=""quarto-0.9.559"">
+
+<meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
+
+
+<title>TensorFlow for R - Regression</title>
+<style>
+code{white-space: pre-wrap;}
+span.smallcaps{font-variant: small-caps;}
+span.underline{text-decoration: underline;}
+div.column{display: inline-block; vertical-align: top; width: 50%;}
+div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
+ul.task-list{list-style: none;}
+pre > code.sourceCode { white-space: pre; position: relative; }
+pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
+pre > code.sourceCode > span:empty { height: 1.2em; }
+.sourceCode { overflow: visible; }
+code.sourceCode > span { color: inherit; text-decoration: inherit; }
+div.sourceCode { margin: 1em 0; }
+pre.sourceCode { margin: 0; }
+@media screen {
+div.sourceCode { overflow: auto; }
+}
+@media print {
+pre > code.sourceCode { white-space: pre-wrap; }
+pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
+}
+pre.numberSource code
+  { counter-reset: source-line 0; }
+pre.numberSource code > span
+  { position: relative; left: -4em; counter-increment: source-line; }
+pre.numberSource code > span > a:first-child::before
+  { content: counter(source-line);
+    position: relative; left: -1em; text-align: right; vertical-align: baseline;
+    border: none; display: inline-block;
+    -webkit-touch-callout: none; -webkit-user-select: none;
+    -khtml-user-select: none; -moz-user-select: none;
+    -ms-user-select: none; user-select: none;
+    padding: 0 4px; width: 4em;
+    color: #aaaaaa;
+  }
+pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
+div.sourceCode
+  {   }
+@media screen {
+pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
+}
+code span.al { color: #ff0000; font-weight: bold; } /* Alert */
+code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
+code span.at { color: #7d9029; } /* Attribute */
+code span.bn { color: #40a070; } /* BaseN */
+code span.bu { } /* BuiltIn */
+code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
+code span.ch { color: #4070a0; } /* Char */
+code span.cn { color: #880000; } /* Constant */
+code span.co { color: #60a0b0; font-style: italic; } /* Comment */
+code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
+code span.do { color: #ba2121; font-style: italic; } /* Documentation */
+code span.dt { color: #902000; } /* DataType */
+code span.dv { color: #40a070; } /* DecVal */
+code span.er { color: #ff0000; font-weight: bold; } /* Error */
+code span.ex { } /* Extension */
+code span.fl { color: #40a070; } /* Float */
+code span.fu { color: #06287e; } /* Function */
+code span.im { } /* Import */
+code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
+code span.kw { color: #007020; font-weight: bold; } /* Keyword */
+code span.op { color: #666666; } /* Operator */
+code span.ot { color: #007020; } /* Other */
+code span.pp { color: #bc7a00; } /* Preprocessor */
+code span.sc { color: #4070a0; } /* SpecialChar */
+code span.ss { color: #bb6688; } /* SpecialString */
+code span.st { color: #4070a0; } /* String */
+code span.va { color: #19177c; } /* Variable */
+code span.vs { color: #4070a0; } /* VerbatimString */
+code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
+</style>
+
+
+<script src=""../../site_libs/quarto-nav/quarto-nav.js""></script>
+<script src=""../../site_libs/quarto-nav/headroom.min.js""></script>
+<script src=""../../site_libs/clipboard/clipboard.min.js""></script>
+<script src=""../../site_libs/quarto-search/autocomplete.umd.js""></script>
+<script src=""../../site_libs/quarto-search/fuse.min.js""></script>
+<script src=""../../site_libs/quarto-search/quarto-search.js""></script>
+<meta name=""quarto:offset"" content=""../../"">
+<link href=""../../tutorials/keras/text_classification_with_hub.html"" rel=""prev"">
+<link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
+<script src=""../../site_libs/quarto-html/quarto.js""></script>
+<script src=""../../site_libs/quarto-html/popper.min.js""></script>
+<script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
+<script src=""../../site_libs/quarto-html/anchor.min.js""></script>
+<link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" class=""quarto-color-scheme"" id=""quarto-text-highlighting-styles"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css"" rel=""prefetch"" class=""quarto-color-scheme quarto-color-alternate"" id=""quarto-text-highlighting-styles"">
+<script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
+<link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
+<link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"" class=""quarto-color-scheme"">
+<link href=""../../site_libs/bootstrap/bootstrap-dark.min.css"" rel=""prefetch"" class=""quarto-color-scheme quarto-color-alternate"">
+<script id=""quarto-search-options"" type=""application/json"">{
+  ""location"": ""navbar"",
+  ""copy-button"": false,
+  ""collapse-after"": 3,
+  ""panel-placement"": ""end"",
+  ""type"": ""overlay"",
+  ""limit"": 20,
+  ""language"": {
+    ""search-no-results-text"": ""No results"",
+    ""search-matching-documents-text"": ""matching documents"",
+    ""search-copy-link-title"": ""Copy link to search"",
+    ""search-hide-matches-text"": ""Hide additional matches"",
+    ""search-more-match-text"": ""more match in this document"",
+    ""search-more-matches-text"": ""more matches in this document"",
+    ""search-clear-button-title"": ""Clear"",
+    ""search-detached-cancel-button-title"": ""Cancel"",
+    ""search-submit-button-title"": ""Submit""
+  }
+}</script>
+
+  <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
+
+</head>
+
+<body class=""nav-sidebar docked nav-fixed"">
+
+<div id=""quarto-search-results""></div>
+  <header id=""quarto-header"" class=""headroom fixed-top"">
+    <nav class=""navbar navbar-expand-lg navbar-dark "">
+      <div class=""navbar-container container-fluid"">
+      <a class=""navbar-brand"" href=""../../index.html"">
+    <img src=""../../images/favicon/icon.png"" alt="""">
+    <span class=""navbar-title"">TensorFlow for R</span>
+  </a>
+          <button class=""navbar-toggler"" type=""button"" data-bs-toggle=""collapse"" data-bs-target=""#navbarCollapse"" aria-controls=""navbarCollapse"" aria-expanded=""false"" aria-label=""Toggle navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+  <span class=""navbar-toggler-icon""></span>
+</button>
+          <div class=""collapse navbar-collapse"" id=""navbarCollapse"">
+            <ul class=""navbar-nav navbar-nav-scroll me-auto"">
+  <li class=""nav-item"">
+    <a class=""nav-link"" href=""../../install/"">Install</a>
+  </li>  
+  <li class=""nav-item"">
+    <a class=""nav-link"" href=""../../tutorials/"">Tutorials</a>
+  </li>  
+  <li class=""nav-item"">
+    <a class=""nav-link"" href=""../../guides/index.html"">Guides</a>
+  </li>  
+  <li class=""nav-item"">
+    <a class=""nav-link"" href=""../../examples/"">Examples</a>
+  </li>  
+  <li class=""nav-item"">
+    <a class=""nav-link"" href=""../../deploy/"">Deploy</a>
+  </li>  
+  <li class=""nav-item"">
+    <a class=""nav-link"" href=""../../tools/"">Tools</a>
+  </li>  
+  <li class=""nav-item"">
+    <a class=""nav-link"" href=""../../reference/"">Reference</a>
+  </li>  
+</ul>
+              <div class=""quarto-toggle-container"">
+                  <a href="""" class=""quarto-color-scheme-toggle nav-link"" onclick=""window.quartoToggleColorScheme(); return false;"" title=""Toggle dark mode""><i class=""bi""></i></a>
+                  <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
+  <div class=""quarto-reader-toggle-btn"">
+  <i class=""bi""></i>
+  </div>
+</a>
+              </div>
+              <div id=""quarto-search"" class="""" title=""Search""></div>
+          </div> <!-- /navcollapse -->
+      </div> <!-- /container-fluid -->
+    </nav>
+  <nav class=""quarto-secondary-nav"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+    <div class=""container-fluid d-flex justify-content-between"">
+      <h1 class=""quarto-secondary-nav-title"">Regression</h1>
+      <button type=""button"" class=""quarto-btn-toggle btn"" aria-label=""Show secondary navigation"">
+        <i class=""bi bi-chevron-right""></i>
+      </button>
+    </div>
+  </nav>
+</header>
+<!-- content -->
+<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
+<!-- sidebar -->
+  <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation docked overflow-auto"">
+    <div class=""sidebar-menu-container""> 
+    <ul class=""list-unstyled mt-1"">
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/index.html"" class=""sidebar-item-text sidebar-link"">Tutorials</a>
+  </div>
+</li>
+        <li class=""sidebar-item sidebar-item-section"">
+    <div class=""sidebar-item-container""> 
+        <a class=""sidebar-item-text sidebar-link text-start"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar-section-1"" aria-expanded=""true"">Quickstart</a>
+      <a class=""sidebar-item-toggle text-start"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar-section-1"" aria-expanded=""true"">
+        <i class=""bi bi-chevron-right ms-2""></i>
+      </a>
+    </div>
+    <ul id=""quarto-sidebar-section-1"" class=""collapse list-unstyled sidebar-section depth1 show"">  
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/quickstart/beginner.html"" class=""sidebar-item-text sidebar-link"">Beginner</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/quickstart/advanced.html"" class=""sidebar-item-text sidebar-link"">Advanced</a>
+  </div>
+</li>
+    </ul>
+  </li>
+        <li class=""sidebar-item sidebar-item-section"">
+    <div class=""sidebar-item-container""> 
+        <a class=""sidebar-item-text sidebar-link text-start"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar-section-2"" aria-expanded=""true"">ML basics with Keras</a>
+      <a class=""sidebar-item-toggle text-start"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar-section-2"" aria-expanded=""true"">
+        <i class=""bi bi-chevron-right ms-2""></i>
+      </a>
+    </div>
+    <ul id=""quarto-sidebar-section-2"" class=""collapse list-unstyled sidebar-section depth1 show"">  
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/classification.html"" class=""sidebar-item-text sidebar-link"">Basic Image Classification</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/text_classification.html"" class=""sidebar-item-text sidebar-link"">Basic text classification</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with Hub</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/regression.html"" class=""sidebar-item-text sidebar-link active"">Regression</a>
+  </div>
+</li>
+    </ul>
+  </li>
+    </ul>
+    </div>
+</nav>
+<!-- margin-sidebar -->
+    <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
+        <nav id=""TOC"" role=""doc-toc"">
+    <h2 id=""toc-title"">On this page</h2>
+   
+  <ul>
+  <li><a href=""#the-auto-mpg-dataset"" id=""toc-the-auto-mpg-dataset"" class=""nav-link active"" data-scroll-target=""#the-auto-mpg-dataset"">The Auto MPG dataset</a>
+  <ul class=""collapse"">
+  <li><a href=""#get-the-data"" id=""toc-get-the-data"" class=""nav-link"" data-scroll-target=""#get-the-data"">Get the data</a></li>
+  <li><a href=""#clean-the-data"" id=""toc-clean-the-data"" class=""nav-link"" data-scroll-target=""#clean-the-data"">Clean the data</a></li>
+  <li><a href=""#split-the-data-into-training-and-test-sets"" id=""toc-split-the-data-into-training-and-test-sets"" class=""nav-link"" data-scroll-target=""#split-the-data-into-training-and-test-sets"">Split the data into training and test sets</a></li>
+  <li><a href=""#inspect-the-data"" id=""toc-inspect-the-data"" class=""nav-link"" data-scroll-target=""#inspect-the-data"">Inspect the data</a></li>
+  <li><a href=""#split-features-from-labels"" id=""toc-split-features-from-labels"" class=""nav-link"" data-scroll-target=""#split-features-from-labels"">Split features from labels</a></li>
+  </ul></li>
+  <li><a href=""#normalization"" id=""toc-normalization"" class=""nav-link"" data-scroll-target=""#normalization"">Normalization</a>
+  <ul class=""collapse"">
+  <li><a href=""#the-normalization-layer"" id=""toc-the-normalization-layer"" class=""nav-link"" data-scroll-target=""#the-normalization-layer"">The Normalization layer</a></li>
+  </ul></li>
+  <li><a href=""#linear-regression"" id=""toc-linear-regression"" class=""nav-link"" data-scroll-target=""#linear-regression"">Linear regression</a>
+  <ul class=""collapse"">
+  <li><a href=""#linear-regression-with-one-variable"" id=""toc-linear-regression-with-one-variable"" class=""nav-link"" data-scroll-target=""#linear-regression-with-one-variable"">Linear regression with one variable</a></li>
+  <li><a href=""#linear-regression-with-multiple-inputs"" id=""toc-linear-regression-with-multiple-inputs"" class=""nav-link"" data-scroll-target=""#linear-regression-with-multiple-inputs"">Linear regression with multiple inputs</a></li>
+  </ul></li>
+  <li><a href=""#regression-with-a-deep-neural-network-dnn"" id=""toc-regression-with-a-deep-neural-network-dnn"" class=""nav-link"" data-scroll-target=""#regression-with-a-deep-neural-network-dnn"">Regression with a deep neural network (DNN)</a>
+  <ul class=""collapse"">
+  <li><a href=""#regression-using-a-dnn-and-a-single-input"" id=""toc-regression-using-a-dnn-and-a-single-input"" class=""nav-link"" data-scroll-target=""#regression-using-a-dnn-and-a-single-input"">Regression using a DNN and a single input</a></li>
+  <li><a href=""#regression-using-a-dnn-and-multiple-inputs"" id=""toc-regression-using-a-dnn-and-multiple-inputs"" class=""nav-link"" data-scroll-target=""#regression-using-a-dnn-and-multiple-inputs"">Regression using a DNN and multiple inputs</a></li>
+  </ul></li>
+  <li><a href=""#performance"" id=""toc-performance"" class=""nav-link"" data-scroll-target=""#performance"">Performance</a>
+  <ul class=""collapse"">
+  <li><a href=""#make-predictions"" id=""toc-make-predictions"" class=""nav-link"" data-scroll-target=""#make-predictions"">Make predictions</a></li>
+  </ul></li>
+  <li><a href=""#conclusion"" id=""toc-conclusion"" class=""nav-link"" data-scroll-target=""#conclusion"">Conclusion</a></li>
+  </ul>
+<div class=""toc-actions""><div><i class=""bi bi-github""></i></div><div class=""action-links""><p><a href=""https://github.com/t-kalinowski/tf-site/edit/main/tutorials/keras/regression.qmd"" class=""toc-action"">Edit this page</a></p><p><a href=""https://github.com/t-kalinowski/tf-site/blob/main/tutorials/keras/regression.qmd"" class=""toc-action"">View source</a></p><p><a href=""https://github.com/t-kalinowski/tf-site/issues/new"" class=""toc-action"">Report an issue</a></p></div></div></nav>
+    </div>
+<!-- main -->
+<main class=""content"" id=""quarto-document-content"">
+
+<header id=""title-block-header"" class=""quarto-title-block default"">
+<div class=""quarto-title"">
+<h1 class=""title d-none d-lg-block"">Regression</h1>
+</div>
+
+
+
+<div class=""quarto-title-meta"">
+
+    
+    
+  </div>
+  
+
+</header>
+
+<p>In a <em>regression</em> problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a <em>classification</em> problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).</p>
+<p>This tutorial uses the classic <a href=""https://archive.ics.uci.edu/ml/datasets/auto+mpg"">Auto MPG</a> dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.</p>
+<p>This example uses the Keras API. (Visit the Keras <a href=""../keras"">tutorials</a> and <a href=""../../guides/keras"">guides</a> to learn more.)</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb1""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb1-1""><a href=""#cb1-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(tensorflow)</span>
+<span id=""cb1-2""><a href=""#cb1-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(keras)</span>
+<span id=""cb1-3""><a href=""#cb1-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(tidyverse)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>── Attaching packages ─────────────────────────────────── tidyverse 1.3.1 ──</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>✔ ggplot2 3.3.5     ✔ purrr   0.3.4
+✔ tibble  3.1.7     ✔ dplyr   1.0.9
+✔ tidyr   1.2.0     ✔ stringr 1.4.0
+✔ readr   2.1.2     ✔ forcats 0.5.1</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'tibble' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'tidyr' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'readr' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'dplyr' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>── Conflicts ────────────────────────────────────── tidyverse_conflicts() ──
+✖ dplyr::filter() masks stats::filter()
+✖ dplyr::lag()    masks stats::lag()</code></pre>
+</div>
+<div class=""sourceCode cell-code"" id=""cb9""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb9-1""><a href=""#cb9-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(tidymodels)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Registered S3 method overwritten by 'tune':
+  method                   from   
+  required_pkgs.model_spec parsnip</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>── Attaching packages ────────────────────────────────── tidymodels 0.1.4 ──</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>✔ broom        0.7.12         ✔ rsample      0.1.0     
+✔ dials        0.0.10         ✔ tune         0.1.6.9000
+✔ infer        1.0.0          ✔ workflows    0.2.3     
+✔ modeldata    0.1.1          ✔ workflowsets 0.1.0     
+✔ parsnip      0.2.1          ✔ yardstick    0.0.8     
+✔ recipes      0.2.0          </code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'broom' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'scales' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'parsnip' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Warning: package 'recipes' was built under R version 4.1.2</code></pre>
+</div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>── Conflicts ───────────────────────────────────── tidymodels_conflicts() ──
+✖ scales::discard()        masks purrr::discard()
+✖ dplyr::filter()          masks stats::filter()
+✖ recipes::fixed()         masks stringr::fixed()
+✖ yardstick::get_weights() masks keras::get_weights()
+✖ dplyr::lag()             masks stats::lag()
+✖ yardstick::spec()        masks readr::spec()
+✖ recipes::step()          masks stats::step()
+✖ tune::tune()             masks parsnip::tune()
+• Use tidymodels_prefer() to resolve common conflicts.</code></pre>
+</div>
+</div>
+<section id=""the-auto-mpg-dataset"" class=""level2"">
+<h2 class=""anchored"" data-anchor-id=""the-auto-mpg-dataset"">The Auto MPG dataset</h2>
+<p>The dataset is available from the <a href=""https://archive.ics.uci.edu/ml/"">UCI Machine Learning Repository</a>.</p>
+<section id=""get-the-data"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""get-the-data"">Get the data</h3>
+<p>First download and import the dataset using pandas:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb18""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb18-1""><a href=""#cb18-1"" aria-hidden=""true"" tabindex=""-1""></a>url <span class=""ot"">&lt;-</span> <span class=""st"">""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data""</span></span>
+<span id=""cb18-2""><a href=""#cb18-2"" aria-hidden=""true"" tabindex=""-1""></a>col_names <span class=""ot"">&lt;-</span> <span class=""fu"">c</span>(<span class=""st"">""mpg""</span>,<span class=""st"">""cylinders""</span>,<span class=""st"">""displacement""</span>,<span class=""st"">""horsepower""</span>,<span class=""st"">""weight""</span>,<span class=""st"">""acceleration""</span>,<span class=""st"">""model_year""</span>, <span class=""st"">""origin""</span>,<span class=""st"">""car_name""</span>)</span>
+<span id=""cb18-3""><a href=""#cb18-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb18-4""><a href=""#cb18-4"" aria-hidden=""true"" tabindex=""-1""></a>raw_dataset <span class=""ot"">&lt;-</span> <span class=""fu"">read.table</span>(</span>
+<span id=""cb18-5""><a href=""#cb18-5"" aria-hidden=""true"" tabindex=""-1""></a>  url, </span>
+<span id=""cb18-6""><a href=""#cb18-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">header =</span> T, </span>
+<span id=""cb18-7""><a href=""#cb18-7"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">col.names =</span> col_names, </span>
+<span id=""cb18-8""><a href=""#cb18-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">na.strings =</span> <span class=""st"">""?""</span></span>
+<span id=""cb18-9""><a href=""#cb18-9"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb19""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb19-1""><a href=""#cb19-1"" aria-hidden=""true"" tabindex=""-1""></a>dataset <span class=""ot"">&lt;-</span> raw_dataset <span class=""sc"">%&gt;%</span> <span class=""fu"">select</span>(<span class=""sc"">-</span>car_name)</span>
+<span id=""cb19-2""><a href=""#cb19-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">tail</span>(dataset)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>    mpg cylinders displacement horsepower weight acceleration model_year
+392  27         4          151         90   2950         17.3         82
+393  27         4          140         86   2790         15.6         82
+394  44         4           97         52   2130         24.6         82
+395  32         4          135         84   2295         11.6         82
+396  28         4          120         79   2625         18.6         82
+397  31         4          119         82   2720         19.4         82
+    origin
+392      1
+393      1
+394      2
+395      1
+396      1
+397      1</code></pre>
+</div>
+</div>
+</section>
+<section id=""clean-the-data"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""clean-the-data"">Clean the data</h3>
+<p>The dataset contains a few unknown values:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb21""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb21-1""><a href=""#cb21-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">lapply</span>(dataset, <span class=""cf"">function</span>(x) <span class=""fu"">sum</span>(<span class=""fu"">is.na</span>(x))) <span class=""sc"">%&gt;%</span> <span class=""fu"">str</span>()</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>List of 8
+ $ mpg         : int 0
+ $ cylinders   : int 0
+ $ displacement: int 0
+ $ horsepower  : int 6
+ $ weight      : int 0
+ $ acceleration: int 0
+ $ model_year  : int 0
+ $ origin      : int 0</code></pre>
+</div>
+</div>
+<p>Drop those rows to keep this initial tutorial simple:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb23""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb23-1""><a href=""#cb23-1"" aria-hidden=""true"" tabindex=""-1""></a>dataset <span class=""ot"">&lt;-</span> <span class=""fu"">na.omit</span>(dataset)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>The <code>""origin""</code> column is categorical, not numeric. So the next step is to one-hot encode the values in the column with the <code>recipes</code> package.</p>
+<p>Note: You can set up the <code>keras_model()</code> to do this kind of transformation for you but that’s beyond the scope of this tutorial. Check out the <a href=""../structured_data/preprocessing_layers.qmd"">Classify structured data using Keras preprocessing layers</a> or <a href=""../load_data/csv.qmd"">Load CSV data</a> tutorials for examples.</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb24""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb24-1""><a href=""#cb24-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(recipes)</span>
+<span id=""cb24-2""><a href=""#cb24-2"" aria-hidden=""true"" tabindex=""-1""></a>dataset <span class=""ot"">&lt;-</span> <span class=""fu"">recipe</span>(mpg <span class=""sc"">~</span> ., dataset) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb24-3""><a href=""#cb24-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">step_num2factor</span>(origin, <span class=""at"">levels =</span> <span class=""fu"">c</span>(<span class=""st"">""USA""</span>, <span class=""st"">""Europe""</span>, <span class=""st"">""Japan""</span>)) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb24-4""><a href=""#cb24-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">step_dummy</span>(origin, <span class=""at"">one_hot =</span> <span class=""cn"">TRUE</span>) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb24-5""><a href=""#cb24-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">prep</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb24-6""><a href=""#cb24-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">bake</span>(<span class=""at"">new_data =</span> <span class=""cn"">NULL</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb25""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb25-1""><a href=""#cb25-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">glimpse</span>(dataset)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Rows: 391
+Columns: 10
+$ cylinders     &lt;int&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 6, 6, 6, 4…
+$ displacement  &lt;dbl&gt; 350, 318, 304, 302, 429, 454, 440, 455, 390, 383, 34…
+$ horsepower    &lt;dbl&gt; 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 16…
+$ weight        &lt;dbl&gt; 3693, 3436, 3433, 3449, 4341, 4354, 4312, 4425, 3850…
+$ acceleration  &lt;dbl&gt; 11.5, 11.0, 12.0, 10.5, 10.0, 9.0, 8.5, 10.0, 8.5, 1…
+$ model_year    &lt;int&gt; 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, …
+$ mpg           &lt;dbl&gt; 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 14, …
+$ origin_USA    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0…
+$ origin_Europe &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
+$ origin_Japan  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…</code></pre>
+</div>
+</div>
+</section>
+<section id=""split-the-data-into-training-and-test-sets"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""split-the-data-into-training-and-test-sets"">Split the data into training and test sets</h3>
+<p>Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb27""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb27-1""><a href=""#cb27-1"" aria-hidden=""true"" tabindex=""-1""></a>split <span class=""ot"">&lt;-</span> <span class=""fu"">initial_split</span>(dataset, <span class=""fl"">0.8</span>)</span>
+<span id=""cb27-2""><a href=""#cb27-2"" aria-hidden=""true"" tabindex=""-1""></a>train_dataset <span class=""ot"">&lt;-</span> <span class=""fu"">training</span>(split)</span>
+<span id=""cb27-3""><a href=""#cb27-3"" aria-hidden=""true"" tabindex=""-1""></a>test_dataset <span class=""ot"">&lt;-</span> <span class=""fu"">testing</span>(split)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+</section>
+<section id=""inspect-the-data"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""inspect-the-data"">Inspect the data</h3>
+<p>Review the joint distribution of a few pairs of columns from the training set.</p>
+<p>The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other.</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb28""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb28-1""><a href=""#cb28-1"" aria-hidden=""true"" tabindex=""-1""></a>train_dataset <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb28-2""><a href=""#cb28-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">select</span>(mpg, cylinders, displacement, weight) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb28-3""><a href=""#cb28-3"" aria-hidden=""true"" tabindex=""-1""></a>  GGally<span class=""sc"">::</span><span class=""fu"">ggpairs</span>()</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Registered S3 method overwritten by 'GGally':
+  method from   
+  +.gg   ggplot2</code></pre>
+</div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-16-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+<p>Let’s also check the overall statistics. Note how each feature covers a very different range:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb30""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb30-1""><a href=""#cb30-1"" aria-hidden=""true"" tabindex=""-1""></a>skimr<span class=""sc"">::</span><span class=""fu"">skim</span>(train_dataset)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<table class=""table table-sm table-striped"">
+<caption>Data summary</caption>
+<tbody>
+<tr class=""odd"">
+<td style=""text-align: left;"">Name</td>
+<td style=""text-align: left;"">train_dataset</td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">Number of rows</td>
+<td style=""text-align: left;"">312</td>
+</tr>
+<tr class=""odd"">
+<td style=""text-align: left;"">Number of columns</td>
+<td style=""text-align: left;"">10</td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">_______________________</td>
+<td style=""text-align: left;""></td>
+</tr>
+<tr class=""odd"">
+<td style=""text-align: left;"">Column type frequency:</td>
+<td style=""text-align: left;""></td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">numeric</td>
+<td style=""text-align: left;"">10</td>
+</tr>
+<tr class=""odd"">
+<td style=""text-align: left;"">________________________</td>
+<td style=""text-align: left;""></td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">Group variables</td>
+<td style=""text-align: left;"">None</td>
+</tr>
+</tbody>
+</table>
+<p><strong>Variable type: numeric</strong></p>
+<table class=""table table-sm table-striped"">
+<colgroup>
+<col style=""width: 14%"">
+<col style=""width: 10%"">
+<col style=""width: 14%"">
+<col style=""width: 8%"">
+<col style=""width: 7%"">
+<col style=""width: 5%"">
+<col style=""width: 8%"">
+<col style=""width: 8%"">
+<col style=""width: 7%"">
+<col style=""width: 7%"">
+<col style=""width: 6%"">
+</colgroup>
+<thead>
+<tr class=""header"">
+<th style=""text-align: left;"">skim_variable</th>
+<th style=""text-align: right;"">n_missing</th>
+<th style=""text-align: right;"">complete_rate</th>
+<th style=""text-align: right;"">mean</th>
+<th style=""text-align: right;"">sd</th>
+<th style=""text-align: right;"">p0</th>
+<th style=""text-align: right;"">p25</th>
+<th style=""text-align: right;"">p50</th>
+<th style=""text-align: right;"">p75</th>
+<th style=""text-align: right;"">p100</th>
+<th style=""text-align: left;"">hist</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td style=""text-align: left;"">cylinders</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">5.50</td>
+<td style=""text-align: right;"">1.72</td>
+<td style=""text-align: right;"">3</td>
+<td style=""text-align: right;"">4.00</td>
+<td style=""text-align: right;"">4.00</td>
+<td style=""text-align: right;"">8.0</td>
+<td style=""text-align: right;"">8.0</td>
+<td style=""text-align: left;"">▇▁▃▁▅</td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">displacement</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">196.99</td>
+<td style=""text-align: right;"">105.81</td>
+<td style=""text-align: right;"">68</td>
+<td style=""text-align: right;"">105.00</td>
+<td style=""text-align: right;"">151.00</td>
+<td style=""text-align: right;"">302.0</td>
+<td style=""text-align: right;"">455.0</td>
+<td style=""text-align: left;"">▇▂▂▃▁</td>
+</tr>
+<tr class=""odd"">
+<td style=""text-align: left;"">horsepower</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">104.93</td>
+<td style=""text-align: right;"">38.85</td>
+<td style=""text-align: right;"">46</td>
+<td style=""text-align: right;"">76.75</td>
+<td style=""text-align: right;"">92.00</td>
+<td style=""text-align: right;"">126.0</td>
+<td style=""text-align: right;"">230.0</td>
+<td style=""text-align: left;"">▆▇▃▂▁</td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">weight</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">2995.04</td>
+<td style=""text-align: right;"">844.26</td>
+<td style=""text-align: right;"">1613</td>
+<td style=""text-align: right;"">2232.50</td>
+<td style=""text-align: right;"">2849.00</td>
+<td style=""text-align: right;"">3622.5</td>
+<td style=""text-align: right;"">4997.0</td>
+<td style=""text-align: left;"">▇▇▆▅▂</td>
+</tr>
+<tr class=""odd"">
+<td style=""text-align: left;"">acceleration</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">15.57</td>
+<td style=""text-align: right;"">2.83</td>
+<td style=""text-align: right;"">8</td>
+<td style=""text-align: right;"">13.88</td>
+<td style=""text-align: right;"">15.50</td>
+<td style=""text-align: right;"">17.2</td>
+<td style=""text-align: right;"">24.8</td>
+<td style=""text-align: left;"">▁▆▇▂▁</td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">model_year</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">76.06</td>
+<td style=""text-align: right;"">3.71</td>
+<td style=""text-align: right;"">70</td>
+<td style=""text-align: right;"">73.00</td>
+<td style=""text-align: right;"">76.00</td>
+<td style=""text-align: right;"">79.0</td>
+<td style=""text-align: right;"">82.0</td>
+<td style=""text-align: left;"">▇▆▇▆▇</td>
+</tr>
+<tr class=""odd"">
+<td style=""text-align: left;"">mpg</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">23.48</td>
+<td style=""text-align: right;"">7.91</td>
+<td style=""text-align: right;"">9</td>
+<td style=""text-align: right;"">17.00</td>
+<td style=""text-align: right;"">22.15</td>
+<td style=""text-align: right;"">29.0</td>
+<td style=""text-align: right;"">46.6</td>
+<td style=""text-align: left;"">▆▇▆▃▁</td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">origin_USA</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">0.62</td>
+<td style=""text-align: right;"">0.48</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">0.00</td>
+<td style=""text-align: right;"">1.00</td>
+<td style=""text-align: right;"">1.0</td>
+<td style=""text-align: right;"">1.0</td>
+<td style=""text-align: left;"">▅▁▁▁▇</td>
+</tr>
+<tr class=""odd"">
+<td style=""text-align: left;"">origin_Europe</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">0.17</td>
+<td style=""text-align: right;"">0.38</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">0.00</td>
+<td style=""text-align: right;"">0.00</td>
+<td style=""text-align: right;"">0.0</td>
+<td style=""text-align: right;"">1.0</td>
+<td style=""text-align: left;"">▇▁▁▁▂</td>
+</tr>
+<tr class=""even"">
+<td style=""text-align: left;"">origin_Japan</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">0.21</td>
+<td style=""text-align: right;"">0.40</td>
+<td style=""text-align: right;"">0</td>
+<td style=""text-align: right;"">0.00</td>
+<td style=""text-align: right;"">0.00</td>
+<td style=""text-align: right;"">0.0</td>
+<td style=""text-align: right;"">1.0</td>
+<td style=""text-align: left;"">▇▁▁▁▂</td>
+</tr>
+</tbody>
+</table>
+</div>
+</div>
+</section>
+<section id=""split-features-from-labels"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""split-features-from-labels"">Split features from labels</h3>
+<p>Separate the target value—the “label”—from the features. This label is the value that you will train the model to predict.</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb31""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb31-1""><a href=""#cb31-1"" aria-hidden=""true"" tabindex=""-1""></a>train_features <span class=""ot"">&lt;-</span> train_dataset <span class=""sc"">%&gt;%</span> <span class=""fu"">select</span>(<span class=""sc"">-</span>mpg)</span>
+<span id=""cb31-2""><a href=""#cb31-2"" aria-hidden=""true"" tabindex=""-1""></a>test_features <span class=""ot"">&lt;-</span> test_dataset <span class=""sc"">%&gt;%</span> <span class=""fu"">select</span>(<span class=""sc"">-</span>mpg)</span>
+<span id=""cb31-3""><a href=""#cb31-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb31-4""><a href=""#cb31-4"" aria-hidden=""true"" tabindex=""-1""></a>train_labels <span class=""ot"">&lt;-</span> train_dataset <span class=""sc"">%&gt;%</span> <span class=""fu"">select</span>(mpg)</span>
+<span id=""cb31-5""><a href=""#cb31-5"" aria-hidden=""true"" tabindex=""-1""></a>test_labels <span class=""ot"">&lt;-</span> test_dataset <span class=""sc"">%&gt;%</span> <span class=""fu"">select</span>(mpg)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+</section>
+</section>
+<section id=""normalization"" class=""level2"">
+<h2 class=""anchored"" data-anchor-id=""normalization"">Normalization</h2>
+<p>In the table of statistics it’s easy to see how different the ranges of each feature are:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb32""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb32-1""><a href=""#cb32-1"" aria-hidden=""true"" tabindex=""-1""></a>my_skim <span class=""ot"">&lt;-</span> skimr<span class=""sc"">::</span><span class=""fu"">skim_with</span>(<span class=""at"">numeric =</span> skimr<span class=""sc"">::</span><span class=""fu"">sfl</span>(mean, sd))</span>
+<span id=""cb32-2""><a href=""#cb32-2"" aria-hidden=""true"" tabindex=""-1""></a>train_dataset <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb32-3""><a href=""#cb32-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">select</span>(<span class=""fu"">where</span>(<span class=""sc"">~</span><span class=""fu"">is.numeric</span>(.x))) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb32-4""><a href=""#cb32-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">pivot_longer</span>(</span>
+<span id=""cb32-5""><a href=""#cb32-5"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">cols =</span> <span class=""fu"">everything</span>(), <span class=""at"">names_to =</span> <span class=""st"">""variable""</span>, <span class=""at"">values_to =</span> <span class=""st"">""values""</span>) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb32-6""><a href=""#cb32-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">group_by</span>(variable) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb32-7""><a href=""#cb32-7"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">summarise</span>(<span class=""at"">mean =</span> <span class=""fu"">mean</span>(values), <span class=""at"">sd =</span> <span class=""fu"">sd</span>(values))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code># A tibble: 10 × 3
+   variable          mean      sd
+   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;
+ 1 acceleration    15.6     2.83 
+ 2 cylinders        5.5     1.72 
+ 3 displacement   197.    106.   
+ 4 horsepower     105.     38.9  
+ 5 model_year      76.1     3.71 
+ 6 mpg             23.5     7.91 
+ 7 origin_Europe    0.170   0.376
+ 8 origin_Japan     0.205   0.404
+ 9 origin_USA       0.625   0.485
+10 weight        2995.    844.   </code></pre>
+</div>
+</div>
+<p>It is good practice to normalize features that use different scales and ranges.</p>
+<p>One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.</p>
+<p>Although a model <em>might</em> converge without feature normalization, normalization makes training much more stable.</p>
+<p>Note: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the <a href=""https://www.tensorflow.org/guide/keras/preprocessing_layers"">Working with preprocessing layers</a> guide and the <a href=""../structured_data/preprocessing_layers.qmd"">Classify structured data using Keras preprocessing layers</a> tutorial.</p>
+<section id=""the-normalization-layer"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""the-normalization-layer"">The Normalization layer</h3>
+<p>The <code>layer_normalization()</code> is a clean and simple way to add feature normalization into your model.</p>
+<p>The first step is to create the layer:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb34""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb34-1""><a href=""#cb34-1"" aria-hidden=""true"" tabindex=""-1""></a>normalizer <span class=""ot"">&lt;-</span> <span class=""fu"">layer_normalization</span>(<span class=""at"">axis =</span> <span class=""sc"">-</span>1L)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Loaded Tensorflow version 2.9.1</code></pre>
+</div>
+</div>
+<p>Then, fit the state of the preprocessing layer to the data by calling <code>adapt()</code>:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb36""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb36-1""><a href=""#cb36-1"" aria-hidden=""true"" tabindex=""-1""></a>normalizer <span class=""sc"">%&gt;%</span> <span class=""fu"">adapt</span>(<span class=""fu"">as.matrix</span>(train_features))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>Calculate the mean and variance, and store them in the layer:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb37""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb37-1""><a href=""#cb37-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(normalizer<span class=""sc"">$</span>mean)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>tf.Tensor(
+[[5.5000000e+00 1.9699361e+02 1.0493270e+02 2.9950388e+03 1.5573077e+01
+  7.6060890e+01 6.2500000e-01 1.6987181e-01 2.0512822e-01]], shape=(1, 9), dtype=float32)</code></pre>
+</div>
+</div>
+<p>When the layer is called, it returns the input data, with each feature independently normalized.</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb39""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb39-1""><a href=""#cb39-1"" aria-hidden=""true"" tabindex=""-1""></a>first <span class=""ot"">&lt;-</span> <span class=""fu"">as.matrix</span>(train_features[<span class=""dv"">1</span>,])</span>
+<span id=""cb39-2""><a href=""#cb39-2"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb39-3""><a href=""#cb39-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">'First example:'</span>, first)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>First example: 8 318 150 4077 14 72 1 0 0</code></pre>
+</div>
+<div class=""sourceCode cell-code"" id=""cb41""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb41-1""><a href=""#cb41-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">'Normalized:'</span>, <span class=""fu"">as.matrix</span>(<span class=""fu"">normalizer</span>(first)))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Normalized: 1.452718 1.145492 1.161751 1.283603 -0.5567052 -1.095416 0.7745966 -0.452364 -0.5080005</code></pre>
+</div>
+</div>
+</section>
+</section>
+<section id=""linear-regression"" class=""level2"">
+<h2 class=""anchored"" data-anchor-id=""linear-regression"">Linear regression</h2>
+<p>Before building a deep neural network model, start with linear regression using one and several variables.</p>
+<section id=""linear-regression-with-one-variable"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""linear-regression-with-one-variable"">Linear regression with one variable</h3>
+<p>Begin with a single-variable linear regression to predict <code>'mpg'</code> from <code>'horsepower'</code>.</p>
+<p>Training a model with Keras typically starts by defining the model architecture. Use a Sequential model, which <a href=""https://www.tensorflow.org/guide/keras/sequential_model"">represents a sequence of steps</a>.</p>
+<p>There are two steps in your single-variable linear regression model:</p>
+<ul>
+<li>Normalize the <code>'horsepower'</code> input features using the <code>normalization</code> preprocessing layer.</li>
+<li>Apply a linear transformation (<span class=""math inline"">\(y = mx+b\)</span>) to produce 1 output using a linear layer (<code>dense</code>).</li>
+</ul>
+<p>The number of <em>inputs</em> can either be set by the <code>input_shape</code> argument, or automatically when the model is run for the first time.</p>
+<p>First, create a matrix made of the <code>'horsepower'</code> features. Then, instantiate the <code>layer_normalization</code> and fit its state to the <code>horsepower</code> data:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb43""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb43-1""><a href=""#cb43-1"" aria-hidden=""true"" tabindex=""-1""></a>horsepower <span class=""ot"">&lt;-</span> <span class=""fu"">matrix</span>(train_features<span class=""sc"">$</span>horsepower)</span>
+<span id=""cb43-2""><a href=""#cb43-2"" aria-hidden=""true"" tabindex=""-1""></a>horsepower_normalizer <span class=""ot"">&lt;-</span> <span class=""fu"">layer_normalization</span>(<span class=""at"">input_shape =</span> <span class=""fu"">shape</span>(<span class=""dv"">1</span>), <span class=""at"">axis =</span> <span class=""cn"">NULL</span>)</span>
+<span id=""cb43-3""><a href=""#cb43-3"" aria-hidden=""true"" tabindex=""-1""></a>horsepower_normalizer <span class=""sc"">%&gt;%</span> <span class=""fu"">adapt</span>(horsepower)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>Build the Keras Sequential model:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb44""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb44-1""><a href=""#cb44-1"" aria-hidden=""true"" tabindex=""-1""></a>horsepower_model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb44-2""><a href=""#cb44-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">horsepower_normalizer</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb44-3""><a href=""#cb44-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">1</span>)</span>
+<span id=""cb44-4""><a href=""#cb44-4"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb44-5""><a href=""#cb44-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(horsepower_model)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: ""sequential""
+____________________________________________________________________________
+ Layer (type)                Output Shape              Param #   Trainable  
+============================================================================
+ normalization_1 (Normalizat  (None, 1)                3         Y          
+ ion)                                                                       
+ dense (Dense)               (None, 1)                 2         Y          
+============================================================================
+Total params: 5
+Trainable params: 2
+Non-trainable params: 3
+____________________________________________________________________________</code></pre>
+</div>
+</div>
+<p>This model will predict <code>'mpg'</code> from <code>'horsepower'</code>.</p>
+<p>Run the untrained model on the first 10 ‘horsepower’ values. The output won’t be good, but notice that it has the expected shape of <code>(10, 1)</code>:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb46""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb46-1""><a href=""#cb46-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">predict</span>(horsepower_model, horsepower[<span class=""dv"">1</span><span class=""sc"">:</span><span class=""dv"">10</span>,])</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>              [,1]
+ [1,]  0.918656170
+ [2,]  0.918656170
+ [3,] -0.182085037
+ [4,] -0.222853214
+ [5,] -0.610151052
+ [6,] -0.161700934
+ [7,]  0.001371827
+ [8,]  0.103292309
+ [9,]  0.103292309
+[10,] -0.447078258</code></pre>
+</div>
+</div>
+<p>Once the model is built, configure the training procedure using the Keras <code>compile()</code> method. The most important arguments to compile are the <code>loss</code> and the <code>optimizer</code>, since these define what will be optimized (<code>mean_absolute_error</code>) and how (using the <code>optimizer_adam</code>).</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb48""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb48-1""><a href=""#cb48-1"" aria-hidden=""true"" tabindex=""-1""></a>horsepower_model <span class=""sc"">%&gt;%</span> <span class=""fu"">compile</span>(</span>
+<span id=""cb48-2""><a href=""#cb48-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">optimizer =</span> <span class=""fu"">optimizer_adam</span>(<span class=""at"">learning_rate =</span> <span class=""fl"">0.1</span>),</span>
+<span id=""cb48-3""><a href=""#cb48-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">loss =</span> <span class=""st"">'mean_absolute_error'</span></span>
+<span id=""cb48-4""><a href=""#cb48-4"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>Use Keras <code>fit()</code> to execute the training for 100 epochs:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb49""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb49-1""><a href=""#cb49-1"" aria-hidden=""true"" tabindex=""-1""></a>history <span class=""ot"">&lt;-</span> horsepower_model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(</span>
+<span id=""cb49-2""><a href=""#cb49-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_features<span class=""sc"">$</span>horsepower),</span>
+<span id=""cb49-3""><a href=""#cb49-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_labels),</span>
+<span id=""cb49-4""><a href=""#cb49-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">epochs =</span> <span class=""dv"">100</span>,</span>
+<span id=""cb49-5""><a href=""#cb49-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""co""># Suppress logging.</span></span>
+<span id=""cb49-6""><a href=""#cb49-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span>,</span>
+<span id=""cb49-7""><a href=""#cb49-7"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""co""># Calculate validation results on 20% of the training data.</span></span>
+<span id=""cb49-8""><a href=""#cb49-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">validation_split =</span> <span class=""fl"">0.2</span></span>
+<span id=""cb49-9""><a href=""#cb49-9"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>Visualize the model’s training progress using the stats stored in the <code>history</code> object:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb50""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb50-1""><a href=""#cb50-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">plot</span>(history)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-42-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+<p>Collect the results on the test set for later:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb51""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb51-1""><a href=""#cb51-1"" aria-hidden=""true"" tabindex=""-1""></a>test_results <span class=""ot"">&lt;-</span> <span class=""fu"">list</span>()</span>
+<span id=""cb51-2""><a href=""#cb51-2"" aria-hidden=""true"" tabindex=""-1""></a>test_results[[<span class=""st"">""horsepower_model""</span>]] <span class=""ot"">&lt;-</span> horsepower_model <span class=""sc"">%&gt;%</span> <span class=""fu"">evaluate</span>(</span>
+<span id=""cb51-3""><a href=""#cb51-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_features<span class=""sc"">$</span>horsepower),</span>
+<span id=""cb51-4""><a href=""#cb51-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_labels), </span>
+<span id=""cb51-5""><a href=""#cb51-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span></span>
+<span id=""cb51-6""><a href=""#cb51-6"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>Since this is a single variable regression, it’s easy to view the model’s predictions as a function of the input:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb52""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb52-1""><a href=""#cb52-1"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> <span class=""fu"">seq</span>(<span class=""dv"">0</span>, <span class=""dv"">250</span>, <span class=""at"">length.out =</span> <span class=""dv"">251</span>)</span>
+<span id=""cb52-2""><a href=""#cb52-2"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">predict</span>(horsepower_model, x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb53""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb53-1""><a href=""#cb53-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">ggplot</span>(train_dataset) <span class=""sc"">+</span></span>
+<span id=""cb53-2""><a href=""#cb53-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">geom_point</span>(<span class=""fu"">aes</span>(<span class=""at"">x =</span> horsepower, <span class=""at"">y =</span> mpg, <span class=""at"">color =</span> <span class=""st"">""data""</span>)) <span class=""sc"">+</span></span>
+<span id=""cb53-3""><a href=""#cb53-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">geom_line</span>(<span class=""at"">data =</span> <span class=""fu"">data.frame</span>(x, y), <span class=""fu"">aes</span>(<span class=""at"">x =</span> x, <span class=""at"">y =</span> y, <span class=""at"">color =</span> <span class=""st"">""prediction""</span>))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-48-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+</section>
+<section id=""linear-regression-with-multiple-inputs"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""linear-regression-with-multiple-inputs"">Linear regression with multiple inputs</h3>
+<p>You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same <span class=""math inline"">\(y = mx+b\)</span> except that <span class=""math inline"">\(m\)</span> is a matrix and <span class=""math inline"">\(b\)</span> is a vector.</p>
+<p>Create a two-step Keras Sequential model again with the first layer being <code>normalizer</code> (<code>layer_normalization(axis = -1)</code>) you defined earlier and adapted to the whole dataset:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb54""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb54-1""><a href=""#cb54-1"" aria-hidden=""true"" tabindex=""-1""></a>linear_model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb54-2""><a href=""#cb54-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">normalizer</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb54-3""><a href=""#cb54-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">1</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>When you call <code>predict()</code> on a batch of inputs, it produces <code>units = 1</code> outputs for each example:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb55""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb55-1""><a href=""#cb55-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">predict</span>(linear_model, <span class=""fu"">as.matrix</span>(train_features[<span class=""dv"">1</span><span class=""sc"">:</span><span class=""dv"">10</span>, ]))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>            [,1]
+ [1,] -2.9093571
+ [2,] -3.0792081
+ [3,]  1.2285657
+ [4,]  1.3383932
+ [5,]  2.0067687
+ [6,]  0.9256226
+ [7,] -0.9580150
+ [8,] -0.7418302
+ [9,] -0.5034288
+[10,]  0.2313302</code></pre>
+</div>
+</div>
+<p>When you call the model, its weight matrices will be built—check that the <code>kernel</code> weights (the <span class=""math inline"">\(m\)</span> in <span class=""math inline"">\(y = mx+b\)</span>) have a shape of <code>(9, 1)</code>:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb57""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb57-1""><a href=""#cb57-1"" aria-hidden=""true"" tabindex=""-1""></a>linear_model<span class=""sc"">$</span>layers[[<span class=""dv"">2</span>]]<span class=""sc"">$</span>kernel</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>&lt;tf.Variable 'dense_1/kernel:0' shape=(9, 1) dtype=float32, numpy=
+array([[ 0.09463781],
+       [-0.04923761],
+       [-0.73521256],
+       [-0.61125565],
+       [ 0.66817605],
+       [ 0.3159597 ],
+       [-0.6834484 ],
+       [ 0.23534286],
+       [-0.00442344]], dtype=float32)&gt;</code></pre>
+</div>
+</div>
+<p>Configure the model with Keras <code>compile()</code> and train with <code>fit()</code> for 100 epochs:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb59""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb59-1""><a href=""#cb59-1"" aria-hidden=""true"" tabindex=""-1""></a>linear_model <span class=""sc"">%&gt;%</span> <span class=""fu"">compile</span>(</span>
+<span id=""cb59-2""><a href=""#cb59-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">optimizer =</span> <span class=""fu"">optimizer_adam</span>(<span class=""at"">learning_rate =</span> <span class=""fl"">0.1</span>),</span>
+<span id=""cb59-3""><a href=""#cb59-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">loss =</span> <span class=""st"">'mean_absolute_error'</span></span>
+<span id=""cb59-4""><a href=""#cb59-4"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb60""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb60-1""><a href=""#cb60-1"" aria-hidden=""true"" tabindex=""-1""></a>history <span class=""ot"">&lt;-</span> linear_model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(</span>
+<span id=""cb60-2""><a href=""#cb60-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_features),</span>
+<span id=""cb60-3""><a href=""#cb60-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_labels),</span>
+<span id=""cb60-4""><a href=""#cb60-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">epochs =</span> <span class=""dv"">100</span>,</span>
+<span id=""cb60-5""><a href=""#cb60-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""co""># Suppress logging.</span></span>
+<span id=""cb60-6""><a href=""#cb60-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span>,</span>
+<span id=""cb60-7""><a href=""#cb60-7"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""co""># Calculate validation results on 20% of the training data.</span></span>
+<span id=""cb60-8""><a href=""#cb60-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">validation_split =</span> <span class=""fl"">0.2</span></span>
+<span id=""cb60-9""><a href=""#cb60-9"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>Using all the inputs in this regression model achieves a much lower training and validation error than the <code>horsepower_model</code>, which had one input:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb61""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb61-1""><a href=""#cb61-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">plot</span>(history)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-60-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+<p>Collect the results on the test set for later:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb62""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb62-1""><a href=""#cb62-1"" aria-hidden=""true"" tabindex=""-1""></a>test_results[[<span class=""st"">'linear_model'</span>]] <span class=""ot"">&lt;-</span> linear_model <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb62-2""><a href=""#cb62-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">evaluate</span>(</span>
+<span id=""cb62-3""><a href=""#cb62-3"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">as.matrix</span>(test_features), </span>
+<span id=""cb62-4""><a href=""#cb62-4"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">as.matrix</span>(test_labels), </span>
+<span id=""cb62-5""><a href=""#cb62-5"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">verbose =</span> <span class=""dv"">0</span></span>
+<span id=""cb62-6""><a href=""#cb62-6"" aria-hidden=""true"" tabindex=""-1""></a>  )</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+</section>
+</section>
+<section id=""regression-with-a-deep-neural-network-dnn"" class=""level2"">
+<h2 class=""anchored"" data-anchor-id=""regression-with-a-deep-neural-network-dnn"">Regression with a deep neural network (DNN)</h2>
+<p>In the previous section, you implemented two linear models for single and multiple inputs.</p>
+<p>Here, you will implement single-input and multiple-input DNN models.</p>
+<p>The code is basically the same except the model is expanded to include some “hidden” non-linear layers. The name “hidden” here just means not directly connected to the inputs or outputs.</p>
+<p>These models will contain a few more layers than the linear model:</p>
+<ul>
+<li>The normalization layer, as before (with <code>horsepower_normalizer</code> for a single-input model and <code>normalizer</code> for a multiple-input model).</li>
+<li>Two hidden, non-linear, <code>Dense</code> layers with the ReLU (<code>relu</code>) activation function nonlinearity.</li>
+<li>A linear <code>Dense</code> single-output layer.</li>
+</ul>
+<p>Both models will use the same training procedure so the <code>compile</code> method is included in the <code>build_and_compile_model</code> function below.</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb63""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb63-1""><a href=""#cb63-1"" aria-hidden=""true"" tabindex=""-1""></a>build_and_compile_model <span class=""ot"">&lt;-</span> <span class=""cf"">function</span>(norm) {</span>
+<span id=""cb63-2""><a href=""#cb63-2"" aria-hidden=""true"" tabindex=""-1""></a>  model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb63-3""><a href=""#cb63-3"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">norm</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb63-4""><a href=""#cb63-4"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">layer_dense</span>(<span class=""dv"">64</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb63-5""><a href=""#cb63-5"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">layer_dense</span>(<span class=""dv"">64</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb63-6""><a href=""#cb63-6"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">layer_dense</span>(<span class=""dv"">1</span>)</span>
+<span id=""cb63-7""><a href=""#cb63-7"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb63-8""><a href=""#cb63-8"" aria-hidden=""true"" tabindex=""-1""></a>  model <span class=""sc"">%&gt;%</span> <span class=""fu"">compile</span>(</span>
+<span id=""cb63-9""><a href=""#cb63-9"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">loss =</span> <span class=""st"">'mean_absolute_error'</span>,</span>
+<span id=""cb63-10""><a href=""#cb63-10"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">optimizer =</span> <span class=""fu"">optimizer_adam</span>(<span class=""fl"">0.001</span>)</span>
+<span id=""cb63-11""><a href=""#cb63-11"" aria-hidden=""true"" tabindex=""-1""></a>  )</span>
+<span id=""cb63-12""><a href=""#cb63-12"" aria-hidden=""true"" tabindex=""-1""></a>  </span>
+<span id=""cb63-13""><a href=""#cb63-13"" aria-hidden=""true"" tabindex=""-1""></a>  model</span>
+<span id=""cb63-14""><a href=""#cb63-14"" aria-hidden=""true"" tabindex=""-1""></a>}</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<section id=""regression-using-a-dnn-and-a-single-input"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""regression-using-a-dnn-and-a-single-input"">Regression using a DNN and a single input</h3>
+<p>Create a DNN model with only <code>'Horsepower'</code> as input and <code>horsepower_normalizer</code> (defined earlier) as the normalization layer:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb64""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb64-1""><a href=""#cb64-1"" aria-hidden=""true"" tabindex=""-1""></a>dnn_horsepower_model <span class=""ot"">&lt;-</span> <span class=""fu"">build_and_compile_model</span>(horsepower_normalizer)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>This model has quite a few more trainable parameters than the linear models:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb65""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb65-1""><a href=""#cb65-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(dnn_horsepower_model)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: ""sequential_2""
+____________________________________________________________________________
+ Layer (type)                Output Shape              Param #   Trainable  
+============================================================================
+ normalization_1 (Normalizat  (None, 1)                3         Y          
+ ion)                                                                       
+ dense_4 (Dense)             (None, 64)                128       Y          
+ dense_3 (Dense)             (None, 64)                4160      Y          
+ dense_2 (Dense)             (None, 1)                 65        Y          
+============================================================================
+Total params: 4,356
+Trainable params: 4,353
+Non-trainable params: 3
+____________________________________________________________________________</code></pre>
+</div>
+</div>
+<p>Train the model with Keras <code>Model$fit</code>:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb67""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb67-1""><a href=""#cb67-1"" aria-hidden=""true"" tabindex=""-1""></a>history <span class=""ot"">&lt;-</span> dnn_horsepower_model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(</span>
+<span id=""cb67-2""><a href=""#cb67-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_features<span class=""sc"">$</span>horsepower),</span>
+<span id=""cb67-3""><a href=""#cb67-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_labels),</span>
+<span id=""cb67-4""><a href=""#cb67-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">validation_split =</span> <span class=""fl"">0.2</span>,</span>
+<span id=""cb67-5""><a href=""#cb67-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span>, </span>
+<span id=""cb67-6""><a href=""#cb67-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">epochs =</span> <span class=""dv"">100</span></span>
+<span id=""cb67-7""><a href=""#cb67-7"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>This model does slightly better than the linear single-input <code>horsepower_model</code>:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb68""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb68-1""><a href=""#cb68-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">plot</span>(history)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-72-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+<p>If you plot the predictions as a function of <code>'horsepower'</code>, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb69""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb69-1""><a href=""#cb69-1"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> <span class=""fu"">seq</span>(<span class=""fl"">0.0</span>, <span class=""dv"">250</span>, <span class=""at"">length.out =</span> <span class=""dv"">251</span>)</span>
+<span id=""cb69-2""><a href=""#cb69-2"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">predict</span>(dnn_horsepower_model, x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb70""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb70-1""><a href=""#cb70-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">ggplot</span>(train_dataset) <span class=""sc"">+</span></span>
+<span id=""cb70-2""><a href=""#cb70-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">geom_point</span>(<span class=""fu"">aes</span>(<span class=""at"">x =</span> horsepower, <span class=""at"">y =</span> mpg, <span class=""at"">color =</span> <span class=""st"">""data""</span>)) <span class=""sc"">+</span></span>
+<span id=""cb70-3""><a href=""#cb70-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">geom_line</span>(<span class=""at"">data =</span> <span class=""fu"">data.frame</span>(x, y), <span class=""fu"">aes</span>(<span class=""at"">x =</span> x, <span class=""at"">y =</span> y, <span class=""at"">color =</span> <span class=""st"">""prediction""</span>))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-76-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+<p>Collect the results on the test set for later:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb71""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb71-1""><a href=""#cb71-1"" aria-hidden=""true"" tabindex=""-1""></a>test_results[[<span class=""st"">'dnn_horsepower_model'</span>]] <span class=""ot"">&lt;-</span> dnn_horsepower_model <span class=""sc"">%&gt;%</span> <span class=""fu"">evaluate</span>(</span>
+<span id=""cb71-2""><a href=""#cb71-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_features<span class=""sc"">$</span>horsepower), </span>
+<span id=""cb71-3""><a href=""#cb71-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_labels),</span>
+<span id=""cb71-4""><a href=""#cb71-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span></span>
+<span id=""cb71-5""><a href=""#cb71-5"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+</section>
+<section id=""regression-using-a-dnn-and-multiple-inputs"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""regression-using-a-dnn-and-multiple-inputs"">Regression using a DNN and multiple inputs</h3>
+<p>Repeat the previous process using all the inputs. The model’s performance slightly improves on the validation dataset.</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb72""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb72-1""><a href=""#cb72-1"" aria-hidden=""true"" tabindex=""-1""></a>dnn_model <span class=""ot"">&lt;-</span> <span class=""fu"">build_and_compile_model</span>(normalizer)</span>
+<span id=""cb72-2""><a href=""#cb72-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(dnn_model)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: ""sequential_3""
+____________________________________________________________________________
+ Layer (type)                Output Shape              Param #   Trainable  
+============================================================================
+ normalization (Normalizatio  (None, 9)                19        Y          
+ n)                                                                         
+ dense_7 (Dense)             (None, 64)                640       Y          
+ dense_6 (Dense)             (None, 64)                4160      Y          
+ dense_5 (Dense)             (None, 1)                 65        Y          
+============================================================================
+Total params: 4,884
+Trainable params: 4,865
+Non-trainable params: 19
+____________________________________________________________________________</code></pre>
+</div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb74""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb74-1""><a href=""#cb74-1"" aria-hidden=""true"" tabindex=""-1""></a>history <span class=""ot"">&lt;-</span> dnn_model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(</span>
+<span id=""cb74-2""><a href=""#cb74-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_features),</span>
+<span id=""cb74-3""><a href=""#cb74-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(train_labels),</span>
+<span id=""cb74-4""><a href=""#cb74-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">validation_split =</span> <span class=""fl"">0.2</span>,</span>
+<span id=""cb74-5""><a href=""#cb74-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span>, </span>
+<span id=""cb74-6""><a href=""#cb74-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">epochs =</span> <span class=""dv"">100</span></span>
+<span id=""cb74-7""><a href=""#cb74-7"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb75""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb75-1""><a href=""#cb75-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">plot</span>(history)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-84-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+<p>Collect the results on the test set:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb76""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb76-1""><a href=""#cb76-1"" aria-hidden=""true"" tabindex=""-1""></a>test_results[[<span class=""st"">'dnn_model'</span>]] <span class=""ot"">&lt;-</span> dnn_model <span class=""sc"">%&gt;%</span> <span class=""fu"">evaluate</span>(</span>
+<span id=""cb76-2""><a href=""#cb76-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_features), </span>
+<span id=""cb76-3""><a href=""#cb76-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_labels), </span>
+<span id=""cb76-4""><a href=""#cb76-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span></span>
+<span id=""cb76-5""><a href=""#cb76-5"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+</section>
+</section>
+<section id=""performance"" class=""level2"">
+<h2 class=""anchored"" data-anchor-id=""performance"">Performance</h2>
+<p>Since all models have been trained, you can review their test set performance:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb77""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb77-1""><a href=""#cb77-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sapply</span>(test_results, <span class=""cf"">function</span>(x) x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>    horsepower_model.loss         linear_model.loss 
+                 3.575989                  2.465263 
+dnn_horsepower_model.loss            dnn_model.loss 
+                 3.201861                  1.801497 </code></pre>
+</div>
+</div>
+<p>These results match the validation error observed during training.</p>
+<section id=""make-predictions"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""make-predictions"">Make predictions</h3>
+<p>You can now make predictions with the <code>dnn_model</code> on the test set using Keras <code>predict()</code> and review the loss:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb79""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb79-1""><a href=""#cb79-1"" aria-hidden=""true"" tabindex=""-1""></a>test_predictions <span class=""ot"">&lt;-</span> <span class=""fu"">predict</span>(dnn_model, <span class=""fu"">as.matrix</span>(test_features))</span>
+<span id=""cb79-2""><a href=""#cb79-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">ggplot</span>(<span class=""fu"">data.frame</span>(<span class=""at"">pred =</span> <span class=""fu"">as.numeric</span>(test_predictions), <span class=""at"">mpg =</span> test_labels<span class=""sc"">$</span>mpg)) <span class=""sc"">+</span></span>
+<span id=""cb79-3""><a href=""#cb79-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">geom_point</span>(<span class=""fu"">aes</span>(<span class=""at"">x =</span> pred, <span class=""at"">y =</span> mpg)) <span class=""sc"">+</span></span>
+<span id=""cb79-4""><a href=""#cb79-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">geom_abline</span>(<span class=""at"">intercept =</span> <span class=""dv"">0</span>, <span class=""at"">slope =</span> <span class=""dv"">1</span>, <span class=""at"">color =</span> <span class=""st"">""blue""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-90-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+</div>
+<p>It appears that the model predicts reasonably well.</p>
+<p>Now, check the error distribution:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb80""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb80-1""><a href=""#cb80-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">qplot</span>(test_predictions <span class=""sc"">-</span> test_labels<span class=""sc"">$</span>mpg, <span class=""at"">geom =</span> <span class=""st"">""density""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output-display"">
+<p><img src=""regression_files/figure-html/unnamed-chunk-92-1.png"" class=""img-fluid"" width=""672""></p>
+</div>
+<div class=""sourceCode cell-code"" id=""cb81""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb81-1""><a href=""#cb81-1"" aria-hidden=""true"" tabindex=""-1""></a>error <span class=""ot"">&lt;-</span> test_predictions <span class=""sc"">-</span> test_labels</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>If you’re happy with the model, save it for later use with <code>Model$save</code>:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb82""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb82-1""><a href=""#cb82-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">save_model_tf</span>(dnn_model, <span class=""st"">'dnn_model'</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<p>If you reload the model, it gives identical output:</p>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb83""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb83-1""><a href=""#cb83-1"" aria-hidden=""true"" tabindex=""-1""></a>reloaded <span class=""ot"">&lt;-</span> <span class=""fu"">load_model_tf</span>(<span class=""st"">'dnn_model'</span>)</span>
+<span id=""cb83-2""><a href=""#cb83-2"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb83-3""><a href=""#cb83-3"" aria-hidden=""true"" tabindex=""-1""></a>test_results[[<span class=""st"">'reloaded'</span>]] <span class=""ot"">&lt;-</span> reloaded <span class=""sc"">%&gt;%</span> <span class=""fu"">evaluate</span>(</span>
+<span id=""cb83-4""><a href=""#cb83-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_features), </span>
+<span id=""cb83-5""><a href=""#cb83-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">as.matrix</span>(test_labels), </span>
+<span id=""cb83-6""><a href=""#cb83-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">verbose =</span> <span class=""dv"">0</span></span>
+<span id=""cb83-7""><a href=""#cb83-7"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</div>
+<div class=""cell"">
+<div class=""sourceCode cell-code"" id=""cb84""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb84-1""><a href=""#cb84-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sapply</span>(test_results, <span class=""cf"">function</span>(x) x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>    horsepower_model.loss         linear_model.loss 
+                 3.575989                  2.465263 
+dnn_horsepower_model.loss            dnn_model.loss 
+                 3.201861                  1.801497 
+            reloaded.loss 
+                 1.801497 </code></pre>
+</div>
+</div>
+</section>
+</section>
+<section id=""conclusion"" class=""level2"">
+<h2 class=""anchored"" data-anchor-id=""conclusion"">Conclusion</h2>
+<p>This notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:</p>
+<ul>
+<li>Mean squared error (MSE) (<code>loss_mean_squared_error()</code>) and mean absolute error (MAE) (<code>loss_mean_absolute_error()</code>) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.</li>
+<li>Similarly, evaluation metrics used for regression differ from classification.</li>
+<li>When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.</li>
+<li>Overfitting is a common problem for DNN models, though it wasn’t a problem for this tutorial. Visit the <a href=""overfit_and_underfit.qmd"">Overfit and underfit</a> tutorial for more help with this.</li>
+</ul>
+
+
+</section>
+
+</main> <!-- /main -->
+<script id=""quarto-html-after-body"" type=""application/javascript"">
+window.document.addEventListener(""DOMContentLoaded"", function (event) {
+  const disableStylesheet = (stylesheets) => {
+    for (let i=0; i < stylesheets.length; i++) {
+      const stylesheet = stylesheets[i];
+      stylesheet.rel = 'prefetch';
+    }
+  }
+  const enableStylesheet = (stylesheets) => {
+    for (let i=0; i < stylesheets.length; i++) {
+      const stylesheet = stylesheets[i];
+      stylesheet.rel = 'stylesheet';
+    }
+  }
+  const manageTransitions = (selector, allowTransitions) => {
+    const els = window.document.querySelectorAll(selector);
+    for (let i=0; i < els.length; i++) {
+      const el = els[i];
+      if (allowTransitions) {
+        el.classList.remove('notransition');
+      } else {
+        el.classList.add('notransition');
+      }
+    }
+  }
+  const toggleColorMode = (alternate) => {
+    // Switch the stylesheets
+    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
+    manageTransitions('#quarto-margin-sidebar .nav-link', false);
+    if (alternate) {
+      enableStylesheet(alternateStylesheets);
+    } else {
+      disableStylesheet(alternateStylesheets);
+    }
+    manageTransitions('#quarto-margin-sidebar .nav-link', true);
+    // Switch the toggles
+    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
+    for (let i=0; i < toggles.length; i++) {
+      const toggle = toggles[i];
+      if (toggle) {
+        if (alternate) {
+          toggle.classList.add(""alternate"");     
+        } else {
+          toggle.classList.remove(""alternate"");
+        }
+      }
+    }
+  }
+  const isFileUrl = () => { 
+    return window.location.protocol === 'file:';
+  }
+  const hasAlternateSentinel = () => {  
+    let styleSentinel = getColorSchemeSentinel();
+    if (styleSentinel !== null) {
+      return styleSentinel === ""alternate"";
+    } else {
+      return false;
+    }
+  }
+  const setStyleSentinel = (alternate) => {
+    const value = alternate ? ""alternate"" : ""default"";
+    if (!isFileUrl()) {
+      window.localStorage.setItem(""quarto-color-scheme"", value);
+    } else {
+      localAlternateSentinel = value;
+    }
+  }
+  const getColorSchemeSentinel = () => {
+    if (!isFileUrl()) {
+      const storageValue = window.localStorage.getItem(""quarto-color-scheme"");
+      return storageValue != null ? storageValue : localAlternateSentinel;
+    } else {
+      return localAlternateSentinel;
+    }
+  }
+  let localAlternateSentinel = 'default';
+  // Dark / light mode switch
+  window.quartoToggleColorScheme = () => {
+    // Read the current dark / light value 
+    let toAlternate = !hasAlternateSentinel();
+    toggleColorMode(toAlternate);
+    setStyleSentinel(toAlternate);
+  };
+  // Ensure there is a toggle, if there isn't float one in the top right
+  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
+    const a = window.document.createElement('a');
+    a.classList.add('top-right');
+    a.classList.add('quarto-color-scheme-toggle');
+    a.href = """";
+    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
+    const i = window.document.createElement(""i"");
+    i.classList.add('bi');
+    a.appendChild(i);
+    window.document.body.appendChild(a);
+  }
+  // Switch to dark mode if need be
+  if (hasAlternateSentinel()) {
+    toggleColorMode(true);
+  } 
+  const icon = """";
+  const anchorJS = new window.AnchorJS();
+  anchorJS.options = {
+    placement: 'right',
+    icon: icon
+  };
+  anchorJS.add('.anchored');
+  const clipboard = new window.ClipboardJS('.code-copy-button', {
+    target: function(trigger) {
+      return trigger.previousElementSibling;
+    }
+  });
+  clipboard.on('success', function(e) {
+    // button target
+    const button = e.trigger;
+    // don't keep focus
+    button.blur();
+    // flash ""checked""
+    button.classList.add('code-copy-button-checked');
+    var currentTitle = button.getAttribute(""title"");
+    button.setAttribute(""title"", ""Copied!"");
+    setTimeout(function() {
+      button.setAttribute(""title"", currentTitle);
+      button.classList.remove('code-copy-button-checked');
+    }, 1000);
+    // clear code selection
+    e.clearSelection();
+  });
+  function tippyHover(el, contentFn) {
+    const config = {
+      allowHTML: true,
+      content: contentFn,
+      maxWidth: 500,
+      delay: 100,
+      arrow: false,
+      appendTo: function(el) {
+          return el.parentElement;
+      },
+      interactive: true,
+      interactiveBorder: 10,
+      theme: 'quarto',
+      placement: 'bottom-start'
+    };
+    window.tippy(el, config); 
+  }
+  const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
+  for (var i=0; i<noterefs.length; i++) {
+    const ref = noterefs[i];
+    tippyHover(ref, function() {
+      let href = ref.getAttribute('href');
+      try { href = new URL(href).hash; } catch {}
+      const id = href.replace(/^#\/?/, """");
+      const note = window.document.getElementById(id);
+      return note.innerHTML;
+    });
+  }
+  var bibliorefs = window.document.querySelectorAll('a[role=""doc-biblioref""]');
+  for (var i=0; i<bibliorefs.length; i++) {
+    const ref = bibliorefs[i];
+    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
+    tippyHover(ref, function() {
+      var popup = window.document.createElement('div');
+      cites.forEach(function(cite) {
+        var citeDiv = window.document.createElement('div');
+        citeDiv.classList.add('hanging-indent');
+        citeDiv.classList.add('csl-entry');
+        var biblioDiv = window.document.getElementById('ref-' + cite);
+        if (biblioDiv) {
+          citeDiv.innerHTML = biblioDiv.innerHTML;
+        }
+        popup.appendChild(citeDiv);
+      });
+      return popup.innerHTML;
+    });
+  }
+});
+</script>
+<nav class=""page-navigation"">
+  <div class=""nav-page nav-page-previous"">
+      <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""pagination-link"">
+        <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text"">Text Classification with Hub</span>
+      </a>          
+  </div>
+  <div class=""nav-page nav-page-next"">
+  </div>
+</nav>
+</div> <!-- /content -->
+
+
+
+</body></html>
\ No newline at end of file

---FILE: _site/tutorials/keras/text_classification.html---
@@ -234,7 +234,12 @@ <h1 class=""quarto-secondary-nav-title"">Basic text classification</h1>
 </li>
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
-  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with TensorFlow Hub</a>
+  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with Hub</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/regression.html"" class=""sidebar-item-text sidebar-link"">Regression</a>
   </div>
 </li>
     </ul>
@@ -917,7 +922,7 @@ <h2 class=""anchored"" data-anchor-id=""learning-more"">Learning more</h2>
   </div>
   <div class=""nav-page nav-page-next"">
       <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""pagination-link"">
-        <span class=""nav-page-text"">Text Classification with TensorFlow Hub</span> <i class=""bi bi-arrow-right-short""></i>
+        <span class=""nav-page-text"">Text Classification with Hub</span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
 </nav>

---FILE: _site/tutorials/keras/text_classification_with_hub.html---
@@ -88,6 +88,7 @@
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
 <meta name=""quarto:offset"" content=""../../"">
+<link href=""../../tutorials/keras/regression.html"" rel=""next"">
 <link href=""../../tutorials/keras/text_classification.html"" rel=""prev"">
 <link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
@@ -235,6 +236,11 @@ <h1 class=""quarto-secondary-nav-title"">Text Classification with Hub</h1>
   <div class=""sidebar-item-container""> 
   <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link active"">Text Classification with Hub</a>
   </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/regression.html"" class=""sidebar-item-text sidebar-link"">Regression</a>
+  </div>
 </li>
     </ul>
   </li>
@@ -656,6 +662,9 @@ <h2 class=""anchored"" data-anchor-id=""further-reading"">Further reading</h2>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
+      <a href=""../../tutorials/keras/regression.html"" class=""pagination-link"">
+        <span class=""nav-page-text"">Regression</span> <i class=""bi bi-arrow-right-short""></i>
+      </a>
   </div>
 </nav>
 </div> <!-- /content -->

---FILE: _site/tutorials/quickstart/advanced.html---
@@ -88,7 +88,7 @@
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
 <meta name=""quarto:offset"" content=""../../"">
-<link href=""../../tutorials/keras/text_classification.html"" rel=""next"">
+<link href=""../../tutorials/keras/classification.html"" rel=""next"">
 <link href=""../../tutorials/quickstart/beginner.html"" rel=""prev"">
 <link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
@@ -224,12 +224,22 @@ <h1 class=""quarto-secondary-nav-title"">Advanced</h1>
     <ul id=""quarto-sidebar-section-2"" class=""collapse list-unstyled sidebar-section depth1 show"">  
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/classification.html"" class=""sidebar-item-text sidebar-link"">Basic Image Classification</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
   <a href=""../../tutorials/keras/text_classification.html"" class=""sidebar-item-text sidebar-link"">Basic text classification</a>
   </div>
 </li>
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
-  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with TensorFlow Hub</a>
+  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with Hub</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/regression.html"" class=""sidebar-item-text sidebar-link"">Regression</a>
   </div>
 </li>
     </ul>
@@ -603,8 +613,8 @@ <h1>TensorFlow 2 quickstart for experts</h1>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""../../tutorials/keras/text_classification.html"" class=""pagination-link"">
-        <span class=""nav-page-text"">Basic text classification</span> <i class=""bi bi-arrow-right-short""></i>
+      <a href=""../../tutorials/keras/classification.html"" class=""pagination-link"">
+        <span class=""nav-page-text"">Basic Image Classification</span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
 </nav>

---FILE: _site/tutorials/quickstart/beginner.html---
@@ -224,12 +224,22 @@ <h1 class=""quarto-secondary-nav-title"">Beginner</h1>
     <ul id=""quarto-sidebar-section-2"" class=""collapse list-unstyled sidebar-section depth1 show"">  
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/classification.html"" class=""sidebar-item-text sidebar-link"">Basic Image Classification</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
   <a href=""../../tutorials/keras/text_classification.html"" class=""sidebar-item-text sidebar-link"">Basic text classification</a>
   </div>
 </li>
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
-  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with TensorFlow Hub</a>
+  <a href=""../../tutorials/keras/text_classification_with_hub.html"" class=""sidebar-item-text sidebar-link"">Text Classification with Hub</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tutorials/keras/regression.html"" class=""sidebar-item-text sidebar-link"">Regression</a>
   </div>
 </li>
     </ul>

---FILE: tutorials/keras/.gitignore---
@@ -1,2 +1,3 @@
 aclImdb
 aclImdb_v1.tar.gz
+dnn_model

---FILE: tutorials/keras/img/.gitignore---
@@ -0,0 +1,2 @@
+auto-mpg.data
+auto-mpg.data-original

---FILE: tutorials/keras/regression.qmd---
@@ -0,0 +1,509 @@
+---
+title: Regression
+---
+
+In a *regression* problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a *classification* problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).
+
+This tutorial uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.
+
+This example uses the Keras API. (Visit the Keras [tutorials](../keras) and [guides](../../guides/keras) to learn more.)
+
+```{r setup}
+library(tensorflow)
+library(keras)
+library(tidyverse)
+library(tidymodels)
+```
+
+## The Auto MPG dataset
+
+The dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).
+
+### Get the data
+
+First download and import the dataset using pandas:
+
+```{r}
+url <- ""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data""
+col_names <- c(""mpg"",""cylinders"",""displacement"",""horsepower"",""weight"",""acceleration"",""model_year"", ""origin"",""car_name"")
+
+raw_dataset <- read.table(
+  url, 
+  header = T, 
+  col.names = col_names, 
+  na.strings = ""?""
+)
+```
+
+```{r}
+dataset <- raw_dataset %>% select(-car_name)
+tail(dataset)
+```
+
+### Clean the data
+
+The dataset contains a few unknown values:
+
+```{r}
+lapply(dataset, function(x) sum(is.na(x))) %>% str()
+```
+
+Drop those rows to keep this initial tutorial simple:
+
+```{r}
+dataset <- na.omit(dataset)
+```
+
+The `""origin""` column is categorical, not numeric. So the next step is to one-hot encode the values in the column with the `recipes` package. 
+
+Note: You can set up the `keras_model()` to do this kind of transformation for you but that's beyond the scope of this tutorial. Check out the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) or [Load CSV data](../load_data/csv.qmd) tutorials for examples.
+
+```{r}
+library(recipes)
+dataset <- recipe(mpg ~ ., dataset) %>% 
+  step_num2factor(origin, levels = c(""USA"", ""Europe"", ""Japan"")) %>% 
+  step_dummy(origin, one_hot = TRUE) %>% 
+  prep() %>% 
+  bake(new_data = NULL)
+```
+
+```{r}
+glimpse(dataset)
+```
+
+### Split the data into training and test sets
+
+Now, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.
+
+```{r}
+split <- initial_split(dataset, 0.8)
+train_dataset <- training(split)
+test_dataset <- testing(split)
+```
+
+### Inspect the data
+
+Review the joint distribution of a few pairs of columns from the training set.
+
+The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other.
+
+```{r}
+train_dataset %>% 
+  select(mpg, cylinders, displacement, weight) %>% 
+  GGally::ggpairs()
+```
+
+Let's also check the overall statistics. Note how each feature covers a very different range:
+
+```{r}
+skimr::skim(train_dataset)
+```
+
+### Split features from labels
+
+Separate the target value—the ""label""—from the features. This label is the value that you will train the model to predict.
+
+```{r}
+train_features <- train_dataset %>% select(-mpg)
+test_features <- test_dataset %>% select(-mpg)
+
+train_labels <- train_dataset %>% select(mpg)
+test_labels <- test_dataset %>% select(mpg)
+```
+
+## Normalization
+
+In the table of statistics it's easy to see how different the ranges of each feature are:
+
+```{r}
+my_skim <- skimr::skim_with(numeric = skimr::sfl(mean, sd))
+train_dataset %>% 
+  select(where(~is.numeric(.x))) %>% 
+  pivot_longer(
+    cols = everything(), names_to = ""variable"", values_to = ""values"") %>% 
+  group_by(variable) %>% 
+  summarise(mean = mean(values), sd = sd(values))
+```
+
+It is good practice to normalize features that use different scales and ranges.
+
+One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.
+
+Although a model *might* converge without feature normalization, normalization makes training much more stable.
+
+Note: There is no advantage to normalizing the one-hot features—it is done here for simplicity. For more details on how to use the preprocessing layers, refer to the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) tutorial.
+
+### The Normalization layer
+
+The `layer_normalization()` is a clean and simple way to add feature normalization into your model.
+
+The first step is to create the layer:
+
+```{r}
+normalizer <- layer_normalization(axis = -1L)
+```
+
+Then, fit the state of the preprocessing layer to the data by calling `adapt()`:
+
+```{r}
+normalizer %>% adapt(as.matrix(train_features))
+```
+
+Calculate the mean and variance, and store them in the layer:
+
+```{r}
+print(normalizer$mean)
+```
+
+When the layer is called, it returns the input data, with each feature independently normalized.
+
+```{r}
+first <- as.matrix(train_features[1,])
+
+cat('First example:', first)
+cat('Normalized:', as.matrix(normalizer(first)))
+```
+
+## Linear regression
+
+Before building a deep neural network model, start with linear regression using one and several variables.
+
+### Linear regression with one variable
+
+Begin with a single-variable linear regression to predict `'mpg'` from `'horsepower'`.
+
+Training a model with Keras typically starts by defining the model architecture. Use a Sequential model, which [represents a sequence of steps](https://www.tensorflow.org/guide/keras/sequential_model).
+
+There are two steps in your single-variable linear regression model:
+
+- Normalize the `'horsepower'` input features using the `normalization` preprocessing layer.
+- Apply a linear transformation ($y = mx+b$) to produce 1 output using a linear layer (`dense`).
+
+The number of _inputs_ can either be set by the `input_shape` argument, or automatically when the model is run for the first time.
+
+First, create a matrix made of the `'horsepower'` features. Then, instantiate the `layer_normalization` and fit its state to the `horsepower` data:
+
+```{r}
+horsepower <- matrix(train_features$horsepower)
+horsepower_normalizer <- layer_normalization(input_shape = shape(1), axis = NULL)
+horsepower_normalizer %>% adapt(horsepower)
+```
+
+Build the Keras Sequential model:
+
+```{r}
+horsepower_model <- keras_model_sequential() %>% 
+  horsepower_normalizer() %>% 
+  layer_dense(units = 1)
+
+summary(horsepower_model)
+```
+
+This model will predict `'mpg'` from `'horsepower'`.
+
+Run the untrained model on the first 10 'horsepower' values. The output won't be good, but notice that it has the expected shape of `(10, 1)`:
+
+```{r}
+predict(horsepower_model, horsepower[1:10,])
+```
+
+Once the model is built, configure the training procedure using the Keras `compile()` method. The most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized (`mean_absolute_error`) and how (using the `optimizer_adam`).
+
+```{r}
+horsepower_model %>% compile(
+  optimizer = optimizer_adam(learning_rate = 0.1),
+  loss = 'mean_absolute_error'
+)
+```
+
+Use Keras `fit()` to execute the training for 100 epochs:
+
+```{r}
+history <- horsepower_model %>% fit(
+  as.matrix(train_features$horsepower),
+  as.matrix(train_labels),
+  epochs = 100,
+  # Suppress logging.
+  verbose = 0,
+  # Calculate validation results on 20% of the training data.
+  validation_split = 0.2
+)
+```
+
+Visualize the model's training progress using the stats stored in the `history` object:
+
+```{r}
+plot(history)
+```
+Collect the results on the test set for later:
+
+```{r}
+test_results <- list()
+test_results[[""horsepower_model""]] <- horsepower_model %>% evaluate(
+  as.matrix(test_features$horsepower),
+  as.matrix(test_labels), 
+  verbose = 0
+)
+```
+
+Since this is a single variable regression, it's easy to view the model's predictions as a function of the input:
+
+```{r}
+x <- seq(0, 250, length.out = 251)
+y <- predict(horsepower_model, x)
+```
+
+```{r}
+ggplot(train_dataset) +
+  geom_point(aes(x = horsepower, y = mpg, color = ""data"")) +
+  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = ""prediction""))
+```
+
+### Linear regression with multiple inputs
+
+
+You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.
+
+Create a two-step Keras Sequential model again with the first layer being `normalizer` (`layer_normalization(axis = -1)`) you defined earlier and adapted to the whole dataset:
+
+```{r}
+linear_model <- keras_model_sequential() %>% 
+  normalizer() %>% 
+  layer_dense(units = 1)
+```
+
+When you call `predict()` on a batch of inputs, it produces `units = 1` outputs for each example:
+
+```{r}
+predict(linear_model, as.matrix(train_features[1:10, ]))
+```
+
+When you call the model, its weight matrices will be built—check that the `kernel` weights (the $m$ in $y = mx+b$) have a shape of `(9, 1)`:
+
+```{r}
+linear_model$layers[[2]]$kernel
+```
+
+Configure the model with Keras `compile()` and train with `fit()` for 100 epochs:
+
+```{r}
+linear_model %>% compile(
+  optimizer = optimizer_adam(learning_rate = 0.1),
+  loss = 'mean_absolute_error'
+)
+```
+
+```{r}
+history <- linear_model %>% fit(
+  as.matrix(train_features),
+  as.matrix(train_labels),
+  epochs = 100,
+  # Suppress logging.
+  verbose = 0,
+  # Calculate validation results on 20% of the training data.
+  validation_split = 0.2
+)
+```
+
+Using all the inputs in this regression model achieves a much lower training and validation error than the `horsepower_model`, which had one input:
+
+```{r}
+plot(history)
+```
+
+Collect the results on the test set for later:
+
+```{r}
+test_results[['linear_model']] <- linear_model %>% 
+  evaluate(
+    as.matrix(test_features), 
+    as.matrix(test_labels), 
+    verbose = 0
+  )
+```
+
+## Regression with a deep neural network (DNN)
+
+
+In the previous section, you implemented two linear models for single and multiple inputs.
+
+Here, you will implement single-input and multiple-input DNN models.
+
+The code is basically the same except the model is expanded to include some ""hidden"" non-linear layers. The name ""hidden"" here just means not directly connected to the inputs or outputs.
+
+These models will contain a few more layers than the linear model:
+
+* The normalization layer, as before (with `horsepower_normalizer` for a single-input model and `normalizer` for a multiple-input model).
+* Two hidden, non-linear, `Dense` layers with the ReLU (`relu`) activation function nonlinearity.
+* A linear `Dense` single-output layer.
+
+Both models will use the same training procedure so the `compile` method is included in the `build_and_compile_model` function below.
+
+```{r}
+build_and_compile_model <- function(norm) {
+  model <- keras_model_sequential() %>% 
+    norm() %>% 
+    layer_dense(64, activation = 'relu') %>% 
+    layer_dense(64, activation = 'relu') %>% 
+    layer_dense(1)
+
+  model %>% compile(
+    loss = 'mean_absolute_error',
+    optimizer = optimizer_adam(0.001)
+  )
+  
+  model
+}
+```
+
+### Regression using a DNN and a single input
+
+
+Create a DNN model with only `'Horsepower'` as input and `horsepower_normalizer` (defined earlier) as the normalization layer:
+
+```{r}
+dnn_horsepower_model <- build_and_compile_model(horsepower_normalizer)
+```
+
+This model has quite a few more trainable parameters than the linear models:
+
+```{r}
+summary(dnn_horsepower_model)
+```
+
+Train the model with Keras `Model$fit`:
+
+```{r}
+history <- dnn_horsepower_model %>% fit(
+  as.matrix(train_features$horsepower),
+  as.matrix(train_labels),
+  validation_split = 0.2,
+  verbose = 0, 
+  epochs = 100
+)
+```
+
+This model does slightly better than the linear single-input `horsepower_model`:
+
+```{r}
+plot(history)
+```
+
+If you plot the predictions as a function of `'horsepower'`, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:
+
+```{r}
+x <- seq(0.0, 250, length.out = 251)
+y <- predict(dnn_horsepower_model, x)
+```
+
+```{r}
+ggplot(train_dataset) +
+  geom_point(aes(x = horsepower, y = mpg, color = ""data"")) +
+  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = ""prediction""))
+```
+
+Collect the results on the test set for later:
+
+```{r}
+test_results[['dnn_horsepower_model']] <- dnn_horsepower_model %>% evaluate(
+  as.matrix(test_features$horsepower), 
+  as.matrix(test_labels),
+  verbose = 0
+)
+```
+
+### Regression using a DNN and multiple inputs
+
+
+Repeat the previous process using all the inputs. The model's performance slightly improves on the validation dataset.
+
+```{r}
+dnn_model <- build_and_compile_model(normalizer)
+summary(dnn_model)
+```
+
+```{r}
+history <- dnn_model %>% fit(
+  as.matrix(train_features),
+  as.matrix(train_labels),
+  validation_split = 0.2,
+  verbose = 0, 
+  epochs = 100
+)
+```
+
+```{r}
+plot(history)
+```
+
+Collect the results on the test set:
+
+```{r}
+test_results[['dnn_model']] <- dnn_model %>% evaluate(
+  as.matrix(test_features), 
+  as.matrix(test_labels), 
+  verbose = 0
+)
+```
+
+## Performance
+
+
+Since all models have been trained, you can review their test set performance:
+
+```{r}
+sapply(test_results, function(x) x)
+```
+
+These results match the validation error observed during training.
+
+### Make predictions
+
+You can now make predictions with the `dnn_model` on the test set using Keras `predict()` and review the loss:
+
+```{r}
+test_predictions <- predict(dnn_model, as.matrix(test_features))
+ggplot(data.frame(pred = as.numeric(test_predictions), mpg = test_labels$mpg)) +
+  geom_point(aes(x = pred, y = mpg)) +
+  geom_abline(intercept = 0, slope = 1, color = ""blue"")
+```
+
+It appears that the model predicts reasonably well.
+
+Now, check the error distribution:
+
+```{r}
+qplot(test_predictions - test_labels$mpg, geom = ""density"")
+error <- test_predictions - test_labels
+```
+
+If you're happy with the model, save it for later use with `Model$save`:
+
+```{r}
+save_model_tf(dnn_model, 'dnn_model')
+```
+
+If you reload the model, it gives identical output:
+
+```{r}
+reloaded <- load_model_tf('dnn_model')
+
+test_results[['reloaded']] <- reloaded %>% evaluate(
+  as.matrix(test_features), 
+  as.matrix(test_labels), 
+  verbose = 0
+)
+```
+
+```{r}
+sapply(test_results, function(x) x)
+```
+
+## Conclusion
+
+This notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:
+
+- Mean squared error (MSE) (`loss_mean_squared_error()`) and mean absolute error (MAE) (`loss_mean_absolute_error()`) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.
+- Similarly, evaluation metrics used for regression differ from classification.
+- When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.
+- Overfitting is a common problem for DNN models, though it wasn't a problem for this tutorial. Visit the [Overfit and underfit](overfit_and_underfit.qmd) tutorial for more help with this."
rstudio,tensorflow.rstudio.com,53bc8a2c665cbae716e69a54f01792e914339b57,Daniel Falbel,dfalbel@gmail.com,2022-06-23T19:31:17Z,Daniel Falbel,dfalbel@gmail.com,2022-06-23T19:31:17Z,fix title,guides/tensorflow/intro_to_graphs.qmd,True,False,True,False,1,1,2,"---FILE: guides/tensorflow/intro_to_graphs.qmd---
@@ -1,5 +1,5 @@
 ---
-title: Intro To_graphs
+title: Introduction to graphs
 ---
 
 ##### Copyright 2020 The TensorFlow Authors."
rstudio,tensorflow.rstudio.com,0663286fbb2b903e683a8a4713bcbe87ad74cac9,Tomasz Kalinowski,kalinowskit@gmail.com,2022-06-06T16:37:01Z,Tomasz Kalinowski,kalinowskit@gmail.com,2022-06-06T16:37:01Z,fix render issues,.Rprofile;_freeze/keras/guides/customizing_what_happens_in_fit/execute-results/html.json;_freeze/keras/guides/functional_api/execute-results/html.json;_freeze/keras/guides/functional_api/figure-html/unnamed-chunk-12-1.png;_freeze/keras/guides/functional_api/figure-html/unnamed-chunk-13-1.png;_freeze/keras/guides/functional_api/figure-html/unnamed-chunk-20-1.png;_freeze/keras/guides/functional_api/figure-html/unnamed-chunk-25-1.png;_freeze/keras/guides/making_new_layers_and_models_via_subclassing/execute-results/html.json;_freeze/keras/guides/preprocessing_layers/execute-results/html.json;_freeze/keras/guides/python_subclasses/execute-results/html.json;_freeze/keras/guides/sequential_model/execute-results/html.json;_freeze/keras/guides/transfer_learning/execute-results/html.json;_freeze/keras/guides/transfer_learning/figure-html/unnamed-chunk-18-1.png;_freeze/keras/guides/working_with_rnns/execute-results/html.json;_freeze/keras/guides/writing_your_own_callbacks/execute-results/html.json;_freeze/tensorflow/guide/autodiff/execute-results/html.json;_freeze/tensorflow/guide/autodiff/figure-html/unnamed-chunk-23-1.png;_freeze/tensorflow/guide/basics/execute-results/html.json;_freeze/tensorflow/guide/basics/libs/bootstrap/bootstrap-icons.css;_freeze/tensorflow/guide/basics/libs/bootstrap/bootstrap-icons.woff;_freeze/tensorflow/guide/basics/libs/bootstrap/bootstrap.min.css;_freeze/tensorflow/guide/basics/libs/bootstrap/bootstrap.min.js;_freeze/tensorflow/guide/basics/libs/clipboard/clipboard.min.js;_freeze/tensorflow/guide/basics/libs/quarto-html/anchor.min.js;_freeze/tensorflow/guide/basics/libs/quarto-html/popper.min.js;_freeze/tensorflow/guide/basics/libs/quarto-html/quarto-syntax-highlighting.css;_freeze/tensorflow/guide/basics/libs/quarto-html/quarto.js;_freeze/tensorflow/guide/basics/libs/quarto-html/tippy.css;_freeze/tensorflow/guide/basics/libs/quarto-html/tippy.umd.min.js;_freeze/tensorflow/guide/intro_to_graphs/execute-results/html.json;_freeze/tensorflow/guide/intro_to_graphs/figure-html/unnamed-chunk-24-1.png;_freeze/tensorflow/guide/ragged_tensor/execute-results/html.json;_freeze/tensorflow/guide/tensor/execute-results/html.json;_freeze/tensorflow/guide/variable/execute-results/html.json;_quarto.yml;_site/css/styles.css;_site/index.html;_site/keras/guides/customizing_what_happens_in_fit.html;_site/keras/guides/functional_api.html;_site/keras/guides/functional_api_files/figure-html/unnamed-chunk-12-1.png;_site/keras/guides/functional_api_files/figure-html/unnamed-chunk-13-1.png;_site/keras/guides/functional_api_files/figure-html/unnamed-chunk-20-1.png;_site/keras/guides/functional_api_files/figure-html/unnamed-chunk-25-1.png;_site/keras/guides/making_new_layers_and_models_via_subclassing.html;_site/keras/guides/preprocessing_layers.html;_site/keras/guides/python_subclasses.html;_site/keras/guides/sequential_model.html;_site/keras/guides/transfer_learning.html;_site/keras/guides/transfer_learning_files/figure-html/unnamed-chunk-18-1.png;_site/keras/guides/working_with_rnns.html;_site/keras/guides/writing_your_own_callbacks.html;_site/listings.json;_site/search.json;_site/site_libs/bootstrap/bootstrap-dark.min.css;_site/site_libs/bootstrap/bootstrap.min.css;_site/site_libs/quarto-html/quarto-syntax-highlighting-dark.css;_site/sitemap.xml;_site/tensorflow/guide/autodiff.html;_site/tensorflow/guide/basics.html;_site/tensorflow/guide/basics_files/figure-html/unnamed-chunk-23-1.png;_site/tensorflow/guide/basics_files/figure-html/unnamed-chunk-26-1.png;_site/tensorflow/guide/basics_files/figure-html/unnamed-chunk-28-1.png;_site/tensorflow/guide/basics_files/figure-html/unnamed-chunk-31-1.png;_site/tensorflow/guide/basics_files/libs/bootstrap/bootstrap-icons.css;_site/tensorflow/guide/basics_files/libs/bootstrap/bootstrap-icons.woff;_site/tensorflow/guide/basics_files/libs/bootstrap/bootstrap.min.css;_site/tensorflow/guide/basics_files/libs/bootstrap/bootstrap.min.js;_site/tensorflow/guide/basics_files/libs/clipboard/clipboard.min.js;_site/tensorflow/guide/basics_files/libs/quarto-html/anchor.min.js;_site/tensorflow/guide/basics_files/libs/quarto-html/popper.min.js;_site/tensorflow/guide/basics_files/libs/quarto-html/quarto-syntax-highlighting.css;_site/tensorflow/guide/basics_files/libs/quarto-html/quarto.js;_site/tensorflow/guide/basics_files/libs/quarto-html/tippy.css;_site/tensorflow/guide/basics_files/libs/quarto-html/tippy.umd.min.js;_site/tensorflow/guide/images/tensor/ragged.png;_site/tensorflow/guide/images/tensor/strings.png;_site/tensorflow/guide/intro_to_graphs.html;_site/tensorflow/guide/intro_to_graphs_files/figure-html/unnamed-chunk-24-1.png;_site/tensorflow/guide/tensor.html;_site/tensorflow/guide/variable.html;css/styles.css;css/theme-dark.scss;css/theme.scss;index.qmd;keras/guides/.Rprofile;keras/guides/functional_api.qmd;keras/guides/sequential_model.qmd;keras/guides/transfer_learning.qmd;tensorflow/guide/.Rprofile;tensorflow/guide/tensor.qmd,True,False,True,False,3851,8325,12176,"---FILE: .Rprofile---
@@ -32,7 +32,15 @@ options(
 setHook(""plot.new"", function() par(las = 1))
 
 setHook(packageEvent(""reticulate"", ""onLoad""),
-        function(...)
+        function(...) {
           try(reticulate::use_virtualenv(""r-tensorflow-site"", required = TRUE))
-)
+        })
+
+# setHook(""reticulate::tensorflow::load"", function() {
+#   # suppressPackageStartupMessages(loadNamespace(""tensorflow""))
+#
+#   msg <- capture.output(print(tensorflow::tf_config()))
+#   writeLines(msg, ""./.tf_config"")
+#   system(paste(""echo"", shQuote(paste0(collapse = ""\n"", msg)), ""1>&2""))
+# })
 

---FILE: _freeze/keras/guides/customizing_what_happens_in_fit/execute-results/html.json---
@@ -1,14 +1,14 @@
 {
   ""hash"": ""700dc92b7c243ecbcbe4fc5842d501ed"",
   ""result"": {
-    ""markdown"": ""---\ntitle: Customizing what happens in `fit()`\nauthor: \n  - name: Francois Chollet\n    url: https://twitter.com/fchollet\n  - name: Tomasz Kalinowski\n    url: https://github.com/t-kalinowski\nexecute:\n  eval: true\n---\n\n## Introduction\n\nWhen you're doing supervised learning, you can use `fit()` and\neverything works smoothly.\n\nWhen you need to write your own training loop from scratch, you can use\nthe `GradientTape` and take control of every little detail.\n\nBut what if you need a custom training algorithm, but you still want to\nbenefit from the convenient features of `fit()`, such as callbacks,\nbuilt-in distribution support, or step fusing?\n\nA core principle of Keras is **progressive disclosure of complexity**.\nYou should always be able to get into lower-level workflows in a gradual\nway. You shouldn't fall off a cliff if the high-level functionality\ndoesn't exactly match your use case. You should be able to gain more\ncontrol over the small details while retaining a commensurate amount of\nhigh-level convenience.\n\nWhen you need to customize what `fit()` does, you should **override the\ntraining step function of the `Model` class**. This is the function that\nis called by `fit()` for every batch of data. You will then be able to\ncall `fit()` as usual -- and it will be running your own learning\nalgorithm.\n\nNote that this pattern does not prevent you from building models with\nthe Functional API. You can do this whether you're building `Sequential`\nmodels, Functional API models, or subclassed models.\n\nLet's see how that works.\n\n## Setup\n\nRequires TensorFlow 2.2 or later.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n## A first simple example\n\nLet's start from a simple example:\n\n-   We create a new model class by calling `new_model_class()`.\n-   We just override the method `train_step(data)`.\n-   We return a dictionary mapping metric names (including the loss) to\n    their current value.\n\nThe input argument `data` is what gets passed to fit as training data:\n\n-   If you pass arrays, by calling `fit(x, y, ...)`, then `data` will be\n    the tuple `(x, y)`\n-   If you pass a `tf$data$Dataset`, by calling `fit(dataset, ...)`,\n    then `data` will be what gets yielded by `dataset` at each batch.\n\nIn the body of the `train_step` method, we implement a regular training\nupdate, similar to what you are already familiar with. Importantly, **we\ncompute the loss via `self$compiled_loss`**, which wraps the loss(es)\nfunction(s) that were passed to `compile()`.\n\nSimilarly, we call `self$compiled_metrics$update_state(y, y_pred)` to\nupdate the state of the metrics that were passed in `compile()`, and we\nquery results from `self$metrics` at the end to retrieve their current\nvalue.\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n```\n\n::: {.cell-output-stderr}\n```\nLoaded Tensorflow version 2.8.0\n```\n:::\n:::\n\nLet's try this out:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \""adam\"",\n                  loss = \""mse\"",\n                  metrics = \""mae\"")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)\n```\n:::\n\n## Going lower-level\n\nNaturally, you could just skip passing a loss function in `compile()`,\nand instead do everything *manually* in `train_step`. Likewise for\nmetrics.\n\nHere's a lower-level example, that only uses `compile()` to configure\nthe optimizer:\n\n-   We start by creating `Metric` instances to track our loss and a MAE\n    score.\n-   We implement a custom `train_step()` that updates the state of these\n    metrics (by calling `update_state()` on them), then query them (via\n    `result()`) to return their current average value, to be displayed\n    by the progress bar and to be pass to any callback.\n-   Note that we would need to call `reset_states()` on our metrics\n    between each epoch! Otherwise calling `result()` would return an\n    average since the start of training, whereas we usually work with\n    per-epoch averages. Thankfully, the framework can do that for us:\n    just list any metric you want to reset in the `metrics` property of\n    the model. The model will call `reset_states()` on any object listed\n    here at the beginning of each `fit()` epoch or at the beginning of a\n    call to `evaluate()`.\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_tracker <- metric_mean(name = \""loss\"")\nmae_metric <- metric_mean_absolute_error(name = \""mae\"")\n\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \""adam\"")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)\n```\n:::\n\n## Supporting `sample_weight` & `class_weight`\n\nYou may have noticed that our first basic example didn't make any\nmention of sample weighting. If you want to support the `fit()`\narguments `sample_weight` and `class_weight`, you'd simply do the\nfollowing:\n\n-   Unpack `sample_weight` from the `data` argument\n-   Pass it to `compiled_loss` & `compiled_metrics` (of course, you\n    could also just apply it manually if you don't rely on `compile()`\n    for losses & metrics)\n-   That's it. That's the list.\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \""adam\"",\n                  loss = \""mse\"",\n                  metrics = \""mae\"")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)\n```\n:::\n\n## Providing your own evaluation step\n\nWhat if you want to do the same for calls to `model$evaluate()`? Then\nyou would override `test_step` in exactly the same way. Here's what it\nlooks like:\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \""mse\"", metrics = \""mae\"")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n```\n\n::: {.cell-output-stdout}\n```\n     loss       mae \n0.2720022 0.4182713 \n```\n:::\n:::\n\n## Wrapping up: an end-to-end GAN example\n\nLet's walk through an end-to-end example that leverages everything you\njust learned.\n\nLet's consider:\n\n-   A generator network meant to generate 28x28x1 images.\n-   A discriminator network meant to classify 28x28x1 images into two\n    classes (\""fake\"" and \""real\"").\n-   One optimizer for each.\n-   A loss function to train the discriminator.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \""discriminator\"",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \""generator\"",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \""same\"", activation = \""sigmoid\"")\n```\n:::\n\nHere's a feature-complete GAN class, overriding `compile()` to use its\nown signature, and implementing the entire GAN algorithm in 17 lines in\n`train_step`:\n\n::: {.cell}\n\n```{.r .cell-code}\nGAN <- new_model_class(\n  classname = \""GAN\"",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \""all real images\""\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n```\n:::\n\nLet's test-drive it:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\""float32\"") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n```\n:::\n\nHappy training!"",
+    ""markdown"": ""---\ntitle: Customizing what happens in `fit()`\nauthor: \n  - name: Francois Chollet\n    url: https://twitter.com/fchollet\n  - name: Tomasz Kalinowski\n    url: https://github.com/t-kalinowski\nexecute:\n  eval: true\n---\n\n\n## Introduction\n\nWhen you're doing supervised learning, you can use `fit()` and\neverything works smoothly.\n\nWhen you need to write your own training loop from scratch, you can use\nthe `GradientTape` and take control of every little detail.\n\nBut what if you need a custom training algorithm, but you still want to\nbenefit from the convenient features of `fit()`, such as callbacks,\nbuilt-in distribution support, or step fusing?\n\nA core principle of Keras is **progressive disclosure of complexity**.\nYou should always be able to get into lower-level workflows in a gradual\nway. You shouldn't fall off a cliff if the high-level functionality\ndoesn't exactly match your use case. You should be able to gain more\ncontrol over the small details while retaining a commensurate amount of\nhigh-level convenience.\n\nWhen you need to customize what `fit()` does, you should **override the\ntraining step function of the `Model` class**. This is the function that\nis called by `fit()` for every batch of data. You will then be able to\ncall `fit()` as usual -- and it will be running your own learning\nalgorithm.\n\nNote that this pattern does not prevent you from building models with\nthe Functional API. You can do this whether you're building `Sequential`\nmodels, Functional API models, or subclassed models.\n\nLet's see how that works.\n\n## Setup\n\nRequires TensorFlow 2.2 or later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## A first simple example\n\nLet's start from a simple example:\n\n-   We create a new model class by calling `new_model_class()`.\n-   We just override the method `train_step(data)`.\n-   We return a dictionary mapping metric names (including the loss) to\n    their current value.\n\nThe input argument `data` is what gets passed to fit as training data:\n\n-   If you pass arrays, by calling `fit(x, y, ...)`, then `data` will be\n    the tuple `(x, y)`\n-   If you pass a `tf$data$Dataset`, by calling `fit(dataset, ...)`,\n    then `data` will be what gets yielded by `dataset` at each batch.\n\nIn the body of the `train_step` method, we implement a regular training\nupdate, similar to what you are already familiar with. Importantly, **we\ncompute the loss via `self$compiled_loss`**, which wraps the loss(es)\nfunction(s) that were passed to `compile()`.\n\nSimilarly, we call `self$compiled_metrics$update_state(y, y_pred)` to\nupdate the state of the metrics that were passed in `compile()`, and we\nquery results from `self$metrics` at the end to retrieve their current\nvalue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and\n    # on what you pass to `fit()`.\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value\n      # (the loss function is configured in `compile()`)\n      loss <-\n        self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    # Update metrics (includes the metric that tracks the loss)\n    self$compiled_metrics$update_state(y, y_pred)\n    \n    # Return a named list mapping metric names to current value\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n:::\n\n\nLet's try this out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct and compile an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>%  layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \""adam\"",\n                  loss = \""mse\"",\n                  metrics = \""mae\"")\n\n# Just use `fit` as usual\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 3)\n```\n:::\n\n\n## Going lower-level\n\nNaturally, you could just skip passing a loss function in `compile()`,\nand instead do everything *manually* in `train_step`. Likewise for\nmetrics.\n\nHere's a lower-level example, that only uses `compile()` to configure\nthe optimizer:\n\n-   We start by creating `Metric` instances to track our loss and a MAE\n    score.\n-   We implement a custom `train_step()` that updates the state of these\n    metrics (by calling `update_state()` on them), then query them (via\n    `result()`) to return their current average value, to be displayed\n    by the progress bar and to be pass to any callback.\n-   Note that we would need to call `reset_states()` on our metrics\n    between each epoch! Otherwise calling `result()` would return an\n    average since the start of training, whereas we usually work with\n    per-epoch averages. Thankfully, the framework can do that for us:\n    just list any metric you want to reset in the `metrics` property of\n    the model. The model will call `reset_states()` on any object listed\n    here at the beginning of each `fit()` epoch or at the beginning of a\n    call to `evaluate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloss_tracker <- metric_mean(name = \""loss\"")\nmae_metric <- metric_mean_absolute_error(name = \""mae\"")\n\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    c(x, y) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute our own loss\n      loss <- keras$losses$mean_squared_error(y, y_pred)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Compute our own metrics\n    loss_tracker$update_state(loss)\n    mae_metric$update_state(y, y_pred)\n    list(loss = loss_tracker$result(), \n         mae = mae_metric$result())\n  },\n  \n  metrics = mark_active(function() {\n    # We list our `Metric` objects here so that `reset_states()` can be\n    # called automatically at the start of each epoch\n    # or at the start of `evaluate()`.\n    # If you don't implement this active property, you have to call\n    # `reset_states()` yourself at the time of your choosing.\n    list(loss_tracker, mae_metric)\n  })\n)\n\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\n\n# We don't pass a loss or metrics here.\nmodel %>% compile(optimizer = \""adam\"")\n\n# Just use `fit` as usual -- you can use callbacks, etc.\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, epochs = 5)\n```\n:::\n\n\n## Supporting `sample_weight` & `class_weight`\n\nYou may have noticed that our first basic example didn't make any\nmention of sample weighting. If you want to support the `fit()`\narguments `sample_weight` and `class_weight`, you'd simply do the\nfollowing:\n\n-   Unpack `sample_weight` from the `data` argument\n-   Pass it to `compiled_loss` & `compiled_metrics` (of course, you\n    could also just apply it manually if you don't rely on `compile()`\n    for losses & metrics)\n-   That's it. That's the list.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    # Unpack the data. Its structure depends on your model and on what you pass\n    # to `fit()`.  A third element in `data` is optional, but if present it's\n    # assigned to sample_weight. If a thrid element is missing, sample_weight\n    # defaults to NULL\n    c(x, y, sample_weight = NULL) %<-% data\n    \n    with(tf$GradientTape() %as% tape, {\n      y_pred <- self(x, training = TRUE)  # Forward pass\n      # Compute the loss value.\n      # The loss function is configured in `compile()`.\n      loss <- self$compiled_loss(y,\n                                 y_pred,\n                                 sample_weight = sample_weight,\n                                 regularization_losses = self$losses)\n    })\n    \n    # Compute gradients\n    trainable_vars <- self$trainable_variables\n    gradients <- tape$gradient(loss, trainable_vars)\n    \n    # Update weights\n    self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))\n    \n    # Update the metrics.\n    # Metrics are configured in `compile()`.\n    self$compiled_metrics$update_state(y, y_pred, sample_weight = sample_weight)\n    \n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n\n# Construct and compile an instance of CustomModel\n\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(optimizer = \""adam\"",\n                  loss = \""mse\"",\n                  metrics = \""mae\"")\n\n# You can now use sample_weight argument\n\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nsw <- k_random_uniform(c(1000, 1))\nmodel %>% fit(x, y, sample_weight = sw, epochs = 3)\n```\n:::\n\n\n## Providing your own evaluation step\n\nWhat if you want to do the same for calls to `model$evaluate()`? Then\nyou would override `test_step` in exactly the same way. Here's what it\nlooks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomModel <- new_model_class(\n  classname = \""CustomModel\"",\n  train_step = function(data) {\n    # Unpack the data\n    c(x, y) %<-% data\n    # Compute predictions\n    y_pred <- self(x, training = FALSE)\n    # Updates the metrics tracking the loss\n    self$compiled_loss(y, y_pred, regularization_losses = self$losses)\n    # Update the metrics.\n    self$compiled_metrics$update_state(y, y_pred)\n    # Return a named list mapping metric names to current value.\n    # Note that it will include the loss (tracked in self$metrics).\n    results <- list()\n    for (m in self$metrics)\n      results[[m$name]] <- m$result()\n    results\n  }\n)\n\n# Construct an instance of CustomModel\ninputs <- layer_input(shape(32))\noutputs <- inputs %>% layer_dense(1)\nmodel <- CustomModel(inputs, outputs)\nmodel %>% compile(loss = \""mse\"", metrics = \""mae\"")\n\n# Evaluate with our custom test_step\nx <- k_random_uniform(c(1000, 32))\ny <- k_random_uniform(c(1000, 1))\nmodel %>% evaluate(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss       mae \n0.2331362 0.3878761 \n```\n:::\n:::\n\n\n## Wrapping up: an end-to-end GAN example\n\nLet's walk through an end-to-end example that leverages everything you\njust learned.\n\nLet's consider:\n\n-   A generator network meant to generate 28x28x1 images.\n-   A discriminator network meant to classify 28x28x1 images into two\n    classes (\""fake\"" and \""real\"").\n-   One optimizer for each.\n-   A loss function to train the discriminator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the discriminator\ndiscriminator <-\n  keras_model_sequential(name = \""discriminator\"",\n                         input_shape = c(28, 28, 1)) %>%\n  layer_conv_2d(64, c(3, 3), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(128, c(3, 3), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_global_max_pooling_2d() %>%\n  layer_dense(1)\n\n# Create the generator\nlatent_dim <- 128\ngenerator <- \n  keras_model_sequential(name = \""generator\"",\n                         input_shape = c(latent_dim)) %>%\n  # We want to generate 128 coefficients to reshape into a 7x7x128 map\n  layer_dense(7 * 7 * 128) %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_reshape(c(7, 7, 128)) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d_transpose(128, c(4, 4), strides = c(2, 2), padding = \""same\"") %>%\n  layer_activation_leaky_relu(alpha = 0.2) %>%\n  layer_conv_2d(1, c(7, 7), padding = \""same\"", activation = \""sigmoid\"")\n```\n:::\n\n\nHere's a feature-complete GAN class, overriding `compile()` to use its\nown signature, and implementing the entire GAN algorithm in 17 lines in\n`train_step`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGAN <- new_model_class(\n  classname = \""GAN\"",\n  initialize = function(discriminator, generator, latent_dim) {\n    super$initialize()\n    self$discriminator <- discriminator\n    self$generator <- generator\n    self$latent_dim <- as.integer(latent_dim)\n  },\n  \n  compile = function(d_optimizer, g_optimizer, loss_fn) {\n    super$compile()\n    self$d_optimizer <- d_optimizer\n    self$g_optimizer <- g_optimizer\n    self$loss_fn <- loss_fn\n  },\n  \n  \n  train_step = function(real_images) {\n    # Sample random points in the latent space\n    batch_size <- tf$shape(real_images)[1]\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Decode them to fake images\n    generated_images <- self$generator(random_latent_vectors)\n    \n    # Combine them with real images\n    combined_images <-\n      tf$concat(list(generated_images, real_images),\n                axis = 0L)\n    \n    # Assemble labels discriminating real from fake images\n    labels <-\n      tf$concat(list(tf$ones(c(batch_size, 1L)),\n                     tf$zeros(c(batch_size, 1L))),\n                axis = 0L)\n    \n    # Add random noise to the labels - important trick!\n    labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))\n    \n    # Train the discriminator\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(combined_images)\n      d_loss <- self$loss_fn(labels, predictions)\n    })\n    grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)\n    self$d_optimizer$apply_gradients(\n      zip_lists(grads, self$discriminator$trainable_weights))\n    \n    # Sample random points in the latent space\n    random_latent_vectors <-\n      tf$random$normal(shape = c(batch_size, self$latent_dim))\n    \n    # Assemble labels that say \""all real images\""\n    misleading_labels <- tf$zeros(c(batch_size, 1L))\n    \n    # Train the generator (note that we should *not* update the weights\n    # of the discriminator)!\n    with(tf$GradientTape() %as% tape, {\n      predictions <- self$discriminator(self$generator(random_latent_vectors))\n      g_loss <- self$loss_fn(misleading_labels, predictions)\n    })\n    grads <- tape$gradient(g_loss, self$generator$trainable_weights)\n    self$g_optimizer$apply_gradients(\n      zip_lists(grads, self$generator$trainable_weights))\n    \n    list(d_loss = d_loss, g_loss = g_loss)\n  }\n)\n```\n:::\n\n\nLet's test-drive it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n# Prepare the dataset. We use both the training & test MNIST digits.\n\nbatch_size <- 64\nall_digits <- dataset_mnist() %>%\n  { k_concatenate(list(.$train$x, .$test$x), axis = 1) } %>%\n  k_cast(\""float32\"") %>%\n  { . / 255 } %>%\n  k_reshape(c(-1, 28, 28, 1))\n\n\ndataset <- tensor_slices_dataset(all_digits) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(batch_size)\n\ngan <-\n  GAN(discriminator = discriminator,\n      generator = generator,\n      latent_dim = latent_dim)\ngan %>% compile(\n  d_optimizer = optimizer_adam(learning_rate = 0.0003),\n  g_optimizer = optimizer_adam(learning_rate = 0.0003),\n  loss_fn = loss_binary_crossentropy(from_logits = TRUE)\n)\n\n# To limit the execution time, we only train on 100 batches. You can train on\n# the entire dataset. You will need about 20 epochs to get nice results.\ngan %>% fit(dataset %>% dataset_take(100), epochs = 1)\n```\n:::\n\n\nHappy training!\n"",
     ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],
-    ""includes"": [],
+    ""includes"": {},
     ""engineDependencies"": {},
     ""preserve"": {},
-    ""postProcess"": null
+    ""postProcess"": true
   }
 }
\ No newline at end of file

---FILE: _freeze/keras/guides/functional_api/execute-results/html.json---
@@ -0,0 +1,16 @@
+{
+  ""hash"": ""0fa4b3cd191cc4b247bef269b8365791"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: The Functional API\nAuthor: \""[fchollet](https://twitter.com/fchollet)\""\ndate-created: 2019/03/01\ndate-last-modified: 2020/04/12\ndescription: Complete guide to the functional API.\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError : 'tf_config' is not an exported object from 'namespace:tensorflow'\n```\n:::\n\n```{.r .cell-code}\nlibrary(keras)\n```\n:::\n\n\n## Introduction\n\nThe Keras *functional API* is a way to create models that are more\nflexible than the sequential API. The functional API can handle models\nwith non-linear topology, shared layers, and even multiple inputs or\noutputs.\n\nThe main idea is that a deep learning model is usually a directed\nacyclic graph (DAG) of layers. So the functional API is a way to build\n*graphs of layers*.\n\nConsider the following model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(input: 784-dimensional vectors)\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (64 units, relu activation)]\n       ↧\n[Dense (10 units, softmax activation)]\n       ↧\n(output: logits of a probability distribution over 10 classes)\n```\n:::\n\n\nThis is a basic graph with three layers. To build this model using the\nfunctional API, start by creating an input node:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = c(784))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n:::\n\n\nThe shape of the data is set as a 784-dimensional vector. The batch size\nis always omitted since only the shape of each sample is specified.\n\nIf, for example, you have an image input with a shape of `(32, 32, 3)`,\nyou would use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Just for demonstration purposes.\nimg_inputs <- layer_input(shape = c(32, 32, 3))\n```\n:::\n\n\nThe `inputs` that is returned contains information about the shape and\n`dtype` of the input data that you feed to your model. Here's the shape:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([None, 784])\n```\n:::\n:::\n\n\nHere's the dtype:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.float32\n```\n:::\n:::\n\n\nYou create a new node in the graph of layers by calling a layer on this\n`inputs` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndense <- layer_dense(units = 64, activation = \""relu\"")\nx <- dense(inputs)\n```\n:::\n\n\nThe \""layer call\"" action is like drawing an arrow from \""inputs\"" to this\nlayer you created. You're \""passing\"" the inputs to the `dense` layer, and\nyou get `x` as the output.\n\nYou can also conveniently create the layer and compose it with `inputs`\nin one step, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- inputs %>% \n  layer_dense(units = 64, activation = \""relu\"") \n```\n:::\n\n\nLet's add a few more layers to the graph of layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutputs <- x %>% \n  layer_dense(64, activation = \""relu\"") %>% \n  layer_dense(10)\n```\n:::\n\n\nAt this point, you can create a `Model` by specifying its inputs and\noutputs in the graph of layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model(inputs = inputs, outputs = outputs, \n                     name = \""mnist_model\"")\n```\n:::\n\n\nLet's check out what the model summary looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""mnist_model\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n input_1 (InputLayer)             [(None, 784)]                 0           \n dense_1 (Dense)                  (None, 64)                    50240       \n dense_3 (Dense)                  (None, 64)                    4160        \n dense_2 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 55,050\nTrainable params: 55,050\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nYou can also plot the model as a graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAnd, optionally, display the input and output shapes of each layer in\nthe plotted graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, show_shapes = TRUE)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThis figure and the code are almost identical. In the code version, the\nconnection arrows are replaced by `%>%` operator.\n\nA \""graph of layers\"" is an intuitive mental image for a deep learning\nmodel, and the functional API is a way to create models that closely\nmirrors this.\n\n## Training, evaluation, and inference\n\nTraining, evaluation, and inference work exactly in the same way for\nmodels built using the functional API as for `Sequential` models.\n\nThe `Model` class offers a built-in training loop (the `fit()` method)\nand a built-in evaluation loop (the `evaluate()` method). Note that you\ncan easily [customize these\nloops](/guides/customizing_what_happens_in_fit/) to implement training\nroutines beyond supervised learning (e.g.\n[GANs](/examples/generative/dcgan_overriding_train_step/)).\n\nHere, load the MNIST image data, reshape it into vectors, fit the model\non the data (while monitoring performance on a validation split), then\nevaluate the model on the test data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_mnist()\n\nx_train <- array_reshape(x_train, c(60000, 784)) / 255\nx_test <-  array_reshape(x_test, c(10000, 784)) / 255 \n\nmodel %>% compile(\n  loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n  optimizer = optimizer_rmsprop(),\n  metrics = \""accuracy\""\n)\n\nhistory <- model %>% fit(\n  x_train, y_train, batch_size = 64, epochs = 2, validation_split = 0.2)\n\ntest_scores <- model %>% evaluate(x_test, y_test, verbose = 2)\nprint(test_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss  accuracy \n0.1352543 0.9590000 \n```\n:::\n:::\n\n\nFor further reading, see the [training and\nevaluation](/guides/training_with_built_in_methods/) guide.\n\n## Save and serialize\n\nSaving the model and serialization work the same way for models built\nusing the functional API as they do for `Sequential` models. The\nstandard way to save a functional model is to call `save_model_tf()` to\nsave the entire model as a single file. You can later recreate the same\nmodel from this file, even if the code that built the model is no longer\navailable.\n\nThis saved file includes the: - model architecture - model weight values\n(that were learned during training) - model training config, if any (as\npassed to `compile`) - optimizer and its state, if any (to restart\ntraining where you left off)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_to_my_model <- tempfile()\nsave_model_tf(model, path_to_my_model)\n\nrm(model)\n# Recreate the exact same model purely from the file:\nmodel <- load_model_tf(path_to_my_model)\n```\n:::\n\n\nFor details, read the model [serialization &\nsaving](/guides/serialization_and_saving/) guide.\n\n## Use the same graph of layers to define multiple models\n\nIn the functional API, models are created by specifying their inputs and\noutputs in a graph of layers. That means that a single graph of layers\ncan be used to generate multiple models.\n\nIn the example below, you use the same stack of layers to instantiate\ntwo models: an `encoder` model that turns image inputs into\n16-dimensional vectors, and an end-to-end `autoencoder` model for\ntraining.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nencoder_input <- layer_input(shape = c(28, 28, 1), \n                             name = \""img\"")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(16, 3, activation = \""relu\"") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, \n                       name = \""encoder\"")\nencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""encoder\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n conv2d_3 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_2 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d (MaxPooling2D)     (None, 8, 8, 32)              0           \n conv2d_1 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d (Conv2D)                  (None, 4, 4, 16)              4624        \n global_max_pooling2d (GlobalMaxP  (None, 16)                   0           \n ooling2D)                                                                  \n============================================================================\nTotal params: 18,672\nTrainable params: 18,672\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\ndecoder_output <- encoder_output %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \""relu\"") %>%\n  layer_conv_2d_transpose(32, 3, activation = \""relu\"") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \""relu\"") %>%\n  layer_conv_2d_transpose(1, 3, activation = \""relu\"")\n\nautoencoder <- keras_model(encoder_input, decoder_output, \n                           name = \""autoencoder\"")\nautoencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""autoencoder\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n conv2d_3 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_2 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d (MaxPooling2D)     (None, 8, 8, 32)              0           \n conv2d_1 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d (Conv2D)                  (None, 4, 4, 16)              4624        \n global_max_pooling2d (GlobalMaxP  (None, 16)                   0           \n ooling2D)                                                                  \n reshape (Reshape)                (None, 4, 4, 1)               0           \n conv2d_transpose_3 (Conv2DTransp  (None, 6, 6, 16)             160         \n ose)                                                                       \n conv2d_transpose_2 (Conv2DTransp  (None, 8, 8, 32)             4640        \n ose)                                                                       \n up_sampling2d (UpSampling2D)     (None, 24, 24, 32)            0           \n conv2d_transpose_1 (Conv2DTransp  (None, 26, 26, 16)           4624        \n ose)                                                                       \n conv2d_transpose (Conv2DTranspos  (None, 28, 28, 1)            145         \n e)                                                                         \n============================================================================\nTotal params: 28,241\nTrainable params: 28,241\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nHere, the decoding architecture is strictly symmetrical to the encoding\narchitecture, so the output shape is the same as the input shape\n`(28, 28, 1)`.\n\nThe reverse of a `Conv2D` layer is a `Conv2DTranspose` layer, and the\nreverse of a `MaxPooling2D` layer is an `UpSampling2D` layer.\n\n## All models are callable, just like layers\n\nYou can treat any model as if it were a layer by invoking it on an\n`Input` or on the output of another layer. By calling a model you aren't\njust reusing the architecture of the model, you're also reusing its\nweights.\n\nTo see this in action, here's a different take on the autoencoder\nexample that creates an encoder model, a decoder model, and chains them\nin two calls to obtain the autoencoder model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nencoder_input <- layer_input(shape = c(28, 28, 1), name = \""original_img\"")\nencoder_output <- encoder_input %>%\n  layer_conv_2d(16, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(16, 3, activation = \""relu\"") %>%\n  layer_global_max_pooling_2d()\n\nencoder <- keras_model(encoder_input, encoder_output, name = \""encoder\"")\nencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""encoder\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n original_img (InputLayer)        [(None, 28, 28, 1)]           0           \n conv2d_7 (Conv2D)                (None, 26, 26, 16)            160         \n conv2d_6 (Conv2D)                (None, 24, 24, 32)            4640        \n max_pooling2d_1 (MaxPooling2D)   (None, 8, 8, 32)              0           \n conv2d_5 (Conv2D)                (None, 6, 6, 32)              9248        \n conv2d_4 (Conv2D)                (None, 4, 4, 16)              4624        \n global_max_pooling2d_1 (GlobalMa  (None, 16)                   0           \n xPooling2D)                                                                \n============================================================================\nTotal params: 18,672\nTrainable params: 18,672\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\ndecoder_input <- layer_input(shape = c(16), name = \""encoded_img\"")\ndecoder_output <- decoder_input %>%\n  layer_reshape(c(4, 4, 1)) %>%\n  layer_conv_2d_transpose(16, 3, activation = \""relu\"") %>%\n  layer_conv_2d_transpose(32, 3, activation = \""relu\"") %>%\n  layer_upsampling_2d(3) %>%\n  layer_conv_2d_transpose(16, 3, activation = \""relu\"") %>%\n  layer_conv_2d_transpose(1, 3, activation = \""relu\"")\n\ndecoder <- keras_model(decoder_input, decoder_output, \n                       name = \""decoder\"")\ndecoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""decoder\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n encoded_img (InputLayer)         [(None, 16)]                  0           \n reshape_1 (Reshape)              (None, 4, 4, 1)               0           \n conv2d_transpose_7 (Conv2DTransp  (None, 6, 6, 16)             160         \n ose)                                                                       \n conv2d_transpose_6 (Conv2DTransp  (None, 8, 8, 32)             4640        \n ose)                                                                       \n up_sampling2d_1 (UpSampling2D)   (None, 24, 24, 32)            0           \n conv2d_transpose_5 (Conv2DTransp  (None, 26, 26, 16)           4624        \n ose)                                                                       \n conv2d_transpose_4 (Conv2DTransp  (None, 28, 28, 1)            145         \n ose)                                                                       \n============================================================================\nTotal params: 9,569\nTrainable params: 9,569\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\nautoencoder_input <- layer_input(shape = c(28, 28, 1), name = \""img\"")\nencoded_img <- encoder(autoencoder_input)\ndecoded_img <- decoder(encoded_img)\nautoencoder <- keras_model(autoencoder_input, decoded_img, \n                           name = \""autoencoder\"")\nautoencoder\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""autoencoder\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n img (InputLayer)                 [(None, 28, 28, 1)]           0           \n encoder (Functional)             (None, 16)                    18672       \n decoder (Functional)             (None, 28, 28, 1)             9569        \n============================================================================\nTotal params: 28,241\nTrainable params: 28,241\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nAs you can see, the model can be nested: a model can contain sub-models\n(since a model is just like a layer). A common use case for model\nnesting is *ensembling*. For example, here's how to ensemble a set of\nmodels into a single model that averages their predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_model <- function() {\n  inputs <- layer_input(shape = c(128))\n  outputs <- inputs %>% layer_dense(1)\n  keras_model(inputs, outputs)\n}\n\nmodel1 <- get_model()\nmodel2 <- get_model()\nmodel3 <- get_model()\n\ninputs <- layer_input(shape = c(128))\ny1 <- model1(inputs)\ny2 <- model2(inputs)\ny3 <- model3(inputs)\noutputs <- layer_average(list(y1, y2, y3))\nensemble_model <- keras_model(inputs = inputs, outputs = outputs)\n```\n:::\n\n\n## Manipulate complex graph topologies\n\n### Models with multiple inputs and outputs\n\nThe functional API makes it easy to manipulate multiple inputs and\noutputs. This cannot be handled with the `Sequential` API.\n\nFor example, if you're building a system for ranking customer issue\ntickets by priority and routing them to the correct department, then the\nmodel will have three inputs:\n\n-   the title of the ticket (text input),\n-   the text body of the ticket (text input), and\n-   any tags added by the user (categorical input)\n\nThis model will have two outputs:\n\n-   the priority score between 0 and 1 (scalar sigmoid output), and\n-   the department that should handle the ticket (softmax output over\n    the set of departments).\n\nYou can build this model in a few lines with the functional API:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_tags <- 12  # Number of unique issue tags\nnum_words <- 10000  # Size of vocabulary obtained when preprocessing text data\nnum_departments <- 4  # Number of departments for predictions\n\ntitle_input <- layer_input(shape = c(NA), name = \""title\"")  # Variable-length sequence of ints\nbody_input <- layer_input(shape = c(NA), name = \""body\"")  # Variable-length sequence of ints\ntags_input <- layer_input(shape = c(num_tags), name = \""tags\"")  # Binary vectors of size `num_tags`\n\n\n# Embed each word in the title into a 64-dimensional vector\ntitle_features <- title_input %>% layer_embedding(num_words, 64)\n\n# Embed each word in the text into a 64-dimensional vector\nbody_features <- body_input %>% layer_embedding(num_words, 64)\n\n# Reduce sequence of embedded words in the title into a single 128-dimensional vector\ntitle_features <- title_features %>% layer_lstm(128)\n\n# Reduce sequence of embedded words in the body into a single 32-dimensional vector\nbody_features <- body_features %>% layer_lstm(32)\n\n# Merge all available features into a single large vector via concatenation\nx <- layer_concatenate(title_features, body_features, tags_input)\n\n# Stick a logistic regression for priority prediction on top of the features\npriority_pred <- x %>% layer_dense(1, name = \""priority\"")\n\n# Stick a department classifier on top of the features\ndepartment_pred <- x %>% layer_dense(num_departments, name = \""department\"")\n\n# Instantiate an end-to-end model predicting both priority and department\nmodel <- keras_model(\n  inputs <- list(title_input, body_input, tags_input),\n  outputs <- list(priority_pred, department_pred)\n)\n```\n:::\n\n\nNow plot the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, show_shapes = TRUE)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nWhen compiling this model, you can assign different losses to each\noutput. You can even assign different weights to each loss -- to\nmodulate their contribution to the total training loss.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    loss_binary_crossentropy(from_logits = TRUE),\n    loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights <- c(1, 0.2)\n)\n```\n:::\n\n\nSince the output layers have different names, you could also specify the\nlosses and loss weights with the corresponding layer names:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = list(\n    priority = loss_binary_crossentropy(from_logits = TRUE),\n    department = loss_categorical_crossentropy(from_logits = TRUE)\n  ),\n  loss_weights = c(priority =  1.0, department = 0.2),\n)\n```\n:::\n\n\nTrain the model by passing lists of NumPy arrays of inputs and targets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# some helpers to generate dummy input data\nrandom_uniform_array <- function(dim) \n  array(runif(prod(dim)), dim)\n\nrandom_vectorized_array <- function(num_words, dim)\n  array(sample(0:(num_words - 1), prod(dim), replace = TRUE), dim)\n\n# Dummy input data\ntitle_data <- random_vectorized_array(num_words, c(1280, 10))\nbody_data <- random_vectorized_array(num_words, c(1280, 100))\ntags_data <- random_vectorized_array(2, c(1280, num_tags))\n# storage.mode(tags_data) <- \""double\"" # from integer\n\n# Dummy target data\npriority_targets <- random_uniform_array(c(1280, 1))\ndept_targets <- random_vectorized_array(2, c(1280, num_departments))\n\nmodel %>% fit(\n  list(title = title_data, body = body_data, tags = tags_data),\n  list(priority = priority_targets, department = dept_targets),\n  epochs = 2,\n  batch_size = 32\n)\n```\n:::\n\n\nWhen calling fit with a `tfdataset` object, it should yield either a\ntuple of lists like\n`tuple(list(title_data, body_data, tags_data), list(priority_targets, dept_targets))`\nor a tuple of named lists like\n`tuple(list(title = title_data, body = body_data, tags = tags_data), list(priority= priority_targets, department= dept_targets))`.\n\nFor more detailed explanation, refer to the [training and\nevaluation](/guides/training_with_built_in_methods/) guide.\n\n### A toy ResNet model\n\nIn addition to models with multiple inputs and outputs, the functional\nAPI makes it easy to manipulate non-linear connectivity topologies --\nthese are models with layers that are not connected sequentially, which\nthe `Sequential` API cannot handle.\n\nA common use case for this is residual connections. Let's build a toy\nResNet model for CIFAR10 to demonstrate this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = c(32, 32, 3), name = \""img\"")\nblock_1_output <- inputs %>% \n  layer_conv_2d(32, 3, activation = \""relu\"") %>% \n  layer_conv_2d(64, 3, activation = \""relu\"") %>% \n  layer_max_pooling_2d(3)\n\nblock_2_output <- block_1_output %>% \n  layer_conv_2d(64, 3, activation = \""relu\"", padding = \""same\"") %>% \n  layer_conv_2d(64, 3, activation = \""relu\"", padding = \""same\"") %>% \n  layer_add(block_1_output)\n\nblock_3_output <- block_2_output %>% \n  layer_conv_2d(64, 3, activation = \""relu\"", padding = \""same\"") %>% \n  layer_conv_2d(64, 3, activation = \""relu\"", padding = \""same\"") %>% \n  layer_add(block_2_output) \n\noutputs <- block_3_output %>%\n  layer_conv_2d(64, 3, activation = \""relu\"") %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(256, activation = \""relu\"") %>%\n  layer_dropout(0.5) %>%\n  layer_dense(10)\n\nmodel <- keras_model(inputs, outputs, name = \""toy_resnet\"")\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""toy_resnet\""\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n img (InputLayer)        [(None, 32, 32  0        []                        \n                         , 3)]                                              \n conv2d_9 (Conv2D)       (None, 30, 30,  896      ['img[0][0]']             \n                          32)                                               \n conv2d_8 (Conv2D)       (None, 28, 28,  18496    ['conv2d_9[0][0]']        \n                          64)                                               \n max_pooling2d_2 (MaxPoo  (None, 9, 9, 6  0       ['conv2d_8[0][0]']        \n ling2D)                 4)                                                 \n conv2d_11 (Conv2D)      (None, 9, 9, 6  36928    ['max_pooling2d_2[0][0]'] \n                         4)                                                 \n conv2d_10 (Conv2D)      (None, 9, 9, 6  36928    ['conv2d_11[0][0]']       \n                         4)                                                 \n add (Add)               (None, 9, 9, 6  0        ['conv2d_10[0][0]',       \n                         4)                        'max_pooling2d_2[0][0]'] \n conv2d_13 (Conv2D)      (None, 9, 9, 6  36928    ['add[0][0]']             \n                         4)                                                 \n conv2d_12 (Conv2D)      (None, 9, 9, 6  36928    ['conv2d_13[0][0]']       \n                         4)                                                 \n add_1 (Add)             (None, 9, 9, 6  0        ['conv2d_12[0][0]',       \n                         4)                        'add[0][0]']             \n conv2d_14 (Conv2D)      (None, 7, 7, 6  36928    ['add_1[0][0]']           \n                         4)                                                 \n global_average_pooling2  (None, 64)     0        ['conv2d_14[0][0]']       \n d (GlobalAveragePooling                                                    \n 2D)                                                                        \n dense_8 (Dense)         (None, 256)     16640    ['global_average_pooling2d\n                                                  [0][0]']                  \n dropout (Dropout)       (None, 256)     0        ['dense_8[0][0]']         \n dense_7 (Dense)         (None, 10)      2570     ['dropout[0][0]']         \n============================================================================\nTotal params: 223,242\nTrainable params: 223,242\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nPlot the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, show_shapes = TRUE)\n```\n\n::: {.cell-output-display}\n![](functional_api_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nNow train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()  \n\nx_train <- x_train / 255\nx_test <- x_test / 255\ny_train <- to_categorical(y_train, 10)\ny_test <- to_categorical(y_test, 10)\n\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(1e-3),\n  loss = loss_categorical_crossentropy(from_logits = TRUE),\n  metrics = \""acc\""\n)\n# We restrict the data to the first 1000 samples so as to limit execution time\n# for this guide. Try to train on the entire dataset until convergence!\nmodel %>% fit(\n  x_train[1:1000, , , ],\n  y_train[1:1000, ],\n  batch_size = 64,\n  epochs = 1,\n  validation_split = 0.2\n)\n```\n:::\n\n\n## Shared layers\n\nAnother good use for the functional API are models that use *shared\nlayers*. Shared layers are layer instances that are reused multiple\ntimes in the same model -- they learn features that correspond to\nmultiple paths in the graph-of-layers.\n\nShared layers are often used to encode inputs from similar spaces (say,\ntwo different pieces of text that feature similar vocabulary). They\nenable sharing of information across these different inputs, and they\nmake it possible to train such a model on less data. If a given word is\nseen in one of the inputs, that will benefit the processing of all\ninputs that pass through the shared layer.\n\nTo share a layer in the functional API, call the same layer instance\nmultiple times. For instance, here's an `Embedding` layer shared across\ntwo different text inputs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Embedding for 1000 unique words mapped to 128-dimensional vectors\nshared_embedding <- layer_embedding(input_dim = 1000, output_dim = 128)\n\n# Variable-length sequence of integers\ntext_input_a <- layer_input(shape = c(NA), dtype = \""int32\"")\n\n# Variable-length sequence of integers\ntext_input_b <- layer_input(shape = c(NA), dtype = \""int32\"")\n\n# Reuse the same layer to encode both inputs\nencoded_input_a <- shared_embedding(text_input_a)\nencoded_input_b <- shared_embedding(text_input_b)\n```\n:::\n\n\n## Extract and reuse nodes in the graph of layers\n\nBecause the graph of layers you are manipulating is a static data\nstructure, it can be accessed and inspected. And this is how you are\nable to plot functional models as images.\n\nThis also means that you can access the activations of intermediate\nlayers (\""nodes\"" in the graph) and reuse them elsewhere -- which is very\nuseful for something like feature extraction.\n\nLet's look at an example. This is a VGG19 model with weights pretrained\non ImageNet:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvgg19 <- application_vgg19()\n```\n:::\n\n\nAnd these are the intermediate activations of the model, obtained by\nquerying the graph data structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\n```\n:::\n\n\nUse these features to create a new feature-extraction model that returns\nthe values of the intermediate layer activations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeat_extraction_model <- \n  keras_model(inputs = vgg19$input, outputs = features_list)\n\nimg <- random_uniform_array(c(1, 224, 224, 3))\nextracted_features <- feat_extraction_model(img)\n```\n:::\n\n\nThis comes in handy for tasks like [neural style\ntransfer](https://keras$io/examples/generative/neural_style_transfer/),\namong other things.\n\n## Extend the API using custom layers\n\n`tf$keras` includes a wide range of built-in layers, for example:\n\n-   Convolutional layers: `Conv1D`, `Conv2D`, `Conv3D`,\n    `Conv2DTranspose`\n-   Pooling layers: `MaxPooling1D`, `MaxPooling2D`, `MaxPooling3D`,\n    `AveragePooling1D`\n-   RNN layers: `GRU`, `LSTM`, `ConvLSTM2D`\n-   `BatchNormalization`, `Dropout`, `Embedding`, etc.\n\nBut if you don't find what you need, it's easy to extend the API by\ncreating your own layers. All layers subclass the `Layer` class and\nimplement:\n\n-   `call` method, that specifies the computation done by the layer.\n-   `build` method, that creates the weights of the layer (this is just\n    a style convention since you can create weights in `__init__`, as\n    well).\n\nTo learn more about creating layers from scratch, read [custom layers\nand models](/guides/making_new_layers_and_models_via_subclassing) guide.\n\nThe following is a basic implementation of `layer_dense()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlayer_custom_dense <- new_layer_class(\n  \""CustomDense\"",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units = as.integer(units)\n  },\n  build = function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  },\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\n```\n:::\n\n\nFor serialization support in your custom layer, define a `get_config`\nmethod that returns the constructor arguments of the layer instance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_custom_dense <- new_layer_class(\n  \""CustomDense\"",\n  initialize = function(units = 32) {\n    super$initialize()\n    self$units <- as.integer(units)\n  },\n  \n  build = function(input_shape) {\n    self$w <-\n      self$add_weight(\n        shape = shape(tail(input_shape, 1), self$units),\n        initializer = \""random_normal\"",\n        trainable = TRUE\n      )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  },\n  \n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  },\n  \n  get_config = function() {\n    list(units = self$units)\n  }\n)\n\n\ninputs <- layer_input(c(4))\noutputs <- inputs %>% layer_custom_dense(10)\n\nmodel <- keras_model(inputs, outputs)\nconfig <- model %>% get_config()\n\nnew_model <- from_config(config, custom_objects = list(layer_custom_dense))\n```\n:::\n\n\nOptionally, implement the class method `from_config(class_constructor, config)` which\nis used when recreating a layer instance given its config.\nThe default implementation of `from_config` is approximately:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrom_config <- function(layer_constructor, config) \n  do.call(layer_constructor, config)\n```\n:::\n\n\n## When to use the functional API\n\nShould you use the Keras functional API to create a new model, or just\nsubclass the `Model` class directly? In general, the functional API is\nhigher-level, easier and safer, and has a number of features that\nsubclassed models do not support.\n\nHowever, model subclassing provides greater flexibility when building\nmodels that are not easily expressible as directed acyclic graphs of\nlayers. For example, you could not implement a Tree-RNN with the\nfunctional API and would have to subclass `Model` directly.\n\nFor an in-depth look at the differences between the functional API and\nmodel subclassing, read [What are Symbolic and Imperative APIs in\nTensorFlow\n2.0?](https://blog$tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n\n### Functional API strengths:\n\nThe following properties are also true for Sequential models (which are\nalso data structures), but are not true for subclassed models (which are\nR code, not data structures).\n\n#### Less verbose\n\nThere is no `super$initialize(...)`, no `call <- function(...) {   }`,\netc.\n\nCompare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape = c(32))\noutputs <- inputs %>% \n  layer_dense(64, activation = 'relu') %>% \n  layer_dense(10)\nmlp <- keras_model(inputs, outputs)\n```\n:::\n\n\nWith the subclassed version:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMLP <- new_model_class(\n  classname = \""MLP\"",\n  \n  initialize = function(...) {\n    super$initialize(...)\n    self$dense_1 <- layer_dense(units = 64, activation = 'relu')\n    self$dense_2 <- layer_dense(units = 10)\n  },\n  \n  call = function(inputs) {\n    inputs %>% \n      self$dense_1() %>% \n      self$dense_2()\n  }\n)\n\n# Instantiate the model.\nmlp <- MLP()\n\n# Necessary to create the model's state.\n# The model doesn't have a state until it's called at least once.\ninvisible(mlp(tf$zeros(shape(1, 32))))\n```\n:::\n\n\n#### Model validation while defining its connectivity graph\n\nIn the functional API, the input specification (shape and dtype) is\ncreated in advance (using `layer_input`). Every time you call a layer, the\nlayer checks that the specification passed to it matches its\nassumptions, and it will raise a helpful error message if not.\n\nThis guarantees that any model you can build with the functional API\nwill run. All debugging -- other than convergence-related debugging --\nhappens statically during the model construction and not at execution\ntime. This is similar to type checking in a compiler.\n\n#### A functional model is plottable and inspectable\n\nYou can plot the model as a graph, and you can easily access\nintermediate nodes in this graph. For example, to extract and reuse the\nactivations of intermediate layers (as seen in a previous example):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures_list <- lapply(vgg19$layers, \\(layer) layer$output)\nfeat_extraction_model <- keras_model(inputs = vgg19$input,\n                                     outputs = features_list)\n```\n:::\n\n\n#### A functional model can be serialized or cloned\n\nBecause a functional model is a data structure rather than a piece of\ncode, it is safely serializable and can be saved as a single file that\nallows you to recreate the exact same model without having access to any\nof the original code. See the [serialization & saving\nguide](/guides/serialization_and_saving/).\n\nTo serialize a subclassed model, it is necessary for the implementer to\nspecify a `get_config()` and `from_config()` method at the model level.\n\n### Functional API weakness:\n\n#### It does not support dynamic architectures\n\nThe functional API treats models as DAGs of layers. This is true for\nmost deep learning architectures, but not all -- for example, recursive\nnetworks or Tree RNNs do not follow this assumption and cannot be\nimplemented in the functional API.\n\n## Mix-and-match API styles\n\nChoosing between the functional API or Model subclassing isn't a binary\ndecision that restricts you into one category of models. All models in\nthe `tf$keras` API can interact with each other, whether they're\n`Sequential` models, functional models, or subclassed models that are\nwritten from scratch.\n\nYou can always use a functional model or `Sequential` model as part of a\nsubclassed model or layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunits <- 32L\ntimesteps <- 10L\ninput_dim <- 5L\n\n# Define a Functional model\n\ninputs <- layer_input(c(NA, units))\noutputs <- inputs %>% \n  layer_global_average_pooling_1d() %>% \n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\n\n\nlayer_custom_rnn <- new_layer_class(\n  \""CustomRNN\"",\n  initialize = function() {\n    super$initialize()\n    self$units <- units\n    self$projection_1 <-\n      layer_dense(units = units, activation = \""tanh\"")\n    self$projection_2 <-\n      layer_dense(units = units, activation = \""tanh\"")\n    # Our previously-defined Functional model\n    self$classifier <- model\n  },\n  \n  call = function(inputs) {\n    message(\""inputs shape: \"", format(inputs$shape))\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\""list\"", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis is 1-based\n    message(\""features shape: \"", format(features$shape))\n    self$classifier(features)\n  }\n)\n\nlayer_custom_rnn(tf$zeros(shape(1, timesteps, input_dim)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ninputs shape: (1, 10, 5)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nfeatures shape: (1, 10, 32)\n```\n:::\n:::\n\n\nYou can use any subclassed layer or model in the functional API as long\nas it implements a `call` method that follows one of the following\npatterns:\n\n-   `call(inputs, ..., training = NULL, mask = NULL)` -- Where `inputs` is a tensor or a\n    nested structure of tensors (e.g. a list of tensors), and where\n    optional named arguments `training` and `mask` can be present. \n    \n    are non-tensor arguments (non-inputs).\n-   `call(self, inputs, training = NULL, **kwargs)` -- Where `training`\n    is a boolean indicating whether the layer should behave in training\n    mode and inference mode.\n-   `call(self, inputs, mask = NULL, **kwargs)` -- Where `mask` is a\n    boolean mask tensor (useful for RNNs, for instance).\n-   `call(self, inputs, training = NULL, mask = NULL, **kwargs)` -- Of\n    course, you can have both masking and training-specific behavior at\n    the same time.\n\nAdditionally, if you implement the `get_config` method on your custom\nLayer or model, the functional models you create will still be\nserializable and cloneable.\n\nHere's a quick example of a custom RNN, written from scratch, being used\nin a functional model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunits <- 32 \ntimesteps <- 10 \ninput_dim <- 5 \nbatch_size <- 16\n\nlayer_custom_rnn <- new_layer_class(\n  \""CustomRNN\"",\n  initialize = function() {\n    super$initialize()\n    self$units <- units  \n    self$projection_1 <- layer_dense(units = units, activation = \""tanh\"")\n    self$projection_2 <- layer_dense(units = units, activation = \""tanh\"")\n    self$classifier <- layer_dense(units = 1)\n  },\n  \n  call = function(inputs) {\n    c(batch_size, timesteps, channels) %<-% dim(inputs)\n    outputs <- vector(\""list\"", timesteps)\n    state <- tf$zeros(shape(batch_size, self$units))\n    for (t in 1:timesteps) {\n      # iterate over each time_step\n      outputs[[t]] <- state <-\n        inputs[, t, ] %>%\n        self$projection_1() %>%\n        { . + self$projection_2(state) }\n    }\n    \n    features <- tf$stack(outputs, axis = 1L) # axis arg is 1-based\n    self$classifier(features)\n  }\n)\n    \n# Note that you specify a static batch size for the inputs with the `batch_shape`\n# arg, because the inner computation of `CustomRNN` requires a static batch size\n# (when you create the `state` zeros tensor).\ninputs <- layer_input(batch_shape = c(batch_size, timesteps, input_dim))\noutputs <- inputs %>% \n  layer_conv_1d(32, 3) %>% \n  layer_custom_rnn()\n\nmodel <- keras_model(inputs, outputs)\nmodel(tf$zeros(shape(1, 10, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]\n  [0.]]], shape=(1, 8, 1), dtype=float32)\n```\n:::\n:::\n"",
+    ""supporting"": [
+      ""functional_api_files""
+    ],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: _freeze/keras/guides/making_new_layers_and_models_via_subclassing/execute-results/html.json---
@@ -1,14 +1,14 @@
 {
   ""hash"": ""1f1bd3ffd98a3d12de8f8cb6a4d80b0d"",
   ""result"": {
-    ""markdown"": ""---\ntitle: \""Writing `Layer` and `Model` objects from scratch.\""\n---\n\n## Setup\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\ntf_version()\n```\n\n::: {.cell-output-stderr}\n```\nLoaded Tensorflow version 2.8.0\n```\n:::\n\n::: {.cell-output-stdout}\n```\n[1] '2.8'\n```\n:::\n:::\n\n## The `Layer` class: a combination of state (weights) and some computation\n\nOne of the central abstractions in Keras is the `Layer` class. A layer\nencapsulates both a state (the layer's \""weights\"") and a transformation\nfrom inputs to outputs (a \""call\"", the layer's forward pass).\n\nHere's a densely-connected layer. It has a state: the variables `w` and\n`b`.\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(\n      initial_value = w_init(\n        shape = shape(input_dim, units),\n        dtype = \""float32\""\n      ),\n      trainable = TRUE\n    )\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(\n      initial_value = b_init(shape = shape(units), dtype = \""float32\""),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\nYou would use a layer by calling it on some tensor input(s), much like a\nregular function.\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[0.11195514 0.06101172 0.05288789 0.03446084]\n [0.11195514 0.06101172 0.05288789 0.03446084]], shape=(2, 4), dtype=float32)\n```\n:::\n:::\n\n`Linear` behaves similarly to a layer present in the Python interface to\nkeras (e.g., `keras$layers$Dense`).\n\nHowever, one additional step is needed to make it behave like the\nbuiltin layers present in the keras R package (e.g., `layer_dense()`).\n\nKeras layers in R are designed to compose nicely with the pipe operator\n(`%>%`), so that the layer instance is conveniently created on demand\nwhen an existing model or tensor is piped in. In order to make a custom\nlayer similarly compose nicely with the pipe, you can call\n`create_layer_wrapper()` on the layer class constructor.\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_linear <- create_layer_wrapper(Linear)\n```\n:::\n\nNow `layer_linear` is a layer constructor that composes nicely with\n`%>%`, just like the built-in layers:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_linear(4, 2)\n\nmodel(k_ones(c(2, 2)))\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[ 0.09593566 -0.0239684  -0.08020082 -0.20181108]\n [ 0.09593566 -0.0239684  -0.08020082 -0.20181108]], shape=(2, 4), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output-stdout}\n```\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n linear_1 (Linear)                (2, 4)                        12          \n============================================================================\nTotal params: 12\nTrainable params: 12\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\nBecause the pattern above is so common, there is a convenience function\nthat combines the steps of subclassing `keras$layers$Layer` and calling\n`create_layer_wrapper` on the output: the `Layer` function. The\n`layer_linear` defined below is identical to the `layer_linear` defined\nabove.\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_linear <- Layer(\n  \""Linear\"",\n  initialize =  function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(initial_value = w_init(shape = shape(input_dim, units),\n                                                 dtype = \""float32\""),\n                          trainable = TRUE)\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(initial_value = b_init(shape = shape(units),\n                                                 dtype = \""float32\""),\n                          trainable = TRUE)\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n```\n:::\n\nFor the remainder of this vignette we'll be using the `%py_class%`\nconstructor. However, in your own code feel free to use\n`create_layer_wrapper` and/or `Layer` if you prefer.\n\nNote that the weights `w` and `b` are automatically tracked by the layer\nupon being set as layer attributes:\n\n::: {.cell}\n\n```{.r .cell-code}\nstopifnot(all.equal(\n  linear_layer$weights,\n  list(linear_layer$w, linear_layer$b)\n))\n```\n:::\n\nYou also have access to a quicker shortcut for adding a weight to a\nlayer: the `add_weight()` method:\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \""zeros\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[-0.10461128  0.03076715  0.03548232 -0.02009123]\n [-0.10461128  0.03076715  0.03548232 -0.02009123]], shape=(2, 4), dtype=float32)\n```\n:::\n:::\n\n## Layers can have non-trainable weights\n\nBesides trainable weights, you can add non-trainable weights to a layer\nas well. Such weights are meant not to be taken into account during\nbackpropagation, when you are training the layer.\n\nHere's how to add and use a non-trainable weight:\n\n::: {.cell}\n\n```{.r .cell-code}\nComputeSum(keras$layers$Layer) %py_class% {\n  initialize <- function(input_dim) {\n    super$initialize()\n    self$total <- tf$Variable(\n      initial_value = tf$zeros(shape(input_dim)),\n      trainable = FALSE\n    )\n  }\n\n  call <- function(inputs) {\n    self$total$assign_add(tf$reduce_sum(inputs, axis = 0L))\n    self$total\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nmy_sum <- ComputeSum(2)\ny <- my_sum(x)\nprint(as.numeric(y))\n```\n\n::: {.cell-output-stdout}\n```\n[1] 2 2\n```\n:::\n\n```{.r .cell-code}\ny <- my_sum(x)\nprint(as.numeric(y))\n```\n\n::: {.cell-output-stdout}\n```\n[1] 4 4\n```\n:::\n:::\n\nIt's part of `layer$weights`, but it gets categorized as a non-trainable\nweight:\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\""weights:\"", length(my_sum$weights), \""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nweights: 1 \n```\n:::\n\n```{.r .cell-code}\ncat(\""non-trainable weights:\"", length(my_sum$non_trainable_weights), \""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nnon-trainable weights: 1 \n```\n:::\n\n```{.r .cell-code}\n# It's not included in the trainable weights:\ncat(\""trainable_weights:\"", my_sum$trainable_weights, \""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\ntrainable_weights:  \n```\n:::\n:::\n\n## Best practice: deferring weight creation until the shape of the inputs is known\n\nOur `Linear` layer above took an `input_dim`argument that was used to\ncompute the shape of the weights `w` and `b` in `initialize()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \""zeros\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\nIn many cases, you may not know in advance the size of your inputs, and\nyou would like to lazily create weights when that value becomes known,\nsome time after instantiating the layer.\n\nIn the Keras API, we recommend creating layer weights in the\n`build(self, inputs_shape)` method of your layer. Like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\nThe `build()` method of your layer will automatically run the first time\nyour layer instance is called. You now have a layer that can handle an\narbitrary number of input features:\n\n::: {.cell}\n\n```{.r .cell-code}\n# At instantiation, we don't know on what inputs this is going to get called\nlinear_layer <- Linear(32)\n\n# The layer's weights are created dynamically the first time the layer is called\ny <- linear_layer(x)\n```\n\n::: {.cell-output-stderr}\n```\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n```\n:::\n:::\n\nImplementing `build()` separately as shown above nicely separates\ncreating weights only once from using weights in every call. However,\nfor some advanced custom layers, it can become impractical to separate\nthe state creation and computation. Layer implementers are allowed to\ndefer weight creation to the first `call()`, but need to take care that\nlater calls use the same weights. In addition, since `call()` is likely\nto be executed for the first time inside a `tf_function()`, any variable\ncreation that takes place in `call()` should be wrapped in a\n`tf$init_scope()`.\n\n## Layers are recursively composable\n\nIf you assign a Layer instance as an attribute of another Layer, the\nouter layer will start tracking the weights created by the inner layer.\n\nWe recommend creating such sublayers in the `initialize()` method and\nleave it to the first `call()` to trigger building their weights.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's assume we are reusing the Linear class\n# with a `build` method that we defined above.\nMLPBlock(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$linear_1 <- Linear(32)\n    self$linear_2 <- Linear(32)\n    self$linear_3 <- Linear(1)\n  }\n\n  call <- function(inputs) {\n    x <- self$linear_1(inputs)\n    x <- tf$nn$relu(x)\n    x <- self$linear_2(x)\n    x <- tf$nn$relu(x)\n    self$linear_3(x)\n  }\n}\n\nmlp <- MLPBlock()\ny <- mlp(tf$ones(shape = shape(3, 64))) # The first call to the `mlp` will create the weights\n```\n\n::: {.cell-output-stderr}\n```\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n\nWarning in is.na(d): is.na() applied to non-(list or vector) of type\n'environment'\n```\n:::\n\n```{.r .cell-code}\ncat(\""weights:\"", length(mlp$weights), \""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nweights: 6 \n```\n:::\n\n```{.r .cell-code}\ncat(\""trainable weights:\"", length(mlp$trainable_weights), \""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\ntrainable weights: 6 \n```\n:::\n:::\n\n## The `add_loss()` method\n\nWhen writing the `call()` method of a layer, you can create loss tensors\nthat you will want to use later, when writing your training loop. This\nis doable by calling `self$add_loss(value)`:\n\n::: {.cell}\n\n```{.r .cell-code}\n# A layer that creates an activity regularization loss\nActivityRegularizationLayer(keras$layers$Layer) %py_class% {\n  initialize <- function(rate = 1e-2) {\n    super$initialize()\n    self$rate <- rate\n  }\n\n  call <- function(inputs) {\n    self$add_loss(self$rate * tf$reduce_sum(inputs))\n    inputs\n  }\n}\n```\n:::\n\nThese losses (including those created by any inner layer) can be\nretrieved via `layer$losses`. This property is reset at the start of\nevery `call()` to the top-level layer, so that `layer$losses` always\ncontains the loss values created during the last forward pass.\n\n::: {.cell}\n\n```{.r .cell-code}\nOuterLayer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$activity_reg <- ActivityRegularizationLayer(1e-2)\n  }\n  call <- function(inputs) {\n    self$activity_reg(inputs)\n  }\n}\n\nlayer <- OuterLayer()\nstopifnot(length(layer$losses) == 0) # No losses yet since the layer has never been called\n\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # We created one loss value\n\n# `layer$losses` gets reset at the start of each call()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # This is the loss created during the call above\n```\n:::\n\nIn addition, the `loss` property also contains regularization losses\ncreated for the weights of any inner layer:\n\n::: {.cell}\n\n```{.r .cell-code}\nOuterLayerWithKernelRegularizer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))\n  }\n  call <- function(inputs) {\n    self$dense(inputs)\n  }\n}\n\nlayer <- OuterLayerWithKernelRegularizer()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\n\n# This is `1e-3 * sum(layer$dense$kernel ** 2)`,\n# created by the `kernel_regularizer` above.\nprint(layer$losses)\n```\n\n::: {.cell-output-stdout}\n```\n[[1]]\ntf.Tensor(0.0018123012, shape=(), dtype=float32)\n```\n:::\n:::\n\nThese losses are meant to be taken into account when writing training\nloops, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Iterate over the batches of a dataset.\ndataset_iterator <- reticulate::as_iterator(train_dataset)\nwhile(!is.null(batch <- iter_next(dataset_iterator))) {\n  c(x_batch_train, y_batch_train) %<-% batch\n  with(tf$GradientTape() %as% tape, {\n    logits <- layer(x_batch_train) # Logits for this minibatch\n    # Loss value for this minibatch\n    loss_value <- loss_fn(y_batch_train, logits)\n    # Add extra losses created during this forward pass:\n    loss_value <- loss_value + sum(model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, model$trainable_weights)))\n}\n```\n:::\n\nFor a detailed guide about writing training loops, see the [guide to\nwriting a training loop from\nscratch](/guides/writing_a_training_loop_from_scratch/).\n\nThese losses also work seamlessly with `fit()` (they get automatically\nsummed and added to the main loss, if any):\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape(3))\noutput <- input %>% layer_activity_regularization()\n# output <- ActivityRegularizationLayer()(input)\nmodel <- keras_model(input, output)\n\n# If there is a loss passed in `compile`, the regularization\n# losses get added to it\nmodel %>% compile(optimizer = \""adam\"", loss = \""mse\"")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n\n# It's also possible not to pass any loss in `compile`,\n# since the model already has a loss to minimize, via the `add_loss`\n# call during the forward pass!\nmodel %>% compile(optimizer = \""adam\"")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n```\n:::\n\n## The `add_metric()` method\n\nSimilarly to `add_loss()`, layers also have an `add_metric()` method for\ntracking the moving average of a quantity during training.\n\nConsider the following layer: a \""logistic endpoint\"" layer. It takes as\ninputs predictions and targets, it computes a loss which it tracks via\n`add_loss()`, and it computes an accuracy scalar, which it tracks via\n`add_metric()`.\n\n::: {.cell}\n\n```{.r .cell-code}\nLogisticEndpoint(keras$layers$Layer) %py_class% {\n  initialize <- function(name = NULL) {\n    super$initialize(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  }\n\n  call <- function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n\n    # Log accuracy as a metric and add it\n    # to the layer using `self.add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \""accuracy\"")\n\n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n}\n```\n:::\n\nMetrics tracked in this way are accessible via `layer$metrics`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- LogisticEndpoint()\n\ntargets <- tf$ones(shape(2, 2))\nlogits <- tf$ones(shape(2, 2))\ny <- layer(targets, logits)\n\ncat(\""layer$metrics: \"")\n```\n\n::: {.cell-output-stdout}\n```\nlayer$metrics: \n```\n:::\n\n```{.r .cell-code}\nstr(layer$metrics)\n```\n\n::: {.cell-output-stdout}\n```\nList of 1\n $ :BinaryAccuracy(name=binary_accuracy,dtype=float32,threshold=0.5)\n```\n:::\n\n```{.r .cell-code}\ncat(\""current accuracy value:\"", as.numeric(layer$metrics[[1]]$result()), \""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\ncurrent accuracy value: 1 \n```\n:::\n:::\n\nJust like for `add_loss()`, these metrics are tracked by `fit()`:\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape(3), name = \""inputs\"")\ntargets <- layer_input(shape(10), name = \""targets\"")\nlogits <- inputs %>% layer_dense(10)\npredictions <- LogisticEndpoint(name = \""predictions\"")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \""adam\"")\n\ndata <- list(\n  inputs = k_random_uniform(c(3, 3)),\n  targets = k_random_uniform(c(3, 10))\n)\n\nmodel %>% fit(data, epochs = 1, verbose = FALSE)\n```\n:::\n\n## You can optionally enable serialization on your layers\n\nIf you need your custom layers to be serializable as part of a\n[Functional model](/guides/functional_api/), you can optionally\nimplement a `get_config()` method:\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    list(units = self$units)\n  }\n}\n\n\n# Now you can recreate the layer from its config:\nlayer <- Linear(64)\nconfig <- layer$get_config()\nprint(config)\n```\n\n::: {.cell-output-stdout}\n```\n$units\n[1] 64\n```\n:::\n\n```{.r .cell-code}\nnew_layer <- Linear$from_config(config)\n```\n:::\n\nNote that the `initialize()` method of the base `Layer` class takes some\nadditional named arguments, in particular a `name` and a `dtype`. It's\ngood practice to pass these arguments to the parent class in\n`initialize()` and to include them in the layer config:\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, ...) {\n    super$initialize(...)\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    config <- super$get_config()\n    config$units <- self$units\n    config\n  }\n}\n\n\nlayer <- Linear(64)\nconfig <- layer$get_config()\nstr(config)\n```\n\n::: {.cell-output-stdout}\n```\nList of 4\n $ name     : chr \""linear_9\""\n $ trainable: logi TRUE\n $ dtype    : chr \""float32\""\n $ units    : num 64\n```\n:::\n\n```{.r .cell-code}\nnew_layer <- Linear$from_config(config)\n```\n:::\n\nIf you need more flexibility when deserializing the layer from its\nconfig, you can also override the `from_config()` class method. This is\nthe base implementation of `from_config()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nfrom_config <- function(cls, config) do.call(cls, config)\n```\n:::\n\nTo learn more about serialization and saving, see the complete [guide to\nsaving and serializing models](/guides/serialization_and_saving/).\n\n## Privileged `training` argument in the `call()` method\n\nSome layers, in particular the `BatchNormalization` layer and the\n`Dropout` layer, have different behaviors during training and inference.\nFor such layers, it is standard practice to expose a `training`\n(boolean) argument in the `call()` method.\n\nBy exposing this argument in `call()`, you enable the built-in training\nand evaluation loops (e.g. `fit()`) to correctly use the layer in\ntraining and inference. Note, the default of `NULL` means that the\ntraining parameter will be inferred by keras from the training context\n(e.g., it will be `TRUE` if called from `fit()`, `FALSE` if called from\n`predict()`)\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomDropout(keras$layers$Layer) %py_class% {\n  initialize <- function(rate, ...) {\n    super$initialize(...)\n    self$rate <- rate\n  }\n  call <- function(inputs, training = NULL) {\n    if (isTRUE(training)) {\n      return(tf$nn$dropout(inputs, rate = self$rate))\n    }\n    inputs\n  }\n}\n```\n:::\n\n## Privileged `mask` argument in the `call()` method\n\nThe other privileged argument supported by `call()` is the `mask`\nargument.\n\nYou will find it in all Keras RNN layers. A mask is a boolean tensor\n(one boolean value per timestep in the input) used to skip certain input\ntimesteps when processing timeseries data.\n\nKeras will automatically pass the correct `mask` argument to `call()`\nfor layers that support it, when a mask is generated by a prior layer.\nMask-generating layers are the `Embedding` layer configured with\n`mask_zero=True`, and the `Masking` layer.\n\nTo learn more about masking and how to write masking-enabled layers,\nplease check out the guide [\""understanding padding and\nmasking\""](/guides/understanding_masking_and_padding/).\n\n## The `Model` class\n\nIn general, you will use the `Layer` class to define inner computation\nblocks, and will use the `Model` class to define the outer model -- the\nobject you will train.\n\nFor instance, in a ResNet50 model, you would have several ResNet blocks\nsubclassing `Layer`, and a single `Model` encompassing the entire\nResNet50 network.\n\nThe `Model` class has the same API as `Layer`, with the following\ndifferences:\n\n-   It has support for built-in training, evaluation, and prediction\n    methods (`fit()`, `evaluate()`, `predict()`).\n-   It exposes the list of its inner layers, via the `model$layers`\n    property.\n-   It exposes saving and serialization APIs (`save_model_tf()`,\n    `save_model_weights_tf()`, ...)\n\nEffectively, the `Layer` class corresponds to what we refer to in the\nliterature as a \""layer\"" (as in \""convolution layer\"" or \""recurrent layer\"")\nor as a \""block\"" (as in \""ResNet block\"" or \""Inception block\"").\n\nMeanwhile, the `Model` class corresponds to what is referred to in the\nliterature as a \""model\"" (as in \""deep learning model\"") or as a \""network\""\n(as in \""deep neural network\"").\n\nSo if you're wondering, \""should I use the `Layer` class or the `Model`\nclass?\"", ask yourself: will I need to call `fit()` on it? Will I need to\ncall `save()` on it? If so, go with `Model`. If not (either because your\nclass is just a block in a bigger system, or because you are writing\ntraining & saving code yourself), use `Layer`.\n\nFor instance, we could take our mini-resnet example above, and use it to\nbuild a `Model` that we could train with `fit()`, and that we could save\nwith `save_model_weights_tf()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nResNet(keras$Model) %py_class% {\n  initialize <- function(num_classes = 1000) {\n    super$initialize()\n    self$block_1 <- ResNetBlock()\n    self$block_2 <- ResNetBlock()\n    self$global_pool <- layer_global_average_pooling_2d()\n    self$classifier <- layer_dense(units = num_classes)\n  }\n\n  call <- function(inputs) {\n    x <- self$block_1(inputs)\n    x <- self$block_2(x)\n    x <- self$global_pool(x)\n    self$classifier(x)\n  }\n}\n\n\nresnet <- ResNet()\ndataset <- ...\nresnet %>% fit(dataset, epochs = 10)\nresnet %>% save_model_tf(filepath)\n```\n:::\n\n## Putting it all together: an end-to-end example\n\nHere's what you've learned so far:\n\n-   A `Layer` encapsulates a state (created in `initialize()` or\n    `build()`), and some computation (defined in `call()`).\n-   Layers can be recursively nested to create new, bigger computation\n    blocks.\n-   Layers can create and track losses (typically regularization losses)\n    as well as metrics, via `add_loss()` and `add_metric()`\n-   The outer container, the thing you want to train, is a `Model`. A\n    `Model` is just like a `Layer`, but with added training and\n    serialization utilities.\n\nLet's put all of these things together into an end-to-end example: we're\ngoing to implement a Variational AutoEncoder (VAE). We'll train it on\nMNIST digits.\n\nOur VAE will be a subclass of `Model`, built as a nested composition of\nlayers that subclass `Layer`. It will feature a regularization loss (KL\ndivergence).\n\n::: {.cell}\n\n```{.r .cell-code}\nSampling(keras$layers$Layer) %py_class% {\n  call <- function(inputs) {\n    c(z_mean, z_log_var) %<-% inputs\n    batch <- tf$shape(z_mean)[1]\n    dim <- tf$shape(z_mean)[2]\n    epsilon <- k_random_normal(shape = c(batch, dim))\n    z_mean + exp(0.5 * z_log_var) * epsilon\n  }\n}\n\n\nEncoder(keras$layers$Layer) %py_class% {\n  \""Maps MNIST digits to a triplet (z_mean, z_log_var, z).\""\n\n  initialize <- function(latent_dim = 32, intermediate_dim = 64, name = \""encoder\"", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \""relu\"")\n    self$dense_mean <- layer_dense(units = latent_dim)\n    self$dense_log_var <- layer_dense(units = latent_dim)\n    self$sampling <- Sampling()\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    z_mean <- self$dense_mean(x)\n    z_log_var <- self$dense_log_var(x)\n    z <- self$sampling(c(z_mean, z_log_var))\n    list(z_mean, z_log_var, z)\n  }\n}\n\n\nDecoder(keras$layers$Layer) %py_class% {\n  \""Converts z, the encoded digit vector, back into a readable digit.\""\n\n  initialize <- function(original_dim, intermediate_dim = 64, name = \""decoder\"", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \""relu\"")\n    self$dense_output <- layer_dense(units = original_dim, activation = \""sigmoid\"")\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    self$dense_output(x)\n  }\n}\n\n\nVariationalAutoEncoder(keras$Model) %py_class% {\n  \""Combines the encoder and decoder into an end-to-end model for training.\""\n\n  initialize <- function(original_dim, intermediate_dim = 64, latent_dim = 32,\n                         name = \""autoencoder\"", ...) {\n    super$initialize(name = name, ...)\n    self$original_dim <- original_dim\n    self$encoder <- Encoder(\n      latent_dim = latent_dim,\n      intermediate_dim = intermediate_dim\n    )\n    self$decoder <- Decoder(original_dim, intermediate_dim = intermediate_dim)\n  }\n\n  call <- function(inputs) {\n    c(z_mean, z_log_var, z) %<-% self$encoder(inputs)\n    reconstructed <- self$decoder(z)\n    # Add KL divergence regularization loss.\n    kl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\n    self$add_loss(kl_loss)\n    reconstructed\n  }\n}\n```\n:::\n\nLet's write a simple training loop on MNIST:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\nlibrary(tfdatasets)\n\n\noriginal_dim <- 784\nvae <- VariationalAutoEncoder(original_dim, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\nmse_loss_fn <- loss_mean_squared_error()\n\nloss_metric <- metric_mean()\n\nx_train <- dataset_mnist()$train$x %>%\n  array_reshape(c(60000, 784)) %>%\n  `/`(255)\n\ntrain_dataset <- tensor_slices_dataset(x_train) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(64)\n\nepochs <- 2\n\n# Iterate over epochs.\nfor (epoch in seq(epochs)) {\n  cat(sprintf(\""Start of epoch %d\\n\"", epoch))\n\n  # Iterate over the batches of the dataset.\n  # autograph lets you use tfdatasets in `for` and `while`\n  autograph({\n    step <- 0\n    for (x_batch_train in train_dataset) {\n      with(tf$GradientTape() %as% tape, {\n        ## Note: we're four opaque contexts deep here (for, autograph, for,\n        ## with), When in doubt about the objects or methods that are available\n        ## (e.g., what is `tape` here?), remember you can always drop into a\n        ## debugger right here:\n        # browser()\n\n        reconstructed <- vae(x_batch_train)\n        # Compute reconstruction loss\n        loss <- mse_loss_fn(x_batch_train, reconstructed)\n\n        loss %<>% add(vae$losses[[1]]) # Add KLD regularization loss\n      })\n      grads <- tape$gradient(loss, vae$trainable_weights)\n      optimizer$apply_gradients(\n        purrr::transpose(list(grads, vae$trainable_weights)))\n\n      loss_metric(loss)\n\n      step %<>% add(1)\n      if (step %% 100 == 0) {\n        cat(sprintf(\""step %d: mean loss = %.4f\\n\"", step, loss_metric$result()))\n      }\n    }\n  })\n}\n```\n\n::: {.cell-output-stdout}\n```\nStart of epoch 1\nstep 100: mean loss = 0.1247\nstep 200: mean loss = 0.0987\nstep 300: mean loss = 0.0888\nstep 400: mean loss = 0.0839\nstep 500: mean loss = 0.0806\nstep 600: mean loss = 0.0785\nstep 700: mean loss = 0.0770\nstep 800: mean loss = 0.0759\nstep 900: mean loss = 0.0749\nStart of epoch 2\nstep 100: mean loss = 0.0739\nstep 200: mean loss = 0.0734\nstep 300: mean loss = 0.0729\nstep 400: mean loss = 0.0726\nstep 500: mean loss = 0.0722\nstep 600: mean loss = 0.0719\nstep 700: mean loss = 0.0716\nstep 800: mean loss = 0.0714\nstep 900: mean loss = 0.0711\n```\n:::\n:::\n\nNote that since the VAE is subclassing `Model`, it features built-in\ntraining loops. So you could also have trained it like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nvae <- VariationalAutoEncoder(784, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\n\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 2, batch_size = 64)\n```\n:::\n\n## Beyond object-oriented development: the Functional API\n\nIf you prefer a less object-oriented way of programming, you can also\nbuild models using the [Functional API](/guides/functional_api/).\nImportantly, choosing one style or another does not prevent you from\nleveraging components written in the other style: you can always\nmix-and-match.\n\nFor instance, the Functional API example below reuses the same\n`Sampling` layer we defined in the example above:\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_dim <- 784\nintermediate_dim <- 64\nlatent_dim <- 32\n\n# Define encoder model.\noriginal_inputs <- layer_input(shape = original_dim, name = \""encoder_input\"")\nx <- layer_dense(units = intermediate_dim, activation = \""relu\"")(original_inputs)\nz_mean <- layer_dense(units = latent_dim, name = \""z_mean\"")(x)\nz_log_var <- layer_dense(units = latent_dim, name = \""z_log_var\"")(x)\nz <- Sampling()(list(z_mean, z_log_var))\nencoder <- keras_model(inputs = original_inputs, outputs = z, name = \""encoder\"")\n\n# Define decoder model.\nlatent_inputs <- layer_input(shape = latent_dim, name = \""z_sampling\"")\nx <- layer_dense(units = intermediate_dim, activation = \""relu\"")(latent_inputs)\noutputs <- layer_dense(units = original_dim, activation = \""sigmoid\"")(x)\ndecoder <- keras_model(inputs = latent_inputs, outputs = outputs, name = \""decoder\"")\n\n# Define VAE model.\noutputs <- decoder(z)\nvae <- keras_model(inputs = original_inputs, outputs = outputs, name = \""vae\"")\n\n# Add KL divergence regularization loss.\nkl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\nvae$add_loss(kl_loss)\n\n# Train.\noptimizer <- keras$optimizers$Adam(learning_rate = 1e-3)\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 3, batch_size = 64)\n```\n:::\n\nFor more information, make sure to read the [Functional API\nguide](/guides/functional_api/).\n\n## Defining custom layers and models in an R package\n\nUnfortunately you can't use anything that creates references to Python\nobjects, at the top-level of an R package.\n\nHere is why: when you build an R package, all the R files in the `R/`\ndirectory get sourced in an R environment (the package namespace), and\nthen that environment is saved as part of the package bundle. Loading\nthe package means restoring the saved R environment. This means that the\nR code only gets sourced once, at build time. If you create references\nto external objects (e.g., Python objects) at package build time, they\nwill be NULL pointers when the package is loaded, because the external\nobjects they pointed to at build time no longer exist at load time.\n\nThe solution is to delay creating references to Python objects until run\ntime. Fortunately, `%py_class%`, `Layer()`, and\n`create_layer_wrapper(R6Class(...))` are all lazy about initializing the\nPython reference, so they are safe to define and export in an R package.\n\nIf you're writing an R package that uses keras and reticulate, [this\narticle](https://rstudio.github.io/reticulate/articles/package.html)\nmight be helpful to read over.\n\n## Summary\n\nIn this guide you learned about creating custom layers and models in\nkeras.\n\n-   The constructors available: `new_layer_class()`, `%py_class%`,\n    `create_layer_wrapper()`, `R6Class()`, `Layer()`.\n-   What methods to you might want to define to your model:\n    `initialize()`, `build()`, `call()`, and `get_config()`.\n-   What convenience methods are available when you subclass\n    `keras$layers$Layer`: `add_weight()`, `add_loss()`, and\n    `add_metric()`"",
+    ""markdown"": ""---\ntitle: \""Writing `Layer` and `Model` objects from scratch.\""\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\ntf_version()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] '2.9'\n```\n:::\n:::\n\n\n## The `Layer` class: a combination of state (weights) and some computation\n\nOne of the central abstractions in Keras is the `Layer` class. A layer\nencapsulates both a state (the layer's \""weights\"") and a transformation\nfrom inputs to outputs (a \""call\"", the layer's forward pass).\n\nHere's a densely-connected layer. It has a state: the variables `w` and\n`b`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(\n      initial_value = w_init(\n        shape = shape(input_dim, units),\n        dtype = \""float32\""\n      ),\n      trainable = TRUE\n    )\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(\n      initial_value = b_init(shape = shape(units), dtype = \""float32\""),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\n\nYou would use a layer by calling it on some tensor input(s), much like a\nregular function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-0.12252003 -0.07721802  0.02356294 -0.04726242]\n [-0.12252003 -0.07721802  0.02356294 -0.04726242]], shape=(2, 4), dtype=float32)\n```\n:::\n:::\n\n\n`Linear` behaves similarly to a layer present in the Python interface to\nkeras (e.g., `keras$layers$Dense`).\n\nHowever, one additional step is needed to make it behave like the\nbuiltin layers present in the keras R package (e.g., `layer_dense()`).\n\nKeras layers in R are designed to compose nicely with the pipe operator\n(`%>%`), so that the layer instance is conveniently created on demand\nwhen an existing model or tensor is piped in. In order to make a custom\nlayer similarly compose nicely with the pipe, you can call\n`create_layer_wrapper()` on the layer class constructor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_linear <- create_layer_wrapper(Linear)\n```\n:::\n\n\nNow `layer_linear` is a layer constructor that composes nicely with\n`%>%`, just like the built-in layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_linear(4, 2)\n\nmodel(k_ones(c(2, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-0.07688868 -0.02687556 -0.04943929  0.00642188]\n [-0.07688868 -0.02687556 -0.04943929  0.00642188]], shape=(2, 4), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n linear_1 (Linear)                (2, 4)                        12          \n============================================================================\nTotal params: 12\nTrainable params: 12\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nBecause the pattern above is so common, there is a convenience function\nthat combines the steps of subclassing `keras$layers$Layer` and calling\n`create_layer_wrapper` on the output: the `Layer` function. The\n`layer_linear` defined below is identical to the `layer_linear` defined\nabove.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer_linear <- Layer(\n  \""Linear\"",\n  initialize =  function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- tf$Variable(initial_value = w_init(shape = shape(input_dim, units),\n                                                 dtype = \""float32\""),\n                          trainable = TRUE)\n    b_init <- tf$zeros_initializer()\n    self$b <- tf$Variable(initial_value = b_init(shape = shape(units),\n                                                 dtype = \""float32\""),\n                          trainable = TRUE)\n  },\n\n  call = function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n)\n```\n:::\n\n\nFor the remainder of this vignette we'll be using the `%py_class%`\nconstructor. However, in your own code feel free to use\n`create_layer_wrapper` and/or `Layer` if you prefer.\n\nNote that the weights `w` and `b` are automatically tracked by the layer\nupon being set as layer attributes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstopifnot(all.equal(\n  linear_layer$weights,\n  list(linear_layer$w, linear_layer$b)\n))\n```\n:::\n\n\nYou also have access to a quicker shortcut for adding a weight to a\nlayer: the `add_weight()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    w_init <- tf$random_normal_initializer()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \""zeros\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nlinear_layer <- Linear(4, 2)\ny <- linear_layer(x)\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[-0.02470569 -0.08144875  0.05874256 -0.06067646]\n [-0.02470569 -0.08144875  0.05874256 -0.06067646]], shape=(2, 4), dtype=float32)\n```\n:::\n:::\n\n\n## Layers can have non-trainable weights\n\nBesides trainable weights, you can add non-trainable weights to a layer\nas well. Such weights are meant not to be taken into account during\nbackpropagation, when you are training the layer.\n\nHere's how to add and use a non-trainable weight:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nComputeSum(keras$layers$Layer) %py_class% {\n  initialize <- function(input_dim) {\n    super$initialize()\n    self$total <- tf$Variable(\n      initial_value = tf$zeros(shape(input_dim)),\n      trainable = FALSE\n    )\n  }\n\n  call <- function(inputs) {\n    self$total$assign_add(tf$reduce_sum(inputs, axis = 0L))\n    self$total\n  }\n}\n\nx <- tf$ones(shape(2, 2))\nmy_sum <- ComputeSum(2)\ny <- my_sum(x)\nprint(as.numeric(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 2\n```\n:::\n\n```{.r .cell-code}\ny <- my_sum(x)\nprint(as.numeric(y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 4\n```\n:::\n:::\n\n\nIt's part of `layer$weights`, but it gets categorized as a non-trainable\nweight:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\""weights:\"", length(my_sum$weights), \""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 1 \n```\n:::\n\n```{.r .cell-code}\ncat(\""non-trainable weights:\"", length(my_sum$non_trainable_weights), \""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon-trainable weights: 1 \n```\n:::\n\n```{.r .cell-code}\n# It's not included in the trainable weights:\ncat(\""trainable_weights:\"", my_sum$trainable_weights, \""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights:  \n```\n:::\n:::\n\n\n## Best practice: deferring weight creation until the shape of the inputs is known\n\nOur `Linear` layer above took an `input_dim`argument that was used to\ncompute the shape of the weights `w` and `b` in `initialize()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, input_dim = 32) {\n    super$initialize()\n    self$w <- self$add_weight(\n      shape = shape(input_dim, units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(units),\n      initializer = \""zeros\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\n\nIn many cases, you may not know in advance the size of your inputs, and\nyou would like to lazily create weights when that value becomes known,\nsome time after instantiating the layer.\n\nIn the Keras API, we recommend creating layer weights in the\n`build(self, inputs_shape)` method of your layer. Like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n}\n```\n:::\n\n\nThe `build()` method of your layer will automatically run the first time\nyour layer instance is called. You now have a layer that can handle an\narbitrary number of input features:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# At instantiation, we don't know on what inputs this is going to get called\nlinear_layer <- Linear(32)\n\n# The layer's weights are created dynamically the first time the layer is called\ny <- linear_layer(x)\n```\n:::\n\n\nImplementing `build()` separately as shown above nicely separates\ncreating weights only once from using weights in every call. However,\nfor some advanced custom layers, it can become impractical to separate\nthe state creation and computation. Layer implementers are allowed to\ndefer weight creation to the first `call()`, but need to take care that\nlater calls use the same weights. In addition, since `call()` is likely\nto be executed for the first time inside a `tf_function()`, any variable\ncreation that takes place in `call()` should be wrapped in a\n`tf$init_scope()`.\n\n## Layers are recursively composable\n\nIf you assign a Layer instance as an attribute of another Layer, the\nouter layer will start tracking the weights created by the inner layer.\n\nWe recommend creating such sublayers in the `initialize()` method and\nleave it to the first `call()` to trigger building their weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's assume we are reusing the Linear class\n# with a `build` method that we defined above.\nMLPBlock(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$linear_1 <- Linear(32)\n    self$linear_2 <- Linear(32)\n    self$linear_3 <- Linear(1)\n  }\n\n  call <- function(inputs) {\n    x <- self$linear_1(inputs)\n    x <- tf$nn$relu(x)\n    x <- self$linear_2(x)\n    x <- tf$nn$relu(x)\n    self$linear_3(x)\n  }\n}\n\nmlp <- MLPBlock()\ny <- mlp(tf$ones(shape = shape(3, 64))) # The first call to the `mlp` will create the weights\ncat(\""weights:\"", length(mlp$weights), \""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 6 \n```\n:::\n\n```{.r .cell-code}\ncat(\""trainable weights:\"", length(mlp$trainable_weights), \""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable weights: 6 \n```\n:::\n:::\n\n\n## The `add_loss()` method\n\nWhen writing the `call()` method of a layer, you can create loss tensors\nthat you will want to use later, when writing your training loop. This\nis doable by calling `self$add_loss(value)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A layer that creates an activity regularization loss\nActivityRegularizationLayer(keras$layers$Layer) %py_class% {\n  initialize <- function(rate = 1e-2) {\n    super$initialize()\n    self$rate <- rate\n  }\n\n  call <- function(inputs) {\n    self$add_loss(self$rate * tf$reduce_sum(inputs))\n    inputs\n  }\n}\n```\n:::\n\n\nThese losses (including those created by any inner layer) can be\nretrieved via `layer$losses`. This property is reset at the start of\nevery `call()` to the top-level layer, so that `layer$losses` always\ncontains the loss values created during the last forward pass.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOuterLayer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$activity_reg <- ActivityRegularizationLayer(1e-2)\n  }\n  call <- function(inputs) {\n    self$activity_reg(inputs)\n  }\n}\n\nlayer <- OuterLayer()\nstopifnot(length(layer$losses) == 0) # No losses yet since the layer has never been called\n\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # We created one loss value\n\n# `layer$losses` gets reset at the start of each call()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\nstopifnot(length(layer$losses) == 1) # This is the loss created during the call above\n```\n:::\n\n\nIn addition, the `loss` property also contains regularization losses\ncreated for the weights of any inner layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOuterLayerWithKernelRegularizer(keras$layers$Layer) %py_class% {\n  initialize <- function() {\n    super$initialize()\n    self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))\n  }\n  call <- function(inputs) {\n    self$dense(inputs)\n  }\n}\n\nlayer <- OuterLayerWithKernelRegularizer()\nlayer(tf$zeros(shape(1, 1))) |> invisible()\n\n# This is `1e-3 * sum(layer$dense$kernel ** 2)`,\n# created by the `kernel_regularizer` above.\nprint(layer$losses)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\ntf.Tensor(0.0019989477, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThese losses are meant to be taken into account when writing training\nloops, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Instantiate an optimizer.\noptimizer <- optimizer_sgd(learning_rate = 1e-3)\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\n# Iterate over the batches of a dataset.\ndataset_iterator <- reticulate::as_iterator(train_dataset)\nwhile(!is.null(batch <- iter_next(dataset_iterator))) {\n  c(x_batch_train, y_batch_train) %<-% batch\n  with(tf$GradientTape() %as% tape, {\n    logits <- layer(x_batch_train) # Logits for this minibatch\n    # Loss value for this minibatch\n    loss_value <- loss_fn(y_batch_train, logits)\n    # Add extra losses created during this forward pass:\n    loss_value <- loss_value + sum(model$losses)\n  })\n  grads <- tape$gradient(loss_value, model$trainable_weights)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, model$trainable_weights)))\n}\n```\n:::\n\n\nFor a detailed guide about writing training loops, see the [guide to\nwriting a training loop from\nscratch](/guides/writing_a_training_loop_from_scratch/).\n\nThese losses also work seamlessly with `fit()` (they get automatically\nsummed and added to the main loss, if any):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape(3))\noutput <- input %>% layer_activity_regularization()\n# output <- ActivityRegularizationLayer()(input)\nmodel <- keras_model(input, output)\n\n# If there is a loss passed in `compile`, the regularization\n# losses get added to it\nmodel %>% compile(optimizer = \""adam\"", loss = \""mse\"")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n\n# It's also possible not to pass any loss in `compile`,\n# since the model already has a loss to minimize, via the `add_loss`\n# call during the forward pass!\nmodel %>% compile(optimizer = \""adam\"")\nmodel %>% fit(k_random_uniform(c(2, 3)),\n  k_random_uniform(c(2, 3)),\n  epochs = 1, verbose = FALSE\n)\n```\n:::\n\n\n## The `add_metric()` method\n\nSimilarly to `add_loss()`, layers also have an `add_metric()` method for\ntracking the moving average of a quantity during training.\n\nConsider the following layer: a \""logistic endpoint\"" layer. It takes as\ninputs predictions and targets, it computes a loss which it tracks via\n`add_loss()`, and it computes an accuracy scalar, which it tracks via\n`add_metric()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLogisticEndpoint(keras$layers$Layer) %py_class% {\n  initialize <- function(name = NULL) {\n    super$initialize(name = name)\n    self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)\n    self$accuracy_fn <- metric_binary_accuracy()\n  }\n\n  call <- function(targets, logits, sample_weights = NULL) {\n    # Compute the training-time loss value and add it\n    # to the layer using `self$add_loss()`.\n    loss <- self$loss_fn(targets, logits, sample_weights)\n    self$add_loss(loss)\n\n    # Log accuracy as a metric and add it\n    # to the layer using `self.add_metric()`.\n    acc <- self$accuracy_fn(targets, logits, sample_weights)\n    self$add_metric(acc, name = \""accuracy\"")\n\n    # Return the inference-time prediction tensor (for `.predict()`).\n    tf$nn$softmax(logits)\n  }\n}\n```\n:::\n\n\nMetrics tracked in this way are accessible via `layer$metrics`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- LogisticEndpoint()\n\ntargets <- tf$ones(shape(2, 2))\nlogits <- tf$ones(shape(2, 2))\ny <- layer(targets, logits)\n\ncat(\""layer$metrics: \"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlayer$metrics: \n```\n:::\n\n```{.r .cell-code}\nstr(layer$metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 1\n $ :BinaryAccuracy(name=binary_accuracy,dtype=float32,threshold=0.5)\n```\n:::\n\n```{.r .cell-code}\ncat(\""current accuracy value:\"", as.numeric(layer$metrics[[1]]$result()), \""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncurrent accuracy value: 1 \n```\n:::\n:::\n\n\nJust like for `add_loss()`, these metrics are tracked by `fit()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(shape(3), name = \""inputs\"")\ntargets <- layer_input(shape(10), name = \""targets\"")\nlogits <- inputs %>% layer_dense(10)\npredictions <- LogisticEndpoint(name = \""predictions\"")(logits, targets)\n\nmodel <- keras_model(inputs = list(inputs, targets), outputs = predictions)\nmodel %>% compile(optimizer = \""adam\"")\n\ndata <- list(\n  inputs = k_random_uniform(c(3, 3)),\n  targets = k_random_uniform(c(3, 10))\n)\n\nmodel %>% fit(data, epochs = 1, verbose = FALSE)\n```\n:::\n\n\n## You can optionally enable serialization on your layers\n\nIf you need your custom layers to be serializable as part of a\n[Functional model](/guides/functional_api/), you can optionally\nimplement a `get_config()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32) {\n    super$initialize()\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    list(units = self$units)\n  }\n}\n\n\n# Now you can recreate the layer from its config:\nlayer <- Linear(64)\nconfig <- layer$get_config()\nprint(config)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$units\n[1] 64\n```\n:::\n\n```{.r .cell-code}\nnew_layer <- Linear$from_config(config)\n```\n:::\n\n\nNote that the `initialize()` method of the base `Layer` class takes some\nadditional named arguments, in particular a `name` and a `dtype`. It's\ngood practice to pass these arguments to the parent class in\n`initialize()` and to include them in the layer config:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLinear(keras$layers$Layer) %py_class% {\n  initialize <- function(units = 32, ...) {\n    super$initialize(...)\n    self$units <- units\n  }\n\n  build <- function(input_shape) {\n    self$w <- self$add_weight(\n      shape = shape(tail(input_shape, 1), self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n    self$b <- self$add_weight(\n      shape = shape(self$units),\n      initializer = \""random_normal\"",\n      trainable = TRUE\n    )\n  }\n\n  call <- function(inputs) {\n    tf$matmul(inputs, self$w) + self$b\n  }\n\n  get_config <- function() {\n    config <- super$get_config()\n    config$units <- self$units\n    config\n  }\n}\n\n\nlayer <- Linear(64)\nconfig <- layer$get_config()\nstr(config)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ name     : chr \""linear_9\""\n $ trainable: logi TRUE\n $ dtype    : chr \""float32\""\n $ units    : num 64\n```\n:::\n\n```{.r .cell-code}\nnew_layer <- Linear$from_config(config)\n```\n:::\n\n\nIf you need more flexibility when deserializing the layer from its\nconfig, you can also override the `from_config()` class method. This is\nthe base implementation of `from_config()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrom_config <- function(cls, config) do.call(cls, config)\n```\n:::\n\n\nTo learn more about serialization and saving, see the complete [guide to\nsaving and serializing models](/guides/serialization_and_saving/).\n\n## Privileged `training` argument in the `call()` method\n\nSome layers, in particular the `BatchNormalization` layer and the\n`Dropout` layer, have different behaviors during training and inference.\nFor such layers, it is standard practice to expose a `training`\n(boolean) argument in the `call()` method.\n\nBy exposing this argument in `call()`, you enable the built-in training\nand evaluation loops (e.g. `fit()`) to correctly use the layer in\ntraining and inference. Note, the default of `NULL` means that the\ntraining parameter will be inferred by keras from the training context\n(e.g., it will be `TRUE` if called from `fit()`, `FALSE` if called from\n`predict()`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomDropout(keras$layers$Layer) %py_class% {\n  initialize <- function(rate, ...) {\n    super$initialize(...)\n    self$rate <- rate\n  }\n  call <- function(inputs, training = NULL) {\n    if (isTRUE(training)) {\n      return(tf$nn$dropout(inputs, rate = self$rate))\n    }\n    inputs\n  }\n}\n```\n:::\n\n\n## Privileged `mask` argument in the `call()` method\n\nThe other privileged argument supported by `call()` is the `mask`\nargument.\n\nYou will find it in all Keras RNN layers. A mask is a boolean tensor\n(one boolean value per timestep in the input) used to skip certain input\ntimesteps when processing timeseries data.\n\nKeras will automatically pass the correct `mask` argument to `call()`\nfor layers that support it, when a mask is generated by a prior layer.\nMask-generating layers are the `Embedding` layer configured with\n`mask_zero=True`, and the `Masking` layer.\n\nTo learn more about masking and how to write masking-enabled layers,\nplease check out the guide [\""understanding padding and\nmasking\""](/guides/understanding_masking_and_padding/).\n\n## The `Model` class\n\nIn general, you will use the `Layer` class to define inner computation\nblocks, and will use the `Model` class to define the outer model -- the\nobject you will train.\n\nFor instance, in a ResNet50 model, you would have several ResNet blocks\nsubclassing `Layer`, and a single `Model` encompassing the entire\nResNet50 network.\n\nThe `Model` class has the same API as `Layer`, with the following\ndifferences:\n\n-   It has support for built-in training, evaluation, and prediction\n    methods (`fit()`, `evaluate()`, `predict()`).\n-   It exposes the list of its inner layers, via the `model$layers`\n    property.\n-   It exposes saving and serialization APIs (`save_model_tf()`,\n    `save_model_weights_tf()`, ...)\n\nEffectively, the `Layer` class corresponds to what we refer to in the\nliterature as a \""layer\"" (as in \""convolution layer\"" or \""recurrent layer\"")\nor as a \""block\"" (as in \""ResNet block\"" or \""Inception block\"").\n\nMeanwhile, the `Model` class corresponds to what is referred to in the\nliterature as a \""model\"" (as in \""deep learning model\"") or as a \""network\""\n(as in \""deep neural network\"").\n\nSo if you're wondering, \""should I use the `Layer` class or the `Model`\nclass?\"", ask yourself: will I need to call `fit()` on it? Will I need to\ncall `save()` on it? If so, go with `Model`. If not (either because your\nclass is just a block in a bigger system, or because you are writing\ntraining & saving code yourself), use `Layer`.\n\nFor instance, we could take our mini-resnet example above, and use it to\nbuild a `Model` that we could train with `fit()`, and that we could save\nwith `save_model_weights_tf()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nResNet(keras$Model) %py_class% {\n  initialize <- function(num_classes = 1000) {\n    super$initialize()\n    self$block_1 <- ResNetBlock()\n    self$block_2 <- ResNetBlock()\n    self$global_pool <- layer_global_average_pooling_2d()\n    self$classifier <- layer_dense(units = num_classes)\n  }\n\n  call <- function(inputs) {\n    x <- self$block_1(inputs)\n    x <- self$block_2(x)\n    x <- self$global_pool(x)\n    self$classifier(x)\n  }\n}\n\n\nresnet <- ResNet()\ndataset <- ...\nresnet %>% fit(dataset, epochs = 10)\nresnet %>% save_model_tf(filepath)\n```\n:::\n\n\n## Putting it all together: an end-to-end example\n\nHere's what you've learned so far:\n\n-   A `Layer` encapsulates a state (created in `initialize()` or\n    `build()`), and some computation (defined in `call()`).\n-   Layers can be recursively nested to create new, bigger computation\n    blocks.\n-   Layers can create and track losses (typically regularization losses)\n    as well as metrics, via `add_loss()` and `add_metric()`\n-   The outer container, the thing you want to train, is a `Model`. A\n    `Model` is just like a `Layer`, but with added training and\n    serialization utilities.\n\nLet's put all of these things together into an end-to-end example: we're\ngoing to implement a Variational AutoEncoder (VAE). We'll train it on\nMNIST digits.\n\nOur VAE will be a subclass of `Model`, built as a nested composition of\nlayers that subclass `Layer`. It will feature a regularization loss (KL\ndivergence).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSampling(keras$layers$Layer) %py_class% {\n  call <- function(inputs) {\n    c(z_mean, z_log_var) %<-% inputs\n    batch <- tf$shape(z_mean)[1]\n    dim <- tf$shape(z_mean)[2]\n    epsilon <- k_random_normal(shape = c(batch, dim))\n    z_mean + exp(0.5 * z_log_var) * epsilon\n  }\n}\n\n\nEncoder(keras$layers$Layer) %py_class% {\n  \""Maps MNIST digits to a triplet (z_mean, z_log_var, z).\""\n\n  initialize <- function(latent_dim = 32, intermediate_dim = 64, name = \""encoder\"", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \""relu\"")\n    self$dense_mean <- layer_dense(units = latent_dim)\n    self$dense_log_var <- layer_dense(units = latent_dim)\n    self$sampling <- Sampling()\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    z_mean <- self$dense_mean(x)\n    z_log_var <- self$dense_log_var(x)\n    z <- self$sampling(c(z_mean, z_log_var))\n    list(z_mean, z_log_var, z)\n  }\n}\n\n\nDecoder(keras$layers$Layer) %py_class% {\n  \""Converts z, the encoded digit vector, back into a readable digit.\""\n\n  initialize <- function(original_dim, intermediate_dim = 64, name = \""decoder\"", ...) {\n    super$initialize(name = name, ...)\n    self$dense_proj <- layer_dense(units = intermediate_dim, activation = \""relu\"")\n    self$dense_output <- layer_dense(units = original_dim, activation = \""sigmoid\"")\n  }\n\n  call <- function(inputs) {\n    x <- self$dense_proj(inputs)\n    self$dense_output(x)\n  }\n}\n\n\nVariationalAutoEncoder(keras$Model) %py_class% {\n  \""Combines the encoder and decoder into an end-to-end model for training.\""\n\n  initialize <- function(original_dim, intermediate_dim = 64, latent_dim = 32,\n                         name = \""autoencoder\"", ...) {\n    super$initialize(name = name, ...)\n    self$original_dim <- original_dim\n    self$encoder <- Encoder(\n      latent_dim = latent_dim,\n      intermediate_dim = intermediate_dim\n    )\n    self$decoder <- Decoder(original_dim, intermediate_dim = intermediate_dim)\n  }\n\n  call <- function(inputs) {\n    c(z_mean, z_log_var, z) %<-% self$encoder(inputs)\n    reconstructed <- self$decoder(z)\n    # Add KL divergence regularization loss.\n    kl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\n    self$add_loss(kl_loss)\n    reconstructed\n  }\n}\n```\n:::\n\n\nLet's write a simple training loop on MNIST:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfautograph)\nlibrary(tfdatasets)\n\n\noriginal_dim <- 784\nvae <- VariationalAutoEncoder(original_dim, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\nmse_loss_fn <- loss_mean_squared_error()\n\nloss_metric <- metric_mean()\n\nx_train <- dataset_mnist()$train$x %>%\n  array_reshape(c(60000, 784)) %>%\n  `/`(255)\n\ntrain_dataset <- tensor_slices_dataset(x_train) %>%\n  dataset_shuffle(buffer_size = 1024) %>%\n  dataset_batch(64)\n\nepochs <- 2\n\n# Iterate over epochs.\nfor (epoch in seq(epochs)) {\n  cat(sprintf(\""Start of epoch %d\\n\"", epoch))\n\n  # Iterate over the batches of the dataset.\n  # autograph lets you use tfdatasets in `for` and `while`\n  autograph({\n    step <- 0\n    for (x_batch_train in train_dataset) {\n      with(tf$GradientTape() %as% tape, {\n        ## Note: we're four opaque contexts deep here (for, autograph, for,\n        ## with), When in doubt about the objects or methods that are available\n        ## (e.g., what is `tape` here?), remember you can always drop into a\n        ## debugger right here:\n        # browser()\n\n        reconstructed <- vae(x_batch_train)\n        # Compute reconstruction loss\n        loss <- mse_loss_fn(x_batch_train, reconstructed)\n\n        loss %<>% add(vae$losses[[1]]) # Add KLD regularization loss\n      })\n      grads <- tape$gradient(loss, vae$trainable_weights)\n      optimizer$apply_gradients(\n        purrr::transpose(list(grads, vae$trainable_weights)))\n\n      loss_metric(loss)\n\n      step %<>% add(1)\n      if (step %% 100 == 0) {\n        cat(sprintf(\""step %d: mean loss = %.4f\\n\"", step, loss_metric$result()))\n      }\n    }\n  })\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart of epoch 1\nstep 100: mean loss = 0.1280\nstep 200: mean loss = 0.1003\nstep 300: mean loss = 0.0899\nstep 400: mean loss = 0.0848\nstep 500: mean loss = 0.0813\nstep 600: mean loss = 0.0791\nstep 700: mean loss = 0.0774\nstep 800: mean loss = 0.0762\nstep 900: mean loss = 0.0752\nStart of epoch 2\nstep 100: mean loss = 0.0742\nstep 200: mean loss = 0.0737\nstep 300: mean loss = 0.0732\nstep 400: mean loss = 0.0729\nstep 500: mean loss = 0.0725\nstep 600: mean loss = 0.0721\nstep 700: mean loss = 0.0718\nstep 800: mean loss = 0.0716\nstep 900: mean loss = 0.0713\n```\n:::\n:::\n\n\nNote that since the VAE is subclassing `Model`, it features built-in\ntraining loops. So you could also have trained it like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvae <- VariationalAutoEncoder(784, 64, 32)\n\noptimizer <- optimizer_adam(learning_rate = 1e-3)\n\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 2, batch_size = 64)\n```\n:::\n\n\n## Beyond object-oriented development: the Functional API\n\nIf you prefer a less object-oriented way of programming, you can also\nbuild models using the [Functional API](/guides/functional_api/).\nImportantly, choosing one style or another does not prevent you from\nleveraging components written in the other style: you can always\nmix-and-match.\n\nFor instance, the Functional API example below reuses the same\n`Sampling` layer we defined in the example above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_dim <- 784\nintermediate_dim <- 64\nlatent_dim <- 32\n\n# Define encoder model.\noriginal_inputs <- layer_input(shape = original_dim, name = \""encoder_input\"")\nx <- layer_dense(units = intermediate_dim, activation = \""relu\"")(original_inputs)\nz_mean <- layer_dense(units = latent_dim, name = \""z_mean\"")(x)\nz_log_var <- layer_dense(units = latent_dim, name = \""z_log_var\"")(x)\nz <- Sampling()(list(z_mean, z_log_var))\nencoder <- keras_model(inputs = original_inputs, outputs = z, name = \""encoder\"")\n\n# Define decoder model.\nlatent_inputs <- layer_input(shape = latent_dim, name = \""z_sampling\"")\nx <- layer_dense(units = intermediate_dim, activation = \""relu\"")(latent_inputs)\noutputs <- layer_dense(units = original_dim, activation = \""sigmoid\"")(x)\ndecoder <- keras_model(inputs = latent_inputs, outputs = outputs, name = \""decoder\"")\n\n# Define VAE model.\noutputs <- decoder(z)\nvae <- keras_model(inputs = original_inputs, outputs = outputs, name = \""vae\"")\n\n# Add KL divergence regularization loss.\nkl_loss <- -0.5 * tf$reduce_mean(z_log_var - tf$square(z_mean) - tf$exp(z_log_var) + 1)\nvae$add_loss(kl_loss)\n\n# Train.\noptimizer <- keras$optimizers$Adam(learning_rate = 1e-3)\nvae %>% compile(optimizer, loss = loss_mean_squared_error())\nvae %>% fit(x_train, x_train, epochs = 3, batch_size = 64)\n```\n:::\n\n\nFor more information, make sure to read the [Functional API\nguide](/guides/functional_api/).\n\n## Defining custom layers and models in an R package\n\nUnfortunately you can't use anything that creates references to Python\nobjects, at the top-level of an R package.\n\nHere is why: when you build an R package, all the R files in the `R/`\ndirectory get sourced in an R environment (the package namespace), and\nthen that environment is saved as part of the package bundle. Loading\nthe package means restoring the saved R environment. This means that the\nR code only gets sourced once, at build time. If you create references\nto external objects (e.g., Python objects) at package build time, they\nwill be NULL pointers when the package is loaded, because the external\nobjects they pointed to at build time no longer exist at load time.\n\nThe solution is to delay creating references to Python objects until run\ntime. Fortunately, `%py_class%`, `Layer()`, and\n`create_layer_wrapper(R6Class(...))` are all lazy about initializing the\nPython reference, so they are safe to define and export in an R package.\n\nIf you're writing an R package that uses keras and reticulate, [this\narticle](https://rstudio.github.io/reticulate/articles/package.html)\nmight be helpful to read over.\n\n## Summary\n\nIn this guide you learned about creating custom layers and models in\nkeras.\n\n-   The constructors available: `new_layer_class()`, `%py_class%`,\n    `create_layer_wrapper()`, `R6Class()`, `Layer()`.\n-   What methods to you might want to define to your model:\n    `initialize()`, `build()`, `call()`, and `get_config()`.\n-   What convenience methods are available when you subclass\n    `keras$layers$Layer`: `add_weight()`, `add_loss()`, and\n    `add_metric()`\n"",
     ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],
-    ""includes"": [],
+    ""includes"": {},
     ""engineDependencies"": {},
     ""preserve"": {},
-    ""postProcess"": null
+    ""postProcess"": true
   }
 }
\ No newline at end of file

---FILE: _freeze/keras/guides/preprocessing_layers/execute-results/html.json---
@@ -1,14 +1,14 @@
 {
   ""hash"": ""811833996f83e3c2fc612b729d5d3c2b"",
   ""result"": {
-    ""markdown"": ""---\ntitle: Working with preprocessing layers\nauthor: Francois Chollet, Mark Omernick, Tomasz Kalinowski\ndescription: Overview of how to leverage preprocessing layers to create end-to-end models.\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n## Keras preprocessing\n\nThe Keras preprocessing layers API allows developers to build\nKeras-native input processing pipelines. These input processing\npipelines can be used as independent preprocessing code in non-Keras\nworkflows, combined directly with Keras models, and exported as part of\na Keras SavedModel.\n\nWith Keras preprocessing layers, you can build and export models that\nare truly end-to-end: models that accept raw images or raw structured\ndata as input; models that handle feature normalization or feature value\nindexing on their own.\n\n## Available preprocessing layers\n\n### Text preprocessing\n\n-   `layer_text_vectorization()`: turns raw strings into an encoded\n    representation that can be read by a `layer_embedding()` or\n    `layer_dense()` layer.\n\n### Numerical features preprocessing\n\n-   `layer_normalization()`: performs feature-wise normalization of\n    input features.\n-   `layer_discretization()`: turns continuous numerical features into\n    integer categorical features.\n\n### Categorical features preprocessing\n\n-   `layer_category_encoding()`: turns integer categorical features into\n    one-hot, multi-hot, or count-based, dense representations.\n-   `layer_hashing()`: performs categorical feature hashing, also known\n    as the \""hashing trick\"".\n-   `layer_string_lookup()`: turns string categorical values into an\n    encoded representation that can be read by an `Embedding` layer or\n    `Dense` layer.\n-   `layer_integer_lookup()`: turns integer categorical values into an\n    encoded representation that can be read by an `Embedding` layer or\n    `Dense` layer.\n\n### Image preprocessing\n\nThese layers are for standardizing the inputs of an image model.\n\n-   `layer_resizing()`: resizes a batch of images to a target size.\n-   `layer_rescaling()`: rescales and offsets the values of a batch of\n    images (e.g., going from inputs in the `[0, 255]` range to inputs in\n    the `[0, 1]` range.\n-   `layer_center_crop()`: returns a center crop of a batch of images.\n\n### Image data augmentation\n\nThese layers apply random augmentation transforms to a batch of images.\nThey are only active during training.\n\n-   `layer_random_crop()`\n-   `layer_random_flip()`\n-   `layer_random_flip()`\n-   `layer_random_translation()`\n-   `layer_random_rotation()`\n-   `layer_random_zoom()`\n-   `layer_random_height()`\n-   `layer_random_width()`\n-   `layer_random_contrast()`\n\n## The `adapt()` function\n\nSome preprocessing layers have an internal state that can be computed\nbased on a sample of the training data. The list of stateful\npreprocessing layers is:\n\n-   `layer_text_vectorization()`: holds a mapping between string tokens\n    and integer indices\n-   `layer_string_lookup()` and `layer_integer_lookup()`: hold a mapping\n    between input values and integer indices.\n-   `layer_normalization()`: holds the mean and standard deviation of\n    the features.\n-   `layer_discretization()`: holds information about value bucket\n    boundaries.\n\nCrucially, these layers are **non-trainable**. Their state is not set\nduring training; it must be set **before training**, either by\ninitializing them from a precomputed constant, or by \""adapting\"" them on\ndata.\n\nYou set the state of a preprocessing layer by exposing it to training\ndata, via `adapt()`:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- rbind(c(0.1, 0.2, 0.3),\n              c(0.8, 0.9, 1.0),\n              c(1.5, 1.6, 1.7))\nlayer <- layer_normalization()\n```\n\n::: {.cell-output-stderr}\n```\nLoaded Tensorflow version 2.8.0\n```\n:::\n\n```{.r .cell-code}\nadapt(layer, data)\nnormalized_data <- as.array(layer(data))\n\nsprintf(\""Features mean: %.2f\"", mean(normalized_data))\n```\n\n::: {.cell-output-stdout}\n```\n[1] \""Features mean: -0.00\""\n```\n:::\n\n```{.r .cell-code}\nsprintf(\""Features std: %.2f\"", sd(normalized_data))\n```\n\n::: {.cell-output-stdout}\n```\n[1] \""Features std: 1.06\""\n```\n:::\n:::\n\n`adapt()` takes either an array or a `tf_dataset`. In the case of\n`layer_string_lookup()` and `layer_text_vectorization()`, you can also\npass a character vector:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- c(\n  \""Congratulations!\"",\n  \""Today is your day.\"",\n  \""You're off to Great Places!\"",\n  \""You're off and away!\"",\n  \""You have brains in your head.\"",\n  \""You have feet in your shoes.\"",\n  \""You can steer yourself\"",\n  \""any direction you choose.\"",\n  \""You're on your own. And you know what you know.\"",\n  \""And YOU are the one who'll decide where to go.\""\n)\n\nlayer = layer_text_vectorization()\nlayer %>% adapt(data)\nvectorized_text <- layer(data)\nprint(vectorized_text)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[31  0  0  0  0  0  0  0  0  0]\n [15 23  3 30  0  0  0  0  0  0]\n [ 4  7  6 25 19  0  0  0  0  0]\n [ 4  7  5 35  0  0  0  0  0  0]\n [ 2 10 34  9  3 24  0  0  0  0]\n [ 2 10 27  9  3 18  0  0  0  0]\n [ 2 33 17 11  0  0  0  0  0  0]\n [37 28  2 32  0  0  0  0  0  0]\n [ 4 22  3 20  5  2  8 14  2  8]\n [ 5  2 36 16 21 12 29 13  6 26]], shape=(10, 10), dtype=int64)\n```\n:::\n:::\n\nIn addition, adaptable layers always expose an option to directly set\nstate via constructor arguments or weight assignment. If the intended\nstate values are known at layer construction time, or are calculated\noutside of the `adapt()` call, they can be set without relying on the\nlayer's internal computation. For instance, if external vocabulary files\nfor the `layer_text_vectorization()`, `layer_string_lookup()`, or\n`layer_integer_lookup()` layers already exist, those can be loaded\ndirectly into the lookup tables by passing a path to the vocabulary file\nin the layer's constructor arguments.\n\nHere's an example where we instantiate a `layer_string_lookup()` layer\nwith precomputed vocabulary:\n\n::: {.cell}\n\n```{.r .cell-code}\nvocab <- c(\""a\"", \""b\"", \""c\"", \""d\"")\ndata <- as_tensor(rbind(c(\""a\"", \""c\"", \""d\""),\n                        c(\""d\"", \""z\"", \""b\"")))\nlayer <- layer_string_lookup(vocabulary=vocab)\nvectorized_data <- layer(data)\nprint(vectorized_data)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[1 3 4]\n [4 0 2]], shape=(2, 3), dtype=int64)\n```\n:::\n:::\n\n## Preprocessing data before the model or inside the model\n\nThere are two ways you could be using preprocessing layers:\n\n**Option 1:** Make them part of the model, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n```\n:::\n\nWith this option, preprocessing will happen on device, synchronously\nwith the rest of the model execution, meaning that it will benefit from\nGPU acceleration. If you're training on GPU, this is the best option for\nthe `layer_normalization()` layer, and for all image preprocessing and\ndata augmentation layers.\n\n**Option 2:** apply it to your `tf_dataset`, so as to obtain a dataset\nthat yields batches of preprocessed data, like this:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n```\n:::\n\nWith this option, your preprocessing will happen on CPU, asynchronously,\nand will be buffered before going into the model. In addition, if you\ncall `tfdatasets::dataset_prefetch()` on your dataset, the preprocessing\nwill happen efficiently in parallel with training:\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n```\n:::\n\nThis is the best option for `layer_text_vectorization()`, and all\nstructured data preprocessing layers. It can also be a good option if\nyou're training on CPU and you use image preprocessing layers.\n\n## Benefits of doing preprocessing inside the model at inference time\n\nEven if you go with option 2, you may later want to export an\ninference-only end-to-end model that will include the preprocessing\nlayers. The key benefit to doing this is that **it makes your model\nportable** and it **helps reduce the [training/serving\nskew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)**.\n\nWhen all data preprocessing is part of the model, other people can load\nand use your model without having to be aware of how each feature is\nexpected to be encoded & normalized. Your inference model will be able\nto process raw images or raw structured data, and will not require users\nof the model to be aware of the details of e.g. the tokenization scheme\nused for text, the indexing scheme used for categorical features,\nwhether image pixel values are normalized to `[-1, +1]` or to `[0, 1]`,\netc. This is especially powerful if you're exporting your model to\nanother runtime, such as TensorFlow.js: you won't have to reimplement\nyour preprocessing pipeline in JavaScript.\n\nIf you initially put your preprocessing layers in your `tf_dataset`\npipeline, you can export an inference model that packages the\npreprocessing. Simply instantiate a new model that chains your\npreprocessing layers and your training model:\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\ninference_model <- keras_model(input, output)\n```\n:::\n\n## Preprocessing during multi-worker training\n\nPreprocessing layers are compatible with the\n[tf.distribute](https://www.tensorflow.org/api_docs/python/tf/distribute)\nAPI for running training across multiple machines.\n\nIn general, preprocessing layers should be placed inside a\n`strategy$scope()` and called either inside or before the model as\ndiscussed above.\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(strategy$scope(), {\n    inputs <- layer_input(shape=input_shape)\n    preprocessing_layer <- layer_hashing(num_bins = 10)\n    dense_layer <- layer_dense(units = 16)\n})\n```\n:::\n\nFor more details, refer to the [preprocessing\nsection](https://www.tensorflow.org/tutorials/distribute/input#data_preprocessing)\nof the distributed input guide.\n\n## Quick recipes\n\n### Image data augmentation\n\nNote that image data augmentation layers are only active during training\n(similar to the `layer_dropout()` layer).\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\""horizontal\"") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\n\n# Load some data\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline of augmented images (and their labels)\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) %>%\n  dataset_map( ~ list(data_augmentation(.x), .y)) # see ?purrr::map to learn about ~ notation\n\n\n# Create a model and train it on the augmented image data\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # Rescale inputs\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \""rmsprop\"", loss = \""sparse_categorical_crossentropy\"") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n```\n:::\n\nYou can see a similar setup in action in the example [image\nclassification from\nscratch](https://keras.io/examples/vision/image_classification_from_scratch/).\n\n### Normalizing numerical features\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\nx_train <- x_train %>%\n  array_reshape(c(dim(x_train)[1], -1L)) # flatten each case\n\ninput_shape <- dim(x_train)[-1] # keras layers automatically add the batch dim\nclasses <- 10\n\n# Create a layer_normalization() layer and set its internal state using the training data\nnormalizer <- layer_normalization()\nnormalizer %>% adapt(x_train)\n\n# Create a model that include the normalization layer\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  normalizer() %>%\n  layer_dense(classes, activation = \""softmax\"")\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \""adam\"",\n          loss = \""sparse_categorical_crossentropy\"")\n\n# Train the model\nmodel %>%\n  fit(x_train, y_train)\n```\n:::\n\n### Encoding string categorical features via one-hot encoding\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some toy data\ndata <- as_tensor(c(\""a\"", \""b\"", \""c\"", \""b\"", \""c\"", \""a\"")) %>%\n  k_reshape(c(-1, 1)) # reshape into matrix with shape: (6, 1)\n\n# Use layer_string_lookup() to build an index of \n# the feature values and encode output.\nlookup <- layer_string_lookup(output_mode=\""one_hot\"")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data = as_tensor(matrix(c(\""a\"", \""b\"", \""c\"", \""d\"", \""e\"", \""\"")))\nencoded_data = lookup(test_data)\nprint(encoded_data)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]], shape=(6, 4), dtype=float32)\n```\n:::\n:::\n\nNote that, here, index 0 is reserved for out-of-vocabulary values\n(values that were not seen during `adapt()`).\n\nYou can see the `layer_string_lookup()` in action in the [Structured\ndata classification from\nscratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\nexample.\n\n### Encoding integer categorical features via one-hot encoding\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some toy data\ndata <- as_tensor(matrix(c(10, 20, 20, 10, 30, 0)), \""int32\"")\n\n# Use layer_integer_lookup() to build an \n# index of the feature values and encode output.\nlookup <- layer_integer_lookup(output_mode=\""one_hot\"")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data <- as_tensor(matrix(c(10, 10, 20, 50, 60, 0)), \""int32\"")\nencoded_data <- lookup(test_data)\nprint(encoded_data)\n```\n\n::: {.cell-output-stdout}\n```\ntf.Tensor(\n[[0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]], shape=(6, 5), dtype=float32)\n```\n:::\n:::\n\nNote that index 0 is reserved for missing values (which you should\nspecify as the value 0), and index 1 is reserved for out-of-vocabulary\nvalues (values that were not seen during `adapt()`). You can configure\nthis by using the `mask_token` and `oov_token` constructor arguments of\n`layer_integer_lookup()`.\n\nYou can see the `layer_integer_lookup()` in action in the example\n[structured data classification from\nscratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/).\n\n### Applying the hashing trick to an integer categorical feature\n\nIf you have a categorical feature that can take many different values\n(on the order of 10e3 or higher), where each value only appears a few\ntimes in the data, it becomes impractical and ineffective to index and\none-hot encode the feature values. Instead, it can be a good idea to\napply the \""hashing trick\"": hash the values to a vector of fixed size.\nThis keeps the size of the feature space manageable, and removes the\nneed for explicit indexing.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample data: 10,000 random integers with values between 0 and 100,000\ndata <- k_random_uniform(shape = c(10000, 1), dtype = \""int64\"")\n\n# Use the Hashing layer to hash the values to the range [0, 64]\nhasher <- layer_hashing(num_bins = 64, salt = 1337)\n\n# Use the CategoryEncoding layer to multi-hot encode the hashed values\nencoder <- layer_category_encoding(num_tokens=64, output_mode=\""multi_hot\"")\nencoded_data <- encoder(hasher(data))\nprint(encoded_data$shape)\n```\n\n::: {.cell-output-stdout}\n```\nTensorShape([10000, 64])\n```\n:::\n:::\n\n### Encoding text as a sequence of token indices\n\nThis is how you should preprocess text to be passed to an `Embedding`\nlayer.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \""The Brain is wider than the Sky\"",\n  \""For put them side by side\"",\n  \""The one the other will contain\"",\n  \""With ease and You beside\""\n))\n\n# Create a layer_text_vectorization() layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\""int\"")\n# Index the vocabulary via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\""Encoded text:\\n\"",\n    as.array(text_vectorizer(\""The Brain is deeper than the sea\"")))\n```\n\n::: {.cell-output-stdout}\n```\nEncoded text:\n 2 19 14 1 9 2 1\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput <- layer_input(shape(NULL), dtype=\""int64\"")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n  c(\""The Brain is deeper than the sea\"", \""for if they are held Blue to Blue\""),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\""Training model...\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer = \""rmsprop\"", loss = \""mse\"") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\""string\"")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\""Calling end-to-end model on test string...\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\""The one the other will absorb\""))\ntest_output <- end_to_end_model(test_data)\ncat(\""Model output:\"", as.array(test_output), \""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nModel output: 0.1437887 \n```\n:::\n:::\n\nYou can see the `layer_text_vectorization()` layer in action, combined\nwith an `Embedding` mode, in the example [text classification from\nscratch](https://keras.io/examples/nlp/text_classification_from_scratch/).\n\nNote that when training such a model, for best performance, you should\nalways use the `layer_text_vectorization()` layer as part of the input\npipeline.\n\n### Encoding text as a dense matrix of ngrams with multi-hot encoding\n\nThis is how you can preprocess text to be passed to a `Dense` layer.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \""The Brain is wider than the Sky\"",\n  \""For put them side by side\"",\n  \""The one the other will contain\"",\n  \""With ease and You beside\""\n))\n\n# Instantiate layer_text_vectorization() with \""multi_hot\"" output_mode\n# and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\""multi_hot\"", ngrams=2)\n# Index the bigrams via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\""Encoded text:\\n\"", \n    as.array(text_vectorizer(\""The Brain is deeper than the sea\"")))\n```\n\n::: {.cell-output-stdout}\n```\nEncoded text:\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput = layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\""int64\"")\n\noutput <- input %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\""The Brain is deeper than the sea\"", \""for if they are held Blue to Blue\""),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\""Training model...\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer=\""rmsprop\"", loss=\""mse\"") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\""string\"")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\""Calling end-to-end model on test string...\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\""The one the other will absorb\""))\ntest_output <- end_to_end_model(test_data)\ncat(\""Model output: \""); print(test_output); cat(\""\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nModel output: \n```\n:::\n\n::: {.cell-output-stdout}\n```\ntf.Tensor([[0.31572872]], shape=(1, 1), dtype=float32)\n```\n:::\n:::\n\n### Encoding text as a dense matrix of ngrams with TF-IDF weighting\n\nThis is an alternative way of preprocessing text before passing it to a\n`layer_dense` layer.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \""The Brain is wider than the Sky\"",\n  \""For put them side by side\"",\n  \""The one the other will contain\"",\n  \""With ease and You beside\""\n))\n\n# Instantiate layer_text_vectorization() with \""tf-idf\"" output_mode\n# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\""tf-idf\"", ngrams=2)\n# Index the bigrams and learn the TF-IDF weights via `adapt()`\n\n\nwith(tf$device(\""CPU\""), {\n  # A bug that prevents this from running on GPU for now.\n  text_vectorizer %>% adapt(adapt_data)\n})\n\n# Try out the layer\ncat(\""Encoded text:\\n\"", \n    as.array(text_vectorizer(\""The Brain is deeper than the sea\"")))\n```\n\n::: {.cell-output-stdout}\n```\nEncoded text:\n 5.461647 1.694596 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.098612 1.098612 1.098612 0 0 0 0 0 0 0 0 0 1.098612 0 0 0 0 0 0 0 1.098612 1.098612 0 0 0\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput <- layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\""int64\"")\noutput <- input %>% layer_dense(1)\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\""The Brain is deeper than the sea\"", \""for if they are held Blue to Blue\""),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n\n# Train the model on the int sequences\ncat(\""Training model...\"")\n```\n\n::: {.cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer=\""rmsprop\"", loss=\""mse\"") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\""string\"")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\""Calling end-to-end model on test string...\\n\"")\n```\n\n::: {.cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\""The one the other will absorb\""))\ntest_output <- end_to_end_model(test_data)\ncat(\""Model output: \""); print(test_output)\n```\n\n::: {.cell-output-stdout}\n```\nModel output: \n```\n:::\n\n::: {.cell-output-stdout}\n```\ntf.Tensor([[0.09238248]], shape=(1, 1), dtype=float32)\n```\n:::\n:::\n\n## Important gotchas\n\n### Working with lookup layers with very large vocabularies\n\nYou may find yourself working with a very large vocabulary in a\n`layer_text_vectorization()`, a `layer_string_lookup()` layer, or an\n`layer_integer_lookup()` layer. Typically, a vocabulary larger than\n500MB would be considered \""very large\"".\n\nIn such case, for best performance, you should avoid using `adapt()`.\nInstead, pre-compute your vocabulary in advance (you could use Apache\nBeam or TF Transform for this) and store it in a file. Then load the\nvocabulary into the layer at construction time by passing the filepath\nas the `vocabulary` argument."",
+    ""markdown"": ""---\ntitle: Working with preprocessing layers\nauthor: Francois Chollet, Mark Omernick, Tomasz Kalinowski\ndescription: Overview of how to leverage preprocessing layers to create end-to-end models.\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Keras preprocessing\n\nThe Keras preprocessing layers API allows developers to build\nKeras-native input processing pipelines. These input processing\npipelines can be used as independent preprocessing code in non-Keras\nworkflows, combined directly with Keras models, and exported as part of\na Keras SavedModel.\n\nWith Keras preprocessing layers, you can build and export models that\nare truly end-to-end: models that accept raw images or raw structured\ndata as input; models that handle feature normalization or feature value\nindexing on their own.\n\n## Available preprocessing layers\n\n### Text preprocessing\n\n-   `layer_text_vectorization()`: turns raw strings into an encoded\n    representation that can be read by a `layer_embedding()` or\n    `layer_dense()` layer.\n\n### Numerical features preprocessing\n\n-   `layer_normalization()`: performs feature-wise normalization of\n    input features.\n-   `layer_discretization()`: turns continuous numerical features into\n    integer categorical features.\n\n### Categorical features preprocessing\n\n-   `layer_category_encoding()`: turns integer categorical features into\n    one-hot, multi-hot, or count-based, dense representations.\n-   `layer_hashing()`: performs categorical feature hashing, also known\n    as the \""hashing trick\"".\n-   `layer_string_lookup()`: turns string categorical values into an\n    encoded representation that can be read by an `Embedding` layer or\n    `Dense` layer.\n-   `layer_integer_lookup()`: turns integer categorical values into an\n    encoded representation that can be read by an `Embedding` layer or\n    `Dense` layer.\n\n### Image preprocessing\n\nThese layers are for standardizing the inputs of an image model.\n\n-   `layer_resizing()`: resizes a batch of images to a target size.\n-   `layer_rescaling()`: rescales and offsets the values of a batch of\n    images (e.g., going from inputs in the `[0, 255]` range to inputs in\n    the `[0, 1]` range.\n-   `layer_center_crop()`: returns a center crop of a batch of images.\n\n### Image data augmentation\n\nThese layers apply random augmentation transforms to a batch of images.\nThey are only active during training.\n\n-   `layer_random_crop()`\n-   `layer_random_flip()`\n-   `layer_random_flip()`\n-   `layer_random_translation()`\n-   `layer_random_rotation()`\n-   `layer_random_zoom()`\n-   `layer_random_height()`\n-   `layer_random_width()`\n-   `layer_random_contrast()`\n\n## The `adapt()` function\n\nSome preprocessing layers have an internal state that can be computed\nbased on a sample of the training data. The list of stateful\npreprocessing layers is:\n\n-   `layer_text_vectorization()`: holds a mapping between string tokens\n    and integer indices\n-   `layer_string_lookup()` and `layer_integer_lookup()`: hold a mapping\n    between input values and integer indices.\n-   `layer_normalization()`: holds the mean and standard deviation of\n    the features.\n-   `layer_discretization()`: holds information about value bucket\n    boundaries.\n\nCrucially, these layers are **non-trainable**. Their state is not set\nduring training; it must be set **before training**, either by\ninitializing them from a precomputed constant, or by \""adapting\"" them on\ndata.\n\nYou set the state of a preprocessing layer by exposing it to training\ndata, via `adapt()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- rbind(c(0.1, 0.2, 0.3),\n              c(0.8, 0.9, 1.0),\n              c(1.5, 1.6, 1.7))\nlayer <- layer_normalization()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nadapt(layer, data)\nnormalized_data <- as.array(layer(data))\n\nsprintf(\""Features mean: %.2f\"", mean(normalized_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \""Features mean: -0.00\""\n```\n:::\n\n```{.r .cell-code}\nsprintf(\""Features std: %.2f\"", sd(normalized_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \""Features std: 1.06\""\n```\n:::\n:::\n\n\n`adapt()` takes either an array or a `tf_dataset`. In the case of\n`layer_string_lookup()` and `layer_text_vectorization()`, you can also\npass a character vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- c(\n  \""Congratulations!\"",\n  \""Today is your day.\"",\n  \""You're off to Great Places!\"",\n  \""You're off and away!\"",\n  \""You have brains in your head.\"",\n  \""You have feet in your shoes.\"",\n  \""You can steer yourself\"",\n  \""any direction you choose.\"",\n  \""You're on your own. And you know what you know.\"",\n  \""And YOU are the one who'll decide where to go.\""\n)\n\nlayer = layer_text_vectorization()\nlayer %>% adapt(data)\nvectorized_text <- layer(data)\nprint(vectorized_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[31  0  0  0  0  0  0  0  0  0]\n [15 23  3 30  0  0  0  0  0  0]\n [ 4  7  6 25 19  0  0  0  0  0]\n [ 4  7  5 35  0  0  0  0  0  0]\n [ 2 10 34  9  3 24  0  0  0  0]\n [ 2 10 27  9  3 18  0  0  0  0]\n [ 2 33 17 11  0  0  0  0  0  0]\n [37 28  2 32  0  0  0  0  0  0]\n [ 4 22  3 20  5  2  8 14  2  8]\n [ 5  2 36 16 21 12 29 13  6 26]], shape=(10, 10), dtype=int64)\n```\n:::\n:::\n\n\nIn addition, adaptable layers always expose an option to directly set\nstate via constructor arguments or weight assignment. If the intended\nstate values are known at layer construction time, or are calculated\noutside of the `adapt()` call, they can be set without relying on the\nlayer's internal computation. For instance, if external vocabulary files\nfor the `layer_text_vectorization()`, `layer_string_lookup()`, or\n`layer_integer_lookup()` layers already exist, those can be loaded\ndirectly into the lookup tables by passing a path to the vocabulary file\nin the layer's constructor arguments.\n\nHere's an example where we instantiate a `layer_string_lookup()` layer\nwith precomputed vocabulary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvocab <- c(\""a\"", \""b\"", \""c\"", \""d\"")\ndata <- as_tensor(rbind(c(\""a\"", \""c\"", \""d\""),\n                        c(\""d\"", \""z\"", \""b\"")))\nlayer <- layer_string_lookup(vocabulary=vocab)\nvectorized_data <- layer(data)\nprint(vectorized_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1 3 4]\n [4 0 2]], shape=(2, 3), dtype=int64)\n```\n:::\n:::\n\n\n## Preprocessing data before the model or inside the model\n\nThere are two ways you could be using preprocessing layers:\n\n**Option 1:** Make them part of the model, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer() %>%\n  rest_of_the_model()\nmodel <- keras_model(input, output)\n```\n:::\n\n\nWith this option, preprocessing will happen on device, synchronously\nwith the rest of the model execution, meaning that it will benefit from\nGPU acceleration. If you're training on GPU, this is the best option for\nthe `layer_normalization()` layer, and for all image preprocessing and\ndata augmentation layers.\n\n**Option 2:** apply it to your `tf_dataset`, so as to obtain a dataset\nthat yields batches of preprocessed data, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\ndataset <- ... # define dataset\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y))\n```\n:::\n\n\nWith this option, your preprocessing will happen on CPU, asynchronously,\nand will be buffered before going into the model. In addition, if you\ncall `tfdatasets::dataset_prefetch()` on your dataset, the preprocessing\nwill happen efficiently in parallel with training:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- dataset %>%\n  dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%\n  dataset_prefetch()\nmodel %>% fit(dataset)\n```\n:::\n\n\nThis is the best option for `layer_text_vectorization()`, and all\nstructured data preprocessing layers. It can also be a good option if\nyou're training on CPU and you use image preprocessing layers.\n\n## Benefits of doing preprocessing inside the model at inference time\n\nEven if you go with option 2, you may later want to export an\ninference-only end-to-end model that will include the preprocessing\nlayers. The key benefit to doing this is that **it makes your model\nportable** and it **helps reduce the [training/serving\nskew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)**.\n\nWhen all data preprocessing is part of the model, other people can load\nand use your model without having to be aware of how each feature is\nexpected to be encoded & normalized. Your inference model will be able\nto process raw images or raw structured data, and will not require users\nof the model to be aware of the details of e.g. the tokenization scheme\nused for text, the indexing scheme used for categorical features,\nwhether image pixel values are normalized to `[-1, +1]` or to `[0, 1]`,\netc. This is especially powerful if you're exporting your model to\nanother runtime, such as TensorFlow.js: you won't have to reimplement\nyour preprocessing pipeline in JavaScript.\n\nIf you initially put your preprocessing layers in your `tf_dataset`\npipeline, you can export an inference model that packages the\npreprocessing. Simply instantiate a new model that chains your\npreprocessing layers and your training model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  preprocessing_layer(input) %>%\n  training_model()\ninference_model <- keras_model(input, output)\n```\n:::\n\n\n## Preprocessing during multi-worker training\n\nPreprocessing layers are compatible with the\n[tf.distribute](https://www.tensorflow.org/api_docs/python/tf/distribute)\nAPI for running training across multiple machines.\n\nIn general, preprocessing layers should be placed inside a\n`strategy$scope()` and called either inside or before the model as\ndiscussed above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(strategy$scope(), {\n    inputs <- layer_input(shape=input_shape)\n    preprocessing_layer <- layer_hashing(num_bins = 10)\n    dense_layer <- layer_dense(units = 16)\n})\n```\n:::\n\n\nFor more details, refer to the [preprocessing\nsection](https://www.tensorflow.org/tutorials/distribute/input#data_preprocessing)\nof the distributed input guide.\n\n## Quick recipes\n\n### Image data augmentation\n\nNote that image data augmentation layers are only active during training\n(similar to the `layer_dropout()` layer).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(tfdatasets)\n\n# Create a data augmentation stage with horizontal flipping, rotations, zooms\ndata_augmentation <-\n  keras_model_sequential() %>%\n  layer_random_flip(\""horizontal\"") %>%\n  layer_random_rotation(0.1) %>%\n  layer_random_zoom(0.1)\n\n\n# Load some data\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\ninput_shape <- dim(x_train)[-1] # drop batch dim\nclasses <- 10\n\n# Create a tf_dataset pipeline of augmented images (and their labels)\ntrain_dataset <- tensor_slices_dataset(list(x_train, y_train)) %>%\n  dataset_batch(16) %>%\n  dataset_map( ~ list(data_augmentation(.x), .y)) # see ?purrr::map to learn about ~ notation\n\n\n# Create a model and train it on the augmented image data\nresnet <- application_resnet50(weights = NULL,\n                               input_shape = input_shape,\n                               classes = classes)\n\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  layer_rescaling(1 / 255) %>%   # Rescale inputs\n  resnet()\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \""rmsprop\"", loss = \""sparse_categorical_crossentropy\"") %>%\n  fit(train_dataset, steps_per_epoch = 5)\n```\n:::\n\n\nYou can see a similar setup in action in the example [image\nclassification from\nscratch](https://keras.io/examples/vision/image_classification_from_scratch/).\n\n### Normalizing numerical features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nc(c(x_train, y_train), ...) %<-% dataset_cifar10()\nx_train <- x_train %>%\n  array_reshape(c(dim(x_train)[1], -1L)) # flatten each case\n\ninput_shape <- dim(x_train)[-1] # keras layers automatically add the batch dim\nclasses <- 10\n\n# Create a layer_normalization() layer and set its internal state using the training data\nnormalizer <- layer_normalization()\nnormalizer %>% adapt(x_train)\n\n# Create a model that include the normalization layer\ninput <- layer_input(shape = input_shape)\noutput <- input %>%\n  normalizer() %>%\n  layer_dense(classes, activation = \""softmax\"")\n\nmodel <- keras_model(input, output) %>%\n  compile(optimizer = \""adam\"",\n          loss = \""sparse_categorical_crossentropy\"")\n\n# Train the model\nmodel %>%\n  fit(x_train, y_train)\n```\n:::\n\n\n### Encoding string categorical features via one-hot encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some toy data\ndata <- as_tensor(c(\""a\"", \""b\"", \""c\"", \""b\"", \""c\"", \""a\"")) %>%\n  k_reshape(c(-1, 1)) # reshape into matrix with shape: (6, 1)\n\n# Use layer_string_lookup() to build an index of \n# the feature values and encode output.\nlookup <- layer_string_lookup(output_mode=\""one_hot\"")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data = as_tensor(matrix(c(\""a\"", \""b\"", \""c\"", \""d\"", \""e\"", \""\"")))\nencoded_data = lookup(test_data)\nprint(encoded_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]], shape=(6, 4), dtype=float32)\n```\n:::\n:::\n\n\nNote that, here, index 0 is reserved for out-of-vocabulary values\n(values that were not seen during `adapt()`).\n\nYou can see the `layer_string_lookup()` in action in the [Structured\ndata classification from\nscratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\nexample.\n\n### Encoding integer categorical features via one-hot encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some toy data\ndata <- as_tensor(matrix(c(10, 20, 20, 10, 30, 0)), \""int32\"")\n\n# Use layer_integer_lookup() to build an \n# index of the feature values and encode output.\nlookup <- layer_integer_lookup(output_mode=\""one_hot\"")\nlookup %>% adapt(data)\n\n# Convert new test data (which includes unknown feature values)\ntest_data <- as_tensor(matrix(c(10, 10, 20, 50, 60, 0)), \""int32\"")\nencoded_data <- lookup(test_data)\nprint(encoded_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[0. 0. 1. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]], shape=(6, 5), dtype=float32)\n```\n:::\n:::\n\n\nNote that index 0 is reserved for missing values (which you should\nspecify as the value 0), and index 1 is reserved for out-of-vocabulary\nvalues (values that were not seen during `adapt()`). You can configure\nthis by using the `mask_token` and `oov_token` constructor arguments of\n`layer_integer_lookup()`.\n\nYou can see the `layer_integer_lookup()` in action in the example\n[structured data classification from\nscratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/).\n\n### Applying the hashing trick to an integer categorical feature\n\nIf you have a categorical feature that can take many different values\n(on the order of 10e3 or higher), where each value only appears a few\ntimes in the data, it becomes impractical and ineffective to index and\none-hot encode the feature values. Instead, it can be a good idea to\napply the \""hashing trick\"": hash the values to a vector of fixed size.\nThis keeps the size of the feature space manageable, and removes the\nneed for explicit indexing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample data: 10,000 random integers with values between 0 and 100,000\ndata <- k_random_uniform(shape = c(10000, 1), dtype = \""int64\"")\n\n# Use the Hashing layer to hash the values to the range [0, 64]\nhasher <- layer_hashing(num_bins = 64, salt = 1337)\n\n# Use the CategoryEncoding layer to multi-hot encode the hashed values\nencoder <- layer_category_encoding(num_tokens=64, output_mode=\""multi_hot\"")\nencoded_data <- encoder(hasher(data))\nprint(encoded_data$shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([10000, 64])\n```\n:::\n:::\n\n\n### Encoding text as a sequence of token indices\n\nThis is how you should preprocess text to be passed to an `Embedding`\nlayer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(tfdatasets)\nlibrary(keras)\n\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \""The Brain is wider than the Sky\"",\n  \""For put them side by side\"",\n  \""The one the other will contain\"",\n  \""With ease and You beside\""\n))\n\n# Create a layer_text_vectorization() layer\ntext_vectorizer <- layer_text_vectorization(output_mode=\""int\"")\n# Index the vocabulary via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\""Encoded text:\\n\"",\n    as.array(text_vectorizer(\""The Brain is deeper than the sea\"")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEncoded text:\n 2 19 14 1 9 2 1\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput <- layer_input(shape(NULL), dtype=\""int64\"")\n\noutput <- input %>%\n  layer_embedding(input_dim = text_vectorizer$vocabulary_size(),\n                  output_dim = 16) %>%\n  layer_gru(8) %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset <- tensor_slices_dataset(list(\n  c(\""The Brain is deeper than the sea\"", \""for if they are held Blue to Blue\""),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\""Training model...\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer = \""rmsprop\"", loss = \""mse\"") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\""string\"")\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model <- keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\""Calling end-to-end model on test string...\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\""The one the other will absorb\""))\ntest_output <- end_to_end_model(test_data)\ncat(\""Model output:\"", as.array(test_output), \""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel output: 0.1344098 \n```\n:::\n:::\n\n\nYou can see the `layer_text_vectorization()` layer in action, combined\nwith an `Embedding` mode, in the example [text classification from\nscratch](https://keras.io/examples/nlp/text_classification_from_scratch/).\n\nNote that when training such a model, for best performance, you should\nalways use the `layer_text_vectorization()` layer as part of the input\npipeline.\n\n### Encoding text as a dense matrix of ngrams with multi-hot encoding\n\nThis is how you can preprocess text to be passed to a `Dense` layer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \""The Brain is wider than the Sky\"",\n  \""For put them side by side\"",\n  \""The one the other will contain\"",\n  \""With ease and You beside\""\n))\n\n# Instantiate layer_text_vectorization() with \""multi_hot\"" output_mode\n# and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\""multi_hot\"", ngrams=2)\n# Index the bigrams via `adapt()`\ntext_vectorizer %>% adapt(adapt_data)\n\n# Try out the layer\ncat(\""Encoded text:\\n\"", \n    as.array(text_vectorizer(\""The Brain is deeper than the sea\"")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEncoded text:\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput = layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\""int64\"")\n\noutput <- input %>%\n  layer_dense(1)\n\nmodel <- keras_model(input, output)\n\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\""The Brain is deeper than the sea\"", \""for if they are held Blue to Blue\""),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n# Train the model on the int sequences\ncat(\""Training model...\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer=\""rmsprop\"", loss=\""mse\"") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\""string\"")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\""Calling end-to-end model on test string...\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\""The one the other will absorb\""))\ntest_output <- end_to_end_model(test_data)\ncat(\""Model output: \""); print(test_output); cat(\""\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel output: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([[0.38880283]], shape=(1, 1), dtype=float32)\n```\n:::\n:::\n\n\n### Encoding text as a dense matrix of ngrams with TF-IDF weighting\n\nThis is an alternative way of preprocessing text before passing it to a\n`layer_dense` layer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define some text data to adapt the layer\nadapt_data <- as_tensor(c(\n  \""The Brain is wider than the Sky\"",\n  \""For put them side by side\"",\n  \""The one the other will contain\"",\n  \""With ease and You beside\""\n))\n\n# Instantiate layer_text_vectorization() with \""tf-idf\"" output_mode\n# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\ntext_vectorizer = layer_text_vectorization(output_mode=\""tf-idf\"", ngrams=2)\n# Index the bigrams and learn the TF-IDF weights via `adapt()`\n\n\nwith(tf$device(\""CPU\""), {\n  # A bug that prevents this from running on GPU for now.\n  text_vectorizer %>% adapt(adapt_data)\n})\n\n# Try out the layer\ncat(\""Encoded text:\\n\"", \n    as.array(text_vectorizer(\""The Brain is deeper than the sea\"")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEncoded text:\n 5.461647 1.694596 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.098612 1.098612 1.098612 0 0 0 0 0 0 0 0 0 1.098612 0 0 0 0 0 0 0 1.098612 1.098612 0 0 0\n```\n:::\n\n```{.r .cell-code}\n# Create a simple model\ninput <- layer_input(shape = text_vectorizer$vocabulary_size(), dtype=\""int64\"")\noutput <- input %>% layer_dense(1)\nmodel <- keras_model(input, output)\n\n# Create a labeled dataset (which includes unknown tokens)\ntrain_dataset = tensor_slices_dataset(list(\n  c(\""The Brain is deeper than the sea\"", \""for if they are held Blue to Blue\""),\n  c(1L, 0L)\n))\n\n# Preprocess the string inputs, turning them into int sequences\ntrain_dataset <- train_dataset %>%\n  dataset_batch(2) %>%\n  dataset_map(~list(text_vectorizer(.x), .y))\n\n\n# Train the model on the int sequences\ncat(\""Training model...\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining model...\n```\n:::\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer=\""rmsprop\"", loss=\""mse\"") %>%\n  fit(train_dataset)\n\n# For inference, you can export a model that accepts strings as input\ninput <- layer_input(shape = 1, dtype=\""string\"")\n\noutput <- input %>%\n  text_vectorizer() %>%\n  model()\n\nend_to_end_model = keras_model(input, output)\n\n# Call the end-to-end model on test data (which includes unknown tokens)\ncat(\""Calling end-to-end model on test string...\\n\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCalling end-to-end model on test string...\n```\n:::\n\n```{.r .cell-code}\ntest_data <- tf$constant(matrix(\""The one the other will absorb\""))\ntest_output <- end_to_end_model(test_data)\ncat(\""Model output: \""); print(test_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel output: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([[-0.8505264]], shape=(1, 1), dtype=float32)\n```\n:::\n:::\n\n\n## Important gotchas\n\n### Working with lookup layers with very large vocabularies\n\nYou may find yourself working with a very large vocabulary in a\n`layer_text_vectorization()`, a `layer_string_lookup()` layer, or an\n`layer_integer_lookup()` layer. Typically, a vocabulary larger than\n500MB would be considered \""very large\"".\n\nIn such case, for best performance, you should avoid using `adapt()`.\nInstead, pre-compute your vocabulary in advance (you could use Apache\nBeam or TF Transform for this) and store it in a file. Then load the\nvocabulary into the layer at construction time by passing the filepath\nas the `vocabulary` argument.\n"",
     ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],
-    ""includes"": [],
+    ""includes"": {},
     ""engineDependencies"": {},
     ""preserve"": {},
-    ""postProcess"": null
+    ""postProcess"": true
   }
 }
\ No newline at end of file

---FILE: _freeze/keras/guides/python_subclasses/execute-results/html.json---
@@ -1,14 +1,14 @@
 {
   ""hash"": ""cd839367bf2a4305aa5c5c73d324a1f1"",
   ""result"": {
-    ""markdown"": ""---\ntitle: \""Python Subclasses\""\n---\n\n\n\nWhen using keras, a desire to create Python-based subclasses can arise in a number of ways. For example, when you want to:\n\n-   define custom layers and/or models\n-   implement custom training logic\n-   create custom losses or optimizers\n-   define custom callbacks\n-   ... and more!\n\nIn such scenarios, the most powerful and flexible approach is to directly inherit from, and then modify and/or enhance an appropriate Python class.\n\nSubclassing a Python class in R is generally straightforward. Two syntaxes are provided: one that adheres to R conventions and uses `R6::R6Class` as the class constructor, and one that adheres more to Python conventions, and attempts to replicate Python syntax in R.\n\n## Examples\n\n### A custom constraint (R6)\n\nFor demonstration purposes, let's say you want to implement a custom keras kernel constraint via subclassing. Using R6:\n\n::: {.cell}\n\n```{.r .cell-code}\nNonNegative <- R6::R6Class(\""NonNegative\"",\n  inherit = keras$constraints$Constraint,\n  public = list(\n    \""__call__\"" = function(x) {\n       w * k_cast(w >= 0, k_floatx())\n    }\n  )\n)\nNonNegative <- r_to_py(NonNegative, convert=TRUE)\n```\n\n::: {.cell-output-stderr}\n```\nLoaded Tensorflow version 2.8.0\n```\n:::\n:::\n\nThe `r_to_py` method will convert an R6 class generator into a Python class generator. After conversion, Python class generators will be different from R6 class generators in a few ways:\n\n-   New class instances are generated by calling the class directly: `NonNegative()` (not `NonNegative$new()`)\n\n-   All methods (functions) are (potentially) modified to ensure their first argument is `self`.\n\n-   All methods have in scope `__class__`, `super` and the class name (`NonNegative`).\n\n-   For convenience, some method names are treated as aliases:\n\n    -   `initialize` is treated as an alias for `__init__`()\n    -   `finalize` is treated as an alias for `__del__`()\n\n-   `super` can be accessed in 3 ways:\n\n    1)  R6 style, which supports only single inheritance (the most common type)\n\n    ``` r\n    super$initialize()\n    ```\n\n    2)  Python 2 style, which requires explicitly providing the class generator and instance\n\n    ``` r\n    super(NonNegative, self)$`__init__`()\n    ```\n\n    3)  Python 3 style\n\n    ``` r\n    super()$`__init__`()\n    ```\n\n-   When subclassing Keras base classes, it is generally your responsibility to call `super$initialize()` if you are masking a superclass initializer by providing your own `initialize` method.\n\n-   Passing `convert=FALSE` to `r_to_py()` will mean that all R methods will receive Python objects as arguments, and are expected to return Python objects. This allows for some features not available with `convert=TRUE`, namely, modifying some Python objects, like dictionaries or lists, in-place.\n\n-   Active bindings (methods supplied to `R6Class(active=...)`) are converted to Python `@property`-decorated methods.\n\n-   R6 classes with private methods or attributes are not supported.\n\n-   The argument supplied to `inherit` can be:\n\n    -   missing or `NULL`\n    -   a Python class generator\n    -   an R6 class generator, as long as it can be converted to a Python class generator as well\n    -   a list of Python/R6 classes (for multiple inheritance)\n    -   A list of superclasses, with optional additional keywords (e.g., `metaclass=`, only for advanced Python use cases)\n\n### A custom constraint (`%py_class%`)\n\nAs an alternative to `r_to_py(R6Class(...))`, we also provide `%py_class%`, a more concise alternative syntax for achieving the same outcome. `%py_class%` is heavily inspired by the Python `class` statement syntax, and is especially convenient when translating Python code to R. Translating the above example, you could write the same using `%py_class%`:\n\n::: {.cell}\n\n```{.r .cell-code}\nNonNegative(keras$constraints$Constraint) %py_class% {\n  \""__call__\"" <- function(x) {\n    w * k_cast(w >= 0, k_floatx())\n  }\n}\n```\n:::\n\nNotice, this is very similar to the equivalent [Python code](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/constraints/Constraint):\n\n::: {.cell}\n\n```{.python .cell-code}\nclass NonNegative(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n```\n:::\n\nSome (potentially surprising) notes about `%py_class%`:\n\n-   Just like the Python `class` statement, it assigns the constructed class in the current scope! (There is no need to write `NonNegative <- ...`).\n\n-   The left hand side can be:\n\n    -   A bare symbol, `ClassName`\n    -   A pseudo-call, with superclasses and keywords as arguments: `ClassName(Superclass1, Superclass2, metaclass=my_metaclass)`\n\n-   The right hand side is evaluated in a new environment to form the namespace for the class methods.\n\n-   `%py_class%` objects can be safely defined at the top level of an R package. (see details about `delay_load` below)\n\n-   Two keywords are treated specially: `convert` and `delay_load`.\n\n-   If you want to call `r_to_py` with `convert=FALSE`, pass it as a keyword:\n\n::: {.cell}\n\n```{.r .cell-code}\nNonNegative(keras$constraints$Constraint, convert=FALSE) %py_class% { ... }\n```\n:::\n\n-   You can delay creating the python type object until this first time a class instance is created by passing `delay_load=TRUE`. The default value is `FALSE` for most contexts, but `TRUE` if you are in an R package. (The actual test performed is `identical(topenv(), globalenv())`). If a `%py_class%` type object is delayed, it will display `\""<<R6type>.ClassName> (delayed)\""` when printed.\n\n-   An additional convenience is that if the first expression of a function body or the class body is a literal character string, it is automatically taken as the `__doc__` attribute of the class or method. The doc string will then be visible to both python and R tools e.g. `reticulate::py_help()`. See `?py_class` for an example.\n\nIn all other regards, `%py_class%` is equivalent to `r_to_py(R6Class())` (indeed, under the hood, they do the same thing).\n\n### A custom layer (R6)\n\nThe same pattern can be extended to all sorts of keras objects. For example, a custom layer can be written by subclassing the base Keras Layer:\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLayer <- r_to_py(R6::R6Class(\n\n  classname = \""CustomLayer\"",\n  inherit = keras$layers$Layer,\n\n  public = list(\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n))\n```\n:::\n\n### A custom layer (`%py_class%`)\n\nor using `%py_class%`:\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLayer(keras$layers$Layer) %py_class% {\n\n  initialize <- function(output_dim) {\n    self$output_dim <- output_dim\n  }\n\n  build <- function(input_shape) {\n    self$kernel <- self$add_weight(\n      name = 'kernel',\n      shape = list(input_shape[[2]], self$output_dim),\n      initializer = initializer_random_normal(),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(x, mask = NULL) {\n    k_dot(x, self$kernel)\n  }\n\n  compute_output_shape <- function(input_shape) {\n    list(input_shape[[1]], self$output_dim)\n  }\n}\n```\n:::"",
+    ""markdown"": ""---\ntitle: \""Python Subclasses\""\n---\n\n\n\n\nWhen using keras, a desire to create Python-based subclasses can arise in a number of ways. For example, when you want to:\n\n-   define custom layers and/or models\n-   implement custom training logic\n-   create custom losses or optimizers\n-   define custom callbacks\n-   ... and more!\n\nIn such scenarios, the most powerful and flexible approach is to directly inherit from, and then modify and/or enhance an appropriate Python class.\n\nSubclassing a Python class in R is generally straightforward. Two syntaxes are provided: one that adheres to R conventions and uses `R6::R6Class` as the class constructor, and one that adheres more to Python conventions, and attempts to replicate Python syntax in R.\n\n## Examples\n\n### A custom constraint (R6)\n\nFor demonstration purposes, let's say you want to implement a custom keras kernel constraint via subclassing. Using R6:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNonNegative <- R6::R6Class(\""NonNegative\"",\n  inherit = keras$constraints$Constraint,\n  public = list(\n    \""__call__\"" = function(x) {\n       w * k_cast(w >= 0, k_floatx())\n    }\n  )\n)\nNonNegative <- r_to_py(NonNegative, convert=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n:::\n\n\nThe `r_to_py` method will convert an R6 class generator into a Python class generator. After conversion, Python class generators will be different from R6 class generators in a few ways:\n\n-   New class instances are generated by calling the class directly: `NonNegative()` (not `NonNegative$new()`)\n\n-   All methods (functions) are (potentially) modified to ensure their first argument is `self`.\n\n-   All methods have in scope `__class__`, `super` and the class name (`NonNegative`).\n\n-   For convenience, some method names are treated as aliases:\n\n    -   `initialize` is treated as an alias for `__init__`()\n    -   `finalize` is treated as an alias for `__del__`()\n\n-   `super` can be accessed in 3 ways:\n\n    1)  R6 style, which supports only single inheritance (the most common type)\n\n    ``` r\n    super$initialize()\n    ```\n\n    2)  Python 2 style, which requires explicitly providing the class generator and instance\n\n    ``` r\n    super(NonNegative, self)$`__init__`()\n    ```\n\n    3)  Python 3 style\n\n    ``` r\n    super()$`__init__`()\n    ```\n\n-   When subclassing Keras base classes, it is generally your responsibility to call `super$initialize()` if you are masking a superclass initializer by providing your own `initialize` method.\n\n-   Passing `convert=FALSE` to `r_to_py()` will mean that all R methods will receive Python objects as arguments, and are expected to return Python objects. This allows for some features not available with `convert=TRUE`, namely, modifying some Python objects, like dictionaries or lists, in-place.\n\n-   Active bindings (methods supplied to `R6Class(active=...)`) are converted to Python `@property`-decorated methods.\n\n-   R6 classes with private methods or attributes are not supported.\n\n-   The argument supplied to `inherit` can be:\n\n    -   missing or `NULL`\n    -   a Python class generator\n    -   an R6 class generator, as long as it can be converted to a Python class generator as well\n    -   a list of Python/R6 classes (for multiple inheritance)\n    -   A list of superclasses, with optional additional keywords (e.g., `metaclass=`, only for advanced Python use cases)\n\n### A custom constraint (`%py_class%`)\n\nAs an alternative to `r_to_py(R6Class(...))`, we also provide `%py_class%`, a more concise alternative syntax for achieving the same outcome. `%py_class%` is heavily inspired by the Python `class` statement syntax, and is especially convenient when translating Python code to R. Translating the above example, you could write the same using `%py_class%`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNonNegative(keras$constraints$Constraint) %py_class% {\n  \""__call__\"" <- function(x) {\n    w * k_cast(w >= 0, k_floatx())\n  }\n}\n```\n:::\n\n\nNotice, this is very similar to the equivalent [Python code](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/constraints/Constraint):\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass NonNegative(tf.keras.constraints.Constraint):\n    def __call__(self, w):\n        return w * tf.cast(tf.math.greater_equal(w, 0.), w.dtype)\n```\n:::\n\n\nSome (potentially surprising) notes about `%py_class%`:\n\n-   Just like the Python `class` statement, it assigns the constructed class in the current scope! (There is no need to write `NonNegative <- ...`).\n\n-   The left hand side can be:\n\n    -   A bare symbol, `ClassName`\n    -   A pseudo-call, with superclasses and keywords as arguments: `ClassName(Superclass1, Superclass2, metaclass=my_metaclass)`\n\n-   The right hand side is evaluated in a new environment to form the namespace for the class methods.\n\n-   `%py_class%` objects can be safely defined at the top level of an R package. (see details about `delay_load` below)\n\n-   Two keywords are treated specially: `convert` and `delay_load`.\n\n-   If you want to call `r_to_py` with `convert=FALSE`, pass it as a keyword:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNonNegative(keras$constraints$Constraint, convert=FALSE) %py_class% { ... }\n```\n:::\n\n\n-   You can delay creating the python type object until this first time a class instance is created by passing `delay_load=TRUE`. The default value is `FALSE` for most contexts, but `TRUE` if you are in an R package. (The actual test performed is `identical(topenv(), globalenv())`). If a `%py_class%` type object is delayed, it will display `\""<<R6type>.ClassName> (delayed)\""` when printed.\n\n-   An additional convenience is that if the first expression of a function body or the class body is a literal character string, it is automatically taken as the `__doc__` attribute of the class or method. The doc string will then be visible to both python and R tools e.g. `reticulate::py_help()`. See `?py_class` for an example.\n\nIn all other regards, `%py_class%` is equivalent to `r_to_py(R6Class())` (indeed, under the hood, they do the same thing).\n\n### A custom layer (R6)\n\nThe same pattern can be extended to all sorts of keras objects. For example, a custom layer can be written by subclassing the base Keras Layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLayer <- r_to_py(R6::R6Class(\n\n  classname = \""CustomLayer\"",\n  inherit = keras$layers$Layer,\n\n  public = list(\n    initialize = function(output_dim) {\n      self$output_dim <- output_dim\n    },\n\n    build = function(input_shape) {\n      self$kernel <- self$add_weight(\n        name = 'kernel',\n        shape = list(input_shape[[2]], self$output_dim),\n        initializer = initializer_random_normal(),\n        trainable = TRUE\n      )\n    },\n\n    call = function(x, mask = NULL) {\n      k_dot(x, self$kernel)\n    },\n\n    compute_output_shape = function(input_shape) {\n      list(input_shape[[1]], self$output_dim)\n    }\n  )\n))\n```\n:::\n\n\n### A custom layer (`%py_class%`)\n\nor using `%py_class%`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLayer(keras$layers$Layer) %py_class% {\n\n  initialize <- function(output_dim) {\n    self$output_dim <- output_dim\n  }\n\n  build <- function(input_shape) {\n    self$kernel <- self$add_weight(\n      name = 'kernel',\n      shape = list(input_shape[[2]], self$output_dim),\n      initializer = initializer_random_normal(),\n      trainable = TRUE\n    )\n  }\n\n  call <- function(x, mask = NULL) {\n    k_dot(x, self$kernel)\n  }\n\n  compute_output_shape <- function(input_shape) {\n    list(input_shape[[1]], self$output_dim)\n  }\n}\n```\n:::\n"",
     ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],
-    ""includes"": [],
+    ""includes"": {},
     ""engineDependencies"": {},
     ""preserve"": {},
-    ""postProcess"": null
+    ""postProcess"": true
   }
 }
\ No newline at end of file

---FILE: _freeze/keras/guides/sequential_model/execute-results/html.json---
@@ -0,0 +1,14 @@
+{
+  ""hash"": ""5c866fbc6ce0da3445b5abc8bc7739dc"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: The Sequential model\nAuthor: \""[fchollet](https://twitter.com/fchollet), Tomasz Kalinowski\""\ndate-created: 2020/04/12\ndate-last-modified: 2020/04/12\ndescription: Complete guide to the Sequential model.\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## When to use a Sequential model\n\nA `Sequential` model is appropriate for **a plain stack of layers**\nwhere each layer has **exactly one input tensor and one output tensor**.\n\nSchematically, the following `Sequential` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define Sequential model with 3 layers\nmodel <- keras_model_sequential() %>% \n  layer_dense(2, activation = \""relu\"", name = \""layer1\"") %>% \n  layer_dense(3, activation = \""relu\"", name = \""layer2\"") %>% \n  layer_dense(4, name = \""layer3\"")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n# Call model on a test input\nx <- tf$ones(shape(3, 3))\ny <- model(x)\n```\n:::\n\n\nis equivalent to this function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 3 layers\nlayer1 <- layer_dense(units = 2, activation = \""relu\"", name = \""layer1\"")\nlayer2 <- layer_dense(units = 3, activation = \""relu\"", name = \""layer2\"")\nlayer3 <- layer_dense(units = 4, name = \""layer3\"")\n\n# Call layers on a test input\nx <- tf$ones(shape(3, 3))\ny <- layer3(layer2(layer1(x)))\n```\n:::\n\n\nA Sequential model is **not appropriate** when:\n\n-   Your model has multiple inputs or multiple outputs\n-   Any of your layers has multiple inputs or multiple outputs\n-   You need to do layer sharing\n-   You want non-linear topology (e.g. a residual connection, a\n    multi-branch model)\n\n## Creating a Sequential model\n\nYou can create a Sequential model by piping a model through a series\nlayers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_dense(2, activation = \""relu\"") %>%\n  layer_dense(3, activation = \""relu\"") %>%\n  layer_dense(4)\n```\n:::\n\n\nIts layers are accessible via the `layers` attribute:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel$layers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<keras.layers.core.dense.Dense object at 0x7f28d0358ee0>\n\n[[2]]\n<keras.layers.core.dense.Dense object at 0x7f28d47f3670>\n\n[[3]]\n<keras.layers.core.dense.Dense object at 0x7f28d47f3580>\n```\n:::\n:::\n\n\nYou can also create a Sequential model incrementally:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential()\nmodel %>% layer_dense(2, activation = \""relu\"")\nmodel %>% layer_dense(3, activation = \""relu\"")\nmodel %>% layer_dense(4)\n```\n:::\n\n\nNote that there's also a corresponding `pop()` method to remove layers:\na Sequential model behaves very much like a stack of layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% pop_layer()\nlength(model$layers)  # 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nAlso note that the Sequential constructor accepts a `name` argument,\njust like any layer or model in Keras. This is useful to annotate\nTensorBoard graphs with semantically meaningful names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(name = \""my_sequential\"")\nmodel %>% layer_dense(2, activation = \""relu\"", name = \""layer1\"")\nmodel %>% layer_dense(3, activation = \""relu\"", name = \""layer2\"")\nmodel %>% layer_dense(4, name = \""layer3\"")\n```\n:::\n\n\n## Specifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in\norder to be able to create their weights. So when you create a layer\nlike this, initially, it has no weights:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 3)\nlayer$weights  # Empty\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlist()\n```\n:::\n:::\n\n\nIt creates its weights the first time it is called on an input, since\nthe shape of the weights depends on the shape of the inputs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Call layer on a test input\nx <- tf$ones(shape(1, 4))\ny <- layer(x)\nlayer$weights  # Now it has weights, of shape (4, 3) and (3,)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=\narray([[-0.0464139 ,  0.05605704, -0.50734866],\n       [ 0.27502584,  0.02782226, -0.81499064],\n       [-0.64200175, -0.55023456,  0.5515981 ],\n       [ 0.56866205, -0.89289516,  0.56033444]], dtype=float32)>\n\n[[2]]\n<tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n```\n:::\n:::\n\n\nNaturally, this also applies to Sequential models. When you instantiate\na Sequential model without an input shape, it isn't \""built\"": it has no\nweights (and calling `model$weights` results in an error stating just\nthis). The weights are created when the model first sees some input\ndata:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n        layer_dense(2, activation = \""relu\"") %>% \n        layer_dense(3, activation = \""relu\"") %>% \n        layer_dense(4)\n\n# No weights at this stage!\n# At this point, you can't do this:\n\ntry(model$weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_get_attr_impl(x, name, silent) : \n  ValueError: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n```\n:::\n\n```{.r .cell-code}\n# The model summary is also not available:\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: <no summary available, model was not built>\n```\n:::\n\n```{.r .cell-code}\n# Call the model on a test input\nx <- tf$ones(shape(1, 4))\ny <- model(x)\ncat(\""Number of weights after calling the model:\"", length(model$weights), \""\\n\"")  # 6\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of weights after calling the model: 6 \n```\n:::\n:::\n\n\nOnce a model is \""built\"", you can call its `summary()` method to display\nits contents (the `summary()` method is also called by the default\n`print()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_3\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_9 (Dense)                  (1, 2)                        10          \n dense_8 (Dense)                  (1, 3)                        9           \n dense_7 (Dense)                  (1, 4)                        16          \n============================================================================\nTotal params: 35\nTrainable params: 35\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nHowever, it can be very useful when building a Sequential model\nincrementally to be able to display the summary of the model so far,\nincluding the current output shape. In this case, you should start your\nmodel by passing an `input_shape` argument to your model, so that it\nknows its input shape from the start:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(4))\nmodel %>% layer_dense(2, activation = \""relu\"")\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_4\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_10 (Dense)                 (None, 2)                     10          \n============================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nModels built with a predefined input shape like this always have weights\n(even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input\nshape of a Sequential model in advance if you know what it is.\n\n## A common debugging workflow: `%>%` + `summary()`\n\nWhen building a new Sequential architecture, it's useful to\nincrementally stack layers and print model summaries. For instance, this\nenables you to monitor how a stack of `Conv2D` and `MaxPooling2D` layers\nis downsampling image feature maps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(250, 250, 3)) # 250x250 RGB images\n  \nmodel %>% \n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3) \n\n# Can you guess what the current output shape is at this point? Probably not.\n# Let's just print it:\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_5\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n============================================================================\nTotal params: 11,680\nTrainable params: 11,680\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# The answer was: (40, 40, 32), so we can keep downsampling...\nmodel %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(3) %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_max_pooling_2d(2) \n\n# And now?\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_5\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        \n conv2d (Conv2D)                  (None, 121, 121, 32)          9248        \n max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           \n conv2d_5 (Conv2D)                (None, 38, 38, 32)            9248        \n conv2d_4 (Conv2D)                (None, 36, 36, 32)            9248        \n max_pooling2d_2 (MaxPooling2D)   (None, 12, 12, 32)            0           \n conv2d_3 (Conv2D)                (None, 10, 10, 32)            9248        \n conv2d_2 (Conv2D)                (None, 8, 8, 32)              9248        \n max_pooling2d_1 (MaxPooling2D)   (None, 4, 4, 32)              0           \n============================================================================\nTotal params: 48,672\nTrainable params: 48,672\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# Now that we have 4x4 feature maps, time to apply global max pooling.\nmodel %>% layer_global_max_pooling_2d()\n\n# Finally, we add a classification layer.\nmodel %>% layer_dense(10)\n```\n:::\n\n\nVery practical, right?\n\n## What to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n\n-   Train your model, evaluate it, and run inference. See our [guide to\n    training & evaluation with the built-in\n    loops](/guides/training_with_built_in_methods/)\n-   Save your model to disk and restore it. See our [guide to\n    serialization & saving](/guides/serialization_and_saving/).\n-   Speed up model training by leveraging multiple GPUs. See our [guide\n    to multi-GPU and distributed\n    training](https://keras.io/guides/distributed_training/).\n\n## Feature extraction with a Sequential model\n\nOnce a Sequential model has been built, it behaves like a [Functional\nAPI model](/guides/functional_api/). This means that every layer has an\n`input` and `output` attribute. These attributes can be used to do neat\nthings, like quickly creating a model that extracts the outputs of all\nintermediate layers in a Sequential model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs = lapply(initial_model$layers, \\(layer) layer$output)\n)\n\n# Call feature extractor on test input.\n\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n```\n:::\n\n\nHere's a similar example that only extract features from one layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_model <-\n  keras_model_sequential(input_shape = c(250, 250, 3)) %>%\n  layer_conv_2d(32, 5, strides = 2, activation = \""relu\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"", name = \""my_intermediate_layer\"") %>%\n  layer_conv_2d(32, 3, activation = \""relu\"")\n\nfeature_extractor <- keras_model(\n  inputs = initial_model$inputs,\n  outputs =  get_layer(initial_model, name = \""my_intermediate_layer\"")$output\n)\n\n# Call feature extractor on test input.\nx <- tf$ones(shape(1, 250, 250, 3))\nfeatures <- feature_extractor(x)\n```\n:::\n\n\n## Transfer learning with a Sequential model\n\nTransfer learning consists of freezing the bottom layers in a model and\nonly training the top layers. If you aren't familiar with it, make sure\nto read our [guide to transfer learning](/guides/transfer_learning/).\n\nHere are two common transfer learning blueprint involving Sequential\nmodels.\n\nFirst, let's say that you have a Sequential model, and you want to\nfreeze all layers except the last one. In this case, you would simply\niterate over `model$layers` and set `layer$trainable = FALSE` on each\nlayer, except the last one. Like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(784)) %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(32, activation = 'relu') %>%\n  layer_dense(10)\n\n\n# Presumably you would want to first load pre-trained weights.\nmodel$load_weights(...)\n\n# Freeze all layers except the last one.\nfor (layer in head(model$layers, -1))\n  layer$trainable <- FALSE\n\n# can also just call: freeze_weights(model, to = -2)\n\n# Recompile and train (this will only update the weights of the last layer).\nmodel %>% compile(...)\nmodel %>% fit(...)\n```\n:::\n\n\nAnother common blueprint is to use a Sequential model to stack a\npre-trained model and some freshly initialized classification layers.\nLike this:\n\n# Load a convolutional base with pre-trained weights\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model <- application_xception(\n    weights = 'imagenet',\n    include_top = FALSE,\n    pooling = 'avg')\n\n# Freeze the base model\nbase_model$trainable <- FALSE\n\n# Use a Sequential model to add a trainable classifier on top\nmodel <- keras_model_sequential() %>%\n  base_model() %>%\n  layer_dense(1000)\n\n# Compile & train\nmodel %>% compile(...)\nmodel %>% fit(...)\n```\n:::\n\n\nIf you do transfer learning, you will probably find yourself frequently\nusing these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n\n-   [Guide to the Functional API](/guides/functional_api/)\n-   [Guide to making new Layers & Models via\n    subclassing](/guides/making_new_layers_and_models_via_subclassing/)\n"",
+    ""supporting"": [],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: _freeze/keras/guides/transfer_learning/execute-results/html.json---
@@ -1,16 +1,16 @@
 {
-  ""hash"": ""dccb052b582475fd48b2b7255d37bcfd"",
+  ""hash"": ""fcbd089a858163073165ca10a2d2c7f7"",
   ""result"": {
-    ""markdown"": ""---\ntitle: \""Transfer learning and fine-tuning\""\n---\n\n## Setup\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nprintf <- function(...) writeLines(sprintf(...))\n```\n:::\n\n## Introduction\n\n**Transfer learning** consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify skunks.\n\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n\nThe most common incarnation of transfer learning in the context of deep learning is the following workflow:\n\n1.  Take layers from a previously trained model.\n2.  Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n3.  Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n4.  Train the new layers on your dataset.\n\nA last, optional step, is **fine-tuning**, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.\n\nFirst, we will go over the Keras `trainable` API in detail, which underlies most transfer learning and fine-tuning workflows.\n\nThen, we'll demonstrate the typical workflow by taking a model pretrained on the ImageNet dataset, and retraining it on the Kaggle \""cats vs dogs\"" classification dataset.\n\nThis is adapted from [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r) and the 2016 blog post [\""building powerful image classification models using very little data\""](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).\n\n## Freezing layers: understanding the `trainable` attribute\n\nLayers and models have three weight attributes:\n\n-   `weights` is the list of all weights variables of the layer.\n-   `trainable_weights` is the list of those that are meant to be updated (via gradient descent) to minimize the loss during training.\n-   `non_trainable_weights` is the list of those that aren't meant to be trained. Typically they are updated by the model during the forward pass.\n\n**Example: the `Dense` layer has 2 trainable weights (kernel and bias)**\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 3)\n```\n\n::: {.cell-output-stderr}\n```\nLoaded Tensorflow version 2.8.0\n```\n:::\n\n```{.r .cell-code}\nlayer$build(shape(NULL, 4))\n\nprintf(\""weights: %s\"", length(layer$weights))\n```\n\n::: {.cell-output-stdout}\n```\nweights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""trainable_weights: %s\"", length(layer$trainable_weights))\n```\n\n::: {.cell-output-stdout}\n```\ntrainable_weights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""non_trainable_weights: %s\"", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output-stdout}\n```\nnon_trainable_weights: 0\n```\n:::\n:::\n\nIn general, all weights are trainable weights. The only built-in layer that has non-trainable weights is `layer_batch_normalization()`. It uses non-trainable weights to keep track of the mean and variance of its inputs during training. To learn how to use non-trainable weights in your own custom layers, see the [guide to writing new layers from scratch](https://keras.rstudio.com/articles/new-guides/making_new_layers_and_models_via_subclassing.html).\n\n**Example: The layer instance returned by `layer_batch_normalization()` has 2 trainable weights and 2 non-trainable weights**\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer <- layer_batch_normalization()\nlayer$build(shape(NULL, 4))\n\nprintf(\""weights: %s\"", length(layer$weights))\n```\n\n::: {.cell-output-stdout}\n```\nweights: 4\n```\n:::\n\n```{.r .cell-code}\nprintf(\""trainable_weights: %s\"", length(layer$trainable_weights))\n```\n\n::: {.cell-output-stdout}\n```\ntrainable_weights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""non_trainable_weights: %s\"", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output-stdout}\n```\nnon_trainable_weights: 2\n```\n:::\n:::\n\nLayers and models also feature a boolean attribute `trainable`. Its value can be changed. Setting `layer$trainable` to `FALSE` moves all the layer's weights from trainable to non-trainable. This is called \""freezing\"" the layer: the state of a frozen layer won't be updated during training (either when training with `fit()` or when training with any custom loop that relies on `trainable_weights` to apply gradient updates).\n\n**Example: setting `trainable` to `False`**\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer = layer_dense(units = 3)\nlayer$build(shape(NULL, 4))  # Create the weights\nlayer$trainable <- FALSE     # Freeze the layer\n\nprintf(\""weights: %s\"", length(layer$weights))\n```\n\n::: {.cell-output-stdout}\n```\nweights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""trainable_weights: %s\"", length(layer$trainable_weights))\n```\n\n::: {.cell-output-stdout}\n```\ntrainable_weights: 0\n```\n:::\n\n```{.r .cell-code}\nprintf(\""non_trainable_weights: %s\"", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output-stdout}\n```\nnon_trainable_weights: 2\n```\n:::\n:::\n\nWhen a trainable weight becomes non-trainable, its value is no longer updated during training.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a model with 2 layers\nlayer1 <- layer_dense(units = 3, activation = \""relu\"")\nlayer2 <- layer_dense(units = 3, activation = \""sigmoid\"")\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  layer1() %>%\n  layer2()\n\n# Freeze the first layer\nlayer1$trainable <- FALSE\n\n# Keep a copy of the weights of layer1 for later reference\ninitial_layer1_weights_values <- get_weights(layer1)\n\n# Train the model\nmodel %>% compile(optimizer = \""adam\"", loss = \""mse\"")\nmodel %>% fit(k_random_normal(c(2, 3)), k_random_normal(c(2, 3)))\n\n# Check that the weights of layer1 have not changed during training\nfinal_layer1_weights_values <- get_weights(layer1)\nstopifnot(all.equal(initial_layer1_weights_values, final_layer1_weights_values))\n```\n:::\n\nDo not confuse the `layer$trainable` attribute with the `training` argument in a layer instance's `call` signature `layer(training =)` (which controls whether the layer should run its forward pass in inference mode or training mode). For more information, see the [Keras FAQ](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n\n## Recursive setting of the `trainable` attribute\n\nIf you set `trainable = FALSE` on a model or on any layer that has sublayers, all child layers become non-trainable as well.\n\n**Example:**\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_model <- keras_model_sequential(input_shape = c(3)) %>%\n  layer_dense(3, activation = \""relu\"") %>%\n  layer_dense(3, activation = \""relu\"")\n\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  inner_model() %>%\n  layer_dense(3, activation = \""sigmoid\"")\n\n\nmodel$trainable <- FALSE  # Freeze the outer model\n\nstopifnot(inner_model$trainable == FALSE)             # All layers in `model` are now frozen\nstopifnot(inner_model$layers[[1]]$trainable == FALSE)  # `trainable` is propagated recursively\n```\n:::\n\n## The typical transfer-learning workflow\n\nThis leads us to how a typical transfer learning workflow can be implemented in Keras:\n\n1.  Instantiate a base model and load pre-trained weights into it.\n2.  Freeze all layers in the base model by setting `trainable = FALSE`.\n3.  Create a new model on top of the output of one (or several) layers from the base model.\n4.  Train your new model on your new dataset.\n\nNote that an alternative, more lightweight workflow could also be:\n\n1.  Instantiate a base model and load pre-trained weights into it.\n2.  Run your new dataset through it and record the output of one (or several) layers from the base model. This is called **feature extraction**.\n3.  Use that output as input data for a new, smaller model.\n\nA key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it's a lot faster and cheaper.\n\nAn issue with that second workflow, though, is that it doesn't allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow.\n\nHere's what the first workflow looks like in Keras:\n\nFirst, instantiate a base model with pre-trained weights.\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model <- application_xception(\n  weights = 'imagenet', # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n```\n:::\n\nThen, freeze the base model.\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model$trainable <- FALSE\n```\n:::\n\nCreate a new model on top.\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(c(150, 150, 3))\n\noutputs <- inputs %>%\n  # We make sure that the base_model is running in inference mode here,\n  # by passing `training=FALSE`. This is important for fine-tuning, as you will\n  # learn in a few paragraphs.\n  base_model(training=FALSE) %>%\n\n  # Convert features of shape `base_model$output_shape[-1]` to vectors\n  layer_global_average_pooling_2d() %>%\n\n  # A Dense classifier with a single unit (binary classification)\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\n```\n:::\n\nTrain the model on new data.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer = optimizer_adam(),\n          loss = loss_binary_crossentropy(from_logits = TRUE),\n          metrics = metric_binary_accuracy()) %>%\n  fit(new_dataset, epochs = 20, callbacks = ..., validation_data = ...)\n```\n:::\n\n## Fine-tuning\n\nOnce your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\n\nThis is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind.\n\nIt is critical to only do this step *after* the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\n\nIt's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to re-adapt the pretrained weights in an incremental way.\n\nThis is how to implement fine-tuning of the whole base model:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unfreeze the base model\nbase_model$trainable <- TRUE\n\n# It's important to recompile your model after you make any changes\n# to the `trainable` attribute of any inner layer, so that your changes\n# are taken into account\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5), # Very low learning rate\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\n# Train end-to-end. Be careful to stop before you overfit!\nmodel %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n```\n:::\n\n**Important note about `compile()` and `trainable`**\n\nCalling `compile()` on a model is meant to \""freeze\"" the behavior of that model. This implies that the `trainable` attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until `compile` is called again. Hence, if you change any `trainable` value, make sure to call `compile()` again on your model for your changes to be taken into account.\n\n**Important notes about `layer_batch_normalization()`**\n\nMany image models contain `BatchNormalization` layers. That layer is a special case on every imaginable count. Here are a few things to keep in mind.\n\n-   `BatchNormalization` contains 2 non-trainable weights that get updated during training. These are the variables tracking the mean and variance of the inputs.\n-   When you set `bn_layer$trainable = FALSE`, the `BatchNormalization` layer will run in inference mode, and will not update its mean and variance statistics. This is not the case for other layers in general, as [weight trainability and inference/training modes are two orthogonal concepts](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute). But the two are tied in the case of the `BatchNormalization` layer.\n-   When you unfreeze a model that contains `BatchNormalization` layers in order to do fine-tuning, you should keep the `BatchNormalization` layers in inference mode by passing `training = FALSE` when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.\n\nYou'll see this pattern in action in the end-to-end example at the end of this guide.\n\n## Transfer learning and fine-tuning with a custom training loop\n\nIf instead of `fit()`, you are using your own low-level training loop, the workflow stays essentially the same. You should be careful to only take into account the list `model$trainable_weights` when applying gradient updates:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create base model\nbase_model = application_xception(\n  weights = 'imagenet',\n  input_shape = c(150, 150, 3),\n  include_top = FALSE\n)\n\n# Freeze base model\nbase_model$trainable = FALSE\n\n# Create new model on top.\ninputs <- layer_input(shape = c(150, 150, 3))\noutputs <- inputs %>%\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\n# helper to zip gradients with weights\nxyz <- function(...) .mapply(c, list(...), NULL)\n\n# Iterate over the batches of a dataset.\nlibrary(tfdatasets)\nnew_dataset <- ...\n\nwhile(!is.null(batch <- iter_next(new_dataset))) {\n  c(inputs, targets) %<-% batch\n  # Open a GradientTape.\n  with(tf$GradientTape() %as% tape, {\n    # Forward pass.\n    predictions = model(inputs)\n    # Compute the loss value for this batch.\n    loss_value = loss_fn(targets, predictions)\n  })\n  # Get gradients of loss w.r.t. the *trainable* weights.\n  gradients <- tape$gradient(loss_value, model$trainable_weights)\n  # Update the weights of the model.\n  optimizer$apply_gradients(xyz(gradients, model$trainable_weights))\n}\n```\n:::\n\nLikewise for fine-tuning.\n\n## An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\n\nTo solidify these concepts, let's walk you through a concrete end-to-end transfer learning and fine-tuning example. We will load the Xception model, pre-trained on ImageNet, and use it on the Kaggle \""cats vs. dogs\"" classification dataset.\n\n### Getting the data\n\nFirst, let's fetch the cats vs. dogs dataset using TFDS. If you have your own dataset, you'll probably want to use the utility `image_dataset_from_directory()` to generate similar labeled dataset objects from a set of images on disk filed into class-specific folders.\n\nTransfer learning is most useful when working with very small datasets. To keep our dataset small, we will use 40% of the original training data (25,000 images) for training, 10% for validation, and 10% for testing.\n\n::: {.cell}\n\n```{.r .cell-code}\n# reticulate::py_install(\""tensorflow_datasets\"", pip = TRUE)\ntfds <- reticulate::import(\""tensorflow_datasets\"")\n\nc(train_ds, validation_ds, test_ds) %<-% tfds$load(\n    \""cats_vs_dogs\"",\n    # Reserve 10% for validation and 10% for test\n    split = c(\""train[:40%]\"", \""train[40%:50%]\"", \""train[50%:60%]\""),\n    as_supervised=TRUE  # Include labels\n)\n\nprintf(\""Number of training samples: %d\"", length(train_ds))\n```\n\n::: {.cell-output-stdout}\n```\nNumber of training samples: 9305\n```\n:::\n\n```{.r .cell-code}\nprintf(\""Number of validation samples: %d\"", length(validation_ds) )\n```\n\n::: {.cell-output-stdout}\n```\nNumber of validation samples: 2326\n```\n:::\n\n```{.r .cell-code}\nprintf(\""Number of test samples: %d\"", length(test_ds))\n```\n\n::: {.cell-output-stdout}\n```\nNumber of test samples: 2326\n```\n:::\n:::\n\nThese are the first 9 images in the training dataset -- as you can see, they're all different sizes.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n\npar(mfrow = c(3, 3), mar = c(1,0,1.5,0))\ntrain_ds %>%\n  dataset_take(9) %>%\n  as_array_iterator() %>%\n  iterate(function(batch) {\n    c(image, label) %<-% batch\n    plot(as.raster(image, max = 255))\n    title(sprintf(\""label: %s   size: %s\"",\n                  label, paste(dim(image), collapse = \"" x \"")))\n  })\n```\n\n::: {.cell-output-display}\n![](transfer_learning_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\nWe can also see that label 1 is \""dog\"" and label 0 is \""cat\"".\n\n### Standardizing the data\n\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn't a great fit for feeding a neural network. We need to do 2 things:\n\n-   Standardize to a fixed image size. We pick 150x150.\n-   Normalize pixel values between -1 and 1. We'll do this using a `layer_normalization()` as part of the model itself.\n\nIn general, it's a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you'll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\n\nHere, we'll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we'll do the input value scaling as part of the model, when we create it.\n\nLet's resize images to 150x150:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr, include.only = \""%<>%\"")\nsize <- as.integer(c(150, 150))\ntrain_ds      %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\nvalidation_ds %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\ntest_ds       %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\n```\n:::\n\nBesides, let's batch the data and use caching and prefetching to optimize loading speed.\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cache_batch_prefetch <- function(dataset, batch_size = 32, buffer_size = 10) {\n  dataset %>%\n    dataset_cache() %>%\n    dataset_batch(batch_size) %>%\n    dataset_prefetch(buffer_size)\n}\n\ntrain_ds      %<>% dataset_cache_batch_prefetch()\nvalidation_ds %<>% dataset_cache_batch_prefetch()\ntest_ds       %<>% dataset_cache_batch_prefetch()\n```\n:::\n\n### Using random data augmentation\n\nWhen you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_augmentation <- keras_model_sequential() %>%\n  layer_random_flip(\""horizontal\"") %>%\n  layer_random_rotation(.1)\n```\n:::\n\nLet's visualize what the first image of the first batch looks like after various random transformations:\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch <- train_ds %>%\n  dataset_take(1) %>%\n  as_iterator() %>% iter_next()\n\nc(images, labels) %<-% batch\nfirst_image <- images[1, all_dims(), drop = FALSE]\naugmented_image <- data_augmentation(first_image, training = TRUE)\n\nplot_image <- function(image, main = deparse1(substitute(image))) {\n  image %>%\n    k_squeeze(1) %>% # drop batch dim\n    as.array() %>%   # convert from tensor to R array\n    as.raster(max = 255) %>%\n    plot()\n\n  if(!is.null(main))\n    title(main)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1))\nplot_image(first_image)\nplot_image(augmented_image)\nplot_image(data_augmentation(first_image, training = TRUE), \""augmented 2\"")\nplot_image(data_augmentation(first_image, training = TRUE), \""augmented 3\"")\n```\n\n::: {.cell-output-display}\n![](transfer_learning_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n## Build a model\n\nNow let's build a model that follows the blueprint we've explained earlier.\n\nNote that:\n\n-   We add `layer_rescaling()` to scale input values (initially in the `[0, 255]` range) to the `[-1, 1]` range.\n-   We add a `layer_dropout()` before the classification layer, for regularization.\n-   We make sure to pass `training = FALSE` when calling the base model, so that it runs in inference mode, so that batchnorm statistics don't get updated even after we unfreeze the base model for fine-tuning.\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model = application_xception(\n  weights = \""imagenet\"", # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\n# Freeze the base_model\nbase_model$trainable <- FALSE\n\n# Create new model on top\ninputs = layer_input(shape = c(150, 150, 3))\n\noutputs <- inputs %>%\n  data_augmentation() %>%   # Apply random data augmentation\n\n  # Pre-trained Xception weights requires that input be scaled\n  # from (0, 255) to a range of (-1., +1.), the rescaling layer\n  # outputs: `(inputs * scale) + offset`\n  layer_rescaling(scale = 1 / 127.5, offset = -1) %>%\n\n  # The base model contains batchnorm layers. We want to keep them in inference mode\n  # when we unfreeze the base model for fine-tuning, so we make sure that the\n  # base_model is running in inference mode here.\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dropout(.2) %>%\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\nmodel\n```\n\n::: {.cell-output-stdout}\n```\nModel: \""model_1\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  N          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 2,049\nNon-trainable params: 20,861,480\n____________________________________________________________________________\n```\n:::\n:::\n\n## Train the top layer\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_adam(),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 20\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n```\n:::\n\n## Do a round of fine-tuning of the entire model\n\nFinally, let's unfreeze the base model and train the entire model end-to-end with a low learning rate.\n\nImportantly, although the base model becomes trainable, it is still running in inference mode since we passed `training = FALSE` when calling it when we built the model. This means that the batch normalization layers inside won't update their batch statistics. If they did, they would wreck havoc on the representations learned by the model so far.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training = FALSE` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model$trainable <- TRUE\nmodel\n```\n\n::: {.cell-output-stdout}\n```\nModel: \""model_1\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  Y          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 20,809,001\nNon-trainable params: 54,528\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 10\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n```\n:::\n\nAfter 10 epochs, fine-tuning gains us a nice improvement here."",
+    ""markdown"": ""---\ntitle: \""Transfer learning and fine-tuning\""\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nprintf <- function(...) writeLines(sprintf(...))\n```\n:::\n\n\n## Introduction\n\n**Transfer learning** consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify skunks.\n\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n\nThe most common incarnation of transfer learning in the context of deep learning is the following workflow:\n\n1.  Take layers from a previously trained model.\n2.  Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n3.  Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n4.  Train the new layers on your dataset.\n\nA last, optional step, is **fine-tuning**, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.\n\nFirst, we will go over the Keras `trainable` API in detail, which underlies most transfer learning and fine-tuning workflows.\n\nThen, we'll demonstrate the typical workflow by taking a model pretrained on the ImageNet dataset, and retraining it on the Kaggle \""cats vs dogs\"" classification dataset.\n\nThis is adapted from [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r) and the 2016 blog post [\""building powerful image classification models using very little data\""](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).\n\n## Freezing layers: understanding the `trainable` attribute\n\nLayers and models have three weight attributes:\n\n-   `weights` is the list of all weights variables of the layer.\n-   `trainable_weights` is the list of those that are meant to be updated (via gradient descent) to minimize the loss during training.\n-   `non_trainable_weights` is the list of those that aren't meant to be trained. Typically they are updated by the model during the forward pass.\n\n**Example: the `Dense` layer has 2 trainable weights (kernel and bias)**\n\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nlayer$build(shape(NULL, 4))\n\nprintf(\""weights: %s\"", length(layer$weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""trainable_weights: %s\"", length(layer$trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""non_trainable_weights: %s\"", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon_trainable_weights: 0\n```\n:::\n:::\n\n\nIn general, all weights are trainable weights. The only built-in layer that has non-trainable weights is `layer_batch_normalization()`. It uses non-trainable weights to keep track of the mean and variance of its inputs during training. To learn how to use non-trainable weights in your own custom layers, see the [guide to writing new layers from scratch](https://keras.rstudio.com/articles/new-guides/making_new_layers_and_models_via_subclassing.html).\n\n**Example: The layer instance returned by `layer_batch_normalization()` has 2 trainable weights and 2 non-trainable weights**\n\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer <- layer_batch_normalization()\nlayer$build(shape(NULL, 4))\n\nprintf(\""weights: %s\"", length(layer$weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 4\n```\n:::\n\n```{.r .cell-code}\nprintf(\""trainable_weights: %s\"", length(layer$trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""non_trainable_weights: %s\"", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon_trainable_weights: 2\n```\n:::\n:::\n\n\nLayers and models also feature a boolean attribute `trainable`. Its value can be changed. Setting `layer$trainable` to `FALSE` moves all the layer's weights from trainable to non-trainable. This is called \""freezing\"" the layer: the state of a frozen layer won't be updated during training (either when training with `fit()` or when training with any custom loop that relies on `trainable_weights` to apply gradient updates).\n\n**Example: setting `trainable` to `False`**\n\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer = layer_dense(units = 3)\nlayer$build(shape(NULL, 4))  # Create the weights\nlayer$trainable <- FALSE     # Freeze the layer\n\nprintf(\""weights: %s\"", length(layer$weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\""trainable_weights: %s\"", length(layer$trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights: 0\n```\n:::\n\n```{.r .cell-code}\nprintf(\""non_trainable_weights: %s\"", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon_trainable_weights: 2\n```\n:::\n:::\n\n\nWhen a trainable weight becomes non-trainable, its value is no longer updated during training.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a model with 2 layers\nlayer1 <- layer_dense(units = 3, activation = \""relu\"")\nlayer2 <- layer_dense(units = 3, activation = \""sigmoid\"")\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  layer1() %>%\n  layer2()\n\n# Freeze the first layer\nlayer1$trainable <- FALSE\n\n# Keep a copy of the weights of layer1 for later reference\ninitial_layer1_weights_values <- get_weights(layer1)\n\n# Train the model\nmodel %>% compile(optimizer = \""adam\"", loss = \""mse\"")\nmodel %>% fit(k_random_normal(c(2, 3)), k_random_normal(c(2, 3)))\n\n# Check that the weights of layer1 have not changed during training\nfinal_layer1_weights_values <- get_weights(layer1)\nstopifnot(all.equal(initial_layer1_weights_values, final_layer1_weights_values))\n```\n:::\n\n\nDo not confuse the `layer$trainable` attribute with the `training` argument in a layer instance's `call` signature `layer(training =)` (which controls whether the layer should run its forward pass in inference mode or training mode). For more information, see the [Keras FAQ](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n\n## Recursive setting of the `trainable` attribute\n\nIf you set `trainable = FALSE` on a model or on any layer that has sublayers, all child layers become non-trainable as well.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_model <- keras_model_sequential(input_shape = c(3)) %>%\n  layer_dense(3, activation = \""relu\"") %>%\n  layer_dense(3, activation = \""relu\"")\n\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  inner_model() %>%\n  layer_dense(3, activation = \""sigmoid\"")\n\n\nmodel$trainable <- FALSE  # Freeze the outer model\n\nstopifnot(inner_model$trainable == FALSE)             # All layers in `model` are now frozen\nstopifnot(inner_model$layers[[1]]$trainable == FALSE)  # `trainable` is propagated recursively\n```\n:::\n\n\n## The typical transfer-learning workflow\n\nThis leads us to how a typical transfer learning workflow can be implemented in Keras:\n\n1.  Instantiate a base model and load pre-trained weights into it.\n2.  Freeze all layers in the base model by setting `trainable = FALSE`.\n3.  Create a new model on top of the output of one (or several) layers from the base model.\n4.  Train your new model on your new dataset.\n\nNote that an alternative, more lightweight workflow could also be:\n\n1.  Instantiate a base model and load pre-trained weights into it.\n2.  Run your new dataset through it and record the output of one (or several) layers from the base model. This is called **feature extraction**.\n3.  Use that output as input data for a new, smaller model.\n\nA key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it's a lot faster and cheaper.\n\nAn issue with that second workflow, though, is that it doesn't allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow.\n\nHere's what the first workflow looks like in Keras:\n\nFirst, instantiate a base model with pre-trained weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model <- application_xception(\n  weights = 'imagenet', # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n```\n:::\n\n\nThen, freeze the base model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model$trainable <- FALSE\n```\n:::\n\n\nCreate a new model on top.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(c(150, 150, 3))\n\noutputs <- inputs %>%\n  # We make sure that the base_model is running in inference mode here,\n  # by passing `training=FALSE`. This is important for fine-tuning, as you will\n  # learn in a few paragraphs.\n  base_model(training=FALSE) %>%\n\n  # Convert features of shape `base_model$output_shape[-1]` to vectors\n  layer_global_average_pooling_2d() %>%\n\n  # A Dense classifier with a single unit (binary classification)\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\n```\n:::\n\n\nTrain the model on new data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer = optimizer_adam(),\n          loss = loss_binary_crossentropy(from_logits = TRUE),\n          metrics = metric_binary_accuracy()) %>%\n  fit(new_dataset, epochs = 20, callbacks = ..., validation_data = ...)\n```\n:::\n\n\n## Fine-tuning\n\nOnce your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\n\nThis is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind.\n\nIt is critical to only do this step *after* the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\n\nIt's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to re-adapt the pretrained weights in an incremental way.\n\nThis is how to implement fine-tuning of the whole base model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unfreeze the base model\nbase_model$trainable <- TRUE\n\n# It's important to recompile your model after you make any changes\n# to the `trainable` attribute of any inner layer, so that your changes\n# are taken into account\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5), # Very low learning rate\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\n# Train end-to-end. Be careful to stop before you overfit!\nmodel %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n```\n:::\n\n\n**Important note about `compile()` and `trainable`**\n\nCalling `compile()` on a model is meant to \""freeze\"" the behavior of that model. This implies that the `trainable` attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until `compile` is called again. Hence, if you change any `trainable` value, make sure to call `compile()` again on your model for your changes to be taken into account.\n\n**Important notes about `layer_batch_normalization()`**\n\nMany image models contain `BatchNormalization` layers. That layer is a special case on every imaginable count. Here are a few things to keep in mind.\n\n-   `BatchNormalization` contains 2 non-trainable weights that get updated during training. These are the variables tracking the mean and variance of the inputs.\n-   When you set `bn_layer$trainable = FALSE`, the `BatchNormalization` layer will run in inference mode, and will not update its mean and variance statistics. This is not the case for other layers in general, as [weight trainability and inference/training modes are two orthogonal concepts](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute). But the two are tied in the case of the `BatchNormalization` layer.\n-   When you unfreeze a model that contains `BatchNormalization` layers in order to do fine-tuning, you should keep the `BatchNormalization` layers in inference mode by passing `training = FALSE` when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.\n\nYou'll see this pattern in action in the end-to-end example at the end of this guide.\n\n## Transfer learning and fine-tuning with a custom training loop\n\nIf instead of `fit()`, you are using your own low-level training loop, the workflow stays essentially the same. You should be careful to only take into account the list `model$trainable_weights` when applying gradient updates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create base model\nbase_model = application_xception(\n  weights = 'imagenet',\n  input_shape = c(150, 150, 3),\n  include_top = FALSE\n)\n\n# Freeze base model\nbase_model$trainable = FALSE\n\n# Create new model on top.\ninputs <- layer_input(shape = c(150, 150, 3))\noutputs <- inputs %>%\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\n# helper to zip gradients with weights\nxyz <- function(...) .mapply(c, list(...), NULL)\n\n# Iterate over the batches of a dataset.\nlibrary(tfdatasets)\nnew_dataset <- ...\n\nwhile(!is.null(batch <- iter_next(new_dataset))) {\n  c(inputs, targets) %<-% batch\n  # Open a GradientTape.\n  with(tf$GradientTape() %as% tape, {\n    # Forward pass.\n    predictions = model(inputs)\n    # Compute the loss value for this batch.\n    loss_value = loss_fn(targets, predictions)\n  })\n  # Get gradients of loss w.r.t. the *trainable* weights.\n  gradients <- tape$gradient(loss_value, model$trainable_weights)\n  # Update the weights of the model.\n  optimizer$apply_gradients(xyz(gradients, model$trainable_weights))\n}\n```\n:::\n\n\nLikewise for fine-tuning.\n\n## An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\n\nTo solidify these concepts, let's walk you through a concrete end-to-end transfer learning and fine-tuning example. We will load the Xception model, pre-trained on ImageNet, and use it on the Kaggle \""cats vs. dogs\"" classification dataset.\n\n### Getting the data\n\nFirst, let's fetch the cats vs. dogs dataset using TFDS. If you have your own dataset, you'll probably want to use the utility `image_dataset_from_directory()` to generate similar labeled dataset objects from a set of images on disk filed into class-specific folders.\n\nTransfer learning is most useful when working with very small datasets. To keep our dataset small, we will use 40% of the original training data (25,000 images) for training, 10% for validation, and 10% for testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reticulate::py_install(\""tensorflow_datasets\"", pip = TRUE)\ntfds <- reticulate::import(\""tensorflow_datasets\"")\n\nc(train_ds, validation_ds, test_ds) %<-% tfds$load(\n    \""cats_vs_dogs\"",\n    # Reserve 10% for validation and 10% for test\n    split = c(\""train[:40%]\"", \""train[40%:50%]\"", \""train[50%:60%]\""),\n    as_supervised=TRUE  # Include labels\n)\n\nprintf(\""Number of training samples: %d\"", length(train_ds))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of training samples: 9305\n```\n:::\n\n```{.r .cell-code}\nprintf(\""Number of validation samples: %d\"", length(validation_ds) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of validation samples: 2326\n```\n:::\n\n```{.r .cell-code}\nprintf(\""Number of test samples: %d\"", length(test_ds))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of test samples: 2326\n```\n:::\n:::\n\n\nThese are the first 9 images in the training dataset -- as you can see, they're all different sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n\npar(mfrow = c(3, 3), mar = c(1,0,1.5,0))\ntrain_ds %>%\n  dataset_take(9) %>%\n  as_array_iterator() %>%\n  iterate(function(batch) {\n    c(image, label) %<-% batch\n    plot(as.raster(image, max = 255))\n    title(sprintf(\""label: %s   size: %s\"",\n                  label, paste(dim(image), collapse = \"" x \"")))\n  })\n```\n\n::: {.cell-output-display}\n![](transfer_learning_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nWe can also see that label 1 is \""dog\"" and label 0 is \""cat\"".\n\n### Standardizing the data\n\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn't a great fit for feeding a neural network. We need to do 2 things:\n\n-   Standardize to a fixed image size. We pick 150x150.\n-   Normalize pixel values between -1 and 1. We'll do this using a `layer_normalization()` as part of the model itself.\n\nIn general, it's a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you'll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\n\nHere, we'll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we'll do the input value scaling as part of the model, when we create it.\n\nLet's resize images to 150x150:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr, include.only = \""%<>%\"")\nsize <- as.integer(c(150, 150))\ntrain_ds      %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\nvalidation_ds %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\ntest_ds       %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\n```\n:::\n\n\nBesides, let's batch the data and use caching and prefetching to optimize loading speed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cache_batch_prefetch <- function(dataset, batch_size = 32, buffer_size = 10) {\n  dataset %>%\n    dataset_cache() %>%\n    dataset_batch(batch_size) %>%\n    dataset_prefetch(buffer_size)\n}\n\ntrain_ds      %<>% dataset_cache_batch_prefetch()\nvalidation_ds %<>% dataset_cache_batch_prefetch()\ntest_ds       %<>% dataset_cache_batch_prefetch()\n```\n:::\n\n\n### Using random data augmentation\n\nWhen you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_augmentation <- keras_model_sequential() %>%\n  layer_random_flip(\""horizontal\"") %>%\n  layer_random_rotation(.1)\n```\n:::\n\n\nLet's visualize what the first image of the first batch looks like after various random transformations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch <- train_ds %>%\n  dataset_take(1) %>%\n  as_iterator() %>% iter_next()\n\nc(images, labels) %<-% batch\nfirst_image <- images[1, all_dims(), drop = FALSE]\naugmented_image <- data_augmentation(first_image, training = TRUE)\n\nplot_image <- function(image, main = deparse1(substitute(image))) {\n  image %>%\n    k_squeeze(1) %>% # drop batch dim\n    as.array() %>%   # convert from tensor to R array\n    as.raster(max = 255) %>%\n    plot()\n\n  if(!is.null(main))\n    title(main)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1))\nplot_image(first_image)\nplot_image(augmented_image)\nplot_image(data_augmentation(first_image, training = TRUE), \""augmented 2\"")\nplot_image(data_augmentation(first_image, training = TRUE), \""augmented 3\"")\n```\n\n::: {.cell-output-display}\n![](transfer_learning_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Build a model\n\nNow let's build a model that follows the blueprint we've explained earlier.\n\nNote that:\n\n-   We add `layer_rescaling()` to scale input values (initially in the `[0, 255]` range) to the `[-1, 1]` range.\n-   We add a `layer_dropout()` before the classification layer, for regularization.\n-   We make sure to pass `training = FALSE` when calling the base model, so that it runs in inference mode, so that batchnorm statistics don't get updated even after we unfreeze the base model for fine-tuning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model = application_xception(\n  weights = \""imagenet\"", # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\n# Freeze the base_model\nbase_model$trainable <- FALSE\n\n# Create new model on top\ninputs <- layer_input(shape = c(150, 150, 3))\n\noutputs <- inputs %>%\n  data_augmentation() %>%   # Apply random data augmentation\n\n  # Pre-trained Xception weights requires that input be scaled\n  # from (0, 255) to a range of (-1., +1.), the rescaling layer\n  # outputs: `(inputs * scale) + offset`\n  layer_rescaling(scale = 1 / 127.5, offset = -1) %>%\n\n  # The base model contains batchnorm layers. We want to keep them in inference mode\n  # when we unfreeze the base model for fine-tuning, so we make sure that the\n  # base_model is running in inference mode here.\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dropout(.2) %>%\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""model_1\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  N          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 2,049\nNon-trainable params: 20,861,480\n____________________________________________________________________________\n```\n:::\n:::\n\n\n## Train the top layer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_adam(),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 2\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n```\n:::\n\n\n## Do a round of fine-tuning of the entire model\n\nFinally, let's unfreeze the base model and train the entire model end-to-end with a low learning rate.\n\nImportantly, although the base model becomes trainable, it is still running in inference mode since we passed `training = FALSE` when calling it when we built the model. This means that the batch normalization layers inside won't update their batch statistics. If they did, they would wreck havoc on the representations learned by the model so far.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training = FALSE` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model$trainable <- TRUE\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""model_1\""\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  Y          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 20,809,001\nNon-trainable params: 54,528\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 1\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n```\n:::\n\n\nAfter 10 epochs, fine-tuning gains us a nice improvement here.\n"",
     ""supporting"": [
       ""transfer_learning_files""
     ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],
-    ""includes"": [],
+    ""includes"": {},
     ""engineDependencies"": {},
     ""preserve"": {},
-    ""postProcess"": null
+    ""postProcess"": true
   }
 }
\ No newline at end of file

---FILE: _freeze/keras/guides/working_with_rnns/execute-results/html.json---
@@ -1,16 +1,16 @@
 {
   ""hash"": ""d69e39a08d03aaf44708dcdfb99c64d3"",
   ""result"": {
-    ""markdown"": ""---\ntitle: Working with RNNs\nauthor: Scott Zhu, Francois Chollet, Tomasz Kalinowski\n---\n\n## Introduction\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\n\nSchematically, a RNN layer uses a `for` loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n-   **Ease of use**: the built-in `layer_rnn()`, `layer_lstm()`, `layer_gru()` layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n\n-   **Ease of customization**: You can also define your own RNN cell layer (the inner part of the `for` loop) with custom behavior, and use it with the generic `layer_rnn` layer (the `for` loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n\n## Setup\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n## Built-in RNN layers: a simple example\n\nThere are three built-in RNN layers in Keras:\n\n1.  `layer_simple_rnn()`, a fully-connected RNN where the output from the previous timestep is to be fed to the next timestep.\n\n2.  `layer_gru()`, first proposed in [Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n\n3.  `layer_lstm()`, first proposed in [Hochreiter & Schmidhuber, 1997](http://www.bioinf.jku.at/publications/older/2604.pdf).\n\nHere is a simple example of a sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a `layer_lstm()`.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n\n  # Add an Embedding layer expecting input vocab of size 1000, and\n  # output embedding dimension of size 64.\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # Add a LSTM layer with 128 internal units.\n  layer_lstm(128) %>%\n\n  # Add a Dense layer with 10 units.\n  layer_dense(10)\n```\n\n::: {.cell-output-stderr}\n```\nLoaded Tensorflow version 2.8.0\n```\n:::\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output-stdout}\n```\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding (Embedding)            (None, None, 64)              64000       \n lstm (LSTM)                      (None, 128)                   98816       \n dense (Dense)                    (None, 10)                    1290        \n============================================================================\nTotal params: 164,106\nTrainable params: 164,106\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\nBuilt-in RNNs support a number of useful features:\n\n-   Recurrent dropout, via the `dropout` and `recurrent_dropout` arguments\n-   Ability to process an input sequence in reverse, via the `go_backwards` argument\n-   Loop unrolling (which can lead to a large speedup when processing short sequences on CPU), via the `unroll` argument\n-   ...and more.\n\nFor more information, see the [RNN API documentation](https://keras.io/api/layers/recurrent_layers/).\n\n## Outputs and states\n\nBy default, the output of a RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is `(batch_size, units)` where `units` corresponds to the `units` argument passed to the layer's constructor.\n\nA RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set `return_sequences = TRUE`. The shape of this output is `(batch_size, timesteps, units)`.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n  layer_gru(256, return_sequences = TRUE) %>%\n\n  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n  layer_simple_rnn(128) %>%\n\n  layer_dense(10)\n\nmodel\n```\n\n::: {.cell-output-stdout}\n```\nModel: \""sequential_1\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding_1 (Embedding)          (None, None, 64)              64000       \n gru (GRU)                        (None, None, 256)             247296      \n simple_rnn (SimpleRNN)           (None, 128)                   49280       \n dense_1 (Dense)                  (None, 10)                    1290        \n============================================================================\nTotal params: 361,866\nTrainable params: 361,866\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\nIn addition, a RNN layer can return its final internal state(s). The returned states can be used to resume the RNN execution later, or [to initialize another RNN](https://arxiv.org/abs/1409.3215). This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.\n\nTo configure a RNN layer to return its internal state, set `return_state = TRUE` when creating the layer. Note that `LSTM` has 2 state tensors, but `GRU` only has one.\n\nTo configure the initial state of the layer, call the layer instance with the additional named argument `initial_state`. Note that the shape of the state needs to match the unit size of the layer, like in the example below.\n\n::: {.cell}\n\n```{.r .cell-code}\nencoder_vocab <- 1000\ndecoder_vocab <- 2000\n\nencoder_input <- layer_input(shape(NULL))\nencoder_embedded <- encoder_input %>%\n  layer_embedding(input_dim=encoder_vocab, output_dim=64)\n\n\n# Return states in addition to output\nc(output, state_h, state_c) %<-%\n  layer_lstm(encoder_embedded, units = 64, return_state=TRUE, name=\""encoder\"")\n\nencoder_state <- list(state_h, state_c)\n\ndecoder_input <- layer_input(shape(NULL))\ndecoder_embedded <- decoder_input %>%\n  layer_embedding(input_dim = decoder_vocab, output_dim = 64)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_lstm_layer <- layer_lstm(units = 64, name = \""decoder\"")\ndecoder_output <- decoder_lstm_layer(decoder_embedded, initial_state = encoder_state)\n\noutput <- decoder_output %>% layer_dense(10)\n\nmodel <- keras_model(inputs = list(encoder_input, decoder_input),\n                     outputs = output)\nmodel\n```\n\n::: {.cell-output-stdout}\n```\nModel: \""model\""\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n input_1 (InputLayer)    [(None, None)]  0        []                        \n input_2 (InputLayer)    [(None, None)]  0        []                        \n embedding_2 (Embedding)  (None, None, 6  64000   ['input_1[0][0]']         \n                         4)                                                 \n embedding_3 (Embedding)  (None, None, 6  128000  ['input_2[0][0]']         \n                         4)                                                 \n encoder (LSTM)          [(None, 64),    33024    ['embedding_2[0][0]']     \n                          (None, 64),                                       \n                          (None, 64)]                                       \n decoder (LSTM)          (None, 64)      33024    ['embedding_3[0][0]',     \n                                                   'encoder[0][1]',         \n                                                   'encoder[0][2]']         \n dense_2 (Dense)         (None, 10)      650      ['decoder[0][0]']         \n============================================================================\nTotal params: 258,698\nTrainable params: 258,698\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n## RNN layers and RNN cells\n\nIn addition to the built-in RNN layers, the RNN API also provides cell-level APIs. Unlike RNN layers, which process whole batches of input sequences, the RNN cell only processes a single timestep.\n\nThe cell is the inside of the `for` loop of a RNN layer. Wrapping a cell inside a `layer_rnn()` layer gives you a layer capable of processing a sequence, e.g. `layer_rnn(layer_lstm_cell(10))`.\n\nMathematically, `layer_rnn(layer_lstm_cell(10))` produces the same result as `layer_lstm(10)`. In fact, the implementation of this layer in TF v1.x was just creating the corresponding RNN cell and wrapping it in a RNN layer. However using the built-in `layer_gru()` and `layer_lstm()` layers enable the use of CuDNN and you may see better performance.\n\nThere are three built-in RNN cells, each of them corresponding to the matching RNN layer.\n\n-   `layer_simple_rnn_cell()` corresponds to the `layer_simple_rnn()` layer.\n\n-   `layer_gru_cell` corresponds to the `layer_gru` layer.\n\n-   `layer_lstm_cell` corresponds to the `layer_lstm` layer.\n\nThe cell abstraction, together with the generic `layer_rnn()` class, makes it very easy to implement custom RNN architectures for your research.\n\n## Cross-batch statefulness\n\nWhen processing very long (possibly infinite) sequences, you may want to use the pattern of **cross-batch statefulness**.\n\nNormally, the internal state of a RNN layer is reset every time it sees a new batch (i.e. every sample seen by the layer is assumed to be independent of the past). The layer will only maintain a state while processing a given sample.\n\nIf you have very long sequences though, it is useful to break them into shorter sequences, and to feed these shorter sequences sequentially into a RNN layer without resetting the layer's state. That way, the layer can retain information about the entirety of the sequence, even though it's only seeing one sub-sequence at a time.\n\nYou can do this by setting `stateful = TRUE` in the constructor.\n\nIf you have a sequence `s = c(t0, t1, ... t1546, t1547)`, you would split it into e.g.\n\n::: {.cell}\n\n```{.r .cell-code}\ns1 = c(t0, t1, ..., t100)\ns2 = c(t101, ..., t201)\n...\ns16 = c(t1501, ..., t1547)\n```\n:::\n\nThen you would process it via:\n\n::: {.cell}\n\n```{.r .cell-code}\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\nfor(s in sub_sequences)\n  output <- lstm_layer(s)\n```\n:::\n\nWhen you want to clear the state, you can use `layer$reset_states()`.\n\n> Note: In this setup, sample `i` in a given batch is assumed to be the continuation of sample `i` in the previous batch. This means that all batches should contain the same number of samples (batch size). E.g. if a batch contains `[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]`, the next batch should contain `[sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]`.\n\nHere is a complete example:\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\noutput <- lstm_layer(paragraph3)\n\n# reset_states() will reset the cached state to the original initial_state.\n# If no initial_state was provided, zero-states will be used by default.\nlstm_layer$reset_states()\n```\n:::\n\n### RNN State Reuse\n\nThe recorded states of the RNN layer are not included in the `layer$weights()`. If you would like to reuse the state from a RNN layer, you can retrieve the states value by `layer$states` and use it as the initial state of a new layer instance via the Keras functional API like `new_layer(inputs, initial_state = layer$states)`, or model subclassing.\n\nPlease also note that a sequential model cannot be used in this case since it only supports layers with single input and output. The extra input of initial state makes it impossible to use here.\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\n\nexisting_state <- lstm_layer$states\n\nnew_lstm_layer <- layer_lstm(units = 64)\nnew_output <- new_lstm_layer(paragraph3, initial_state = existing_state)\n```\n:::\n\n## Bidirectional RNNs\n\nFor sequences other than time series (e.g. text), it is often the case that a RNN model can perform better if it not only processes sequence from start to end, but also backwards. For example, to predict the next word in a sentence, it is often useful to have the context around the word, not only just the words that come before it.\n\nKeras provides an easy API for you to build such bidirectional RNNs: the `bidirectional()` wrapper.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = shape(5, 10)) %>%\n  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units = 32)) %>%\n  layer_dense(10)\n\nmodel\n```\n\n::: {.cell-output-stdout}\n```\nModel: \""sequential_2\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n bidirectional_1 (Bidirectional)  (None, 5, 128)                38400       \n bidirectional (Bidirectional)    (None, 64)                    41216       \n dense_3 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 80,266\nTrainable params: 80,266\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\nUnder the hood, `bidirectional()` will copy the RNN layer passed in, and flip the `go_backwards` field of the newly copied layer, so that it will process the inputs in reverse order.\n\nThe output of the `bidirectional` RNN will be, by default, the concatenation of the forward layer output and the backward layer output. If you need a different merging behavior, e.g. averaging, change the `merge_mode` parameter in the `bidirectional` wrapper constructor. For more details about `bidirectional`, please check [the API docs](https://keras.io/api/layers/recurrent_layers/bidirectional/).\n\n## Performance optimization and CuDNN kernels\n\nIn TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN kernels by default when a GPU is available. With this change, the prior `layer_cudnn_gru/layer_cudnn_lstm` layers have been deprecated, and you can build your model without worrying about the hardware it will run on.\n\nSince the CuDNN kernel is built with certain assumptions, this means the layer **will not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or GRU layers**. E.g.:\n\n-   Changing the `activation` function from `\""tanh\""` to something else.\n-   Changing the `recurrent_activation` function from `\""sigmoid\""` to something else.\n-   Using `recurrent_dropout > 0`.\n-   Setting `unroll` to `TRUE`, which forces LSTM/GRU to decompose the inner `tf$while_loop` into an unrolled `for` loop.\n-   Setting `use_bias` to `FALSE`.\n-   Using masking when the input data is not strictly right padded (if the mask corresponds to strictly right padded data, CuDNN can still be used. This is the most common case).\n\nFor the detailed list of constraints, please see the documentation for the [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) and [GRU](https://keras.io/api/layers/recurrent_layers/gru/) layers.\n\n### Using CuDNN kernels when available\n\nLet's build a simple LSTM model to demonstrate the performance difference.\n\nWe'll use as input sequences the sequence of rows of MNIST digits (treating each row of pixels as a timestep), and we'll predict the digit's label.\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch_size <- 64\n# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n# Each input sequence will be of size (28, 28) (height is treated like time).\ninput_dim <- 28\n\nunits <- 64\noutput_size <- 10  # labels are from 0 to 9\n\n# Build the RNN model\nbuild_model <- function(allow_cudnn_kernel = TRUE) {\n  # CuDNN is only available at the layer level, and not at the cell level.\n  # This means `layer_lstm(units = units)` will use the CuDNN kernel,\n  # while layer_rnn(cell = layer_lstm_cell(units)) will run on non-CuDNN kernel.\n  if (allow_cudnn_kernel)\n    # The LSTM layer with default options uses CuDNN.\n    lstm_layer <- layer_lstm(units = units)\n  else\n    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n    lstm_layer <- layer_rnn(cell = layer_lstm_cell(units = units))\n\n  model <-\n    keras_model_sequential(input_shape = shape(NULL, input_dim)) %>%\n    lstm_layer() %>%\n    layer_batch_normalization() %>%\n    layer_dense(output_size)\n\n  model\n}\n```\n:::\n\nLet's load the MNIST dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x / 255\nmnist$test$x <- mnist$test$x / 255\nc(sample, sample_label) %<-% with(mnist$train, list(x[1,,], y[1]))\n```\n:::\n\nLet's create a model instance and train it.\n\nWe choose `sparse_categorical_crossentropy()` as the loss function for the model. The output of the model has shape of `(batch_size, 10)`. The target for the model is an integer vector, each of the integer is in the range of 0 to 9.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- build_model(allow_cudnn_kernel = TRUE) %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \""sgd\"",\n    metrics = \""accuracy\""\n  )\n\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n```\n:::\n\nNow, let's compare to a model that does not use the CuDNN kernel:\n\n::: {.cell}\n\n```{.r .cell-code}\nnoncudnn_model <- build_model(allow_cudnn_kernel=FALSE)\nnoncudnn_model$set_weights(model$get_weights())\nnoncudnn_model %>% compile(\n    loss=loss_sparse_categorical_crossentropy(from_logits=TRUE),\n    optimizer=\""sgd\"",\n    metrics=\""accuracy\"",\n)\n\nnoncudnn_model %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n```\n:::\n\nWhen running on a machine with a NVIDIA GPU and CuDNN installed, the model built with CuDNN is much faster to train compared to the model that uses the regular TensorFlow kernel.\n\nThe same CuDNN-enabled model can also be used to run inference in a CPU-only environment. The `tf$device()` annotation below is just forcing the device placement. The model will run on CPU by default if no GPU is available.\n\nYou simply don't have to worry about the hardware you're running on anymore. Isn't that pretty cool?\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$device(\""CPU:0\""), {\n    cpu_model <- build_model(allow_cudnn_kernel=TRUE)\n    cpu_model$set_weights(model$get_weights())\n\n    result <- cpu_model %>%\n      predict_on_batch(k_expand_dims(sample, 1)) %>%\n      k_argmax(axis = 2)\n\n    cat(sprintf(\n        \""Predicted result is: %s, target result is: %s\\n\"", as.numeric(result), sample_label))\n\n    # show mnist image\n    sample %>%\n      apply(2, rev) %>% # flip\n      t() %>%           # rotate\n      image(axes = FALSE, asp = 1, col = grey(seq(0, 1, length.out = 256)))\n})\n```\n\n::: {.cell-output-stdout}\n```\nPredicted result is: 3, target result is: 5\n```\n:::\n\n::: {.cell-output-display}\n![](working_with_rnns_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n## RNNs with list/dict inputs, or nested inputs\n\nNested structures allow implementers to include more information within a single timestep. For example, a video frame could have audio and video input at the same time. The data shape in this case could be:\n\n`[batch, timestep, {\""video\"": [height, width, channel], \""audio\"": [frequency]}]`\n\nIn another example, handwriting data could have both coordinates x and y for the current position of the pen, as well as pressure information. So the data representation could be:\n\n`[batch, timestep, {\""location\"": [x, y], \""pressure\"": [force]}]`\n\nThe following code provides an example of how to build a custom RNN cell that accepts such structured inputs.\n\n### Define a custom cell that supports nested input/output\n\nSee [Making new Layers & Models via subclassing](/guides/making_new_layers_and_models_via_subclassing/) for details on writing your own layers.\n\n::: {.cell}\n\n```{.r .cell-code}\nNestedCell(keras$layers$Layer) %py_class% {\n\n  initialize <- function(unit_1, unit_2, unit_3, ...) {\n    self$unit_1 <- unit_1\n    self$unit_2 <- unit_2\n    self$unit_3 <- unit_3\n    self$state_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    self$output_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    super$initialize(...)\n  }\n\n  build <- function(self, input_shapes) {\n    # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n    # dput(input_shapes) gives: list(list(NULL, 32L), list(NULL, 64L, 32L))\n    i1 <- input_shapes[[c(1, 2)]] # 32\n    i2 <- input_shapes[[c(2, 2)]] # 64\n    i3 <- input_shapes[[c(2, 3)]] # 32\n\n    self$kernel_1 = self$add_weight(\n      shape = shape(i1, self$unit_1),\n      initializer = \""uniform\"",\n      name = \""kernel_1\""\n    )\n    self$kernel_2_3 = self$add_weight(\n      shape = shape(i2, i3, self$unit_2, self$unit_3),\n      initializer = \""uniform\"",\n      name = \""kernel_2_3\""\n    )\n  }\n\n  call <- function(inputs, states) {\n    # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n    # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n    # Don't forget you can call `browser()` here while the layer is being traced!\n    c(input_1, input_2) %<-% tf$nest$flatten(inputs)\n    c(s1, s2) %<-% states\n\n    output_1 <- tf$matmul(input_1, self$kernel_1)\n    output_2_3 <- tf$einsum(\""bij,ijkl->bkl\"", input_2, self$kernel_2_3)\n    state_1 <- s1 + output_1\n    state_2_3 <- s2 + output_2_3\n\n    output <- tuple(output_1, output_2_3)\n    new_states <- tuple(state_1, state_2_3)\n\n    tuple(output, new_states)\n  }\n\n  get_config <- function() {\n    list(\""unit_1\"" = self$unit_1,\n         \""unit_2\"" = self$unit_2,\n         \""unit_3\"" = self$unit_3)\n  }\n}\n```\n:::\n\n### Build a RNN model with nested input/output\n\nLet's build a Keras model that uses a `layer_rnn` layer and the custom cell we just defined.\n\n::: {.cell}\n\n```{.r .cell-code}\nunit_1 <- 10\nunit_2 <- 20\nunit_3 <- 30\n\ni1 <- 32\ni2 <- 64\ni3 <- 32\nbatch_size <- 64\nnum_batches <- 10\ntimestep <- 50\n\ncell <- NestedCell(unit_1, unit_2, unit_3)\nrnn <- layer_rnn(cell = cell)\n\ninput_1 = layer_input(shape(NULL, i1))\ninput_2 = layer_input(shape(NULL, i2, i3))\n\noutputs = rnn(tuple(input_1, input_2))\n\nmodel = keras_model(list(input_1, input_2), outputs)\n\nmodel %>% compile(optimizer=\""adam\"", loss=\""mse\"", metrics=\""accuracy\"")\n```\n:::\n\n### Train the model with randomly generated data\n\nSince there isn't a good candidate dataset for this model, we use random data for demonstration.\n\n::: {.cell}\n\n```{.r .cell-code}\ninput_1_data <- k_random_uniform(c(batch_size * num_batches, timestep, i1))\ninput_2_data <- k_random_uniform(c(batch_size * num_batches, timestep, i2, i3))\ntarget_1_data <- k_random_uniform(c(batch_size * num_batches, unit_1))\ntarget_2_data <- k_random_uniform(c(batch_size * num_batches, unit_2, unit_3))\ninput_data <- list(input_1_data, input_2_data)\ntarget_data <- list(target_1_data, target_2_data)\n\nmodel %>% fit(input_data, target_data, batch_size=batch_size)\n```\n:::\n\nWith `keras::layer_rnn()`, you are only expected to define the math logic for an individual step within the sequence, and the `layer_rnn()` will handle the sequence iteration for you. It's an incredibly powerful way to quickly prototype new kinds of RNNs (e.g. a LSTM variant).\n\nFor more details, please visit the [API docs](https://keras.io/api/layers/recurrent_layers/rnn/)."",
+    ""markdown"": ""---\ntitle: Working with RNNs\nauthor: Scott Zhu, Francois Chollet, Tomasz Kalinowski\n---\n\n\n## Introduction\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\n\nSchematically, a RNN layer uses a `for` loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n-   **Ease of use**: the built-in `layer_rnn()`, `layer_lstm()`, `layer_gru()` layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n\n-   **Ease of customization**: You can also define your own RNN cell layer (the inner part of the `for` loop) with custom behavior, and use it with the generic `layer_rnn` layer (the `for` loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Built-in RNN layers: a simple example\n\nThere are three built-in RNN layers in Keras:\n\n1.  `layer_simple_rnn()`, a fully-connected RNN where the output from the previous timestep is to be fed to the next timestep.\n\n2.  `layer_gru()`, first proposed in [Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n\n3.  `layer_lstm()`, first proposed in [Hochreiter & Schmidhuber, 1997](http://www.bioinf.jku.at/publications/older/2604.pdf).\n\nHere is a simple example of a sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a `layer_lstm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n\n  # Add an Embedding layer expecting input vocab of size 1000, and\n  # output embedding dimension of size 64.\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # Add a LSTM layer with 128 internal units.\n  layer_lstm(128) %>%\n\n  # Add a Dense layer with 10 units.\n  layer_dense(10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding (Embedding)            (None, None, 64)              64000       \n lstm (LSTM)                      (None, 128)                   98816       \n dense (Dense)                    (None, 10)                    1290        \n============================================================================\nTotal params: 164,106\nTrainable params: 164,106\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nBuilt-in RNNs support a number of useful features:\n\n-   Recurrent dropout, via the `dropout` and `recurrent_dropout` arguments\n-   Ability to process an input sequence in reverse, via the `go_backwards` argument\n-   Loop unrolling (which can lead to a large speedup when processing short sequences on CPU), via the `unroll` argument\n-   ...and more.\n\nFor more information, see the [RNN API documentation](https://keras.io/api/layers/recurrent_layers/).\n\n## Outputs and states\n\nBy default, the output of a RNN layer contains a single vector per sample. This vector is the RNN cell output corresponding to the last timestep, containing information about the entire input sequence. The shape of this output is `(batch_size, units)` where `units` corresponds to the `units` argument passed to the layer's constructor.\n\nA RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set `return_sequences = TRUE`. The shape of this output is `(batch_size, timesteps, units)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_embedding(input_dim = 1000, output_dim = 64) %>%\n\n  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n  layer_gru(256, return_sequences = TRUE) %>%\n\n  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n  layer_simple_rnn(128) %>%\n\n  layer_dense(10)\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_1\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n embedding_1 (Embedding)          (None, None, 64)              64000       \n gru (GRU)                        (None, None, 256)             247296      \n simple_rnn (SimpleRNN)           (None, 128)                   49280       \n dense_1 (Dense)                  (None, 10)                    1290        \n============================================================================\nTotal params: 361,866\nTrainable params: 361,866\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nIn addition, a RNN layer can return its final internal state(s). The returned states can be used to resume the RNN execution later, or [to initialize another RNN](https://arxiv.org/abs/1409.3215). This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.\n\nTo configure a RNN layer to return its internal state, set `return_state = TRUE` when creating the layer. Note that `LSTM` has 2 state tensors, but `GRU` only has one.\n\nTo configure the initial state of the layer, call the layer instance with the additional named argument `initial_state`. Note that the shape of the state needs to match the unit size of the layer, like in the example below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nencoder_vocab <- 1000\ndecoder_vocab <- 2000\n\nencoder_input <- layer_input(shape(NULL))\nencoder_embedded <- encoder_input %>%\n  layer_embedding(input_dim=encoder_vocab, output_dim=64)\n\n\n# Return states in addition to output\nc(output, state_h, state_c) %<-%\n  layer_lstm(encoder_embedded, units = 64, return_state=TRUE, name=\""encoder\"")\n\nencoder_state <- list(state_h, state_c)\n\ndecoder_input <- layer_input(shape(NULL))\ndecoder_embedded <- decoder_input %>%\n  layer_embedding(input_dim = decoder_vocab, output_dim = 64)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_lstm_layer <- layer_lstm(units = 64, name = \""decoder\"")\ndecoder_output <- decoder_lstm_layer(decoder_embedded, initial_state = encoder_state)\n\noutput <- decoder_output %>% layer_dense(10)\n\nmodel <- keras_model(inputs = list(encoder_input, decoder_input),\n                     outputs = output)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""model\""\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n input_1 (InputLayer)    [(None, None)]  0        []                        \n input_2 (InputLayer)    [(None, None)]  0        []                        \n embedding_2 (Embedding)  (None, None, 6  64000   ['input_1[0][0]']         \n                         4)                                                 \n embedding_3 (Embedding)  (None, None, 6  128000  ['input_2[0][0]']         \n                         4)                                                 \n encoder (LSTM)          [(None, 64),    33024    ['embedding_2[0][0]']     \n                          (None, 64),                                       \n                          (None, 64)]                                       \n decoder (LSTM)          (None, 64)      33024    ['embedding_3[0][0]',     \n                                                   'encoder[0][1]',         \n                                                   'encoder[0][2]']         \n dense_2 (Dense)         (None, 10)      650      ['decoder[0][0]']         \n============================================================================\nTotal params: 258,698\nTrainable params: 258,698\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\n## RNN layers and RNN cells\n\nIn addition to the built-in RNN layers, the RNN API also provides cell-level APIs. Unlike RNN layers, which process whole batches of input sequences, the RNN cell only processes a single timestep.\n\nThe cell is the inside of the `for` loop of a RNN layer. Wrapping a cell inside a `layer_rnn()` layer gives you a layer capable of processing a sequence, e.g. `layer_rnn(layer_lstm_cell(10))`.\n\nMathematically, `layer_rnn(layer_lstm_cell(10))` produces the same result as `layer_lstm(10)`. In fact, the implementation of this layer in TF v1.x was just creating the corresponding RNN cell and wrapping it in a RNN layer. However using the built-in `layer_gru()` and `layer_lstm()` layers enable the use of CuDNN and you may see better performance.\n\nThere are three built-in RNN cells, each of them corresponding to the matching RNN layer.\n\n-   `layer_simple_rnn_cell()` corresponds to the `layer_simple_rnn()` layer.\n\n-   `layer_gru_cell` corresponds to the `layer_gru` layer.\n\n-   `layer_lstm_cell` corresponds to the `layer_lstm` layer.\n\nThe cell abstraction, together with the generic `layer_rnn()` class, makes it very easy to implement custom RNN architectures for your research.\n\n## Cross-batch statefulness\n\nWhen processing very long (possibly infinite) sequences, you may want to use the pattern of **cross-batch statefulness**.\n\nNormally, the internal state of a RNN layer is reset every time it sees a new batch (i.e. every sample seen by the layer is assumed to be independent of the past). The layer will only maintain a state while processing a given sample.\n\nIf you have very long sequences though, it is useful to break them into shorter sequences, and to feed these shorter sequences sequentially into a RNN layer without resetting the layer's state. That way, the layer can retain information about the entirety of the sequence, even though it's only seeing one sub-sequence at a time.\n\nYou can do this by setting `stateful = TRUE` in the constructor.\n\nIf you have a sequence `s = c(t0, t1, ... t1546, t1547)`, you would split it into e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns1 = c(t0, t1, ..., t100)\ns2 = c(t101, ..., t201)\n...\ns16 = c(t1501, ..., t1547)\n```\n:::\n\n\nThen you would process it via:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\nfor(s in sub_sequences)\n  output <- lstm_layer(s)\n```\n:::\n\n\nWhen you want to clear the state, you can use `layer$reset_states()`.\n\n> Note: In this setup, sample `i` in a given batch is assumed to be the continuation of sample `i` in the previous batch. This means that all batches should contain the same number of samples (batch size). E.g. if a batch contains `[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]`, the next batch should contain `[sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]`.\n\nHere is a complete example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\noutput <- lstm_layer(paragraph3)\n\n# reset_states() will reset the cached state to the original initial_state.\n# If no initial_state was provided, zero-states will be used by default.\nlstm_layer$reset_states()\n```\n:::\n\n\n### RNN State Reuse\n\nThe recorded states of the RNN layer are not included in the `layer$weights()`. If you would like to reuse the state from a RNN layer, you can retrieve the states value by `layer$states` and use it as the initial state of a new layer instance via the Keras functional API like `new_layer(inputs, initial_state = layer$states)`, or model subclassing.\n\nPlease also note that a sequential model cannot be used in this case since it only supports layers with single input and output. The extra input of initial state makes it impossible to use here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparagraph1 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph2 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\nparagraph3 <- k_random_uniform(c(20, 10, 50), dtype = \""float32\"")\n\nlstm_layer <- layer_lstm(units = 64, stateful = TRUE)\noutput <- lstm_layer(paragraph1)\noutput <- lstm_layer(paragraph2)\n\nexisting_state <- lstm_layer$states\n\nnew_lstm_layer <- layer_lstm(units = 64)\nnew_output <- new_lstm_layer(paragraph3, initial_state = existing_state)\n```\n:::\n\n\n## Bidirectional RNNs\n\nFor sequences other than time series (e.g. text), it is often the case that a RNN model can perform better if it not only processes sequence from start to end, but also backwards. For example, to predict the next word in a sentence, it is often useful to have the context around the word, not only just the words that come before it.\n\nKeras provides an easy API for you to build such bidirectional RNNs: the `bidirectional()` wrapper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = shape(5, 10)) %>%\n  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) %>%\n  bidirectional(layer_lstm(units = 32)) %>%\n  layer_dense(10)\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \""sequential_2\""\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n bidirectional_1 (Bidirectional)  (None, 5, 128)                38400       \n bidirectional (Bidirectional)    (None, 64)                    41216       \n dense_3 (Dense)                  (None, 10)                    650         \n============================================================================\nTotal params: 80,266\nTrainable params: 80,266\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\nUnder the hood, `bidirectional()` will copy the RNN layer passed in, and flip the `go_backwards` field of the newly copied layer, so that it will process the inputs in reverse order.\n\nThe output of the `bidirectional` RNN will be, by default, the concatenation of the forward layer output and the backward layer output. If you need a different merging behavior, e.g. averaging, change the `merge_mode` parameter in the `bidirectional` wrapper constructor. For more details about `bidirectional`, please check [the API docs](https://keras.io/api/layers/recurrent_layers/bidirectional/).\n\n## Performance optimization and CuDNN kernels\n\nIn TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN kernels by default when a GPU is available. With this change, the prior `layer_cudnn_gru/layer_cudnn_lstm` layers have been deprecated, and you can build your model without worrying about the hardware it will run on.\n\nSince the CuDNN kernel is built with certain assumptions, this means the layer **will not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or GRU layers**. E.g.:\n\n-   Changing the `activation` function from `\""tanh\""` to something else.\n-   Changing the `recurrent_activation` function from `\""sigmoid\""` to something else.\n-   Using `recurrent_dropout > 0`.\n-   Setting `unroll` to `TRUE`, which forces LSTM/GRU to decompose the inner `tf$while_loop` into an unrolled `for` loop.\n-   Setting `use_bias` to `FALSE`.\n-   Using masking when the input data is not strictly right padded (if the mask corresponds to strictly right padded data, CuDNN can still be used. This is the most common case).\n\nFor the detailed list of constraints, please see the documentation for the [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) and [GRU](https://keras.io/api/layers/recurrent_layers/gru/) layers.\n\n### Using CuDNN kernels when available\n\nLet's build a simple LSTM model to demonstrate the performance difference.\n\nWe'll use as input sequences the sequence of rows of MNIST digits (treating each row of pixels as a timestep), and we'll predict the digit's label.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch_size <- 64\n# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n# Each input sequence will be of size (28, 28) (height is treated like time).\ninput_dim <- 28\n\nunits <- 64\noutput_size <- 10  # labels are from 0 to 9\n\n# Build the RNN model\nbuild_model <- function(allow_cudnn_kernel = TRUE) {\n  # CuDNN is only available at the layer level, and not at the cell level.\n  # This means `layer_lstm(units = units)` will use the CuDNN kernel,\n  # while layer_rnn(cell = layer_lstm_cell(units)) will run on non-CuDNN kernel.\n  if (allow_cudnn_kernel)\n    # The LSTM layer with default options uses CuDNN.\n    lstm_layer <- layer_lstm(units = units)\n  else\n    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n    lstm_layer <- layer_rnn(cell = layer_lstm_cell(units = units))\n\n  model <-\n    keras_model_sequential(input_shape = shape(NULL, input_dim)) %>%\n    lstm_layer() %>%\n    layer_batch_normalization() %>%\n    layer_dense(output_size)\n\n  model\n}\n```\n:::\n\n\nLet's load the MNIST dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x / 255\nmnist$test$x <- mnist$test$x / 255\nc(sample, sample_label) %<-% with(mnist$train, list(x[1,,], y[1]))\n```\n:::\n\n\nLet's create a model instance and train it.\n\nWe choose `sparse_categorical_crossentropy()` as the loss function for the model. The output of the model has shape of `(batch_size, 10)`. The target for the model is an integer vector, each of the integer is in the range of 0 to 9.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- build_model(allow_cudnn_kernel = TRUE) %>%\n  compile(\n    loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),\n    optimizer = \""sgd\"",\n    metrics = \""accuracy\""\n  )\n\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n```\n:::\n\n\nNow, let's compare to a model that does not use the CuDNN kernel:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnoncudnn_model <- build_model(allow_cudnn_kernel=FALSE)\nnoncudnn_model$set_weights(model$get_weights())\nnoncudnn_model %>% compile(\n    loss=loss_sparse_categorical_crossentropy(from_logits=TRUE),\n    optimizer=\""sgd\"",\n    metrics=\""accuracy\"",\n)\n\nnoncudnn_model %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  validation_data = with(mnist$test, list(x, y)),\n  batch_size = batch_size,\n  epochs = 1\n)\n```\n:::\n\n\nWhen running on a machine with a NVIDIA GPU and CuDNN installed, the model built with CuDNN is much faster to train compared to the model that uses the regular TensorFlow kernel.\n\nThe same CuDNN-enabled model can also be used to run inference in a CPU-only environment. The `tf$device()` annotation below is just forcing the device placement. The model will run on CPU by default if no GPU is available.\n\nYou simply don't have to worry about the hardware you're running on anymore. Isn't that pretty cool?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$device(\""CPU:0\""), {\n    cpu_model <- build_model(allow_cudnn_kernel=TRUE)\n    cpu_model$set_weights(model$get_weights())\n\n    result <- cpu_model %>%\n      predict_on_batch(k_expand_dims(sample, 1)) %>%\n      k_argmax(axis = 2)\n\n    cat(sprintf(\n        \""Predicted result is: %s, target result is: %s\\n\"", as.numeric(result), sample_label))\n\n    # show mnist image\n    sample %>%\n      apply(2, rev) %>% # flip\n      t() %>%           # rotate\n      image(axes = FALSE, asp = 1, col = grey(seq(0, 1, length.out = 256)))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted result is: 3, target result is: 5\n```\n:::\n\n::: {.cell-output-display}\n![](working_with_rnns_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## RNNs with list/dict inputs, or nested inputs\n\nNested structures allow implementers to include more information within a single timestep. For example, a video frame could have audio and video input at the same time. The data shape in this case could be:\n\n`[batch, timestep, {\""video\"": [height, width, channel], \""audio\"": [frequency]}]`\n\nIn another example, handwriting data could have both coordinates x and y for the current position of the pen, as well as pressure information. So the data representation could be:\n\n`[batch, timestep, {\""location\"": [x, y], \""pressure\"": [force]}]`\n\nThe following code provides an example of how to build a custom RNN cell that accepts such structured inputs.\n\n### Define a custom cell that supports nested input/output\n\nSee [Making new Layers & Models via subclassing](/guides/making_new_layers_and_models_via_subclassing/) for details on writing your own layers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nNestedCell(keras$layers$Layer) %py_class% {\n\n  initialize <- function(unit_1, unit_2, unit_3, ...) {\n    self$unit_1 <- unit_1\n    self$unit_2 <- unit_2\n    self$unit_3 <- unit_3\n    self$state_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    self$output_size <- list(shape(unit_1), shape(unit_2, unit_3))\n    super$initialize(...)\n  }\n\n  build <- function(self, input_shapes) {\n    # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n    # dput(input_shapes) gives: list(list(NULL, 32L), list(NULL, 64L, 32L))\n    i1 <- input_shapes[[c(1, 2)]] # 32\n    i2 <- input_shapes[[c(2, 2)]] # 64\n    i3 <- input_shapes[[c(2, 3)]] # 32\n\n    self$kernel_1 = self$add_weight(\n      shape = shape(i1, self$unit_1),\n      initializer = \""uniform\"",\n      name = \""kernel_1\""\n    )\n    self$kernel_2_3 = self$add_weight(\n      shape = shape(i2, i3, self$unit_2, self$unit_3),\n      initializer = \""uniform\"",\n      name = \""kernel_2_3\""\n    )\n  }\n\n  call <- function(inputs, states) {\n    # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n    # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n    # Don't forget you can call `browser()` here while the layer is being traced!\n    c(input_1, input_2) %<-% tf$nest$flatten(inputs)\n    c(s1, s2) %<-% states\n\n    output_1 <- tf$matmul(input_1, self$kernel_1)\n    output_2_3 <- tf$einsum(\""bij,ijkl->bkl\"", input_2, self$kernel_2_3)\n    state_1 <- s1 + output_1\n    state_2_3 <- s2 + output_2_3\n\n    output <- tuple(output_1, output_2_3)\n    new_states <- tuple(state_1, state_2_3)\n\n    tuple(output, new_states)\n  }\n\n  get_config <- function() {\n    list(\""unit_1\"" = self$unit_1,\n         \""unit_2\"" = self$unit_2,\n         \""unit_3\"" = self$unit_3)\n  }\n}\n```\n:::\n\n\n### Build a RNN model with nested input/output\n\nLet's build a Keras model that uses a `layer_rnn` layer and the custom cell we just defined.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunit_1 <- 10\nunit_2 <- 20\nunit_3 <- 30\n\ni1 <- 32\ni2 <- 64\ni3 <- 32\nbatch_size <- 64\nnum_batches <- 10\ntimestep <- 50\n\ncell <- NestedCell(unit_1, unit_2, unit_3)\nrnn <- layer_rnn(cell = cell)\n\ninput_1 = layer_input(shape(NULL, i1))\ninput_2 = layer_input(shape(NULL, i2, i3))\n\noutputs = rnn(tuple(input_1, input_2))\n\nmodel = keras_model(list(input_1, input_2), outputs)\n\nmodel %>% compile(optimizer=\""adam\"", loss=\""mse\"", metrics=\""accuracy\"")\n```\n:::\n\n\n### Train the model with randomly generated data\n\nSince there isn't a good candidate dataset for this model, we use random data for demonstration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput_1_data <- k_random_uniform(c(batch_size * num_batches, timestep, i1))\ninput_2_data <- k_random_uniform(c(batch_size * num_batches, timestep, i2, i3))\ntarget_1_data <- k_random_uniform(c(batch_size * num_batches, unit_1))\ntarget_2_data <- k_random_uniform(c(batch_size * num_batches, unit_2, unit_3))\ninput_data <- list(input_1_data, input_2_data)\ntarget_data <- list(target_1_data, target_2_data)\n\nmodel %>% fit(input_data, target_data, batch_size=batch_size)\n```\n:::\n\n\nWith `keras::layer_rnn()`, you are only expected to define the math logic for an individual step within the sequence, and the `layer_rnn()` will handle the sequence iteration for you. It's an incredibly powerful way to quickly prototype new kinds of RNNs (e.g. a LSTM variant).\n\nFor more details, please visit the [API docs](https://keras.io/api/layers/recurrent_layers/rnn/).\n"",
     ""supporting"": [
       ""working_with_rnns_files""
     ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],
-    ""includes"": [],
+    ""includes"": {},
     ""engineDependencies"": {},
     ""preserve"": {},
-    ""postProcess"": null
+    ""postProcess"": true
   }
 }
\ No newline at end of file

---FILE: _freeze/keras/guides/writing_your_own_callbacks/execute-results/html.json---
@@ -1,14 +1,14 @@
 {
   ""hash"": ""27983fb11834f67012d0696b21084a4c"",
   ""result"": {
-    ""markdown"": ""---\ntitle: Writing your own callbacks\nauthor: Rick Chao, Francois Chollet, Tomasz Kalinowski\n---\n\n## Introduction\n\nA callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include `callback_tensorboard()` to visualize training progress and results with TensorBoard, or `callback_model_checkpoint()` to periodically save your model during training.\n\nIn this guide, you will learn what a Keras callback is, what it can do, and how you can build your own. We provide a few demos of simple callback applications to get you started.\n\n\n\n## Setup\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(magrittr, `%<>%`)\nenvir::import_from(dplyr, last)\n\ntf_version()\n```\n:::\n\n## Keras callbacks overview\n\nAll callbacks subclass the `keras$callbacks$Callback` class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\n\nYou can pass a list of callbacks (as a named argument `callbacks`) to the following keras model methods:\n\n-   `fit()`\n-   `evaluate()`\n-   `predict()`\n\n<!-- -->\n\n## An overview of callback methods\n\n### Global methods\n\n#### `on_(train|test|predict)_begin(self, logs=None)`\n\nCalled at the beginning of `fit`/`evaluate`/`predict`.\n\n#### `on_(train|test|predict)_end(self, logs=None)`\n\nCalled at the end of `fit`/`evaluate`/`predict`.\n\n### Batch-level methods for training/testing/predicting\n\n#### `on_(train|test|predict)_batch_begin(self, batch, logs=None)`\n\nCalled right before processing a batch during training/testing/predicting.\n\n#### `on_(train|test|predict)_batch_end(self, batch, logs=None)`\n\nCalled at the end of training/testing/predicting a batch. Within this method, `logs` is a dict containing the metrics results.\n\n### Epoch-level methods (training only)\n\n#### `on_epoch_begin(self, epoch, logs=None)`\n\nCalled at the beginning of an epoch during training.\n\n#### `on_epoch_end(self, epoch, logs=None)`\n\nCalled at the end of an epoch during training.\n\n## A basic example\n\nLet's take a look at a concrete example. To get started, let's import tensorflow and define a simple Sequential Keras model:\n\n::: {.cell}\n\n```{.r .cell-code}\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \""mean_squared_error\"",\n      metrics = \""mean_absolute_error\""\n    )\n  model\n}\n```\n:::\n\nThen, load the MNIST data for training and testing from Keras datasets API:\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500,]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500,]\nmnist$test$y  <- mnist$test$y[1:500]\n```\n:::\n\nNow, define a simple custom callback that logs:\n\n-   When `fit`/`evaluate`/`predict` starts & ends\n-   When each epoch starts & ends\n-   When each training batch starts & ends\n-   When each evaluation (test) batch starts & ends\n-   When each inference (prediction) batch starts & ends\n\n::: {.cell}\n\n```{.r .cell-code}\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \""got logs: \"", sep = \""; \"")\n  str(logs); cat(\""\\n\"")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\""Starting training\"", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\""Stop training\"", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\""Start epoch {epoch} of training\"", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\""End epoch {epoch} of training\"", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\""Start testing\"", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\""Stop testing\"", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\""Start predicting\"", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\""Stop predicting\"", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\""...Training: start of batch {batch}\"", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\""...Training: end of batch {batch}\"",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\""...Evaluating: start of batch {batch}\"", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\""...Evaluating: end of batch {batch}\"", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\""...Predicting: start of batch {batch}\"", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\""...Predicting: end of batch {batch}\"", logs)\n}\n```\n:::\n\nLet's try it out:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n```\n:::\n\n### Usage of `logs` dict\n\nThe `logs` dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\n\n::: {.cell}\n\n```{.r .cell-code}\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\""Up to batch %i, the average loss is %7.2f.\\n\"",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\""Up to batch %i, the average loss is %7.2f.\\n\"",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \""The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\"",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n```\n:::\n\n## Usage of `self$model` attribute\n\nIn addition to receiving log information when one of their methods is called, callbacks have access to the model associated with the current round of training/evaluation/inference: `self$model`.\n\nHere are of few of the things you can do with `self$model` in a callback:\n\n-   Set `self$model$stop_training <- TRUE` to immediately interrupt training.\n-   Mutate hyperparameters of the optimizer (available as `self$model$optimizer`), such as `self$model$optimizer$learning_rate`.\n-   Save the model at period intervals.\n-   Record the output of `predict(model)` on a few test samples at the end of each epoch, to use as a sanity check during training.\n-   Extract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\n-   etc.\n\nLet's see this in action in a couple of examples.\n\n## Examples of Keras callback applications\n\n### Early stopping at minimum loss\n\nThis first example shows the creation of a `Callback` that stops training when the minimum of loss has been reached, by setting the attribute `self$model$stop_training` (boolean). Optionally, you can provide an argument `patience` to specify how many epochs we should wait before stopping after having reached a local minimum.\n\n`keras$callbacks$EarlyStopping` provides a more complete and general implementation.\n\n::: {.cell}\n\n```{.r .cell-code}\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \""Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \""\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\""Restoring model weights from the end of the best epoch.\\n\"")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\""Epoch %05d: early stopping\\n\"", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n```\n:::\n\n### Learning rate scheduling\n\nIn this example, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training.\n\nSee `keras$callbacks$LearningRateScheduler` for a more general implementations (in RStudio, press F1 while the cursor is over `LearningRateScheduler` and a browser will open to [this page](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/callbacks/LearningRateScheduler)).\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \""Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \""\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\""learning_rate\"" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \""learning_rate\"" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\""\\nEpoch %05d: Learning rate is %6.4f.\\n\"", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                               0, .1,\n                               3, 0.05,\n                               6, 0.01,\n                               9, 0.005,\n                               12, 0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \""Helper function to retrieve the scheduled learning rate based on epoch.\""\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n```\n:::\n\n### Built-in Keras callbacks\n\nBe sure to check out the existing Keras callbacks by reading the [API docs](https://keras.io/api/callbacks/). Applications include logging to CSV, saving the model, visualizing metrics in TensorBoard, and a lot more!"",
+    ""markdown"": ""---\ntitle: Writing your own callbacks\nauthor: Rick Chao, Francois Chollet, Tomasz Kalinowski\n---\n\n\n## Introduction\n\nA callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include `callback_tensorboard()` to visualize training progress and results with TensorBoard, or `callback_model_checkpoint()` to periodically save your model during training.\n\nIn this guide, you will learn what a Keras callback is, what it can do, and how you can build your own. We provide a few demos of simple callback applications to get you started.\n\n\n\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nenvir::import_from(magrittr, `%<>%`)\nenvir::import_from(dplyr, last)\n\ntf_version()\n```\n:::\n\n\n## Keras callbacks overview\n\nAll callbacks subclass the `keras$callbacks$Callback` class, and override a set of methods called at various stages of training, testing, and predicting. Callbacks are useful to get a view on internal states and statistics of the model during training.\n\nYou can pass a list of callbacks (as a named argument `callbacks`) to the following keras model methods:\n\n-   `fit()`\n-   `evaluate()`\n-   `predict()`\n\n<!-- -->\n\n## An overview of callback methods\n\n### Global methods\n\n#### `on_(train|test|predict)_begin(self, logs=None)`\n\nCalled at the beginning of `fit`/`evaluate`/`predict`.\n\n#### `on_(train|test|predict)_end(self, logs=None)`\n\nCalled at the end of `fit`/`evaluate`/`predict`.\n\n### Batch-level methods for training/testing/predicting\n\n#### `on_(train|test|predict)_batch_begin(self, batch, logs=None)`\n\nCalled right before processing a batch during training/testing/predicting.\n\n#### `on_(train|test|predict)_batch_end(self, batch, logs=None)`\n\nCalled at the end of training/testing/predicting a batch. Within this method, `logs` is a dict containing the metrics results.\n\n### Epoch-level methods (training only)\n\n#### `on_epoch_begin(self, epoch, logs=None)`\n\nCalled at the beginning of an epoch during training.\n\n#### `on_epoch_end(self, epoch, logs=None)`\n\nCalled at the end of an epoch during training.\n\n## A basic example\n\nLet's take a look at a concrete example. To get started, let's import tensorflow and define a simple Sequential Keras model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_model <- function() {\n  model <- keras_model_sequential() %>%\n    layer_dense(1, input_shape = 784) %>%\n    compile(\n      optimizer = optimizer_rmsprop(learning_rate=0.1),\n      loss = \""mean_squared_error\"",\n      metrics = \""mean_absolute_error\""\n    )\n  model\n}\n```\n:::\n\n\nThen, load the MNIST data for training and testing from Keras datasets API:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- dataset_mnist()\n\nflatten_and_rescale <- function(x) {\n  x <- array_reshape(x, c(-1, 784))\n  x <- x / 255\n  x\n}\n\nmnist$train$x <- flatten_and_rescale(mnist$train$x)\nmnist$test$x  <- flatten_and_rescale(mnist$test$x)\n\n# limit to 500 samples\nmnist$train$x <- mnist$train$x[1:500,]\nmnist$train$y <- mnist$train$y[1:500]\nmnist$test$x  <- mnist$test$x[1:500,]\nmnist$test$y  <- mnist$test$y[1:500]\n```\n:::\n\n\nNow, define a simple custom callback that logs:\n\n-   When `fit`/`evaluate`/`predict` starts & ends\n-   When each epoch starts & ends\n-   When each training batch starts & ends\n-   When each evaluation (test) batch starts & ends\n-   When each inference (prediction) batch starts & ends\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow <- function(msg, logs) {\n  cat(glue::glue(msg, .envir = parent.frame()),\n      \""got logs: \"", sep = \""; \"")\n  str(logs); cat(\""\\n\"")\n}\n\nCustomCallback(keras$callbacks$Callback) %py_class% {\n  on_train_begin <- function(logs = NULL)\n    show(\""Starting training\"", logs)\n\n  on_train_end <- function(logs = NULL)\n    show(\""Stop training\"", logs)\n\n  on_epoch_begin <- function(epoch, logs = NULL)\n    show(\""Start epoch {epoch} of training\"", logs)\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    show(\""End epoch {epoch} of training\"", logs)\n\n  on_test_begin <- function(logs = NULL)\n    show(\""Start testing\"", logs)\n\n  on_test_end <- function(logs = NULL)\n    show(\""Stop testing\"", logs)\n\n  on_predict_begin <- function(logs = NULL)\n    show(\""Start predicting\"", logs)\n\n  on_predict_end <- function(logs = NULL)\n    show(\""Stop predicting\"", logs)\n\n  on_train_batch_begin <- function(batch, logs = NULL)\n    show(\""...Training: start of batch {batch}\"", logs)\n\n  on_train_batch_end <- function(batch, logs = NULL)\n    show(\""...Training: end of batch {batch}\"",  logs)\n\n  on_test_batch_begin <- function(batch, logs = NULL)\n    show(\""...Evaluating: start of batch {batch}\"", logs)\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    show(\""...Evaluating: end of batch {batch}\"", logs)\n\n  on_predict_batch_begin <- function(batch, logs = NULL)\n    show(\""...Predicting: start of batch {batch}\"", logs)\n\n  on_predict_batch_end <- function(batch, logs = NULL)\n    show(\""...Predicting: end of batch {batch}\"", logs)\n}\n```\n:::\n\n\nLet's try it out:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  validation_split = 0.5,\n  callbacks = list(CustomCallback())\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  evaluate(\n    mnist$test$x,\n    mnist$test$y,\n    batch_size = 128,\n    verbose = 0,\n    callbacks = list(CustomCallback())\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- model %>%\n  predict(mnist$test$x,\n          batch_size = 128,\n          callbacks = list(CustomCallback()))\n```\n:::\n\n\n### Usage of `logs` dict\n\nThe `logs` dict contains the loss value, and all the metrics at the end of a batch or epoch. Example includes the loss and mean absolute error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLossAndErrorPrintingCallback(keras$callbacks$Callback) %py_class% {\n  on_train_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\""Up to batch %i, the average loss is %7.2f.\\n\"",\n                batch,  logs$loss))\n\n  on_test_batch_end <- function(batch, logs = NULL)\n    cat(sprintf(\""Up to batch %i, the average loss is %7.2f.\\n\"",\n                batch, logs$loss))\n\n  on_epoch_end <- function(epoch, logs = NULL)\n    cat(sprintf(\n      \""The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\"",\n      epoch, logs$loss, logs$mean_absolute_error\n    ))\n}\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 128,\n  epochs = 2,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n\nres = model %>% evaluate(\n  mnist$test$x,\n  mnist$test$y,\n  batch_size = 128,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback())\n)\n```\n:::\n\n\n## Usage of `self$model` attribute\n\nIn addition to receiving log information when one of their methods is called, callbacks have access to the model associated with the current round of training/evaluation/inference: `self$model`.\n\nHere are of few of the things you can do with `self$model` in a callback:\n\n-   Set `self$model$stop_training <- TRUE` to immediately interrupt training.\n-   Mutate hyperparameters of the optimizer (available as `self$model$optimizer`), such as `self$model$optimizer$learning_rate`.\n-   Save the model at period intervals.\n-   Record the output of `predict(model)` on a few test samples at the end of each epoch, to use as a sanity check during training.\n-   Extract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\n-   etc.\n\nLet's see this in action in a couple of examples.\n\n## Examples of Keras callback applications\n\n### Early stopping at minimum loss\n\nThis first example shows the creation of a `Callback` that stops training when the minimum of loss has been reached, by setting the attribute `self$model$stop_training` (boolean). Optionally, you can provide an argument `patience` to specify how many epochs we should wait before stopping after having reached a local minimum.\n\n`keras$callbacks$EarlyStopping` provides a more complete and general implementation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEarlyStoppingAtMinLoss(keras$callbacks$Callback) %py_class% {\n  \""Stop training when the loss is at its min, i.e. the loss stops decreasing.\n\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n        number of no improvement, training stops.\n  \""\n\n  initialize <- function(patience = 0) {\n    # call keras$callbacks$Callback$__init__(), so it can setup `self`\n    super$initialize()\n    self$patience <- patience\n    # best_weights to store the weights at which the minimum loss occurs.\n    self$best_weights <- NULL\n  }\n\n  on_train_begin <- function(logs = NULL) {\n    # The number of epoch it has waited when loss is no longer minimum.\n    self$wait <- 0\n    # The epoch the training stops at.\n    self$stopped_epoch <- 0\n    # Initialize the best as infinity.\n    self$best <- Inf\n  }\n\n  on_epoch_end <- function(epoch, logs = NULL) {\n    current <- logs$loss\n    if (current < self$best) {\n      self$best <- current\n      self$wait <- 0\n      # Record the best weights if current results is better (less).\n      self$best_weights <- self$model$get_weights()\n    } else {\n      self$wait %<>% `+`(1)\n      if (self$wait >= self$patience) {\n        self$stopped_epoch <- epoch\n        self$model$stop_training <- TRUE\n        cat(\""Restoring model weights from the end of the best epoch.\\n\"")\n        self$model$set_weights(self$best_weights)\n      }\n    }\n  }\n\n  on_train_end <- function(logs = NULL)\n    if (self$stopped_epoch > 0)\n      cat(sprintf(\""Epoch %05d: early stopping\\n\"", self$stopped_epoch + 1))\n\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 30,\n  verbose = 0,\n  callbacks = list(LossAndErrorPrintingCallback(),\n                   EarlyStoppingAtMinLoss())\n)\n```\n:::\n\n\n### Learning rate scheduling\n\nIn this example, we show how a custom Callback can be used to dynamically change the learning rate of the optimizer during the course of training.\n\nSee `keras$callbacks$LearningRateScheduler` for a more general implementations (in RStudio, press F1 while the cursor is over `LearningRateScheduler` and a browser will open to [this page](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/callbacks/LearningRateScheduler)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCustomLearningRateScheduler(keras$callbacks$Callback) %py_class% {\n  \""Learning rate scheduler which sets the learning rate according to schedule.\n\n  Arguments:\n      schedule: a function that takes an epoch index\n          (integer, indexed from 0) and current learning rate\n          as inputs and returns a new learning rate as output (float).\n  \""\n\n  `__init__` <- function(schedule) {\n    super()$`__init__`()\n    self$schedule <- schedule\n  }\n\n  on_epoch_begin <- function(epoch, logs = NULL) {\n    ## When in doubt about what types of objects are in scope (e.g., self$model)\n    ## use a debugger to interact with the actual objects at the console!\n    # browser()\n\n    if (!\""learning_rate\"" %in% names(self$model$optimizer))\n      stop('Optimizer must have a \""learning_rate\"" attribute.')\n\n    # # Get the current learning rate from model's optimizer.\n    # use as.numeric() to convert the tf.Variable to an R numeric\n    lr <- as.numeric(self$model$optimizer$learning_rate)\n    # # Call schedule function to get the scheduled learning rate.\n    scheduled_lr <- self$schedule(epoch, lr)\n    # # Set the value back to the optimizer before this epoch starts\n    self$model$optimizer$learning_rate <- scheduled_lr\n    cat(sprintf(\""\\nEpoch %05d: Learning rate is %6.4f.\\n\"", epoch, scheduled_lr))\n  }\n}\n\n\nLR_SCHEDULE <- tibble::tribble(~ start_epoch, ~ learning_rate,\n                               0, .1,\n                               3, 0.05,\n                               6, 0.01,\n                               9, 0.005,\n                               12, 0.001)\n\n\nlr_schedule <- function(epoch, learning_rate) {\n  \""Helper function to retrieve the scheduled learning rate based on epoch.\""\n  if (epoch <= last(LR_SCHEDULE$start_epoch))\n    with(LR_SCHEDULE, learning_rate[which.min(epoch > start_epoch)])\n  else\n    learning_rate\n}\n\n\nmodel <- get_model()\nmodel %>% fit(\n  mnist$train$x,\n  mnist$train$y,\n  batch_size = 64,\n  steps_per_epoch = 5,\n  epochs = 15,\n  verbose = 0,\n  callbacks = list(\n    LossAndErrorPrintingCallback(),\n    CustomLearningRateScheduler(lr_schedule)\n  )\n)\n```\n:::\n\n\n### Built-in Keras callbacks\n\nBe sure to check out the existing Keras callbacks by reading the [API docs](https://keras.io/api/callbacks/). Applications include logging to CSV, saving the model, visualizing metrics in TensorBoard, and a lot more!\n"",
     ""supporting"": [],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],
-    ""includes"": [],
+    ""includes"": {},
     ""engineDependencies"": {},
     ""preserve"": {},
-    ""postProcess"": null
+    ""postProcess"": true
   }
 }
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/autodiff/execute-results/html.json---
@@ -0,0 +1,16 @@
+{
+  ""hash"": ""c8101cba0c29bc197ac812b23832c6c1"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: Introduction to gradients and automatic differentiation\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#@title Licensed under the Apache License, Version 2.0 (the \""License\"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \""AS IS\"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\n## Automatic Differentiation and Gradients\n\n[Automatic\ndifferentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\nis useful for implementing machine learning algorithms such as\n[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for\ntraining neural networks.\n\nIn this guide, you will explore ways to compute gradients with\nTensorFlow, especially in eager execution.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Computing gradients\n\nTo differentiate automatically, TensorFlow needs to remember what\noperations happen in what order during the *forward* pass. Then, during\nthe *backward pass*, TensorFlow traverses this list of operations in\nreverse order to compute gradients.\n\n## Gradient tapes\n\nTensorFlow provides the `tf$GradientTape()` API for automatic\ndifferentiation; that is, computing the gradient of a computation with\nrespect to some inputs, usually `tf$Variable`s. TensorFlow \""records\""\nrelevant operations executed inside the context of a `tf$GradientTape()`\nonto a \""tape\"". TensorFlow then uses that tape to compute the gradients\nof a \""recorded\"" computation using [reverse mode\ndifferentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n\nHere is a simple example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% tape, {\n  y <- x ^ 2\n})\n```\n:::\n\n\nOnce you've recorded some operations, use\n`GradientTape$gradient(target, sources)` to calculate the gradient of\nsome target (often a loss) relative to some source (often the model's\nvariables):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dy = 2x * dx\n\ndy_dx <- tape$gradient(y, x)\ndy_dx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThe above example uses scalars, but `tf$GradientTape` works as easily on\nany tensor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- tf$Variable(tf$random$normal(c(3L, 2L)), name = 'w')\nb <- tf$Variable(tf$zeros(2L, dtype = tf$float32), name = 'b')\nx <- as_tensor(1:3, \""float32\"", shape = c(1, 3))\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y <- tf$matmul(x, w) + b\n  loss <- mean(y ^ 2)\n})\n```\n:::\n\n\nTo get the gradient of `loss` with respect to both variables, you can\npass both as sources to the `gradient` method. The tape is flexible\nabout how sources are passed and will accept any nested combination of\nlists or dictionaries and return the gradient structured the same way\n(see `tf$nest`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(dl_dw, dl_db) %<-% tape$gradient(loss, c(w, b))\n```\n:::\n\n\nThe gradient with respect to each source has the shape of the source:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([3, 2])\n```\n:::\n\n```{.r .cell-code}\ndl_dw$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([3, 2])\n```\n:::\n:::\n\n\nHere is the gradient calculation again, this time passing a named list\nof variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_vars <- list(w = w,\n                b = b)\n\ngrad <- tape$gradient(loss, my_vars)\ngrad$b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([ 2.0255458 -0.5183691], shape=(2), dtype=float32)\n```\n:::\n:::\n\n\n## Gradients with respect to a model\n\nIt's common to collect `tf$Variables` into a `tf$Module` or one of its\nsubclasses (`tf$keras$layers$Layer`, `tf$keras$Model`) for\n[checkpointing](checkpoint.qmd) and [exporting](saved_model.qmd).\n\nIn most cases, you will want to calculate gradients with respect to a\nmodel's trainable variables. Since all subclasses of `tf$Module`\naggregate their variables in the `Module$trainable_variables` property,\nyou can calculate these gradients in a few lines of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 2, activation = 'relu')\nx <- as_tensor(1:3, \""float32\"", shape = c(1, -1))\n\nwith(tf$GradientTape() %as% tape, {\n  # Forward pass\n  y <- layer(x)\n  loss <- mean(y ^ 2)\n})\n\n# Calculate gradients with respect to every trainable variable\ngrad <- tape$gradient(loss, layer$trainable_variables)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (pair in zip_lists(layer$trainable_variables, grad)) {\n  c(var, g) %<-% pair\n  print(glue::glue('{var$name}, shape: {format(g$shape)}'))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndense/kernel:0, shape: (3, 2)\ndense/bias:0, shape: (2)\n```\n:::\n:::\n\n\n## Controlling what the tape watches\n\nThe default behavior is to record all operations after accessing a\ntrainable `tf$Variable`. The reasons for this are:\n\n-   The tape needs to know which operations to record in the forward\n    pass to calculate the gradients in the backwards pass.\n-   The tape holds references to intermediate outputs, so you don't want\n    to record unnecessary operations.\n-   The most common use case involves calculating the gradient of a loss\n    with respect to all a model's trainable variables.\n\nFor example, the following fails to calculate a gradient because the\n`tf$Tensor` is not \""watched\"" by default, and the `tf$Variable` is not\ntrainable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A trainable variable\nx0 <- tf$Variable(3.0, name = 'x0')\n\n# Not trainable\nx1 <- tf$Variable(3.0, name = 'x1', trainable = FALSE)\n\n# Not a Variable: A variable + tensor returns a tensor.\nx2 <- tf$Variable(2.0, name = 'x2') + 1.0\n\n# Not a variable\nx3 <- as_tensor(3.0, name = 'x3')\n\nwith(tf$GradientTape() %as% tape, {\n  y <- (x0 ^ 2) + (x1 ^ 2) + (x2 ^ 2)\n})\n\ngrad <- tape$gradient(y, list(x0, x1, x2, x3))\n\nstr(grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n $ : NULL\n $ : NULL\n $ : NULL\n```\n:::\n:::\n\n\nYou can list the variables being watched by the tape using the\n`GradientTape$watched_variables` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntape$watched_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>\n```\n:::\n:::\n\n\n`tf$GradientTape` provides hooks that give the user control over what is\nor is not watched.\n\nTo record gradients with respect to a `tf$Tensor`, you need to call\n`GradientTape$watch(x)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(3.0)\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x ^ 2\n})\n\n# dy = 2x * dx\ndy_dx <- tape$gradient(y, x)\nas.array(dy_dx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\nConversely, to disable the default behavior of watching all\n`tf$Variables`, set `watch_accessed_variables = FALSE` when creating the\ngradient tape. This calculation uses two variables, but only connects\nthe gradient for one of the variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- tf$Variable(0.0)\nx1 <- tf$Variable(10.0)\n\nwith(tf$GradientTape(watch_accessed_variables = FALSE) %as% tape, {\n  tape$watch(x1)\n  y0 <- sin(x0)\n  y1 <- tf$nn$softplus(x1)\n  y <- y0 + y1\n  ys <- sum(y)\n})\n```\n:::\n\n\nSince `GradientTape$watch` was not called on `x0`, no gradient is\ncomputed with respect to it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\ngrad <- tape$gradient(ys, list(x0 = x0, x1 = x1))\n\ncat('dy/dx0: ', grad$x0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndy/dx0: \n```\n:::\n\n```{.r .cell-code}\ncat('dy/dx1: ', as.array(grad$x1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndy/dx1:  0.9999546\n```\n:::\n:::\n\n\n## Intermediate results\n\nYou can also request gradients of the output with respect to\nintermediate values computed inside the `tf$GradientTape` context.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(3.0)\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\n# Use the tape to compute the gradient of z with respect to the\n# intermediate value y.\n# dz_dy = 2 * y and y = x ^ 2 = 9\ntape$gradient(z, y) |> as.array()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18\n```\n:::\n:::\n\n\nBy default, the resources held by a `GradientTape` are released as soon\nas the `GradientTape$gradient` method is called. To compute multiple\ngradients over the same computation, create a gradient tape with\n`persistent = TRUE`. This allows multiple calls to the `gradient` method\nas resources are released when the tape object is garbage collected. For\nexample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(c(1, 3.0))\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\nas.array(tape$gradient(z, x))  # c(4.0, 108.0); (4 * x^3 at x = c(1.0, 3.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   4 108\n```\n:::\n\n```{.r .cell-code}\nas.array(tape$gradient(y, x))  # c(2.0, 6.0);   (2 * x at x = c(1.0, 3.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 6\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(tape)   # Drop the reference to the tape\n```\n:::\n\n\n## Notes on performance\n\n-   There is a tiny overhead associated with doing operations inside a\n    gradient tape context. For most eager execution this will not be a\n    noticeable cost, but you should still use tape context around the\n    areas only where it is required.\n\n-   Gradient tapes use memory to store intermediate results, including\n    inputs and outputs, for use during the backwards pass.\n\n    For efficiency, some ops (like `ReLU`) don't need to keep their\n    intermediate results and they are pruned during the forward pass.\n    However, if you use `persistent = TRUE` on your tape, *nothing is\n    discarded* and your peak memory usage will be higher.\n\n## Gradients of non-scalar targets\n\nA gradient is fundamentally an operation on a scalar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2.0)\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y0 <- x ^ 2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(y0, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n\n```{.r .cell-code}\nas.array(tape$gradient(y1, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.25\n```\n:::\n:::\n\n\nThus, if you ask for the gradient of multiple targets, the result for\neach source is:\n\n-   The gradient of the sum of the targets, or equivalently\n-   The sum of the gradients of each target.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2.0)\nwith(tf$GradientTape() %as% tape, {\n  y0 <- x^2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(list(y0 = y0, y1 = y1), x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.75\n```\n:::\n:::\n\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is\ncalculated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x * c(3, 4)\n})\n\nas.array(tape$gradient(y, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7\n```\n:::\n:::\n\n\nThis makes it simple to take the gradient of the sum of a collection of\nlosses, or the gradient of the sum of an element-wise loss calculation.\n\nIf you need a separate gradient for each item, refer to\n[Jacobians](advanced_autodiff$ipynb#jacobians).\n\nIn some cases you can skip the Jacobian. For an element-wise\ncalculation, the gradient of the sum gives the derivative of each\nelement with respect to its input-element, since each element is\nindependent:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$linspace(-10.0, 10.0, as.integer(200+1))\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- tf$nn$sigmoid(x)\n})\n\ndy_dx <- tape$gradient(y, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(var in alist(x, y, dy_dx))\n  eval(bquote(.(var) <- as.array(.(var))))\nplot(NULL, xlim = range(x), ylim = range(y), ann=F, frame.plot = F)\nlines(x, y, col = \""royalblue\"", lwd = 2)\nlines(x, dy_dx, col = \""coral\"", lwd=2)\nlegend(\""topleft\"", inset = .05,\n       expression(y, dy/dx),\n       col = c(\""royalblue\"", \""coral\""), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](autodiff_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n## Control flow\n\nBecause a gradient tape records operations as they are executed, Python\ncontrol flow is naturally handled (for example, `if` and `while`\nstatements).\n\nHere a different variable is used on each branch of an `if`. The\ngradient only connects to the variable that was used:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(1.0)\n\nv0 <- tf$Variable(2.0)\nv1 <- tf$Variable(2.0)\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  tape$watch(x)\n  if (as.logical(x > 0.0))\n    result <- v0\n  else\n    result <- v1 ^ 2\n})\n\nc(dv0, dv1) %<-% tape$gradient(result, list(v0, v1))\n\ndv0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(1.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\ndv1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nJust remember that the control statements themselves are not\ndifferentiable, so they are invisible to gradient-based optimizers.\n\nDepending on the value of `x` in the above example, the tape either\nrecords `result = v0` or `result = v1 ^ 2`. The gradient with respect to\n`x` is always `NULL`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(dx <- tape$gradient(result, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\n## Getting a gradient of `NULL`\n\nWhen a target is not connected to a source you will get a gradient of\n`NULL`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2)\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y * y\n})\ntape$gradient(z, x)\n```\n:::\n\n\nHere `z` is obviously not connected to `x`, but there are several\nless-obvious ways that a gradient can be disconnected.\n\n### 1. Replaced a variable with a tensor\n\nIn the section on [\""controlling what the tape watches\""](#watches) you\nsaw that the tape will automatically watch a `tf$Variable` but not a\n`tf$Tensor`.\n\nOne common error is to inadvertently replace a `tf$Variable` with a\n`tf$Tensor`, instead of using `Variable$assign` to update the\n`tf$Variable`. Here is an example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2.0)\n\nfor (epoch in seq(2)) {\n\n  with(tf$GradientTape() %as% tape,\n       {  y <- x+1 })\n\n  cat(x$`__class__`$`__name__`, \"": \"")\n  print(tape$gradient(y, x))\n  x <- x + 1   # This should be `x$assign_add(1)`\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\nEagerTensor : NULL\n```\n:::\n:::\n\n\n### 2. Did calculations outside of TensorFlow\n\nThe tape can't record the gradient path if the calculation exits\nTensorFlow. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnp <- reticulate::import(\""numpy\"", convert = FALSE)\nx <- tf$Variable(as_tensor(1:4, dtype=tf$float32, shape = c(2, 2)))\n\nwith(tf$GradientTape() %as% tape, {\n  x2 <- x ^ 2\n\n  # This step is calculated with NumPy\n  y <- np$mean(x2, axis = 0L)\n\n  # Like most tf ops, reduce_mean will cast the NumPy array to a constant tensor\n  # using `tf$convert_to_tensor`.\n  y <- tf$reduce_mean(y, axis = 0L)\n})\n\nprint(tape$gradient(y, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\n### 3. Took gradients through an integer or string\n\nIntegers and strings are not differentiable. If a calculation path uses\nthese data types there will be no gradient.\n\nNobody expects strings to be differentiable, but it's easy to\naccidentally create an `int` constant or variable if you don't specify\nthe `dtype`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(10L)\n\nwith(tf$GradientTape() %as% g, {\n  g$watch(x)\n  y <- x * x\n})\n\ng$gradient(y, x)\n```\n:::\n\n\n````default\nWARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nWARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n````\n\n\nTensorFlow doesn't automatically cast between types, so, in practice,\nyou'll often get a type error instead of a missing gradient.\n\n### 4. Took gradients through a stateful object\n\nState stops gradients. When you read from a stateful object, the tape\ncan only observe the current state, not the history that lead to it.\n\nA `tf$Tensor` is immutable. You can't change a tensor once it's created.\nIt has a *value*, but no *state*. All the operations discussed so far\nare also stateless: the output of a `tf$matmul` only depends on its\ninputs.\n\nA `tf$Variable` has internal state---its value. When you use the\nvariable, the state is read. It's normal to calculate a gradient with\nrespect to a variable, but the variable's state blocks gradient\ncalculations from going farther back. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- tf$Variable(3.0)\nx1 <- tf$Variable(0.0)\n\nwith(tf$GradientTape() %as% tape, {\n  # Update x1 <- x1 + x0.\n  x1$assign_add(x0)\n  # The tape starts recording from x1.\n  y <- x1^2   # y = (x1 + x0)^2\n})\n\n# This doesn't work.\nprint(tape$gradient(y, x0))  #dy/dx0 = 2*(x1 + x0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nSimilarly, `tf$data$Dataset` iterators and `tf$queue`s are stateful, and\nwill stop all gradients on tensors that pass through them.\n\n## No gradient registered\n\nSome `tf$Operation`s are **registered as being non-differentiable\\* and\nwill return `NULL`. Others have** no gradient registered\\*\\*.\n\nThe\n[`tf$raw_ops`](https://www.tensorflow.org/api_docs/python/tf/raw_ops)\npage shows which low-level ops have gradients registered.\n\nIf you attempt to take a gradient through a float op that has no\ngradient registered the tape will throw an error instead of silently\nreturning `NULL`. This way you know something has gone wrong.\n\nFor example, the `tf$image$adjust_contrast` function wraps\n`raw_ops$AdjustContrastv2`, which could have a gradient but the gradient\nis not implemented:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimage <- tf$Variable(array(c(0.5, 0, 0), c(1,1,1)))\ndelta <- tf$Variable(0.1)\n\nwith(tf$GradientTape() %as% tape, {\n  new_image <- tf$image$adjust_contrast(image, delta)\n})\n\ntry(print(tape$gradient(new_image, list(image, delta))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  LookupError: gradient registry has no entry for: AdjustContrastv2\n```\n:::\n:::\n\n\nIf you need to differentiate through this op, you'll either need to\nimplement the gradient and register it (using `tf$RegisterGradient`) or\nre-implement the function using other ops.\n\n## Zeros instead of NULL\n\nIn some cases it would be convenient to get 0 instead of `NULL` for\nunconnected gradients. You can decide what to return when you have\nunconnected gradients using the `unconnected_gradients` argument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(c(2, 2))\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y^2\n})\ntape$gradient(z, x, unconnected_gradients = tf$UnconnectedGradients$ZERO)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([0. 0.], shape=(2), dtype=float32)\n```\n:::\n:::\n"",
+    ""supporting"": [
+      ""autodiff_files""
+    ],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/basics/execute-results/html.json---
@@ -0,0 +1,14 @@
+{
+  ""hash"": ""ef591513643f6d1d55de6da1da8e90d0"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: Tensorflow Basics\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\nThis guide provides a quick overview of *TensorFlow basics*. Each\nsection of this doc is an overview of a larger topic---you can find\nlinks to full guides at the end of each section.\n\nTensorFlow is an end-to-end platform for machine learning. It supports\nthe following:\n\n-   Multidimensional-array based numeric computation (similar to\n    [Numpy](https://numpy.org)\n\n-   GPU and distributed processing\n\n-   Automatic differentiation\n\n-   Model construction, training, and export\n\n-   And more\n\n## Tensors\n\nTensorFlow operates on multidimensional arrays or *tensors* represented\nas `tensorflow.tensor` objects. Here is a two-dimensional tensor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n\nx <- as_tensor(1:6, dtype = \""float32\"", shape = c(2, 3))\n\nx\nx$shape\nx$dtype\n```\n:::\n\n\nThe most important attributes of a tensor are its `shape` and `dtype`:\n\n-   `tensor$shape`: tells you the size of the tensor along each of its\n    axes.\n-   `tensor$dtype`: tells you the type of all the elements in the\n    tensor.\n\nTensorFlow implements standard mathematical operations on tensors, as\nwell as many operations specialized for machine learning.\n\nFor example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx + x\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n5 * x\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$matmul(x, t(x)) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$concat(list(x, x, x), axis = 0L)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$nn$softmax(x, axis = -1L)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(x) # same as tf$reduce_sum(x)\n```\n:::\n\n\nRunning large calculations on CPU can be slow. When properly configured,\nTensorFlow can use accelerator hardware like GPUs to execute operations\nvery quickly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (length(tf$config$list_physical_devices('GPU')))\n  message(\""TensorFlow **IS** using the GPU\"") else\n  message(\""TensorFlow **IS NOT** using the GPU\"")\n```\n:::\n\n\nRefer to the [Tensor guide](tensorflow/guide/tensor.qmd) for details.\n\n## Variables\n\nNormal tensor objects are immutable. To store model weights (or other\nmutable state) in TensorFlow use a `tf$Variable`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar <- tf$Variable(c(0, 0, 0))\nvar\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar$assign(c(1, 2, 3))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar$assign_add(c(1, 1, 1))\n```\n:::\n\n\nRefer to the [Variables guide](variable$ipynb) for details.\n\n## Automatic differentiation\n\n[*Gradient descent*](https://en.wikipedia.org/wiki/Gradient_descent) and\nrelated algorithms are a cornerstone of modern machine learning.\n\nTo enable this, TensorFlow implements automatic differentiation\n(autodiff), which uses calculus to compute gradients. Typically you'll\nuse this to calculate the gradient of a model's *error* or *loss* with\nrespect to its weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(1.0)\n\nf <- function(x)\n  x^2 + 2*x - 5\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf(x)\n```\n:::\n\n\nAt `x = 1.0`, `y = f(x) = (1^2 + 2*1 - 5) = -2`.\n\nThe derivative of `y` is `y' = f'(x) = (2*x + 2) = 4`. TensorFlow can\ncalculate this automatically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% tape, {\n  y <- f(x)\n})\n\ng_x <- tape$gradient(y, x)  # g(x) = dy/dx\n\ng_x\n```\n:::\n\n\nThis simplified example only takes the derivative with respect to a\nsingle scalar (`x`), but TensorFlow can compute the gradient with\nrespect to any number of non-scalar tensors simultaneously.\n\nRefer to the [Autodiff guide](tensorflow/guide/autodiff.qmd) for\ndetails.\n\n## Graphs and `tf_function`\n\nWhile you can use TensorFlow interactively like any R library,\nTensorFlow also provides tools for:\n\n-   **Performance optimization**: to speed up training and inference.\n-   **Export**: so you can save your model when it's done training.\n\nThese require that you use `tf_function()` to separate your\npure-TensorFlow code from R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_func <- tf_function(function(x) {\n  message('Tracing.')\n  tf$reduce_sum(x)\n})\n```\n:::\n\n\nThe first time you run the `tf_function`, although it executes in R, it\ncaptures a complete, optimized graph representing the TensorFlow\ncomputations done within the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(1:3)\nmy_func(x)\n```\n:::\n\n\nOn subsequent calls TensorFlow only executes the optimized graph,\nskipping any non-TensorFlow steps. Below, note that `my_func` doesn't\nprint `\""Tracing.\""` since `message` is an R function, not a TensorFlow\nfunction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(10:8)\nmy_func(x)\n```\n:::\n\n\nA graph may not be reusable for inputs with a different *signature*\n(`shape` and `dtype`), so a new graph is generated instead:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(c(10.0, 9.1, 8.2), dtype=tf$dtypes$float32)\nmy_func(x)\n```\n:::\n\n\nThese captured graphs provide two benefits:\n\n-   In many cases they provide a significant speedup in execution\n    (though not this trivial example).\n-   You can export these graphs, using `tf$saved_model`, to run on other\n    systems like a\n    [server](https://www.tensorflow.org/tfx/serving/docker) or a [mobile\n    device](https://www.tensorflow.org/lite/guide), no Python\n    installation required.\n\nRefer to [Intro to graphs](tensorflow/guide/intro_to_graphs.qmd) for\nmore details.\n\n## Modules, layers, and models\n\n`tf$Module` is a class for managing your `tf$Variable` objects, and the\n`tf_function` objects that operate on them. The `tf$Module` class is\nnecessary to support two significant features:\n\n1.  You can save and restore the values of your variables using\n    `tf$train$Checkpoint`. This is useful during training as it is quick\n    to save and restore a model's state.\n2.  You can import and export the `tf$Variable` values *and* the\n    `tf$function` graphs using `tf$saved_model`. This allows you to run\n    your model independently of the Python program that created it.\n\nHere is a complete example exporting a simple `tf$Module` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) # %py_class% is exported by the keras package at this time\nMyModule(tf$Module) %py_class% {\n  initialize <- function(self, value) {\n    self$weight <- tf$Variable(value)\n  }\n  \n  multiply <- tf_function(function(self, x) {\n    x * self$weight\n  })\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- MyModule(3)\nmod$multiply(as_tensor(c(1, 2, 3)))\n```\n:::\n\n\nSave the `Module`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_path <- tempfile()\ntf$saved_model$save(mod, save_path)\n```\n:::\n\n\nThe resulting SavedModel is independent of the code that created it. You\ncan load a SavedModel from R, Python, other language bindings, or\n[TensorFlow Serving](https://www.tensorflow.org/tfx/serving/docker). You\ncan also convert it to run with [TensorFlow\nLite](https://www.tensorflow.org/lite/guide) or [TensorFlow\nJS](https://www.tensorflow.org/js/guide).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- tf$saved_model$load(save_path)\nreloaded$multiply(as_tensor(c(1, 2, 3)))\n```\n:::\n\n\nThe `tf$keras$layers$Layer` and `tf$keras$Model` classes build on\n`tf$Module` providing additional functionality and convenience methods\nfor building, training, and saving models. Some of these are\ndemonstrated in the next section.\n\nRefer to [Intro to modules](tensorflow/guide/intro_to_modules.qmd) for\ndetails.\n\n## Training loops\n\nNow put this all together to build a basic model and train it from\nscratch.\n\nFirst, create some example data. This generates a cloud of points that\nloosely follows a quadratic curve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(seq(-2, 2, length.out = 201))\n\nf <- function(x)\n  x^2 + 2*x - 5\n\nground_truth <- f(x) \ny <- ground_truth + tf$random$normal(shape(201))\n\nx %<>% as.array()\ny %<>% as.array()\nground_truth %<>% as.array()\n\nplot(x, y, type = 'p', col = \""deepskyblue2\"", pch = 19)\nlines(x, ground_truth, col = \""tomato2\"", lwd = 3)\nlegend(\""topleft\"", \n       col = c(\""deepskyblue2\"", \""tomato2\""),\n       lty = c(NA, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\""Data\"", \""Ground Truth\""))\n```\n:::\n\n\nCreate a model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel(tf$keras$Model) %py_class% {\n  initialize <- function(units) {\n    super$initialize()\n    self$dense1 <- layer_dense(\n      units = units,\n      activation = tf$nn$relu,\n      kernel_initializer = tf$random$normal,\n      bias_initializer = tf$random$normal\n    )\n    self$dense2 <- layer_dense(units = 1)\n  }\n  \n  call <- function(x, training = TRUE) {\n    x %>% \n      .[, tf$newaxis] %>% \n      self$dense1() %>% \n      self$dense2() %>% \n      .[, 1] \n  }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- Model(64)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuntrained_predictions <- model(as_tensor(x))\n\nplot(x, y, type = 'p', col = \""deepskyblue2\"", pch = 19)\nlines(x, ground_truth, col = \""tomato2\"", lwd = 3)\nlines(x, untrained_predictions, col = \""forestgreen\"", lwd = 3)\nlegend(\""topleft\"", \n       col = c(\""deepskyblue2\"", \""tomato2\"", \""forestgreen\""),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\""Data\"", \""Ground Truth\"", \""Untrained predictions\""))\ntitle(\""Before training\"")\n```\n:::\n\n\nWrite a basic training loop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariables <- model$variables\n\noptimizer <- tf$optimizers$SGD(learning_rate=0.01)\n\nfor (step in seq(1000)) {\n  \n  with(tf$GradientTape() %as% tape, {\n    prediction <- model(x)\n    error <- (y - prediction) ^ 2\n    mean_error <- mean(error)\n  })\n  gradient <- tape$gradient(mean_error, variables)\n  optimizer$apply_gradients(zip_lists(gradient, variables))\n\n  if (step %% 100 == 0)\n    message(sprintf('Mean squared error: %.3f', as.array(mean_error)))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrained_predictions <- model(x)\nplot(x, y, type = 'p', col = \""deepskyblue2\"", pch = 19)\nlines(x, ground_truth, col = \""tomato2\"", lwd = 3)\nlines(x, trained_predictions, col = \""forestgreen\"", lwd = 3)\nlegend(\""topleft\"", \n       col = c(\""deepskyblue2\"", \""tomato2\"", \""forestgreen\""),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\""Data\"", \""Ground Truth\"", \""Trained predictions\""))\ntitle(\""After training\"")\n```\n:::\n\n\nThat's working, but remember that implementations of common training\nutilities are available in the `tf$keras` module. So consider using\nthose before writing your own. To start with, the `compile` and `fit`\nmethods for Keras `Model`s implement a training loop for you:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_model <- Model(64)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_model %>% compile(\n  loss = tf$keras$losses$MSE,\n  optimizer = tf$optimizers$SGD(learning_rate = 0.01)\n)\n\nhistory <- new_model %>% \n  fit(x, y,\n      epochs = 100,\n      batch_size = 32,\n      verbose = 0)\n\nmodel$save('./my_model')\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history, metrics = 'loss', method = \""base\"") \n# see ?plot.keras_training_history for more options.\n```\n:::\n\n\nRefer to [Basic training loops](basic_training_loops.qmd) and the [Keras\nguide](https://www.tensorflow.org/guide/keras) for more details.\n"",
+    ""supporting"": [],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/basics/libs/bootstrap/bootstrap-icons.css---
@@ -1,1704 +0,0 @@
-@font-face {
-  font-family: ""bootstrap-icons"";
-  src: 
-url(""./bootstrap-icons.woff?524846017b983fc8ded9325d94ed40f3"") format(""woff"");
-}
-
-.bi::before,
-[class^=""bi-""]::before,
-[class*="" bi-""]::before {
-  display: inline-block;
-  font-family: bootstrap-icons !important;
-  font-style: normal;
-  font-weight: normal !important;
-  font-variant: normal;
-  text-transform: none;
-  line-height: 1;
-  vertical-align: -.125em;
-  -webkit-font-smoothing: antialiased;
-  -moz-osx-font-smoothing: grayscale;
-}
-
-.bi-123::before { content: ""\f67f""; }
-.bi-alarm-fill::before { content: ""\f101""; }
-.bi-alarm::before { content: ""\f102""; }
-.bi-align-bottom::before { content: ""\f103""; }
-.bi-align-center::before { content: ""\f104""; }
-.bi-align-end::before { content: ""\f105""; }
-.bi-align-middle::before { content: ""\f106""; }
-.bi-align-start::before { content: ""\f107""; }
-.bi-align-top::before { content: ""\f108""; }
-.bi-alt::before { content: ""\f109""; }
-.bi-app-indicator::before { content: ""\f10a""; }
-.bi-app::before { content: ""\f10b""; }
-.bi-archive-fill::before { content: ""\f10c""; }
-.bi-archive::before { content: ""\f10d""; }
-.bi-arrow-90deg-down::before { content: ""\f10e""; }
-.bi-arrow-90deg-left::before { content: ""\f10f""; }
-.bi-arrow-90deg-right::before { content: ""\f110""; }
-.bi-arrow-90deg-up::before { content: ""\f111""; }
-.bi-arrow-bar-down::before { content: ""\f112""; }
-.bi-arrow-bar-left::before { content: ""\f113""; }
-.bi-arrow-bar-right::before { content: ""\f114""; }
-.bi-arrow-bar-up::before { content: ""\f115""; }
-.bi-arrow-clockwise::before { content: ""\f116""; }
-.bi-arrow-counterclockwise::before { content: ""\f117""; }
-.bi-arrow-down-circle-fill::before { content: ""\f118""; }
-.bi-arrow-down-circle::before { content: ""\f119""; }
-.bi-arrow-down-left-circle-fill::before { content: ""\f11a""; }
-.bi-arrow-down-left-circle::before { content: ""\f11b""; }
-.bi-arrow-down-left-square-fill::before { content: ""\f11c""; }
-.bi-arrow-down-left-square::before { content: ""\f11d""; }
-.bi-arrow-down-left::before { content: ""\f11e""; }
-.bi-arrow-down-right-circle-fill::before { content: ""\f11f""; }
-.bi-arrow-down-right-circle::before { content: ""\f120""; }
-.bi-arrow-down-right-square-fill::before { content: ""\f121""; }
-.bi-arrow-down-right-square::before { content: ""\f122""; }
-.bi-arrow-down-right::before { content: ""\f123""; }
-.bi-arrow-down-short::before { content: ""\f124""; }
-.bi-arrow-down-square-fill::before { content: ""\f125""; }
-.bi-arrow-down-square::before { content: ""\f126""; }
-.bi-arrow-down-up::before { content: ""\f127""; }
-.bi-arrow-down::before { content: ""\f128""; }
-.bi-arrow-left-circle-fill::before { content: ""\f129""; }
-.bi-arrow-left-circle::before { content: ""\f12a""; }
-.bi-arrow-left-right::before { content: ""\f12b""; }
-.bi-arrow-left-short::before { content: ""\f12c""; }
-.bi-arrow-left-square-fill::before { content: ""\f12d""; }
-.bi-arrow-left-square::before { content: ""\f12e""; }
-.bi-arrow-left::before { content: ""\f12f""; }
-.bi-arrow-repeat::before { content: ""\f130""; }
-.bi-arrow-return-left::before { content: ""\f131""; }
-.bi-arrow-return-right::before { content: ""\f132""; }
-.bi-arrow-right-circle-fill::before { content: ""\f133""; }
-.bi-arrow-right-circle::before { content: ""\f134""; }
-.bi-arrow-right-short::before { content: ""\f135""; }
-.bi-arrow-right-square-fill::before { content: ""\f136""; }
-.bi-arrow-right-square::before { content: ""\f137""; }
-.bi-arrow-right::before { content: ""\f138""; }
-.bi-arrow-up-circle-fill::before { content: ""\f139""; }
-.bi-arrow-up-circle::before { content: ""\f13a""; }
-.bi-arrow-up-left-circle-fill::before { content: ""\f13b""; }
-.bi-arrow-up-left-circle::before { content: ""\f13c""; }
-.bi-arrow-up-left-square-fill::before { content: ""\f13d""; }
-.bi-arrow-up-left-square::before { content: ""\f13e""; }
-.bi-arrow-up-left::before { content: ""\f13f""; }
-.bi-arrow-up-right-circle-fill::before { content: ""\f140""; }
-.bi-arrow-up-right-circle::before { content: ""\f141""; }
-.bi-arrow-up-right-square-fill::before { content: ""\f142""; }
-.bi-arrow-up-right-square::before { content: ""\f143""; }
-.bi-arrow-up-right::before { content: ""\f144""; }
-.bi-arrow-up-short::before { content: ""\f145""; }
-.bi-arrow-up-square-fill::before { content: ""\f146""; }
-.bi-arrow-up-square::before { content: ""\f147""; }
-.bi-arrow-up::before { content: ""\f148""; }
-.bi-arrows-angle-contract::before { content: ""\f149""; }
-.bi-arrows-angle-expand::before { content: ""\f14a""; }
-.bi-arrows-collapse::before { content: ""\f14b""; }
-.bi-arrows-expand::before { content: ""\f14c""; }
-.bi-arrows-fullscreen::before { content: ""\f14d""; }
-.bi-arrows-move::before { content: ""\f14e""; }
-.bi-aspect-ratio-fill::before { content: ""\f14f""; }
-.bi-aspect-ratio::before { content: ""\f150""; }
-.bi-asterisk::before { content: ""\f151""; }
-.bi-at::before { content: ""\f152""; }
-.bi-award-fill::before { content: ""\f153""; }
-.bi-award::before { content: ""\f154""; }
-.bi-back::before { content: ""\f155""; }
-.bi-backspace-fill::before { content: ""\f156""; }
-.bi-backspace-reverse-fill::before { content: ""\f157""; }
-.bi-backspace-reverse::before { content: ""\f158""; }
-.bi-backspace::before { content: ""\f159""; }
-.bi-badge-3d-fill::before { content: ""\f15a""; }
-.bi-badge-3d::before { content: ""\f15b""; }
-.bi-badge-4k-fill::before { content: ""\f15c""; }
-.bi-badge-4k::before { content: ""\f15d""; }
-.bi-badge-8k-fill::before { content: ""\f15e""; }
-.bi-badge-8k::before { content: ""\f15f""; }
-.bi-badge-ad-fill::before { content: ""\f160""; }
-.bi-badge-ad::before { content: ""\f161""; }
-.bi-badge-ar-fill::before { content: ""\f162""; }
-.bi-badge-ar::before { content: ""\f163""; }
-.bi-badge-cc-fill::before { content: ""\f164""; }
-.bi-badge-cc::before { content: ""\f165""; }
-.bi-badge-hd-fill::before { content: ""\f166""; }
-.bi-badge-hd::before { content: ""\f167""; }
-.bi-badge-tm-fill::before { content: ""\f168""; }
-.bi-badge-tm::before { content: ""\f169""; }
-.bi-badge-vo-fill::before { content: ""\f16a""; }
-.bi-badge-vo::before { content: ""\f16b""; }
-.bi-badge-vr-fill::before { content: ""\f16c""; }
-.bi-badge-vr::before { content: ""\f16d""; }
-.bi-badge-wc-fill::before { content: ""\f16e""; }
-.bi-badge-wc::before { content: ""\f16f""; }
-.bi-bag-check-fill::before { content: ""\f170""; }
-.bi-bag-check::before { content: ""\f171""; }
-.bi-bag-dash-fill::before { content: ""\f172""; }
-.bi-bag-dash::before { content: ""\f173""; }
-.bi-bag-fill::before { content: ""\f174""; }
-.bi-bag-plus-fill::before { content: ""\f175""; }
-.bi-bag-plus::before { content: ""\f176""; }
-.bi-bag-x-fill::before { content: ""\f177""; }
-.bi-bag-x::before { content: ""\f178""; }
-.bi-bag::before { content: ""\f179""; }
-.bi-bar-chart-fill::before { content: ""\f17a""; }
-.bi-bar-chart-line-fill::before { content: ""\f17b""; }
-.bi-bar-chart-line::before { content: ""\f17c""; }
-.bi-bar-chart-steps::before { content: ""\f17d""; }
-.bi-bar-chart::before { content: ""\f17e""; }
-.bi-basket-fill::before { content: ""\f17f""; }
-.bi-basket::before { content: ""\f180""; }
-.bi-basket2-fill::before { content: ""\f181""; }
-.bi-basket2::before { content: ""\f182""; }
-.bi-basket3-fill::before { content: ""\f183""; }
-.bi-basket3::before { content: ""\f184""; }
-.bi-battery-charging::before { content: ""\f185""; }
-.bi-battery-full::before { content: ""\f186""; }
-.bi-battery-half::before { content: ""\f187""; }
-.bi-battery::before { content: ""\f188""; }
-.bi-bell-fill::before { content: ""\f189""; }
-.bi-bell::before { content: ""\f18a""; }
-.bi-bezier::before { content: ""\f18b""; }
-.bi-bezier2::before { content: ""\f18c""; }
-.bi-bicycle::before { content: ""\f18d""; }
-.bi-binoculars-fill::before { content: ""\f18e""; }
-.bi-binoculars::before { content: ""\f18f""; }
-.bi-blockquote-left::before { content: ""\f190""; }
-.bi-blockquote-right::before { content: ""\f191""; }
-.bi-book-fill::before { content: ""\f192""; }
-.bi-book-half::before { content: ""\f193""; }
-.bi-book::before { content: ""\f194""; }
-.bi-bookmark-check-fill::before { content: ""\f195""; }
-.bi-bookmark-check::before { content: ""\f196""; }
-.bi-bookmark-dash-fill::before { content: ""\f197""; }
-.bi-bookmark-dash::before { content: ""\f198""; }
-.bi-bookmark-fill::before { content: ""\f199""; }
-.bi-bookmark-heart-fill::before { content: ""\f19a""; }
-.bi-bookmark-heart::before { content: ""\f19b""; }
-.bi-bookmark-plus-fill::before { content: ""\f19c""; }
-.bi-bookmark-plus::before { content: ""\f19d""; }
-.bi-bookmark-star-fill::before { content: ""\f19e""; }
-.bi-bookmark-star::before { content: ""\f19f""; }
-.bi-bookmark-x-fill::before { content: ""\f1a0""; }
-.bi-bookmark-x::before { content: ""\f1a1""; }
-.bi-bookmark::before { content: ""\f1a2""; }
-.bi-bookmarks-fill::before { content: ""\f1a3""; }
-.bi-bookmarks::before { content: ""\f1a4""; }
-.bi-bookshelf::before { content: ""\f1a5""; }
-.bi-bootstrap-fill::before { content: ""\f1a6""; }
-.bi-bootstrap-reboot::before { content: ""\f1a7""; }
-.bi-bootstrap::before { content: ""\f1a8""; }
-.bi-border-all::before { content: ""\f1a9""; }
-.bi-border-bottom::before { content: ""\f1aa""; }
-.bi-border-center::before { content: ""\f1ab""; }
-.bi-border-inner::before { content: ""\f1ac""; }
-.bi-border-left::before { content: ""\f1ad""; }
-.bi-border-middle::before { content: ""\f1ae""; }
-.bi-border-outer::before { content: ""\f1af""; }
-.bi-border-right::before { content: ""\f1b0""; }
-.bi-border-style::before { content: ""\f1b1""; }
-.bi-border-top::before { content: ""\f1b2""; }
-.bi-border-width::before { content: ""\f1b3""; }
-.bi-border::before { content: ""\f1b4""; }
-.bi-bounding-box-circles::before { content: ""\f1b5""; }
-.bi-bounding-box::before { content: ""\f1b6""; }
-.bi-box-arrow-down-left::before { content: ""\f1b7""; }
-.bi-box-arrow-down-right::before { content: ""\f1b8""; }
-.bi-box-arrow-down::before { content: ""\f1b9""; }
-.bi-box-arrow-in-down-left::before { content: ""\f1ba""; }
-.bi-box-arrow-in-down-right::before { content: ""\f1bb""; }
-.bi-box-arrow-in-down::before { content: ""\f1bc""; }
-.bi-box-arrow-in-left::before { content: ""\f1bd""; }
-.bi-box-arrow-in-right::before { content: ""\f1be""; }
-.bi-box-arrow-in-up-left::before { content: ""\f1bf""; }
-.bi-box-arrow-in-up-right::before { content: ""\f1c0""; }
-.bi-box-arrow-in-up::before { content: ""\f1c1""; }
-.bi-box-arrow-left::before { content: ""\f1c2""; }
-.bi-box-arrow-right::before { content: ""\f1c3""; }
-.bi-box-arrow-up-left::before { content: ""\f1c4""; }
-.bi-box-arrow-up-right::before { content: ""\f1c5""; }
-.bi-box-arrow-up::before { content: ""\f1c6""; }
-.bi-box-seam::before { content: ""\f1c7""; }
-.bi-box::before { content: ""\f1c8""; }
-.bi-braces::before { content: ""\f1c9""; }
-.bi-bricks::before { content: ""\f1ca""; }
-.bi-briefcase-fill::before { content: ""\f1cb""; }
-.bi-briefcase::before { content: ""\f1cc""; }
-.bi-brightness-alt-high-fill::before { content: ""\f1cd""; }
-.bi-brightness-alt-high::before { content: ""\f1ce""; }
-.bi-brightness-alt-low-fill::before { content: ""\f1cf""; }
-.bi-brightness-alt-low::before { content: ""\f1d0""; }
-.bi-brightness-high-fill::before { content: ""\f1d1""; }
-.bi-brightness-high::before { content: ""\f1d2""; }
-.bi-brightness-low-fill::before { content: ""\f1d3""; }
-.bi-brightness-low::before { content: ""\f1d4""; }
-.bi-broadcast-pin::before { content: ""\f1d5""; }
-.bi-broadcast::before { content: ""\f1d6""; }
-.bi-brush-fill::before { content: ""\f1d7""; }
-.bi-brush::before { content: ""\f1d8""; }
-.bi-bucket-fill::before { content: ""\f1d9""; }
-.bi-bucket::before { content: ""\f1da""; }
-.bi-bug-fill::before { content: ""\f1db""; }
-.bi-bug::before { content: ""\f1dc""; }
-.bi-building::before { content: ""\f1dd""; }
-.bi-bullseye::before { content: ""\f1de""; }
-.bi-calculator-fill::before { content: ""\f1df""; }
-.bi-calculator::before { content: ""\f1e0""; }
-.bi-calendar-check-fill::before { content: ""\f1e1""; }
-.bi-calendar-check::before { content: ""\f1e2""; }
-.bi-calendar-date-fill::before { content: ""\f1e3""; }
-.bi-calendar-date::before { content: ""\f1e4""; }
-.bi-calendar-day-fill::before { content: ""\f1e5""; }
-.bi-calendar-day::before { content: ""\f1e6""; }
-.bi-calendar-event-fill::before { content: ""\f1e7""; }
-.bi-calendar-event::before { content: ""\f1e8""; }
-.bi-calendar-fill::before { content: ""\f1e9""; }
-.bi-calendar-minus-fill::before { content: ""\f1ea""; }
-.bi-calendar-minus::before { content: ""\f1eb""; }
-.bi-calendar-month-fill::before { content: ""\f1ec""; }
-.bi-calendar-month::before { content: ""\f1ed""; }
-.bi-calendar-plus-fill::before { content: ""\f1ee""; }
-.bi-calendar-plus::before { content: ""\f1ef""; }
-.bi-calendar-range-fill::before { content: ""\f1f0""; }
-.bi-calendar-range::before { content: ""\f1f1""; }
-.bi-calendar-week-fill::before { content: ""\f1f2""; }
-.bi-calendar-week::before { content: ""\f1f3""; }
-.bi-calendar-x-fill::before { content: ""\f1f4""; }
-.bi-calendar-x::before { content: ""\f1f5""; }
-.bi-calendar::before { content: ""\f1f6""; }
-.bi-calendar2-check-fill::before { content: ""\f1f7""; }
-.bi-calendar2-check::before { content: ""\f1f8""; }
-.bi-calendar2-date-fill::before { content: ""\f1f9""; }
-.bi-calendar2-date::before { content: ""\f1fa""; }
-.bi-calendar2-day-fill::before { content: ""\f1fb""; }
-.bi-calendar2-day::before { content: ""\f1fc""; }
-.bi-calendar2-event-fill::before { content: ""\f1fd""; }
-.bi-calendar2-event::before { content: ""\f1fe""; }
-.bi-calendar2-fill::before { content: ""\f1ff""; }
-.bi-calendar2-minus-fill::before { content: ""\f200""; }
-.bi-calendar2-minus::before { content: ""\f201""; }
-.bi-calendar2-month-fill::before { content: ""\f202""; }
-.bi-calendar2-month::before { content: ""\f203""; }
-.bi-calendar2-plus-fill::before { content: ""\f204""; }
-.bi-calendar2-plus::before { content: ""\f205""; }
-.bi-calendar2-range-fill::before { content: ""\f206""; }
-.bi-calendar2-range::before { content: ""\f207""; }
-.bi-calendar2-week-fill::before { content: ""\f208""; }
-.bi-calendar2-week::before { content: ""\f209""; }
-.bi-calendar2-x-fill::before { content: ""\f20a""; }
-.bi-calendar2-x::before { content: ""\f20b""; }
-.bi-calendar2::before { content: ""\f20c""; }
-.bi-calendar3-event-fill::before { content: ""\f20d""; }
-.bi-calendar3-event::before { content: ""\f20e""; }
-.bi-calendar3-fill::before { content: ""\f20f""; }
-.bi-calendar3-range-fill::before { content: ""\f210""; }
-.bi-calendar3-range::before { content: ""\f211""; }
-.bi-calendar3-week-fill::before { content: ""\f212""; }
-.bi-calendar3-week::before { content: ""\f213""; }
-.bi-calendar3::before { content: ""\f214""; }
-.bi-calendar4-event::before { content: ""\f215""; }
-.bi-calendar4-range::before { content: ""\f216""; }
-.bi-calendar4-week::before { content: ""\f217""; }
-.bi-calendar4::before { content: ""\f218""; }
-.bi-camera-fill::before { content: ""\f219""; }
-.bi-camera-reels-fill::before { content: ""\f21a""; }
-.bi-camera-reels::before { content: ""\f21b""; }
-.bi-camera-video-fill::before { content: ""\f21c""; }
-.bi-camera-video-off-fill::before { content: ""\f21d""; }
-.bi-camera-video-off::before { content: ""\f21e""; }
-.bi-camera-video::before { content: ""\f21f""; }
-.bi-camera::before { content: ""\f220""; }
-.bi-camera2::before { content: ""\f221""; }
-.bi-capslock-fill::before { content: ""\f222""; }
-.bi-capslock::before { content: ""\f223""; }
-.bi-card-checklist::before { content: ""\f224""; }
-.bi-card-heading::before { content: ""\f225""; }
-.bi-card-image::before { content: ""\f226""; }
-.bi-card-list::before { content: ""\f227""; }
-.bi-card-text::before { content: ""\f228""; }
-.bi-caret-down-fill::before { content: ""\f229""; }
-.bi-caret-down-square-fill::before { content: ""\f22a""; }
-.bi-caret-down-square::before { content: ""\f22b""; }
-.bi-caret-down::before { content: ""\f22c""; }
-.bi-caret-left-fill::before { content: ""\f22d""; }
-.bi-caret-left-square-fill::before { content: ""\f22e""; }
-.bi-caret-left-square::before { content: ""\f22f""; }
-.bi-caret-left::before { content: ""\f230""; }
-.bi-caret-right-fill::before { content: ""\f231""; }
-.bi-caret-right-square-fill::before { content: ""\f232""; }
-.bi-caret-right-square::before { content: ""\f233""; }
-.bi-caret-right::before { content: ""\f234""; }
-.bi-caret-up-fill::before { content: ""\f235""; }
-.bi-caret-up-square-fill::before { content: ""\f236""; }
-.bi-caret-up-square::before { content: ""\f237""; }
-.bi-caret-up::before { content: ""\f238""; }
-.bi-cart-check-fill::before { content: ""\f239""; }
-.bi-cart-check::before { content: ""\f23a""; }
-.bi-cart-dash-fill::before { content: ""\f23b""; }
-.bi-cart-dash::before { content: ""\f23c""; }
-.bi-cart-fill::before { content: ""\f23d""; }
-.bi-cart-plus-fill::before { content: ""\f23e""; }
-.bi-cart-plus::before { content: ""\f23f""; }
-.bi-cart-x-fill::before { content: ""\f240""; }
-.bi-cart-x::before { content: ""\f241""; }
-.bi-cart::before { content: ""\f242""; }
-.bi-cart2::before { content: ""\f243""; }
-.bi-cart3::before { content: ""\f244""; }
-.bi-cart4::before { content: ""\f245""; }
-.bi-cash-stack::before { content: ""\f246""; }
-.bi-cash::before { content: ""\f247""; }
-.bi-cast::before { content: ""\f248""; }
-.bi-chat-dots-fill::before { content: ""\f249""; }
-.bi-chat-dots::before { content: ""\f24a""; }
-.bi-chat-fill::before { content: ""\f24b""; }
-.bi-chat-left-dots-fill::before { content: ""\f24c""; }
-.bi-chat-left-dots::before { content: ""\f24d""; }
-.bi-chat-left-fill::before { content: ""\f24e""; }
-.bi-chat-left-quote-fill::before { content: ""\f24f""; }
-.bi-chat-left-quote::before { content: ""\f250""; }
-.bi-chat-left-text-fill::before { content: ""\f251""; }
-.bi-chat-left-text::before { content: ""\f252""; }
-.bi-chat-left::before { content: ""\f253""; }
-.bi-chat-quote-fill::before { content: ""\f254""; }
-.bi-chat-quote::before { content: ""\f255""; }
-.bi-chat-right-dots-fill::before { content: ""\f256""; }
-.bi-chat-right-dots::before { content: ""\f257""; }
-.bi-chat-right-fill::before { content: ""\f258""; }
-.bi-chat-right-quote-fill::before { content: ""\f259""; }
-.bi-chat-right-quote::before { content: ""\f25a""; }
-.bi-chat-right-text-fill::before { content: ""\f25b""; }
-.bi-chat-right-text::before { content: ""\f25c""; }
-.bi-chat-right::before { content: ""\f25d""; }
-.bi-chat-square-dots-fill::before { content: ""\f25e""; }
-.bi-chat-square-dots::before { content: ""\f25f""; }
-.bi-chat-square-fill::before { content: ""\f260""; }
-.bi-chat-square-quote-fill::before { content: ""\f261""; }
-.bi-chat-square-quote::before { content: ""\f262""; }
-.bi-chat-square-text-fill::before { content: ""\f263""; }
-.bi-chat-square-text::before { content: ""\f264""; }
-.bi-chat-square::before { content: ""\f265""; }
-.bi-chat-text-fill::before { content: ""\f266""; }
-.bi-chat-text::before { content: ""\f267""; }
-.bi-chat::before { content: ""\f268""; }
-.bi-check-all::before { content: ""\f269""; }
-.bi-check-circle-fill::before { content: ""\f26a""; }
-.bi-check-circle::before { content: ""\f26b""; }
-.bi-check-square-fill::before { content: ""\f26c""; }
-.bi-check-square::before { content: ""\f26d""; }
-.bi-check::before { content: ""\f26e""; }
-.bi-check2-all::before { content: ""\f26f""; }
-.bi-check2-circle::before { content: ""\f270""; }
-.bi-check2-square::before { content: ""\f271""; }
-.bi-check2::before { content: ""\f272""; }
-.bi-chevron-bar-contract::before { content: ""\f273""; }
-.bi-chevron-bar-down::before { content: ""\f274""; }
-.bi-chevron-bar-expand::before { content: ""\f275""; }
-.bi-chevron-bar-left::before { content: ""\f276""; }
-.bi-chevron-bar-right::before { content: ""\f277""; }
-.bi-chevron-bar-up::before { content: ""\f278""; }
-.bi-chevron-compact-down::before { content: ""\f279""; }
-.bi-chevron-compact-left::before { content: ""\f27a""; }
-.bi-chevron-compact-right::before { content: ""\f27b""; }
-.bi-chevron-compact-up::before { content: ""\f27c""; }
-.bi-chevron-contract::before { content: ""\f27d""; }
-.bi-chevron-double-down::before { content: ""\f27e""; }
-.bi-chevron-double-left::before { content: ""\f27f""; }
-.bi-chevron-double-right::before { content: ""\f280""; }
-.bi-chevron-double-up::before { content: ""\f281""; }
-.bi-chevron-down::before { content: ""\f282""; }
-.bi-chevron-expand::before { content: ""\f283""; }
-.bi-chevron-left::before { content: ""\f284""; }
-.bi-chevron-right::before { content: ""\f285""; }
-.bi-chevron-up::before { content: ""\f286""; }
-.bi-circle-fill::before { content: ""\f287""; }
-.bi-circle-half::before { content: ""\f288""; }
-.bi-circle-square::before { content: ""\f289""; }
-.bi-circle::before { content: ""\f28a""; }
-.bi-clipboard-check::before { content: ""\f28b""; }
-.bi-clipboard-data::before { content: ""\f28c""; }
-.bi-clipboard-minus::before { content: ""\f28d""; }
-.bi-clipboard-plus::before { content: ""\f28e""; }
-.bi-clipboard-x::before { content: ""\f28f""; }
-.bi-clipboard::before { content: ""\f290""; }
-.bi-clock-fill::before { content: ""\f291""; }
-.bi-clock-history::before { content: ""\f292""; }
-.bi-clock::before { content: ""\f293""; }
-.bi-cloud-arrow-down-fill::before { content: ""\f294""; }
-.bi-cloud-arrow-down::before { content: ""\f295""; }
-.bi-cloud-arrow-up-fill::before { content: ""\f296""; }
-.bi-cloud-arrow-up::before { content: ""\f297""; }
-.bi-cloud-check-fill::before { content: ""\f298""; }
-.bi-cloud-check::before { content: ""\f299""; }
-.bi-cloud-download-fill::before { content: ""\f29a""; }
-.bi-cloud-download::before { content: ""\f29b""; }
-.bi-cloud-drizzle-fill::before { content: ""\f29c""; }
-.bi-cloud-drizzle::before { content: ""\f29d""; }
-.bi-cloud-fill::before { content: ""\f29e""; }
-.bi-cloud-fog-fill::before { content: ""\f29f""; }
-.bi-cloud-fog::before { content: ""\f2a0""; }
-.bi-cloud-fog2-fill::before { content: ""\f2a1""; }
-.bi-cloud-fog2::before { content: ""\f2a2""; }
-.bi-cloud-hail-fill::before { content: ""\f2a3""; }
-.bi-cloud-hail::before { content: ""\f2a4""; }
-.bi-cloud-haze-1::before { content: ""\f2a5""; }
-.bi-cloud-haze-fill::before { content: ""\f2a6""; }
-.bi-cloud-haze::before { content: ""\f2a7""; }
-.bi-cloud-haze2-fill::before { content: ""\f2a8""; }
-.bi-cloud-lightning-fill::before { content: ""\f2a9""; }
-.bi-cloud-lightning-rain-fill::before { content: ""\f2aa""; }
-.bi-cloud-lightning-rain::before { content: ""\f2ab""; }
-.bi-cloud-lightning::before { content: ""\f2ac""; }
-.bi-cloud-minus-fill::before { content: ""\f2ad""; }
-.bi-cloud-minus::before { content: ""\f2ae""; }
-.bi-cloud-moon-fill::before { content: ""\f2af""; }
-.bi-cloud-moon::before { content: ""\f2b0""; }
-.bi-cloud-plus-fill::before { content: ""\f2b1""; }
-.bi-cloud-plus::before { content: ""\f2b2""; }
-.bi-cloud-rain-fill::before { content: ""\f2b3""; }
-.bi-cloud-rain-heavy-fill::before { content: ""\f2b4""; }
-.bi-cloud-rain-heavy::before { content: ""\f2b5""; }
-.bi-cloud-rain::before { content: ""\f2b6""; }
-.bi-cloud-slash-fill::before { content: ""\f2b7""; }
-.bi-cloud-slash::before { content: ""\f2b8""; }
-.bi-cloud-sleet-fill::before { content: ""\f2b9""; }
-.bi-cloud-sleet::before { content: ""\f2ba""; }
-.bi-cloud-snow-fill::before { content: ""\f2bb""; }
-.bi-cloud-snow::before { content: ""\f2bc""; }
-.bi-cloud-sun-fill::before { content: ""\f2bd""; }
-.bi-cloud-sun::before { content: ""\f2be""; }
-.bi-cloud-upload-fill::before { content: ""\f2bf""; }
-.bi-cloud-upload::before { content: ""\f2c0""; }
-.bi-cloud::before { content: ""\f2c1""; }
-.bi-clouds-fill::before { content: ""\f2c2""; }
-.bi-clouds::before { content: ""\f2c3""; }
-.bi-cloudy-fill::before { content: ""\f2c4""; }
-.bi-cloudy::before { content: ""\f2c5""; }
-.bi-code-slash::before { content: ""\f2c6""; }
-.bi-code-square::before { content: ""\f2c7""; }
-.bi-code::before { content: ""\f2c8""; }
-.bi-collection-fill::before { content: ""\f2c9""; }
-.bi-collection-play-fill::before { content: ""\f2ca""; }
-.bi-collection-play::before { content: ""\f2cb""; }
-.bi-collection::before { content: ""\f2cc""; }
-.bi-columns-gap::before { content: ""\f2cd""; }
-.bi-columns::before { content: ""\f2ce""; }
-.bi-command::before { content: ""\f2cf""; }
-.bi-compass-fill::before { content: ""\f2d0""; }
-.bi-compass::before { content: ""\f2d1""; }
-.bi-cone-striped::before { content: ""\f2d2""; }
-.bi-cone::before { content: ""\f2d3""; }
-.bi-controller::before { content: ""\f2d4""; }
-.bi-cpu-fill::before { content: ""\f2d5""; }
-.bi-cpu::before { content: ""\f2d6""; }
-.bi-credit-card-2-back-fill::before { content: ""\f2d7""; }
-.bi-credit-card-2-back::before { content: ""\f2d8""; }
-.bi-credit-card-2-front-fill::before { content: ""\f2d9""; }
-.bi-credit-card-2-front::before { content: ""\f2da""; }
-.bi-credit-card-fill::before { content: ""\f2db""; }
-.bi-credit-card::before { content: ""\f2dc""; }
-.bi-crop::before { content: ""\f2dd""; }
-.bi-cup-fill::before { content: ""\f2de""; }
-.bi-cup-straw::before { content: ""\f2df""; }
-.bi-cup::before { content: ""\f2e0""; }
-.bi-cursor-fill::before { content: ""\f2e1""; }
-.bi-cursor-text::before { content: ""\f2e2""; }
-.bi-cursor::before { content: ""\f2e3""; }
-.bi-dash-circle-dotted::before { content: ""\f2e4""; }
-.bi-dash-circle-fill::before { content: ""\f2e5""; }
-.bi-dash-circle::before { content: ""\f2e6""; }
-.bi-dash-square-dotted::before { content: ""\f2e7""; }
-.bi-dash-square-fill::before { content: ""\f2e8""; }
-.bi-dash-square::before { content: ""\f2e9""; }
-.bi-dash::before { content: ""\f2ea""; }
-.bi-diagram-2-fill::before { content: ""\f2eb""; }
-.bi-diagram-2::before { content: ""\f2ec""; }
-.bi-diagram-3-fill::before { content: ""\f2ed""; }
-.bi-diagram-3::before { content: ""\f2ee""; }
-.bi-diamond-fill::before { content: ""\f2ef""; }
-.bi-diamond-half::before { content: ""\f2f0""; }
-.bi-diamond::before { content: ""\f2f1""; }
-.bi-dice-1-fill::before { content: ""\f2f2""; }
-.bi-dice-1::before { content: ""\f2f3""; }
-.bi-dice-2-fill::before { content: ""\f2f4""; }
-.bi-dice-2::before { content: ""\f2f5""; }
-.bi-dice-3-fill::before { content: ""\f2f6""; }
-.bi-dice-3::before { content: ""\f2f7""; }
-.bi-dice-4-fill::before { content: ""\f2f8""; }
-.bi-dice-4::before { content: ""\f2f9""; }
-.bi-dice-5-fill::before { content: ""\f2fa""; }
-.bi-dice-5::before { content: ""\f2fb""; }
-.bi-dice-6-fill::before { content: ""\f2fc""; }
-.bi-dice-6::before { content: ""\f2fd""; }
-.bi-disc-fill::before { content: ""\f2fe""; }
-.bi-disc::before { content: ""\f2ff""; }
-.bi-discord::before { content: ""\f300""; }
-.bi-display-fill::before { content: ""\f301""; }
-.bi-display::before { content: ""\f302""; }
-.bi-distribute-horizontal::before { content: ""\f303""; }
-.bi-distribute-vertical::before { content: ""\f304""; }
-.bi-door-closed-fill::before { content: ""\f305""; }
-.bi-door-closed::before { content: ""\f306""; }
-.bi-door-open-fill::before { content: ""\f307""; }
-.bi-door-open::before { content: ""\f308""; }
-.bi-dot::before { content: ""\f309""; }
-.bi-download::before { content: ""\f30a""; }
-.bi-droplet-fill::before { content: ""\f30b""; }
-.bi-droplet-half::before { content: ""\f30c""; }
-.bi-droplet::before { content: ""\f30d""; }
-.bi-earbuds::before { content: ""\f30e""; }
-.bi-easel-fill::before { content: ""\f30f""; }
-.bi-easel::before { content: ""\f310""; }
-.bi-egg-fill::before { content: ""\f311""; }
-.bi-egg-fried::before { content: ""\f312""; }
-.bi-egg::before { content: ""\f313""; }
-.bi-eject-fill::before { content: ""\f314""; }
-.bi-eject::before { content: ""\f315""; }
-.bi-emoji-angry-fill::before { content: ""\f316""; }
-.bi-emoji-angry::before { content: ""\f317""; }
-.bi-emoji-dizzy-fill::before { content: ""\f318""; }
-.bi-emoji-dizzy::before { content: ""\f319""; }
-.bi-emoji-expressionless-fill::before { content: ""\f31a""; }
-.bi-emoji-expressionless::before { content: ""\f31b""; }
-.bi-emoji-frown-fill::before { content: ""\f31c""; }
-.bi-emoji-frown::before { content: ""\f31d""; }
-.bi-emoji-heart-eyes-fill::before { content: ""\f31e""; }
-.bi-emoji-heart-eyes::before { content: ""\f31f""; }
-.bi-emoji-laughing-fill::before { content: ""\f320""; }
-.bi-emoji-laughing::before { content: ""\f321""; }
-.bi-emoji-neutral-fill::before { content: ""\f322""; }
-.bi-emoji-neutral::before { content: ""\f323""; }
-.bi-emoji-smile-fill::before { content: ""\f324""; }
-.bi-emoji-smile-upside-down-fill::before { content: ""\f325""; }
-.bi-emoji-smile-upside-down::before { content: ""\f326""; }
-.bi-emoji-smile::before { content: ""\f327""; }
-.bi-emoji-sunglasses-fill::before { content: ""\f328""; }
-.bi-emoji-sunglasses::before { content: ""\f329""; }
-.bi-emoji-wink-fill::before { content: ""\f32a""; }
-.bi-emoji-wink::before { content: ""\f32b""; }
-.bi-envelope-fill::before { content: ""\f32c""; }
-.bi-envelope-open-fill::before { content: ""\f32d""; }
-.bi-envelope-open::before { content: ""\f32e""; }
-.bi-envelope::before { content: ""\f32f""; }
-.bi-eraser-fill::before { content: ""\f330""; }
-.bi-eraser::before { content: ""\f331""; }
-.bi-exclamation-circle-fill::before { content: ""\f332""; }
-.bi-exclamation-circle::before { content: ""\f333""; }
-.bi-exclamation-diamond-fill::before { content: ""\f334""; }
-.bi-exclamation-diamond::before { content: ""\f335""; }
-.bi-exclamation-octagon-fill::before { content: ""\f336""; }
-.bi-exclamation-octagon::before { content: ""\f337""; }
-.bi-exclamation-square-fill::before { content: ""\f338""; }
-.bi-exclamation-square::before { content: ""\f339""; }
-.bi-exclamation-triangle-fill::before { content: ""\f33a""; }
-.bi-exclamation-triangle::before { content: ""\f33b""; }
-.bi-exclamation::before { content: ""\f33c""; }
-.bi-exclude::before { content: ""\f33d""; }
-.bi-eye-fill::before { content: ""\f33e""; }
-.bi-eye-slash-fill::before { content: ""\f33f""; }
-.bi-eye-slash::before { content: ""\f340""; }
-.bi-eye::before { content: ""\f341""; }
-.bi-eyedropper::before { content: ""\f342""; }
-.bi-eyeglasses::before { content: ""\f343""; }
-.bi-facebook::before { content: ""\f344""; }
-.bi-file-arrow-down-fill::before { content: ""\f345""; }
-.bi-file-arrow-down::before { content: ""\f346""; }
-.bi-file-arrow-up-fill::before { content: ""\f347""; }
-.bi-file-arrow-up::before { content: ""\f348""; }
-.bi-file-bar-graph-fill::before { content: ""\f349""; }
-.bi-file-bar-graph::before { content: ""\f34a""; }
-.bi-file-binary-fill::before { content: ""\f34b""; }
-.bi-file-binary::before { content: ""\f34c""; }
-.bi-file-break-fill::before { content: ""\f34d""; }
-.bi-file-break::before { content: ""\f34e""; }
-.bi-file-check-fill::before { content: ""\f34f""; }
-.bi-file-check::before { content: ""\f350""; }
-.bi-file-code-fill::before { content: ""\f351""; }
-.bi-file-code::before { content: ""\f352""; }
-.bi-file-diff-fill::before { content: ""\f353""; }
-.bi-file-diff::before { content: ""\f354""; }
-.bi-file-earmark-arrow-down-fill::before { content: ""\f355""; }
-.bi-file-earmark-arrow-down::before { content: ""\f356""; }
-.bi-file-earmark-arrow-up-fill::before { content: ""\f357""; }
-.bi-file-earmark-arrow-up::before { content: ""\f358""; }
-.bi-file-earmark-bar-graph-fill::before { content: ""\f359""; }
-.bi-file-earmark-bar-graph::before { content: ""\f35a""; }
-.bi-file-earmark-binary-fill::before { content: ""\f35b""; }
-.bi-file-earmark-binary::before { content: ""\f35c""; }
-.bi-file-earmark-break-fill::before { content: ""\f35d""; }
-.bi-file-earmark-break::before { content: ""\f35e""; }
-.bi-file-earmark-check-fill::before { content: ""\f35f""; }
-.bi-file-earmark-check::before { content: ""\f360""; }
-.bi-file-earmark-code-fill::before { content: ""\f361""; }
-.bi-file-earmark-code::before { content: ""\f362""; }
-.bi-file-earmark-diff-fill::before { content: ""\f363""; }
-.bi-file-earmark-diff::before { content: ""\f364""; }
-.bi-file-earmark-easel-fill::before { content: ""\f365""; }
-.bi-file-earmark-easel::before { content: ""\f366""; }
-.bi-file-earmark-excel-fill::before { content: ""\f367""; }
-.bi-file-earmark-excel::before { content: ""\f368""; }
-.bi-file-earmark-fill::before { content: ""\f369""; }
-.bi-file-earmark-font-fill::before { content: ""\f36a""; }
-.bi-file-earmark-font::before { content: ""\f36b""; }
-.bi-file-earmark-image-fill::before { content: ""\f36c""; }
-.bi-file-earmark-image::before { content: ""\f36d""; }
-.bi-file-earmark-lock-fill::before { content: ""\f36e""; }
-.bi-file-earmark-lock::before { content: ""\f36f""; }
-.bi-file-earmark-lock2-fill::before { content: ""\f370""; }
-.bi-file-earmark-lock2::before { content: ""\f371""; }
-.bi-file-earmark-medical-fill::before { content: ""\f372""; }
-.bi-file-earmark-medical::before { content: ""\f373""; }
-.bi-file-earmark-minus-fill::before { content: ""\f374""; }
-.bi-file-earmark-minus::before { content: ""\f375""; }
-.bi-file-earmark-music-fill::before { content: ""\f376""; }
-.bi-file-earmark-music::before { content: ""\f377""; }
-.bi-file-earmark-person-fill::before { content: ""\f378""; }
-.bi-file-earmark-person::before { content: ""\f379""; }
-.bi-file-earmark-play-fill::before { content: ""\f37a""; }
-.bi-file-earmark-play::before { content: ""\f37b""; }
-.bi-file-earmark-plus-fill::before { content: ""\f37c""; }
-.bi-file-earmark-plus::before { content: ""\f37d""; }
-.bi-file-earmark-post-fill::before { content: ""\f37e""; }
-.bi-file-earmark-post::before { content: ""\f37f""; }
-.bi-file-earmark-ppt-fill::before { content: ""\f380""; }
-.bi-file-earmark-ppt::before { content: ""\f381""; }
-.bi-file-earmark-richtext-fill::before { content: ""\f382""; }
-.bi-file-earmark-richtext::before { content: ""\f383""; }
-.bi-file-earmark-ruled-fill::before { content: ""\f384""; }
-.bi-file-earmark-ruled::before { content: ""\f385""; }
-.bi-file-earmark-slides-fill::before { content: ""\f386""; }
-.bi-file-earmark-slides::before { content: ""\f387""; }
-.bi-file-earmark-spreadsheet-fill::before { content: ""\f388""; }
-.bi-file-earmark-spreadsheet::before { content: ""\f389""; }
-.bi-file-earmark-text-fill::before { content: ""\f38a""; }
-.bi-file-earmark-text::before { content: ""\f38b""; }
-.bi-file-earmark-word-fill::before { content: ""\f38c""; }
-.bi-file-earmark-word::before { content: ""\f38d""; }
-.bi-file-earmark-x-fill::before { content: ""\f38e""; }
-.bi-file-earmark-x::before { content: ""\f38f""; }
-.bi-file-earmark-zip-fill::before { content: ""\f390""; }
-.bi-file-earmark-zip::before { content: ""\f391""; }
-.bi-file-earmark::before { content: ""\f392""; }
-.bi-file-easel-fill::before { content: ""\f393""; }
-.bi-file-easel::before { content: ""\f394""; }
-.bi-file-excel-fill::before { content: ""\f395""; }
-.bi-file-excel::before { content: ""\f396""; }
-.bi-file-fill::before { content: ""\f397""; }
-.bi-file-font-fill::before { content: ""\f398""; }
-.bi-file-font::before { content: ""\f399""; }
-.bi-file-image-fill::before { content: ""\f39a""; }
-.bi-file-image::before { content: ""\f39b""; }
-.bi-file-lock-fill::before { content: ""\f39c""; }
-.bi-file-lock::before { content: ""\f39d""; }
-.bi-file-lock2-fill::before { content: ""\f39e""; }
-.bi-file-lock2::before { content: ""\f39f""; }
-.bi-file-medical-fill::before { content: ""\f3a0""; }
-.bi-file-medical::before { content: ""\f3a1""; }
-.bi-file-minus-fill::before { content: ""\f3a2""; }
-.bi-file-minus::before { content: ""\f3a3""; }
-.bi-file-music-fill::before { content: ""\f3a4""; }
-.bi-file-music::before { content: ""\f3a5""; }
-.bi-file-person-fill::before { content: ""\f3a6""; }
-.bi-file-person::before { content: ""\f3a7""; }
-.bi-file-play-fill::before { content: ""\f3a8""; }
-.bi-file-play::before { content: ""\f3a9""; }
-.bi-file-plus-fill::before { content: ""\f3aa""; }
-.bi-file-plus::before { content: ""\f3ab""; }
-.bi-file-post-fill::before { content: ""\f3ac""; }
-.bi-file-post::before { content: ""\f3ad""; }
-.bi-file-ppt-fill::before { content: ""\f3ae""; }
-.bi-file-ppt::before { content: ""\f3af""; }
-.bi-file-richtext-fill::before { content: ""\f3b0""; }
-.bi-file-richtext::before { content: ""\f3b1""; }
-.bi-file-ruled-fill::before { content: ""\f3b2""; }
-.bi-file-ruled::before { content: ""\f3b3""; }
-.bi-file-slides-fill::before { content: ""\f3b4""; }
-.bi-file-slides::before { content: ""\f3b5""; }
-.bi-file-spreadsheet-fill::before { content: ""\f3b6""; }
-.bi-file-spreadsheet::before { content: ""\f3b7""; }
-.bi-file-text-fill::before { content: ""\f3b8""; }
-.bi-file-text::before { content: ""\f3b9""; }
-.bi-file-word-fill::before { content: ""\f3ba""; }
-.bi-file-word::before { content: ""\f3bb""; }
-.bi-file-x-fill::before { content: ""\f3bc""; }
-.bi-file-x::before { content: ""\f3bd""; }
-.bi-file-zip-fill::before { content: ""\f3be""; }
-.bi-file-zip::before { content: ""\f3bf""; }
-.bi-file::before { content: ""\f3c0""; }
-.bi-files-alt::before { content: ""\f3c1""; }
-.bi-files::before { content: ""\f3c2""; }
-.bi-film::before { content: ""\f3c3""; }
-.bi-filter-circle-fill::before { content: ""\f3c4""; }
-.bi-filter-circle::before { content: ""\f3c5""; }
-.bi-filter-left::before { content: ""\f3c6""; }
-.bi-filter-right::before { content: ""\f3c7""; }
-.bi-filter-square-fill::before { content: ""\f3c8""; }
-.bi-filter-square::before { content: ""\f3c9""; }
-.bi-filter::before { content: ""\f3ca""; }
-.bi-flag-fill::before { content: ""\f3cb""; }
-.bi-flag::before { content: ""\f3cc""; }
-.bi-flower1::before { content: ""\f3cd""; }
-.bi-flower2::before { content: ""\f3ce""; }
-.bi-flower3::before { content: ""\f3cf""; }
-.bi-folder-check::before { content: ""\f3d0""; }
-.bi-folder-fill::before { content: ""\f3d1""; }
-.bi-folder-minus::before { content: ""\f3d2""; }
-.bi-folder-plus::before { content: ""\f3d3""; }
-.bi-folder-symlink-fill::before { content: ""\f3d4""; }
-.bi-folder-symlink::before { content: ""\f3d5""; }
-.bi-folder-x::before { content: ""\f3d6""; }
-.bi-folder::before { content: ""\f3d7""; }
-.bi-folder2-open::before { content: ""\f3d8""; }
-.bi-folder2::before { content: ""\f3d9""; }
-.bi-fonts::before { content: ""\f3da""; }
-.bi-forward-fill::before { content: ""\f3db""; }
-.bi-forward::before { content: ""\f3dc""; }
-.bi-front::before { content: ""\f3dd""; }
-.bi-fullscreen-exit::before { content: ""\f3de""; }
-.bi-fullscreen::before { content: ""\f3df""; }
-.bi-funnel-fill::before { content: ""\f3e0""; }
-.bi-funnel::before { content: ""\f3e1""; }
-.bi-gear-fill::before { content: ""\f3e2""; }
-.bi-gear-wide-connected::before { content: ""\f3e3""; }
-.bi-gear-wide::before { content: ""\f3e4""; }
-.bi-gear::before { content: ""\f3e5""; }
-.bi-gem::before { content: ""\f3e6""; }
-.bi-geo-alt-fill::before { content: ""\f3e7""; }
-.bi-geo-alt::before { content: ""\f3e8""; }
-.bi-geo-fill::before { content: ""\f3e9""; }
-.bi-geo::before { content: ""\f3ea""; }
-.bi-gift-fill::before { content: ""\f3eb""; }
-.bi-gift::before { content: ""\f3ec""; }
-.bi-github::before { content: ""\f3ed""; }
-.bi-globe::before { content: ""\f3ee""; }
-.bi-globe2::before { content: ""\f3ef""; }
-.bi-google::before { content: ""\f3f0""; }
-.bi-graph-down::before { content: ""\f3f1""; }
-.bi-graph-up::before { content: ""\f3f2""; }
-.bi-grid-1x2-fill::before { content: ""\f3f3""; }
-.bi-grid-1x2::before { content: ""\f3f4""; }
-.bi-grid-3x2-gap-fill::before { content: ""\f3f5""; }
-.bi-grid-3x2-gap::before { content: ""\f3f6""; }
-.bi-grid-3x2::before { content: ""\f3f7""; }
-.bi-grid-3x3-gap-fill::before { content: ""\f3f8""; }
-.bi-grid-3x3-gap::before { content: ""\f3f9""; }
-.bi-grid-3x3::before { content: ""\f3fa""; }
-.bi-grid-fill::before { content: ""\f3fb""; }
-.bi-grid::before { content: ""\f3fc""; }
-.bi-grip-horizontal::before { content: ""\f3fd""; }
-.bi-grip-vertical::before { content: ""\f3fe""; }
-.bi-hammer::before { content: ""\f3ff""; }
-.bi-hand-index-fill::before { content: ""\f400""; }
-.bi-hand-index-thumb-fill::before { content: ""\f401""; }
-.bi-hand-index-thumb::before { content: ""\f402""; }
-.bi-hand-index::before { content: ""\f403""; }
-.bi-hand-thumbs-down-fill::before { content: ""\f404""; }
-.bi-hand-thumbs-down::before { content: ""\f405""; }
-.bi-hand-thumbs-up-fill::before { content: ""\f406""; }
-.bi-hand-thumbs-up::before { content: ""\f407""; }
-.bi-handbag-fill::before { content: ""\f408""; }
-.bi-handbag::before { content: ""\f409""; }
-.bi-hash::before { content: ""\f40a""; }
-.bi-hdd-fill::before { content: ""\f40b""; }
-.bi-hdd-network-fill::before { content: ""\f40c""; }
-.bi-hdd-network::before { content: ""\f40d""; }
-.bi-hdd-rack-fill::before { content: ""\f40e""; }
-.bi-hdd-rack::before { content: ""\f40f""; }
-.bi-hdd-stack-fill::before { content: ""\f410""; }
-.bi-hdd-stack::before { content: ""\f411""; }
-.bi-hdd::before { content: ""\f412""; }
-.bi-headphones::before { content: ""\f413""; }
-.bi-headset::before { content: ""\f414""; }
-.bi-heart-fill::before { content: ""\f415""; }
-.bi-heart-half::before { content: ""\f416""; }
-.bi-heart::before { content: ""\f417""; }
-.bi-heptagon-fill::before { content: ""\f418""; }
-.bi-heptagon-half::before { content: ""\f419""; }
-.bi-heptagon::before { content: ""\f41a""; }
-.bi-hexagon-fill::before { content: ""\f41b""; }
-.bi-hexagon-half::before { content: ""\f41c""; }
-.bi-hexagon::before { content: ""\f41d""; }
-.bi-hourglass-bottom::before { content: ""\f41e""; }
-.bi-hourglass-split::before { content: ""\f41f""; }
-.bi-hourglass-top::before { content: ""\f420""; }
-.bi-hourglass::before { content: ""\f421""; }
-.bi-house-door-fill::before { content: ""\f422""; }
-.bi-house-door::before { content: ""\f423""; }
-.bi-house-fill::before { content: ""\f424""; }
-.bi-house::before { content: ""\f425""; }
-.bi-hr::before { content: ""\f426""; }
-.bi-hurricane::before { content: ""\f427""; }
-.bi-image-alt::before { content: ""\f428""; }
-.bi-image-fill::before { content: ""\f429""; }
-.bi-image::before { content: ""\f42a""; }
-.bi-images::before { content: ""\f42b""; }
-.bi-inbox-fill::before { content: ""\f42c""; }
-.bi-inbox::before { content: ""\f42d""; }
-.bi-inboxes-fill::before { content: ""\f42e""; }
-.bi-inboxes::before { content: ""\f42f""; }
-.bi-info-circle-fill::before { content: ""\f430""; }
-.bi-info-circle::before { content: ""\f431""; }
-.bi-info-square-fill::before { content: ""\f432""; }
-.bi-info-square::before { content: ""\f433""; }
-.bi-info::before { content: ""\f434""; }
-.bi-input-cursor-text::before { content: ""\f435""; }
-.bi-input-cursor::before { content: ""\f436""; }
-.bi-instagram::before { content: ""\f437""; }
-.bi-intersect::before { content: ""\f438""; }
-.bi-journal-album::before { content: ""\f439""; }
-.bi-journal-arrow-down::before { content: ""\f43a""; }
-.bi-journal-arrow-up::before { content: ""\f43b""; }
-.bi-journal-bookmark-fill::before { content: ""\f43c""; }
-.bi-journal-bookmark::before { content: ""\f43d""; }
-.bi-journal-check::before { content: ""\f43e""; }
-.bi-journal-code::before { content: ""\f43f""; }
-.bi-journal-medical::before { content: ""\f440""; }
-.bi-journal-minus::before { content: ""\f441""; }
-.bi-journal-plus::before { content: ""\f442""; }
-.bi-journal-richtext::before { content: ""\f443""; }
-.bi-journal-text::before { content: ""\f444""; }
-.bi-journal-x::before { content: ""\f445""; }
-.bi-journal::before { content: ""\f446""; }
-.bi-journals::before { content: ""\f447""; }
-.bi-joystick::before { content: ""\f448""; }
-.bi-justify-left::before { content: ""\f449""; }
-.bi-justify-right::before { content: ""\f44a""; }
-.bi-justify::before { content: ""\f44b""; }
-.bi-kanban-fill::before { content: ""\f44c""; }
-.bi-kanban::before { content: ""\f44d""; }
-.bi-key-fill::before { content: ""\f44e""; }
-.bi-key::before { content: ""\f44f""; }
-.bi-keyboard-fill::before { content: ""\f450""; }
-.bi-keyboard::before { content: ""\f451""; }
-.bi-ladder::before { content: ""\f452""; }
-.bi-lamp-fill::before { content: ""\f453""; }
-.bi-lamp::before { content: ""\f454""; }
-.bi-laptop-fill::before { content: ""\f455""; }
-.bi-laptop::before { content: ""\f456""; }
-.bi-layer-backward::before { content: ""\f457""; }
-.bi-layer-forward::before { content: ""\f458""; }
-.bi-layers-fill::before { content: ""\f459""; }
-.bi-layers-half::before { content: ""\f45a""; }
-.bi-layers::before { content: ""\f45b""; }
-.bi-layout-sidebar-inset-reverse::before { content: ""\f45c""; }
-.bi-layout-sidebar-inset::before { content: ""\f45d""; }
-.bi-layout-sidebar-reverse::before { content: ""\f45e""; }
-.bi-layout-sidebar::before { content: ""\f45f""; }
-.bi-layout-split::before { content: ""\f460""; }
-.bi-layout-text-sidebar-reverse::before { content: ""\f461""; }
-.bi-layout-text-sidebar::before { content: ""\f462""; }
-.bi-layout-text-window-reverse::before { content: ""\f463""; }
-.bi-layout-text-window::before { content: ""\f464""; }
-.bi-layout-three-columns::before { content: ""\f465""; }
-.bi-layout-wtf::before { content: ""\f466""; }
-.bi-life-preserver::before { content: ""\f467""; }
-.bi-lightbulb-fill::before { content: ""\f468""; }
-.bi-lightbulb-off-fill::before { content: ""\f469""; }
-.bi-lightbulb-off::before { content: ""\f46a""; }
-.bi-lightbulb::before { content: ""\f46b""; }
-.bi-lightning-charge-fill::before { content: ""\f46c""; }
-.bi-lightning-charge::before { content: ""\f46d""; }
-.bi-lightning-fill::before { content: ""\f46e""; }
-.bi-lightning::before { content: ""\f46f""; }
-.bi-link-45deg::before { content: ""\f470""; }
-.bi-link::before { content: ""\f471""; }
-.bi-linkedin::before { content: ""\f472""; }
-.bi-list-check::before { content: ""\f473""; }
-.bi-list-nested::before { content: ""\f474""; }
-.bi-list-ol::before { content: ""\f475""; }
-.bi-list-stars::before { content: ""\f476""; }
-.bi-list-task::before { content: ""\f477""; }
-.bi-list-ul::before { content: ""\f478""; }
-.bi-list::before { content: ""\f479""; }
-.bi-lock-fill::before { content: ""\f47a""; }
-.bi-lock::before { content: ""\f47b""; }
-.bi-mailbox::before { content: ""\f47c""; }
-.bi-mailbox2::before { content: ""\f47d""; }
-.bi-map-fill::before { content: ""\f47e""; }
-.bi-map::before { content: ""\f47f""; }
-.bi-markdown-fill::before { content: ""\f480""; }
-.bi-markdown::before { content: ""\f481""; }
-.bi-mask::before { content: ""\f482""; }
-.bi-megaphone-fill::before { content: ""\f483""; }
-.bi-megaphone::before { content: ""\f484""; }
-.bi-menu-app-fill::before { content: ""\f485""; }
-.bi-menu-app::before { content: ""\f486""; }
-.bi-menu-button-fill::before { content: ""\f487""; }
-.bi-menu-button-wide-fill::before { content: ""\f488""; }
-.bi-menu-button-wide::before { content: ""\f489""; }
-.bi-menu-button::before { content: ""\f48a""; }
-.bi-menu-down::before { content: ""\f48b""; }
-.bi-menu-up::before { content: ""\f48c""; }
-.bi-mic-fill::before { content: ""\f48d""; }
-.bi-mic-mute-fill::before { content: ""\f48e""; }
-.bi-mic-mute::before { content: ""\f48f""; }
-.bi-mic::before { content: ""\f490""; }
-.bi-minecart-loaded::before { content: ""\f491""; }
-.bi-minecart::before { content: ""\f492""; }
-.bi-moisture::before { content: ""\f493""; }
-.bi-moon-fill::before { content: ""\f494""; }
-.bi-moon-stars-fill::before { content: ""\f495""; }
-.bi-moon-stars::before { content: ""\f496""; }
-.bi-moon::before { content: ""\f497""; }
-.bi-mouse-fill::before { content: ""\f498""; }
-.bi-mouse::before { content: ""\f499""; }
-.bi-mouse2-fill::before { content: ""\f49a""; }
-.bi-mouse2::before { content: ""\f49b""; }
-.bi-mouse3-fill::before { content: ""\f49c""; }
-.bi-mouse3::before { content: ""\f49d""; }
-.bi-music-note-beamed::before { content: ""\f49e""; }
-.bi-music-note-list::before { content: ""\f49f""; }
-.bi-music-note::before { content: ""\f4a0""; }
-.bi-music-player-fill::before { content: ""\f4a1""; }
-.bi-music-player::before { content: ""\f4a2""; }
-.bi-newspaper::before { content: ""\f4a3""; }
-.bi-node-minus-fill::before { content: ""\f4a4""; }
-.bi-node-minus::before { content: ""\f4a5""; }
-.bi-node-plus-fill::before { content: ""\f4a6""; }
-.bi-node-plus::before { content: ""\f4a7""; }
-.bi-nut-fill::before { content: ""\f4a8""; }
-.bi-nut::before { content: ""\f4a9""; }
-.bi-octagon-fill::before { content: ""\f4aa""; }
-.bi-octagon-half::before { content: ""\f4ab""; }
-.bi-octagon::before { content: ""\f4ac""; }
-.bi-option::before { content: ""\f4ad""; }
-.bi-outlet::before { content: ""\f4ae""; }
-.bi-paint-bucket::before { content: ""\f4af""; }
-.bi-palette-fill::before { content: ""\f4b0""; }
-.bi-palette::before { content: ""\f4b1""; }
-.bi-palette2::before { content: ""\f4b2""; }
-.bi-paperclip::before { content: ""\f4b3""; }
-.bi-paragraph::before { content: ""\f4b4""; }
-.bi-patch-check-fill::before { content: ""\f4b5""; }
-.bi-patch-check::before { content: ""\f4b6""; }
-.bi-patch-exclamation-fill::before { content: ""\f4b7""; }
-.bi-patch-exclamation::before { content: ""\f4b8""; }
-.bi-patch-minus-fill::before { content: ""\f4b9""; }
-.bi-patch-minus::before { content: ""\f4ba""; }
-.bi-patch-plus-fill::before { content: ""\f4bb""; }
-.bi-patch-plus::before { content: ""\f4bc""; }
-.bi-patch-question-fill::before { content: ""\f4bd""; }
-.bi-patch-question::before { content: ""\f4be""; }
-.bi-pause-btn-fill::before { content: ""\f4bf""; }
-.bi-pause-btn::before { content: ""\f4c0""; }
-.bi-pause-circle-fill::before { content: ""\f4c1""; }
-.bi-pause-circle::before { content: ""\f4c2""; }
-.bi-pause-fill::before { content: ""\f4c3""; }
-.bi-pause::before { content: ""\f4c4""; }
-.bi-peace-fill::before { content: ""\f4c5""; }
-.bi-peace::before { content: ""\f4c6""; }
-.bi-pen-fill::before { content: ""\f4c7""; }
-.bi-pen::before { content: ""\f4c8""; }
-.bi-pencil-fill::before { content: ""\f4c9""; }
-.bi-pencil-square::before { content: ""\f4ca""; }
-.bi-pencil::before { content: ""\f4cb""; }
-.bi-pentagon-fill::before { content: ""\f4cc""; }
-.bi-pentagon-half::before { content: ""\f4cd""; }
-.bi-pentagon::before { content: ""\f4ce""; }
-.bi-people-fill::before { content: ""\f4cf""; }
-.bi-people::before { content: ""\f4d0""; }
-.bi-percent::before { content: ""\f4d1""; }
-.bi-person-badge-fill::before { content: ""\f4d2""; }
-.bi-person-badge::before { content: ""\f4d3""; }
-.bi-person-bounding-box::before { content: ""\f4d4""; }
-.bi-person-check-fill::before { content: ""\f4d5""; }
-.bi-person-check::before { content: ""\f4d6""; }
-.bi-person-circle::before { content: ""\f4d7""; }
-.bi-person-dash-fill::before { content: ""\f4d8""; }
-.bi-person-dash::before { content: ""\f4d9""; }
-.bi-person-fill::before { content: ""\f4da""; }
-.bi-person-lines-fill::before { content: ""\f4db""; }
-.bi-person-plus-fill::before { content: ""\f4dc""; }
-.bi-person-plus::before { content: ""\f4dd""; }
-.bi-person-square::before { content: ""\f4de""; }
-.bi-person-x-fill::before { content: ""\f4df""; }
-.bi-person-x::before { content: ""\f4e0""; }
-.bi-person::before { content: ""\f4e1""; }
-.bi-phone-fill::before { content: ""\f4e2""; }
-.bi-phone-landscape-fill::before { content: ""\f4e3""; }
-.bi-phone-landscape::before { content: ""\f4e4""; }
-.bi-phone-vibrate-fill::before { content: ""\f4e5""; }
-.bi-phone-vibrate::before { content: ""\f4e6""; }
-.bi-phone::before { content: ""\f4e7""; }
-.bi-pie-chart-fill::before { content: ""\f4e8""; }
-.bi-pie-chart::before { content: ""\f4e9""; }
-.bi-pin-angle-fill::before { content: ""\f4ea""; }
-.bi-pin-angle::before { content: ""\f4eb""; }
-.bi-pin-fill::before { content: ""\f4ec""; }
-.bi-pin::before { content: ""\f4ed""; }
-.bi-pip-fill::before { content: ""\f4ee""; }
-.bi-pip::before { content: ""\f4ef""; }
-.bi-play-btn-fill::before { content: ""\f4f0""; }
-.bi-play-btn::before { content: ""\f4f1""; }
-.bi-play-circle-fill::before { content: ""\f4f2""; }
-.bi-play-circle::before { content: ""\f4f3""; }
-.bi-play-fill::before { content: ""\f4f4""; }
-.bi-play::before { content: ""\f4f5""; }
-.bi-plug-fill::before { content: ""\f4f6""; }
-.bi-plug::before { content: ""\f4f7""; }
-.bi-plus-circle-dotted::before { content: ""\f4f8""; }
-.bi-plus-circle-fill::before { content: ""\f4f9""; }
-.bi-plus-circle::before { content: ""\f4fa""; }
-.bi-plus-square-dotted::before { content: ""\f4fb""; }
-.bi-plus-square-fill::before { content: ""\f4fc""; }
-.bi-plus-square::before { content: ""\f4fd""; }
-.bi-plus::before { content: ""\f4fe""; }
-.bi-power::before { content: ""\f4ff""; }
-.bi-printer-fill::before { content: ""\f500""; }
-.bi-printer::before { content: ""\f501""; }
-.bi-puzzle-fill::before { content: ""\f502""; }
-.bi-puzzle::before { content: ""\f503""; }
-.bi-question-circle-fill::before { content: ""\f504""; }
-.bi-question-circle::before { content: ""\f505""; }
-.bi-question-diamond-fill::before { content: ""\f506""; }
-.bi-question-diamond::before { content: ""\f507""; }
-.bi-question-octagon-fill::before { content: ""\f508""; }
-.bi-question-octagon::before { content: ""\f509""; }
-.bi-question-square-fill::before { content: ""\f50a""; }
-.bi-question-square::before { content: ""\f50b""; }
-.bi-question::before { content: ""\f50c""; }
-.bi-rainbow::before { content: ""\f50d""; }
-.bi-receipt-cutoff::before { content: ""\f50e""; }
-.bi-receipt::before { content: ""\f50f""; }
-.bi-reception-0::before { content: ""\f510""; }
-.bi-reception-1::before { content: ""\f511""; }
-.bi-reception-2::before { content: ""\f512""; }
-.bi-reception-3::before { content: ""\f513""; }
-.bi-reception-4::before { content: ""\f514""; }
-.bi-record-btn-fill::before { content: ""\f515""; }
-.bi-record-btn::before { content: ""\f516""; }
-.bi-record-circle-fill::before { content: ""\f517""; }
-.bi-record-circle::before { content: ""\f518""; }
-.bi-record-fill::before { content: ""\f519""; }
-.bi-record::before { content: ""\f51a""; }
-.bi-record2-fill::before { content: ""\f51b""; }
-.bi-record2::before { content: ""\f51c""; }
-.bi-reply-all-fill::before { content: ""\f51d""; }
-.bi-reply-all::before { content: ""\f51e""; }
-.bi-reply-fill::before { content: ""\f51f""; }
-.bi-reply::before { content: ""\f520""; }
-.bi-rss-fill::before { content: ""\f521""; }
-.bi-rss::before { content: ""\f522""; }
-.bi-rulers::before { content: ""\f523""; }
-.bi-save-fill::before { content: ""\f524""; }
-.bi-save::before { content: ""\f525""; }
-.bi-save2-fill::before { content: ""\f526""; }
-.bi-save2::before { content: ""\f527""; }
-.bi-scissors::before { content: ""\f528""; }
-.bi-screwdriver::before { content: ""\f529""; }
-.bi-search::before { content: ""\f52a""; }
-.bi-segmented-nav::before { content: ""\f52b""; }
-.bi-server::before { content: ""\f52c""; }
-.bi-share-fill::before { content: ""\f52d""; }
-.bi-share::before { content: ""\f52e""; }
-.bi-shield-check::before { content: ""\f52f""; }
-.bi-shield-exclamation::before { content: ""\f530""; }
-.bi-shield-fill-check::before { content: ""\f531""; }
-.bi-shield-fill-exclamation::before { content: ""\f532""; }
-.bi-shield-fill-minus::before { content: ""\f533""; }
-.bi-shield-fill-plus::before { content: ""\f534""; }
-.bi-shield-fill-x::before { content: ""\f535""; }
-.bi-shield-fill::before { content: ""\f536""; }
-.bi-shield-lock-fill::before { content: ""\f537""; }
-.bi-shield-lock::before { content: ""\f538""; }
-.bi-shield-minus::before { content: ""\f539""; }
-.bi-shield-plus::before { content: ""\f53a""; }
-.bi-shield-shaded::before { content: ""\f53b""; }
-.bi-shield-slash-fill::before { content: ""\f53c""; }
-.bi-shield-slash::before { content: ""\f53d""; }
-.bi-shield-x::before { content: ""\f53e""; }
-.bi-shield::before { content: ""\f53f""; }
-.bi-shift-fill::before { content: ""\f540""; }
-.bi-shift::before { content: ""\f541""; }
-.bi-shop-window::before { content: ""\f542""; }
-.bi-shop::before { content: ""\f543""; }
-.bi-shuffle::before { content: ""\f544""; }
-.bi-signpost-2-fill::before { content: ""\f545""; }
-.bi-signpost-2::before { content: ""\f546""; }
-.bi-signpost-fill::before { content: ""\f547""; }
-.bi-signpost-split-fill::before { content: ""\f548""; }
-.bi-signpost-split::before { content: ""\f549""; }
-.bi-signpost::before { content: ""\f54a""; }
-.bi-sim-fill::before { content: ""\f54b""; }
-.bi-sim::before { content: ""\f54c""; }
-.bi-skip-backward-btn-fill::before { content: ""\f54d""; }
-.bi-skip-backward-btn::before { content: ""\f54e""; }
-.bi-skip-backward-circle-fill::before { content: ""\f54f""; }
-.bi-skip-backward-circle::before { content: ""\f550""; }
-.bi-skip-backward-fill::before { content: ""\f551""; }
-.bi-skip-backward::before { content: ""\f552""; }
-.bi-skip-end-btn-fill::before { content: ""\f553""; }
-.bi-skip-end-btn::before { content: ""\f554""; }
-.bi-skip-end-circle-fill::before { content: ""\f555""; }
-.bi-skip-end-circle::before { content: ""\f556""; }
-.bi-skip-end-fill::before { content: ""\f557""; }
-.bi-skip-end::before { content: ""\f558""; }
-.bi-skip-forward-btn-fill::before { content: ""\f559""; }
-.bi-skip-forward-btn::before { content: ""\f55a""; }
-.bi-skip-forward-circle-fill::before { content: ""\f55b""; }
-.bi-skip-forward-circle::before { content: ""\f55c""; }
-.bi-skip-forward-fill::before { content: ""\f55d""; }
-.bi-skip-forward::before { content: ""\f55e""; }
-.bi-skip-start-btn-fill::before { content: ""\f55f""; }
-.bi-skip-start-btn::before { content: ""\f560""; }
-.bi-skip-start-circle-fill::before { content: ""\f561""; }
-.bi-skip-start-circle::before { content: ""\f562""; }
-.bi-skip-start-fill::before { content: ""\f563""; }
-.bi-skip-start::before { content: ""\f564""; }
-.bi-slack::before { content: ""\f565""; }
-.bi-slash-circle-fill::before { content: ""\f566""; }
-.bi-slash-circle::before { content: ""\f567""; }
-.bi-slash-square-fill::before { content: ""\f568""; }
-.bi-slash-square::before { content: ""\f569""; }
-.bi-slash::before { content: ""\f56a""; }
-.bi-sliders::before { content: ""\f56b""; }
-.bi-smartwatch::before { content: ""\f56c""; }
-.bi-snow::before { content: ""\f56d""; }
-.bi-snow2::before { content: ""\f56e""; }
-.bi-snow3::before { content: ""\f56f""; }
-.bi-sort-alpha-down-alt::before { content: ""\f570""; }
-.bi-sort-alpha-down::before { content: ""\f571""; }
-.bi-sort-alpha-up-alt::before { content: ""\f572""; }
-.bi-sort-alpha-up::before { content: ""\f573""; }
-.bi-sort-down-alt::before { content: ""\f574""; }
-.bi-sort-down::before { content: ""\f575""; }
-.bi-sort-numeric-down-alt::before { content: ""\f576""; }
-.bi-sort-numeric-down::before { content: ""\f577""; }
-.bi-sort-numeric-up-alt::before { content: ""\f578""; }
-.bi-sort-numeric-up::before { content: ""\f579""; }
-.bi-sort-up-alt::before { content: ""\f57a""; }
-.bi-sort-up::before { content: ""\f57b""; }
-.bi-soundwave::before { content: ""\f57c""; }
-.bi-speaker-fill::before { content: ""\f57d""; }
-.bi-speaker::before { content: ""\f57e""; }
-.bi-speedometer::before { content: ""\f57f""; }
-.bi-speedometer2::before { content: ""\f580""; }
-.bi-spellcheck::before { content: ""\f581""; }
-.bi-square-fill::before { content: ""\f582""; }
-.bi-square-half::before { content: ""\f583""; }
-.bi-square::before { content: ""\f584""; }
-.bi-stack::before { content: ""\f585""; }
-.bi-star-fill::before { content: ""\f586""; }
-.bi-star-half::before { content: ""\f587""; }
-.bi-star::before { content: ""\f588""; }
-.bi-stars::before { content: ""\f589""; }
-.bi-stickies-fill::before { content: ""\f58a""; }
-.bi-stickies::before { content: ""\f58b""; }
-.bi-sticky-fill::before { content: ""\f58c""; }
-.bi-sticky::before { content: ""\f58d""; }
-.bi-stop-btn-fill::before { content: ""\f58e""; }
-.bi-stop-btn::before { content: ""\f58f""; }
-.bi-stop-circle-fill::before { content: ""\f590""; }
-.bi-stop-circle::before { content: ""\f591""; }
-.bi-stop-fill::before { content: ""\f592""; }
-.bi-stop::before { content: ""\f593""; }
-.bi-stoplights-fill::before { content: ""\f594""; }
-.bi-stoplights::before { content: ""\f595""; }
-.bi-stopwatch-fill::before { content: ""\f596""; }
-.bi-stopwatch::before { content: ""\f597""; }
-.bi-subtract::before { content: ""\f598""; }
-.bi-suit-club-fill::before { content: ""\f599""; }
-.bi-suit-club::before { content: ""\f59a""; }
-.bi-suit-diamond-fill::before { content: ""\f59b""; }
-.bi-suit-diamond::before { content: ""\f59c""; }
-.bi-suit-heart-fill::before { content: ""\f59d""; }
-.bi-suit-heart::before { content: ""\f59e""; }
-.bi-suit-spade-fill::before { content: ""\f59f""; }
-.bi-suit-spade::before { content: ""\f5a0""; }
-.bi-sun-fill::before { content: ""\f5a1""; }
-.bi-sun::before { content: ""\f5a2""; }
-.bi-sunglasses::before { content: ""\f5a3""; }
-.bi-sunrise-fill::before { content: ""\f5a4""; }
-.bi-sunrise::before { content: ""\f5a5""; }
-.bi-sunset-fill::before { content: ""\f5a6""; }
-.bi-sunset::before { content: ""\f5a7""; }
-.bi-symmetry-horizontal::before { content: ""\f5a8""; }
-.bi-symmetry-vertical::before { content: ""\f5a9""; }
-.bi-table::before { content: ""\f5aa""; }
-.bi-tablet-fill::before { content: ""\f5ab""; }
-.bi-tablet-landscape-fill::before { content: ""\f5ac""; }
-.bi-tablet-landscape::before { content: ""\f5ad""; }
-.bi-tablet::before { content: ""\f5ae""; }
-.bi-tag-fill::before { content: ""\f5af""; }
-.bi-tag::before { content: ""\f5b0""; }
-.bi-tags-fill::before { content: ""\f5b1""; }
-.bi-tags::before { content: ""\f5b2""; }
-.bi-telegram::before { content: ""\f5b3""; }
-.bi-telephone-fill::before { content: ""\f5b4""; }
-.bi-telephone-forward-fill::before { content: ""\f5b5""; }
-.bi-telephone-forward::before { content: ""\f5b6""; }
-.bi-telephone-inbound-fill::before { content: ""\f5b7""; }
-.bi-telephone-inbound::before { content: ""\f5b8""; }
-.bi-telephone-minus-fill::before { content: ""\f5b9""; }
-.bi-telephone-minus::before { content: ""\f5ba""; }
-.bi-telephone-outbound-fill::before { content: ""\f5bb""; }
-.bi-telephone-outbound::before { content: ""\f5bc""; }
-.bi-telephone-plus-fill::before { content: ""\f5bd""; }
-.bi-telephone-plus::before { content: ""\f5be""; }
-.bi-telephone-x-fill::before { content: ""\f5bf""; }
-.bi-telephone-x::before { content: ""\f5c0""; }
-.bi-telephone::before { content: ""\f5c1""; }
-.bi-terminal-fill::before { content: ""\f5c2""; }
-.bi-terminal::before { content: ""\f5c3""; }
-.bi-text-center::before { content: ""\f5c4""; }
-.bi-text-indent-left::before { content: ""\f5c5""; }
-.bi-text-indent-right::before { content: ""\f5c6""; }
-.bi-text-left::before { content: ""\f5c7""; }
-.bi-text-paragraph::before { content: ""\f5c8""; }
-.bi-text-right::before { content: ""\f5c9""; }
-.bi-textarea-resize::before { content: ""\f5ca""; }
-.bi-textarea-t::before { content: ""\f5cb""; }
-.bi-textarea::before { content: ""\f5cc""; }
-.bi-thermometer-half::before { content: ""\f5cd""; }
-.bi-thermometer-high::before { content: ""\f5ce""; }
-.bi-thermometer-low::before { content: ""\f5cf""; }
-.bi-thermometer-snow::before { content: ""\f5d0""; }
-.bi-thermometer-sun::before { content: ""\f5d1""; }
-.bi-thermometer::before { content: ""\f5d2""; }
-.bi-three-dots-vertical::before { content: ""\f5d3""; }
-.bi-three-dots::before { content: ""\f5d4""; }
-.bi-toggle-off::before { content: ""\f5d5""; }
-.bi-toggle-on::before { content: ""\f5d6""; }
-.bi-toggle2-off::before { content: ""\f5d7""; }
-.bi-toggle2-on::before { content: ""\f5d8""; }
-.bi-toggles::before { content: ""\f5d9""; }
-.bi-toggles2::before { content: ""\f5da""; }
-.bi-tools::before { content: ""\f5db""; }
-.bi-tornado::before { content: ""\f5dc""; }
-.bi-trash-fill::before { content: ""\f5dd""; }
-.bi-trash::before { content: ""\f5de""; }
-.bi-trash2-fill::before { content: ""\f5df""; }
-.bi-trash2::before { content: ""\f5e0""; }
-.bi-tree-fill::before { content: ""\f5e1""; }
-.bi-tree::before { content: ""\f5e2""; }
-.bi-triangle-fill::before { content: ""\f5e3""; }
-.bi-triangle-half::before { content: ""\f5e4""; }
-.bi-triangle::before { content: ""\f5e5""; }
-.bi-trophy-fill::before { content: ""\f5e6""; }
-.bi-trophy::before { content: ""\f5e7""; }
-.bi-tropical-storm::before { content: ""\f5e8""; }
-.bi-truck-flatbed::before { content: ""\f5e9""; }
-.bi-truck::before { content: ""\f5ea""; }
-.bi-tsunami::before { content: ""\f5eb""; }
-.bi-tv-fill::before { content: ""\f5ec""; }
-.bi-tv::before { content: ""\f5ed""; }
-.bi-twitch::before { content: ""\f5ee""; }
-.bi-twitter::before { content: ""\f5ef""; }
-.bi-type-bold::before { content: ""\f5f0""; }
-.bi-type-h1::before { content: ""\f5f1""; }
-.bi-type-h2::before { content: ""\f5f2""; }
-.bi-type-h3::before { content: ""\f5f3""; }
-.bi-type-italic::before { content: ""\f5f4""; }
-.bi-type-strikethrough::before { content: ""\f5f5""; }
-.bi-type-underline::before { content: ""\f5f6""; }
-.bi-type::before { content: ""\f5f7""; }
-.bi-ui-checks-grid::before { content: ""\f5f8""; }
-.bi-ui-checks::before { content: ""\f5f9""; }
-.bi-ui-radios-grid::before { content: ""\f5fa""; }
-.bi-ui-radios::before { content: ""\f5fb""; }
-.bi-umbrella-fill::before { content: ""\f5fc""; }
-.bi-umbrella::before { content: ""\f5fd""; }
-.bi-union::before { content: ""\f5fe""; }
-.bi-unlock-fill::before { content: ""\f5ff""; }
-.bi-unlock::before { content: ""\f600""; }
-.bi-upc-scan::before { content: ""\f601""; }
-.bi-upc::before { content: ""\f602""; }
-.bi-upload::before { content: ""\f603""; }
-.bi-vector-pen::before { content: ""\f604""; }
-.bi-view-list::before { content: ""\f605""; }
-.bi-view-stacked::before { content: ""\f606""; }
-.bi-vinyl-fill::before { content: ""\f607""; }
-.bi-vinyl::before { content: ""\f608""; }
-.bi-voicemail::before { content: ""\f609""; }
-.bi-volume-down-fill::before { content: ""\f60a""; }
-.bi-volume-down::before { content: ""\f60b""; }
-.bi-volume-mute-fill::before { content: ""\f60c""; }
-.bi-volume-mute::before { content: ""\f60d""; }
-.bi-volume-off-fill::before { content: ""\f60e""; }
-.bi-volume-off::before { content: ""\f60f""; }
-.bi-volume-up-fill::before { content: ""\f610""; }
-.bi-volume-up::before { content: ""\f611""; }
-.bi-vr::before { content: ""\f612""; }
-.bi-wallet-fill::before { content: ""\f613""; }
-.bi-wallet::before { content: ""\f614""; }
-.bi-wallet2::before { content: ""\f615""; }
-.bi-watch::before { content: ""\f616""; }
-.bi-water::before { content: ""\f617""; }
-.bi-whatsapp::before { content: ""\f618""; }
-.bi-wifi-1::before { content: ""\f619""; }
-.bi-wifi-2::before { content: ""\f61a""; }
-.bi-wifi-off::before { content: ""\f61b""; }
-.bi-wifi::before { content: ""\f61c""; }
-.bi-wind::before { content: ""\f61d""; }
-.bi-window-dock::before { content: ""\f61e""; }
-.bi-window-sidebar::before { content: ""\f61f""; }
-.bi-window::before { content: ""\f620""; }
-.bi-wrench::before { content: ""\f621""; }
-.bi-x-circle-fill::before { content: ""\f622""; }
-.bi-x-circle::before { content: ""\f623""; }
-.bi-x-diamond-fill::before { content: ""\f624""; }
-.bi-x-diamond::before { content: ""\f625""; }
-.bi-x-octagon-fill::before { content: ""\f626""; }
-.bi-x-octagon::before { content: ""\f627""; }
-.bi-x-square-fill::before { content: ""\f628""; }
-.bi-x-square::before { content: ""\f629""; }
-.bi-x::before { content: ""\f62a""; }
-.bi-youtube::before { content: ""\f62b""; }
-.bi-zoom-in::before { content: ""\f62c""; }
-.bi-zoom-out::before { content: ""\f62d""; }
-.bi-bank::before { content: ""\f62e""; }
-.bi-bank2::before { content: ""\f62f""; }
-.bi-bell-slash-fill::before { content: ""\f630""; }
-.bi-bell-slash::before { content: ""\f631""; }
-.bi-cash-coin::before { content: ""\f632""; }
-.bi-check-lg::before { content: ""\f633""; }
-.bi-coin::before { content: ""\f634""; }
-.bi-currency-bitcoin::before { content: ""\f635""; }
-.bi-currency-dollar::before { content: ""\f636""; }
-.bi-currency-euro::before { content: ""\f637""; }
-.bi-currency-exchange::before { content: ""\f638""; }
-.bi-currency-pound::before { content: ""\f639""; }
-.bi-currency-yen::before { content: ""\f63a""; }
-.bi-dash-lg::before { content: ""\f63b""; }
-.bi-exclamation-lg::before { content: ""\f63c""; }
-.bi-file-earmark-pdf-fill::before { content: ""\f63d""; }
-.bi-file-earmark-pdf::before { content: ""\f63e""; }
-.bi-file-pdf-fill::before { content: ""\f63f""; }
-.bi-file-pdf::before { content: ""\f640""; }
-.bi-gender-ambiguous::before { content: ""\f641""; }
-.bi-gender-female::before { content: ""\f642""; }
-.bi-gender-male::before { content: ""\f643""; }
-.bi-gender-trans::before { content: ""\f644""; }
-.bi-headset-vr::before { content: ""\f645""; }
-.bi-info-lg::before { content: ""\f646""; }
-.bi-mastodon::before { content: ""\f647""; }
-.bi-messenger::before { content: ""\f648""; }
-.bi-piggy-bank-fill::before { content: ""\f649""; }
-.bi-piggy-bank::before { content: ""\f64a""; }
-.bi-pin-map-fill::before { content: ""\f64b""; }
-.bi-pin-map::before { content: ""\f64c""; }
-.bi-plus-lg::before { content: ""\f64d""; }
-.bi-question-lg::before { content: ""\f64e""; }
-.bi-recycle::before { content: ""\f64f""; }
-.bi-reddit::before { content: ""\f650""; }
-.bi-safe-fill::before { content: ""\f651""; }
-.bi-safe2-fill::before { content: ""\f652""; }
-.bi-safe2::before { content: ""\f653""; }
-.bi-sd-card-fill::before { content: ""\f654""; }
-.bi-sd-card::before { content: ""\f655""; }
-.bi-skype::before { content: ""\f656""; }
-.bi-slash-lg::before { content: ""\f657""; }
-.bi-translate::before { content: ""\f658""; }
-.bi-x-lg::before { content: ""\f659""; }
-.bi-safe::before { content: ""\f65a""; }
-.bi-apple::before { content: ""\f65b""; }
-.bi-microsoft::before { content: ""\f65d""; }
-.bi-windows::before { content: ""\f65e""; }
-.bi-behance::before { content: ""\f65c""; }
-.bi-dribbble::before { content: ""\f65f""; }
-.bi-line::before { content: ""\f660""; }
-.bi-medium::before { content: ""\f661""; }
-.bi-paypal::before { content: ""\f662""; }
-.bi-pinterest::before { content: ""\f663""; }
-.bi-signal::before { content: ""\f664""; }
-.bi-snapchat::before { content: ""\f665""; }
-.bi-spotify::before { content: ""\f666""; }
-.bi-stack-overflow::before { content: ""\f667""; }
-.bi-strava::before { content: ""\f668""; }
-.bi-wordpress::before { content: ""\f669""; }
-.bi-vimeo::before { content: ""\f66a""; }
-.bi-activity::before { content: ""\f66b""; }
-.bi-easel2-fill::before { content: ""\f66c""; }
-.bi-easel2::before { content: ""\f66d""; }
-.bi-easel3-fill::before { content: ""\f66e""; }
-.bi-easel3::before { content: ""\f66f""; }
-.bi-fan::before { content: ""\f670""; }
-.bi-fingerprint::before { content: ""\f671""; }
-.bi-graph-down-arrow::before { content: ""\f672""; }
-.bi-graph-up-arrow::before { content: ""\f673""; }
-.bi-hypnotize::before { content: ""\f674""; }
-.bi-magic::before { content: ""\f675""; }
-.bi-person-rolodex::before { content: ""\f676""; }
-.bi-person-video::before { content: ""\f677""; }
-.bi-person-video2::before { content: ""\f678""; }
-.bi-person-video3::before { content: ""\f679""; }
-.bi-person-workspace::before { content: ""\f67a""; }
-.bi-radioactive::before { content: ""\f67b""; }
-.bi-webcam-fill::before { content: ""\f67c""; }
-.bi-webcam::before { content: ""\f67d""; }
-.bi-yin-yang::before { content: ""\f67e""; }
-.bi-bandaid-fill::before { content: ""\f680""; }
-.bi-bandaid::before { content: ""\f681""; }
-.bi-bluetooth::before { content: ""\f682""; }
-.bi-body-text::before { content: ""\f683""; }
-.bi-boombox::before { content: ""\f684""; }
-.bi-boxes::before { content: ""\f685""; }
-.bi-dpad-fill::before { content: ""\f686""; }
-.bi-dpad::before { content: ""\f687""; }
-.bi-ear-fill::before { content: ""\f688""; }
-.bi-ear::before { content: ""\f689""; }
-.bi-envelope-check-1::before { content: ""\f68a""; }
-.bi-envelope-check-fill::before { content: ""\f68b""; }
-.bi-envelope-check::before { content: ""\f68c""; }
-.bi-envelope-dash-1::before { content: ""\f68d""; }
-.bi-envelope-dash-fill::before { content: ""\f68e""; }
-.bi-envelope-dash::before { content: ""\f68f""; }
-.bi-envelope-exclamation-1::before { content: ""\f690""; }
-.bi-envelope-exclamation-fill::before { content: ""\f691""; }
-.bi-envelope-exclamation::before { content: ""\f692""; }
-.bi-envelope-plus-fill::before { content: ""\f693""; }
-.bi-envelope-plus::before { content: ""\f694""; }
-.bi-envelope-slash-1::before { content: ""\f695""; }
-.bi-envelope-slash-fill::before { content: ""\f696""; }
-.bi-envelope-slash::before { content: ""\f697""; }
-.bi-envelope-x-1::before { content: ""\f698""; }
-.bi-envelope-x-fill::before { content: ""\f699""; }
-.bi-envelope-x::before { content: ""\f69a""; }
-.bi-explicit-fill::before { content: ""\f69b""; }
-.bi-explicit::before { content: ""\f69c""; }
-.bi-git::before { content: ""\f69d""; }
-.bi-infinity::before { content: ""\f69e""; }
-.bi-list-columns-reverse::before { content: ""\f69f""; }
-.bi-list-columns::before { content: ""\f6a0""; }
-.bi-meta::before { content: ""\f6a1""; }
-.bi-mortorboard-fill::before { content: ""\f6a2""; }
-.bi-mortorboard::before { content: ""\f6a3""; }
-.bi-nintendo-switch::before { content: ""\f6a4""; }
-.bi-pc-display-horizontal::before { content: ""\f6a5""; }
-.bi-pc-display::before { content: ""\f6a6""; }
-.bi-pc-horizontal::before { content: ""\f6a7""; }
-.bi-pc::before { content: ""\f6a8""; }
-.bi-playstation::before { content: ""\f6a9""; }
-.bi-plus-slash-minus::before { content: ""\f6aa""; }
-.bi-projector-fill::before { content: ""\f6ab""; }
-.bi-projector::before { content: ""\f6ac""; }
-.bi-qr-code-scan::before { content: ""\f6ad""; }
-.bi-qr-code::before { content: ""\f6ae""; }
-.bi-quora::before { content: ""\f6af""; }
-.bi-quote::before { content: ""\f6b0""; }
-.bi-robot::before { content: ""\f6b1""; }
-.bi-send-check-fill::before { content: ""\f6b2""; }
-.bi-send-check::before { content: ""\f6b3""; }
-.bi-send-dash-fill::before { content: ""\f6b4""; }
-.bi-send-dash::before { content: ""\f6b5""; }
-.bi-send-exclamation-1::before { content: ""\f6b6""; }
-.bi-send-exclamation-fill::before { content: ""\f6b7""; }
-.bi-send-exclamation::before { content: ""\f6b8""; }
-.bi-send-fill::before { content: ""\f6b9""; }
-.bi-send-plus-fill::before { content: ""\f6ba""; }
-.bi-send-plus::before { content: ""\f6bb""; }
-.bi-send-slash-fill::before { content: ""\f6bc""; }
-.bi-send-slash::before { content: ""\f6bd""; }
-.bi-send-x-fill::before { content: ""\f6be""; }
-.bi-send-x::before { content: ""\f6bf""; }
-.bi-send::before { content: ""\f6c0""; }
-.bi-steam::before { content: ""\f6c1""; }
-.bi-terminal-dash-1::before { content: ""\f6c2""; }
-.bi-terminal-dash::before { content: ""\f6c3""; }
-.bi-terminal-plus::before { content: ""\f6c4""; }
-.bi-terminal-split::before { content: ""\f6c5""; }
-.bi-ticket-detailed-fill::before { content: ""\f6c6""; }
-.bi-ticket-detailed::before { content: ""\f6c7""; }
-.bi-ticket-fill::before { content: ""\f6c8""; }
-.bi-ticket-perforated-fill::before { content: ""\f6c9""; }
-.bi-ticket-perforated::before { content: ""\f6ca""; }
-.bi-ticket::before { content: ""\f6cb""; }
-.bi-tiktok::before { content: ""\f6cc""; }
-.bi-window-dash::before { content: ""\f6cd""; }
-.bi-window-desktop::before { content: ""\f6ce""; }
-.bi-window-fullscreen::before { content: ""\f6cf""; }
-.bi-window-plus::before { content: ""\f6d0""; }
-.bi-window-split::before { content: ""\f6d1""; }
-.bi-window-stack::before { content: ""\f6d2""; }
-.bi-window-x::before { content: ""\f6d3""; }
-.bi-xbox::before { content: ""\f6d4""; }
-.bi-ethernet::before { content: ""\f6d5""; }
-.bi-hdmi-fill::before { content: ""\f6d6""; }
-.bi-hdmi::before { content: ""\f6d7""; }
-.bi-usb-c-fill::before { content: ""\f6d8""; }
-.bi-usb-c::before { content: ""\f6d9""; }
-.bi-usb-fill::before { content: ""\f6da""; }
-.bi-usb-plug-fill::before { content: ""\f6db""; }
-.bi-usb-plug::before { content: ""\f6dc""; }
-.bi-usb-symbol::before { content: ""\f6dd""; }
-.bi-usb::before { content: ""\f6de""; }
-.bi-boombox-fill::before { content: ""\f6df""; }
-.bi-displayport-1::before { content: ""\f6e0""; }
-.bi-displayport::before { content: ""\f6e1""; }
-.bi-gpu-card::before { content: ""\f6e2""; }
-.bi-memory::before { content: ""\f6e3""; }
-.bi-modem-fill::before { content: ""\f6e4""; }
-.bi-modem::before { content: ""\f6e5""; }
-.bi-motherboard-fill::before { content: ""\f6e6""; }
-.bi-motherboard::before { content: ""\f6e7""; }
-.bi-optical-audio-fill::before { content: ""\f6e8""; }
-.bi-optical-audio::before { content: ""\f6e9""; }
-.bi-pci-card::before { content: ""\f6ea""; }
-.bi-router-fill::before { content: ""\f6eb""; }
-.bi-router::before { content: ""\f6ec""; }
-.bi-ssd-fill::before { content: ""\f6ed""; }
-.bi-ssd::before { content: ""\f6ee""; }
-.bi-thunderbolt-fill::before { content: ""\f6ef""; }
-.bi-thunderbolt::before { content: ""\f6f0""; }
-.bi-usb-drive-fill::before { content: ""\f6f1""; }
-.bi-usb-drive::before { content: ""\f6f2""; }
-.bi-usb-micro-fill::before { content: ""\f6f3""; }
-.bi-usb-micro::before { content: ""\f6f4""; }
-.bi-usb-mini-fill::before { content: ""\f6f5""; }
-.bi-usb-mini::before { content: ""\f6f6""; }
-.bi-cloud-haze2::before { content: ""\f6f7""; }
-.bi-device-hdd-fill::before { content: ""\f6f8""; }
-.bi-device-hdd::before { content: ""\f6f9""; }
-.bi-device-ssd-fill::before { content: ""\f6fa""; }
-.bi-device-ssd::before { content: ""\f6fb""; }
-.bi-displayport-fill::before { content: ""\f6fc""; }
-.bi-mortarboard-fill::before { content: ""\f6fd""; }
-.bi-mortarboard::before { content: ""\f6fe""; }
-.bi-terminal-x::before { content: ""\f6ff""; }
-.bi-arrow-through-heart-fill::before { content: ""\f700""; }
-.bi-arrow-through-heart::before { content: ""\f701""; }
-.bi-badge-sd-fill::before { content: ""\f702""; }
-.bi-badge-sd::before { content: ""\f703""; }
-.bi-bag-heart-fill::before { content: ""\f704""; }
-.bi-bag-heart::before { content: ""\f705""; }
-.bi-balloon-fill::before { content: ""\f706""; }
-.bi-balloon-heart-fill::before { content: ""\f707""; }
-.bi-balloon-heart::before { content: ""\f708""; }
-.bi-balloon::before { content: ""\f709""; }
-.bi-box2-fill::before { content: ""\f70a""; }
-.bi-box2-heart-fill::before { content: ""\f70b""; }
-.bi-box2-heart::before { content: ""\f70c""; }
-.bi-box2::before { content: ""\f70d""; }
-.bi-braces-asterisk::before { content: ""\f70e""; }
-.bi-calendar-heart-fill::before { content: ""\f70f""; }
-.bi-calendar-heart::before { content: ""\f710""; }
-.bi-calendar2-heart-fill::before { content: ""\f711""; }
-.bi-calendar2-heart::before { content: ""\f712""; }
-.bi-chat-heart-fill::before { content: ""\f713""; }
-.bi-chat-heart::before { content: ""\f714""; }
-.bi-chat-left-heart-fill::before { content: ""\f715""; }
-.bi-chat-left-heart::before { content: ""\f716""; }
-.bi-chat-right-heart-fill::before { content: ""\f717""; }
-.bi-chat-right-heart::before { content: ""\f718""; }
-.bi-chat-square-heart-fill::before { content: ""\f719""; }
-.bi-chat-square-heart::before { content: ""\f71a""; }
-.bi-clipboard-check-fill::before { content: ""\f71b""; }
-.bi-clipboard-data-fill::before { content: ""\f71c""; }
-.bi-clipboard-fill::before { content: ""\f71d""; }
-.bi-clipboard-heart-fill::before { content: ""\f71e""; }
-.bi-clipboard-heart::before { content: ""\f71f""; }
-.bi-clipboard-minus-fill::before { content: ""\f720""; }
-.bi-clipboard-plus-fill::before { content: ""\f721""; }
-.bi-clipboard-pulse::before { content: ""\f722""; }
-.bi-clipboard-x-fill::before { content: ""\f723""; }
-.bi-clipboard2-check-fill::before { content: ""\f724""; }
-.bi-clipboard2-check::before { content: ""\f725""; }
-.bi-clipboard2-data-fill::before { content: ""\f726""; }
-.bi-clipboard2-data::before { content: ""\f727""; }
-.bi-clipboard2-fill::before { content: ""\f728""; }
-.bi-clipboard2-heart-fill::before { content: ""\f729""; }
-.bi-clipboard2-heart::before { content: ""\f72a""; }
-.bi-clipboard2-minus-fill::before { content: ""\f72b""; }
-.bi-clipboard2-minus::before { content: ""\f72c""; }
-.bi-clipboard2-plus-fill::before { content: ""\f72d""; }
-.bi-clipboard2-plus::before { content: ""\f72e""; }
-.bi-clipboard2-pulse-fill::before { content: ""\f72f""; }
-.bi-clipboard2-pulse::before { content: ""\f730""; }
-.bi-clipboard2-x-fill::before { content: ""\f731""; }
-.bi-clipboard2-x::before { content: ""\f732""; }
-.bi-clipboard2::before { content: ""\f733""; }
-.bi-emoji-kiss-fill::before { content: ""\f734""; }
-.bi-emoji-kiss::before { content: ""\f735""; }
-.bi-envelope-heart-fill::before { content: ""\f736""; }
-.bi-envelope-heart::before { content: ""\f737""; }
-.bi-envelope-open-heart-fill::before { content: ""\f738""; }
-.bi-envelope-open-heart::before { content: ""\f739""; }
-.bi-envelope-paper-fill::before { content: ""\f73a""; }
-.bi-envelope-paper-heart-fill::before { content: ""\f73b""; }
-.bi-envelope-paper-heart::before { content: ""\f73c""; }
-.bi-envelope-paper::before { content: ""\f73d""; }
-.bi-filetype-aac::before { content: ""\f73e""; }
-.bi-filetype-ai::before { content: ""\f73f""; }
-.bi-filetype-bmp::before { content: ""\f740""; }
-.bi-filetype-cs::before { content: ""\f741""; }
-.bi-filetype-css::before { content: ""\f742""; }
-.bi-filetype-csv::before { content: ""\f743""; }
-.bi-filetype-doc::before { content: ""\f744""; }
-.bi-filetype-docx::before { content: ""\f745""; }
-.bi-filetype-exe::before { content: ""\f746""; }
-.bi-filetype-gif::before { content: ""\f747""; }
-.bi-filetype-heic::before { content: ""\f748""; }
-.bi-filetype-html::before { content: ""\f749""; }
-.bi-filetype-java::before { content: ""\f74a""; }
-.bi-filetype-jpg::before { content: ""\f74b""; }
-.bi-filetype-js::before { content: ""\f74c""; }
-.bi-filetype-jsx::before { content: ""\f74d""; }
-.bi-filetype-key::before { content: ""\f74e""; }
-.bi-filetype-m4p::before { content: ""\f74f""; }
-.bi-filetype-md::before { content: ""\f750""; }
-.bi-filetype-mdx::before { content: ""\f751""; }
-.bi-filetype-mov::before { content: ""\f752""; }
-.bi-filetype-mp3::before { content: ""\f753""; }
-.bi-filetype-mp4::before { content: ""\f754""; }
-.bi-filetype-otf::before { content: ""\f755""; }
-.bi-filetype-pdf::before { content: ""\f756""; }
-.bi-filetype-php::before { content: ""\f757""; }
-.bi-filetype-png::before { content: ""\f758""; }
-.bi-filetype-ppt-1::before { content: ""\f759""; }
-.bi-filetype-ppt::before { content: ""\f75a""; }
-.bi-filetype-psd::before { content: ""\f75b""; }
-.bi-filetype-py::before { content: ""\f75c""; }
-.bi-filetype-raw::before { content: ""\f75d""; }
-.bi-filetype-rb::before { content: ""\f75e""; }
-.bi-filetype-sass::before { content: ""\f75f""; }
-.bi-filetype-scss::before { content: ""\f760""; }
-.bi-filetype-sh::before { content: ""\f761""; }
-.bi-filetype-svg::before { content: ""\f762""; }
-.bi-filetype-tiff::before { content: ""\f763""; }
-.bi-filetype-tsx::before { content: ""\f764""; }
-.bi-filetype-ttf::before { content: ""\f765""; }
-.bi-filetype-txt::before { content: ""\f766""; }
-.bi-filetype-wav::before { content: ""\f767""; }
-.bi-filetype-woff::before { content: ""\f768""; }
-.bi-filetype-xls-1::before { content: ""\f769""; }
-.bi-filetype-xls::before { content: ""\f76a""; }
-.bi-filetype-xml::before { content: ""\f76b""; }
-.bi-filetype-yml::before { content: ""\f76c""; }
-.bi-heart-arrow::before { content: ""\f76d""; }
-.bi-heart-pulse-fill::before { content: ""\f76e""; }
-.bi-heart-pulse::before { content: ""\f76f""; }
-.bi-heartbreak-fill::before { content: ""\f770""; }
-.bi-heartbreak::before { content: ""\f771""; }
-.bi-hearts::before { content: ""\f772""; }
-.bi-hospital-fill::before { content: ""\f773""; }
-.bi-hospital::before { content: ""\f774""; }
-.bi-house-heart-fill::before { content: ""\f775""; }
-.bi-house-heart::before { content: ""\f776""; }
-.bi-incognito::before { content: ""\f777""; }
-.bi-magnet-fill::before { content: ""\f778""; }
-.bi-magnet::before { content: ""\f779""; }
-.bi-person-heart::before { content: ""\f77a""; }
-.bi-person-hearts::before { content: ""\f77b""; }
-.bi-phone-flip::before { content: ""\f77c""; }
-.bi-plugin::before { content: ""\f77d""; }
-.bi-postage-fill::before { content: ""\f77e""; }
-.bi-postage-heart-fill::before { content: ""\f77f""; }
-.bi-postage-heart::before { content: ""\f780""; }
-.bi-postage::before { content: ""\f781""; }
-.bi-postcard-fill::before { content: ""\f782""; }
-.bi-postcard-heart-fill::before { content: ""\f783""; }
-.bi-postcard-heart::before { content: ""\f784""; }
-.bi-postcard::before { content: ""\f785""; }
-.bi-search-heart-fill::before { content: ""\f786""; }
-.bi-search-heart::before { content: ""\f787""; }
-.bi-sliders2-vertical::before { content: ""\f788""; }
-.bi-sliders2::before { content: ""\f789""; }
-.bi-trash3-fill::before { content: ""\f78a""; }
-.bi-trash3::before { content: ""\f78b""; }
-.bi-valentine::before { content: ""\f78c""; }
-.bi-valentine2::before { content: ""\f78d""; }
-.bi-wrench-adjustable-circle-fill::before { content: ""\f78e""; }
-.bi-wrench-adjustable-circle::before { content: ""\f78f""; }
-.bi-wrench-adjustable::before { content: ""\f790""; }
-.bi-filetype-json::before { content: ""\f791""; }
-.bi-filetype-pptx::before { content: ""\f792""; }
-.bi-filetype-xlsx::before { content: ""\f793""; }

---FILE: _freeze/tensorflow/guide/basics/libs/bootstrap/bootstrap.min.js---
@@ -1,7 +0,0 @@
-/*!
-  * Bootstrap v5.1.3 (https://getbootstrap.com/)
-  * Copyright 2011-2021 The Bootstrap Authors (https://github.com/twbs/bootstrap/graphs/contributors)
-  * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE)
-  */
-!function(t,e){""object""==typeof exports&&""undefined""!=typeof module?module.exports=e():""function""==typeof define&&define.amd?define(e):(t=""undefined""!=typeof globalThis?globalThis:t||self).bootstrap=e()}(this,(function(){""use strict"";const t=""transitionend"",e=t=>{let e=t.getAttribute(""data-bs-target"");if(!e||""#""===e){let i=t.getAttribute(""href"");if(!i||!i.includes(""#"")&&!i.startsWith("".""))return null;i.includes(""#"")&&!i.startsWith(""#"")&&(i=`#${i.split(""#"")[1]}`),e=i&&""#""!==i?i.trim():null}return e},i=t=>{const i=e(t);return i&&document.querySelector(i)?i:null},n=t=>{const i=e(t);return i?document.querySelector(i):null},s=e=>{e.dispatchEvent(new Event(t))},o=t=>!(!t||""object""!=typeof t)&&(void 0!==t.jquery&&(t=t[0]),void 0!==t.nodeType),r=t=>o(t)?t.jquery?t[0]:t:""string""==typeof t&&t.length>0?document.querySelector(t):null,a=(t,e,i)=>{Object.keys(i).forEach((n=>{const s=i[n],r=e[n],a=r&&o(r)?""element"":null==(l=r)?`${l}`:{}.toString.call(l).match(/\s([a-z]+)/i)[1].toLowerCase();var l;if(!new RegExp(s).test(a))throw new TypeError(`${t.toUpperCase()}: Option ""${n}"" provided type ""${a}"" but expected type ""${s}"".`)}))},l=t=>!(!o(t)||0===t.getClientRects().length)&&""visible""===getComputedStyle(t).getPropertyValue(""visibility""),c=t=>!t||t.nodeType!==Node.ELEMENT_NODE||!!t.classList.contains(""disabled"")||(void 0!==t.disabled?t.disabled:t.hasAttribute(""disabled"")&&""false""!==t.getAttribute(""disabled"")),h=t=>{if(!document.documentElement.attachShadow)return null;if(""function""==typeof t.getRootNode){const e=t.getRootNode();return e instanceof ShadowRoot?e:null}return t instanceof ShadowRoot?t:t.parentNode?h(t.parentNode):null},d=()=>{},u=t=>{t.offsetHeight},f=()=>{const{jQuery:t}=window;return t&&!document.body.hasAttribute(""data-bs-no-jquery"")?t:null},p=[],m=()=>""rtl""===document.documentElement.dir,g=t=>{var e;e=()=>{const e=f();if(e){const i=t.NAME,n=e.fn[i];e.fn[i]=t.jQueryInterface,e.fn[i].Constructor=t,e.fn[i].noConflict=()=>(e.fn[i]=n,t.jQueryInterface)}},""loading""===document.readyState?(p.length||document.addEventListener(""DOMContentLoaded"",(()=>{p.forEach((t=>t()))})),p.push(e)):e()},_=t=>{""function""==typeof t&&t()},b=(e,i,n=!0)=>{if(!n)return void _(e);const o=(t=>{if(!t)return 0;let{transitionDuration:e,transitionDelay:i}=window.getComputedStyle(t);const n=Number.parseFloat(e),s=Number.parseFloat(i);return n||s?(e=e.split("","")[0],i=i.split("","")[0],1e3*(Number.parseFloat(e)+Number.parseFloat(i))):0})(i)+5;let r=!1;const a=({target:n})=>{n===i&&(r=!0,i.removeEventListener(t,a),_(e))};i.addEventListener(t,a),setTimeout((()=>{r||s(i)}),o)},v=(t,e,i,n)=>{let s=t.indexOf(e);if(-1===s)return t[!i&&n?t.length-1:0];const o=t.length;return s+=i?1:-1,n&&(s=(s+o)%o),t[Math.max(0,Math.min(s,o-1))]},y=/[^.]*(?=\..*)\.|.*/,w=/\..*/,E=/::\d+$/,A={};let T=1;const O={mouseenter:""mouseover"",mouseleave:""mouseout""},C=/^(mouseenter|mouseleave)/i,k=new Set([""click"",""dblclick"",""mouseup"",""mousedown"",""contextmenu"",""mousewheel"",""DOMMouseScroll"",""mouseover"",""mouseout"",""mousemove"",""selectstart"",""selectend"",""keydown"",""keypress"",""keyup"",""orientationchange"",""touchstart"",""touchmove"",""touchend"",""touchcancel"",""pointerdown"",""pointermove"",""pointerup"",""pointerleave"",""pointercancel"",""gesturestart"",""gesturechange"",""gestureend"",""focus"",""blur"",""change"",""reset"",""select"",""submit"",""focusin"",""focusout"",""load"",""unload"",""beforeunload"",""resize"",""move"",""DOMContentLoaded"",""readystatechange"",""error"",""abort"",""scroll""]);function L(t,e){return e&&`${e}::${T++}`||t.uidEvent||T++}function x(t){const e=L(t);return t.uidEvent=e,A[e]=A[e]||{},A[e]}function D(t,e,i=null){const n=Object.keys(t);for(let s=0,o=n.length;s<o;s++){const o=t[n[s]];if(o.originalHandler===e&&o.delegationSelector===i)return o}return null}function S(t,e,i){const n=""string""==typeof e,s=n?i:e;let o=P(t);return k.has(o)||(o=t),[n,s,o]}function N(t,e,i,n,s){if(""string""!=typeof e||!t)return;if(i||(i=n,n=null),C.test(e)){const t=t=>function(e){if(!e.relatedTarget||e.relatedTarget!==e.delegateTarget&&!e.delegateTarget.contains(e.relatedTarget))return t.call(this,e)};n?n=t(n):i=t(i)}const[o,r,a]=S(e,i,n),l=x(t),c=l[a]||(l[a]={}),h=D(c,r,o?i:null);if(h)return void(h.oneOff=h.oneOff&&s);const d=L(r,e.replace(y,"""")),u=o?function(t,e,i){return function n(s){const o=t.querySelectorAll(e);for(let{target:r}=s;r&&r!==this;r=r.parentNode)for(let a=o.length;a--;)if(o[a]===r)return s.delegateTarget=r,n.oneOff&&j.off(t,s.type,e,i),i.apply(r,[s]);return null}}(t,i,n):function(t,e){return function i(n){return n.delegateTarget=t,i.oneOff&&j.off(t,n.type,e),e.apply(t,[n])}}(t,i);u.delegationSelector=o?i:null,u.originalHandler=r,u.oneOff=s,u.uidEvent=d,c[d]=u,t.addEventListener(a,u,o)}function I(t,e,i,n,s){const o=D(e[i],n,s);o&&(t.removeEventListener(i,o,Boolean(s)),delete e[i][o.uidEvent])}function P(t){return t=t.replace(w,""""),O[t]||t}const j={on(t,e,i,n){N(t,e,i,n,!1)},one(t,e,i,n){N(t,e,i,n,!0)},off(t,e,i,n){if(""string""!=typeof e||!t)return;const[s,o,r]=S(e,i,n),a=r!==e,l=x(t),c=e.startsWith(""."");if(void 0!==o){if(!l||!l[r])return;return void I(t,l,r,o,s?i:null)}c&&Object.keys(l).forEach((i=>{!function(t,e,i,n){const s=e[i]||{};Object.keys(s).forEach((o=>{if(o.includes(n)){const n=s[o];I(t,e,i,n.originalHandler,n.delegationSelector)}}))}(t,l,i,e.slice(1))}));const h=l[r]||{};Object.keys(h).forEach((i=>{const n=i.replace(E,"""");if(!a||e.includes(n)){const e=h[i];I(t,l,r,e.originalHandler,e.delegationSelector)}}))},trigger(t,e,i){if(""string""!=typeof e||!t)return null;const n=f(),s=P(e),o=e!==s,r=k.has(s);let a,l=!0,c=!0,h=!1,d=null;return o&&n&&(a=n.Event(e,i),n(t).trigger(a),l=!a.isPropagationStopped(),c=!a.isImmediatePropagationStopped(),h=a.isDefaultPrevented()),r?(d=document.createEvent(""HTMLEvents""),d.initEvent(s,l,!0)):d=new CustomEvent(e,{bubbles:l,cancelable:!0}),void 0!==i&&Object.keys(i).forEach((t=>{Object.defineProperty(d,t,{get:()=>i[t]})})),h&&d.preventDefault(),c&&t.dispatchEvent(d),d.defaultPrevented&&void 0!==a&&a.preventDefault(),d}},M=new Map,H={set(t,e,i){M.has(t)||M.set(t,new Map);const n=M.get(t);n.has(e)||0===n.size?n.set(e,i):console.error(`Bootstrap doesn't allow more than one instance per element. Bound instance: ${Array.from(n.keys())[0]}.`)},get:(t,e)=>M.has(t)&&M.get(t).get(e)||null,remove(t,e){if(!M.has(t))return;const i=M.get(t);i.delete(e),0===i.size&&M.delete(t)}};class B{constructor(t){(t=r(t))&&(this._element=t,H.set(this._element,this.constructor.DATA_KEY,this))}dispose(){H.remove(this._element,this.constructor.DATA_KEY),j.off(this._element,this.constructor.EVENT_KEY),Object.getOwnPropertyNames(this).forEach((t=>{this[t]=null}))}_queueCallback(t,e,i=!0){b(t,e,i)}static getInstance(t){return H.get(r(t),this.DATA_KEY)}static getOrCreateInstance(t,e={}){return this.getInstance(t)||new this(t,""object""==typeof e?e:null)}static get VERSION(){return""5.1.3""}static get NAME(){throw new Error('You have to implement the static method ""NAME"", for each component!')}static get DATA_KEY(){return`bs.${this.NAME}`}static get EVENT_KEY(){return`.${this.DATA_KEY}`}}const R=(t,e=""hide"")=>{const i=`click.dismiss${t.EVENT_KEY}`,s=t.NAME;j.on(document,i,`[data-bs-dismiss=""${s}""]`,(function(i){if([""A"",""AREA""].includes(this.tagName)&&i.preventDefault(),c(this))return;const o=n(this)||this.closest(`.${s}`);t.getOrCreateInstance(o)[e]()}))};class W extends B{static get NAME(){return""alert""}close(){if(j.trigger(this._element,""close.bs.alert"").defaultPrevented)return;this._element.classList.remove(""show"");const t=this._element.classList.contains(""fade"");this._queueCallback((()=>this._destroyElement()),this._element,t)}_destroyElement(){this._element.remove(),j.trigger(this._element,""closed.bs.alert""),this.dispose()}static jQueryInterface(t){return this.each((function(){const e=W.getOrCreateInstance(this);if(""string""==typeof t){if(void 0===e[t]||t.startsWith(""_"")||""constructor""===t)throw new TypeError(`No method named ""${t}""`);e[t](this)}}))}}R(W,""close""),g(W);const $='[data-bs-toggle=""button""]';class z extends B{static get NAME(){return""button""}toggle(){this._element.setAttribute(""aria-pressed"",this._element.classList.toggle(""active""))}static jQueryInterface(t){return this.each((function(){const e=z.getOrCreateInstance(this);""toggle""===t&&e[t]()}))}}function q(t){return""true""===t||""false""!==t&&(t===Number(t).toString()?Number(t):""""===t||""null""===t?null:t)}function F(t){return t.replace(/[A-Z]/g,(t=>`-${t.toLowerCase()}`))}j.on(document,""click.bs.button.data-api"",$,(t=>{t.preventDefault();const e=t.target.closest($);z.getOrCreateInstance(e).toggle()})),g(z);const U={setDataAttribute(t,e,i){t.setAttribute(`data-bs-${F(e)}`,i)},removeDataAttribute(t,e){t.removeAttribute(`data-bs-${F(e)}`)},getDataAttributes(t){if(!t)return{};const e={};return Object.keys(t.dataset).filter((t=>t.startsWith(""bs""))).forEach((i=>{let n=i.replace(/^bs/,"""");n=n.charAt(0).toLowerCase()+n.slice(1,n.length),e[n]=q(t.dataset[i])})),e},getDataAttribute:(t,e)=>q(t.getAttribute(`data-bs-${F(e)}`)),offset(t){const e=t.getBoundingClientRect();return{top:e.top+window.pageYOffset,left:e.left+window.pageXOffset}},position:t=>({top:t.offsetTop,left:t.offsetLeft})},V={find:(t,e=document.documentElement)=>[].concat(...Element.prototype.querySelectorAll.call(e,t)),findOne:(t,e=document.documentElement)=>Element.prototype.querySelector.call(e,t),children:(t,e)=>[].concat(...t.children).filter((t=>t.matches(e))),parents(t,e){const i=[];let n=t.parentNode;for(;n&&n.nodeType===Node.ELEMENT_NODE&&3!==n.nodeType;)n.matches(e)&&i.push(n),n=n.parentNode;return i},prev(t,e){let i=t.previousElementSibling;for(;i;){if(i.matches(e))return[i];i=i.previousElementSibling}return[]},next(t,e){let i=t.nextElementSibling;for(;i;){if(i.matches(e))return[i];i=i.nextElementSibling}return[]},focusableChildren(t){const e=[""a"",""button"",""input"",""textarea"",""select"",""details"",""[tabindex]"",'[contenteditable=""true""]'].map((t=>`${t}:not([tabindex^=""-""])`)).join("", "");return this.find(e,t).filter((t=>!c(t)&&l(t)))}},K=""carousel"",X={interval:5e3,keyboard:!0,slide:!1,pause:""hover"",wrap:!0,touch:!0},Y={interval:""(number|boolean)"",keyboard:""boolean"",slide:""(boolean|string)"",pause:""(string|boolean)"",wrap:""boolean"",touch:""boolean""},Q=""next"",G=""prev"",Z=""left"",J=""right"",tt={ArrowLeft:J,ArrowRight:Z},et=""slid.bs.carousel"",it=""active"",nt="".active.carousel-item"";class st extends B{constructor(t,e){super(t),this._items=null,this._interval=null,this._activeElement=null,this._isPaused=!1,this._isSliding=!1,this.touchTimeout=null,this.touchStartX=0,this.touchDeltaX=0,this._config=this._getConfig(e),this._indicatorsElement=V.findOne("".carousel-indicators"",this._element),this._touchSupported=""ontouchstart""in document.documentElement||navigator.maxTouchPoints>0,this._pointerEvent=Boolean(window.PointerEvent),this._addEventListeners()}static get Default(){return X}static get NAME(){return K}next(){this._slide(Q)}nextWhenVisible(){!document.hidden&&l(this._element)&&this.next()}prev(){this._slide(G)}pause(t){t||(this._isPaused=!0),V.findOne("".carousel-item-next, .carousel-item-prev"",this._element)&&(s(this._element),this.cycle(!0)),clearInterval(this._interval),this._interval=null}cycle(t){t||(this._isPaused=!1),this._interval&&(clearInterval(this._interval),this._interval=null),this._config&&this._config.interval&&!this._isPaused&&(this._updateInterval(),this._interval=setInterval((document.visibilityState?this.nextWhenVisible:this.next).bind(this),this._config.interval))}to(t){this._activeElement=V.findOne(nt,this._element);const e=this._getItemIndex(this._activeElement);if(t>this._items.length-1||t<0)return;if(this._isSliding)return void j.one(this._element,et,(()=>this.to(t)));if(e===t)return this.pause(),void this.cycle();const i=t>e?Q:G;this._slide(i,this._items[t])}_getConfig(t){return t={...X,...U.getDataAttributes(this._element),...""object""==typeof t?t:{}},a(K,t,Y),t}_handleSwipe(){const t=Math.abs(this.touchDeltaX);if(t<=40)return;const e=t/this.touchDeltaX;this.touchDeltaX=0,e&&this._slide(e>0?J:Z)}_addEventListeners(){this._config.keyboard&&j.on(this._element,""keydown.bs.carousel"",(t=>this._keydown(t))),""hover""===this._config.pause&&(j.on(this._element,""mouseenter.bs.carousel"",(t=>this.pause(t))),j.on(this._element,""mouseleave.bs.carousel"",(t=>this.cycle(t)))),this._config.touch&&this._touchSupported&&this._addTouchEventListeners()}_addTouchEventListeners(){const t=t=>this._pointerEvent&&(""pen""===t.pointerType||""touch""===t.pointerType),e=e=>{t(e)?this.touchStartX=e.clientX:this._pointerEvent||(this.touchStartX=e.touches[0].clientX)},i=t=>{this.touchDeltaX=t.touches&&t.touches.length>1?0:t.touches[0].clientX-this.touchStartX},n=e=>{t(e)&&(this.touchDeltaX=e.clientX-this.touchStartX),this._handleSwipe(),""hover""===this._config.pause&&(this.pause(),this.touchTimeout&&clearTimeout(this.touchTimeout),this.touchTimeout=setTimeout((t=>this.cycle(t)),500+this._config.interval))};V.find("".carousel-item img"",this._element).forEach((t=>{j.on(t,""dragstart.bs.carousel"",(t=>t.preventDefault()))})),this._pointerEvent?(j.on(this._element,""pointerdown.bs.carousel"",(t=>e(t))),j.on(this._element,""pointerup.bs.carousel"",(t=>n(t))),this._element.classList.add(""pointer-event"")):(j.on(this._element,""touchstart.bs.carousel"",(t=>e(t))),j.on(this._element,""touchmove.bs.carousel"",(t=>i(t))),j.on(this._element,""touchend.bs.carousel"",(t=>n(t))))}_keydown(t){if(/input|textarea/i.test(t.target.tagName))return;const e=tt[t.key];e&&(t.preventDefault(),this._slide(e))}_getItemIndex(t){return this._items=t&&t.parentNode?V.find("".carousel-item"",t.parentNode):[],this._items.indexOf(t)}_getItemByOrder(t,e){const i=t===Q;return v(this._items,e,i,this._config.wrap)}_triggerSlideEvent(t,e){const i=this._getItemIndex(t),n=this._getItemIndex(V.findOne(nt,this._element));return j.trigger(this._element,""slide.bs.carousel"",{relatedTarget:t,direction:e,from:n,to:i})}_setActiveIndicatorElement(t){if(this._indicatorsElement){const e=V.findOne("".active"",this._indicatorsElement);e.classList.remove(it),e.removeAttribute(""aria-current"");const i=V.find(""[data-bs-target]"",this._indicatorsElement);for(let e=0;e<i.length;e++)if(Number.parseInt(i[e].getAttribute(""data-bs-slide-to""),10)===this._getItemIndex(t)){i[e].classList.add(it),i[e].setAttribute(""aria-current"",""true"");break}}}_updateInterval(){const t=this._activeElement||V.findOne(nt,this._element);if(!t)return;const e=Number.parseInt(t.getAttribute(""data-bs-interval""),10);e?(this._config.defaultInterval=this._config.defaultInterval||this._config.interval,this._config.interval=e):this._config.interval=this._config.defaultInterval||this._config.interval}_slide(t,e){const i=this._directionToOrder(t),n=V.findOne(nt,this._element),s=this._getItemIndex(n),o=e||this._getItemByOrder(i,n),r=this._getItemIndex(o),a=Boolean(this._interval),l=i===Q,c=l?""carousel-item-start"":""carousel-item-end"",h=l?""carousel-item-next"":""carousel-item-prev"",d=this._orderToDirection(i);if(o&&o.classList.contains(it))return void(this._isSliding=!1);if(this._isSliding)return;if(this._triggerSlideEvent(o,d).defaultPrevented)return;if(!n||!o)return;this._isSliding=!0,a&&this.pause(),this._setActiveIndicatorElement(o),this._activeElement=o;const f=()=>{j.trigger(this._element,et,{relatedTarget:o,direction:d,from:s,to:r})};if(this._element.classList.contains(""slide"")){o.classList.add(h),u(o),n.classList.add(c),o.classList.add(c);const t=()=>{o.classList.remove(c,h),o.classList.add(it),n.classList.remove(it,h,c),this._isSliding=!1,setTimeout(f,0)};this._queueCallback(t,n,!0)}else n.classList.remove(it),o.classList.add(it),this._isSliding=!1,f();a&&this.cycle()}_directionToOrder(t){return[J,Z].includes(t)?m()?t===Z?G:Q:t===Z?Q:G:t}_orderToDirection(t){return[Q,G].includes(t)?m()?t===G?Z:J:t===G?J:Z:t}static carouselInterface(t,e){const i=st.getOrCreateInstance(t,e);let{_config:n}=i;""object""==typeof e&&(n={...n,...e});const s=""string""==typeof e?e:n.slide;if(""number""==typeof e)i.to(e);else if(""string""==typeof s){if(void 0===i[s])throw new TypeError(`No method named ""${s}""`);i[s]()}else n.interval&&n.ride&&(i.pause(),i.cycle())}static jQueryInterface(t){return this.each((function(){st.carouselInterface(this,t)}))}static dataApiClickHandler(t){const e=n(this);if(!e||!e.classList.contains(""carousel""))return;const i={...U.getDataAttributes(e),...U.getDataAttributes(this)},s=this.getAttribute(""data-bs-slide-to"");s&&(i.interval=!1),st.carouselInterface(e,i),s&&st.getInstance(e).to(s),t.preventDefault()}}j.on(document,""click.bs.carousel.data-api"",""[data-bs-slide], [data-bs-slide-to]"",st.dataApiClickHandler),j.on(window,""load.bs.carousel.data-api"",(()=>{const t=V.find('[data-bs-ride=""carousel""]');for(let e=0,i=t.length;e<i;e++)st.carouselInterface(t[e],st.getInstance(t[e]))})),g(st);const ot=""collapse"",rt={toggle:!0,parent:null},at={toggle:""boolean"",parent:""(null|element)""},lt=""show"",ct=""collapse"",ht=""collapsing"",dt=""collapsed"",ut="":scope .collapse .collapse"",ft='[data-bs-toggle=""collapse""]';class pt extends B{constructor(t,e){super(t),this._isTransitioning=!1,this._config=this._getConfig(e),this._triggerArray=[];const n=V.find(ft);for(let t=0,e=n.length;t<e;t++){const e=n[t],s=i(e),o=V.find(s).filter((t=>t===this._element));null!==s&&o.length&&(this._selector=s,this._triggerArray.push(e))}this._initializeChildren(),this._config.parent||this._addAriaAndCollapsedClass(this._triggerArray,this._isShown()),this._config.toggle&&this.toggle()}static get Default(){return rt}static get NAME(){return ot}toggle(){this._isShown()?this.hide():this.show()}show(){if(this._isTransitioning||this._isShown())return;let t,e=[];if(this._config.parent){const t=V.find(ut,this._config.parent);e=V.find("".collapse.show, .collapse.collapsing"",this._config.parent).filter((e=>!t.includes(e)))}const i=V.findOne(this._selector);if(e.length){const n=e.find((t=>i!==t));if(t=n?pt.getInstance(n):null,t&&t._isTransitioning)return}if(j.trigger(this._element,""show.bs.collapse"").defaultPrevented)return;e.forEach((e=>{i!==e&&pt.getOrCreateInstance(e,{toggle:!1}).hide(),t||H.set(e,""bs.collapse"",null)}));const n=this._getDimension();this._element.classList.remove(ct),this._element.classList.add(ht),this._element.style[n]=0,this._addAriaAndCollapsedClass(this._triggerArray,!0),this._isTransitioning=!0;const s=`scroll${n[0].toUpperCase()+n.slice(1)}`;this._queueCallback((()=>{this._isTransitioning=!1,this._element.classList.remove(ht),this._element.classList.add(ct,lt),this._element.style[n]="""",j.trigger(this._element,""shown.bs.collapse"")}),this._element,!0),this._element.style[n]=`${this._element[s]}px`}hide(){if(this._isTransitioning||!this._isShown())return;if(j.trigger(this._element,""hide.bs.collapse"").defaultPrevented)return;const t=this._getDimension();this._element.style[t]=`${this._element.getBoundingClientRect()[t]}px`,u(this._element),this._element.classList.add(ht),this._element.classList.remove(ct,lt);const e=this._triggerArray.length;for(let t=0;t<e;t++){const e=this._triggerArray[t],i=n(e);i&&!this._isShown(i)&&this._addAriaAndCollapsedClass([e],!1)}this._isTransitioning=!0,this._element.style[t]="""",this._queueCallback((()=>{this._isTransitioning=!1,this._element.classList.remove(ht),this._element.classList.add(ct),j.trigger(this._element,""hidden.bs.collapse"")}),this._element,!0)}_isShown(t=this._element){return t.classList.contains(lt)}_getConfig(t){return(t={...rt,...U.getDataAttributes(this._element),...t}).toggle=Boolean(t.toggle),t.parent=r(t.parent),a(ot,t,at),t}_getDimension(){return this._element.classList.contains(""collapse-horizontal"")?""width"":""height""}_initializeChildren(){if(!this._config.parent)return;const t=V.find(ut,this._config.parent);V.find(ft,this._config.parent).filter((e=>!t.includes(e))).forEach((t=>{const e=n(t);e&&this._addAriaAndCollapsedClass([t],this._isShown(e))}))}_addAriaAndCollapsedClass(t,e){t.length&&t.forEach((t=>{e?t.classList.remove(dt):t.classList.add(dt),t.setAttribute(""aria-expanded"",e)}))}static jQueryInterface(t){return this.each((function(){const e={};""string""==typeof t&&/show|hide/.test(t)&&(e.toggle=!1);const i=pt.getOrCreateInstance(this,e);if(""string""==typeof t){if(void 0===i[t])throw new TypeError(`No method named ""${t}""`);i[t]()}}))}}j.on(document,""click.bs.collapse.data-api"",ft,(function(t){(""A""===t.target.tagName||t.delegateTarget&&""A""===t.delegateTarget.tagName)&&t.preventDefault();const e=i(this);V.find(e).forEach((t=>{pt.getOrCreateInstance(t,{toggle:!1}).toggle()}))})),g(pt);var mt=""top"",gt=""bottom"",_t=""right"",bt=""left"",vt=""auto"",yt=[mt,gt,_t,bt],wt=""start"",Et=""end"",At=""clippingParents"",Tt=""viewport"",Ot=""popper"",Ct=""reference"",kt=yt.reduce((function(t,e){return t.concat([e+""-""+wt,e+""-""+Et])}),[]),Lt=[].concat(yt,[vt]).reduce((function(t,e){return t.concat([e,e+""-""+wt,e+""-""+Et])}),[]),xt=""beforeRead"",Dt=""read"",St=""afterRead"",Nt=""beforeMain"",It=""main"",Pt=""afterMain"",jt=""beforeWrite"",Mt=""write"",Ht=""afterWrite"",Bt=[xt,Dt,St,Nt,It,Pt,jt,Mt,Ht];function Rt(t){return t?(t.nodeName||"""").toLowerCase():null}function Wt(t){if(null==t)return window;if(""[object Window]""!==t.toString()){var e=t.ownerDocument;return e&&e.defaultView||window}return t}function $t(t){return t instanceof Wt(t).Element||t instanceof Element}function zt(t){return t instanceof Wt(t).HTMLElement||t instanceof HTMLElement}function qt(t){return""undefined""!=typeof ShadowRoot&&(t instanceof Wt(t).ShadowRoot||t instanceof ShadowRoot)}const Ft={name:""applyStyles"",enabled:!0,phase:""write"",fn:function(t){var e=t.state;Object.keys(e.elements).forEach((function(t){var i=e.styles[t]||{},n=e.attributes[t]||{},s=e.elements[t];zt(s)&&Rt(s)&&(Object.assign(s.style,i),Object.keys(n).forEach((function(t){var e=n[t];!1===e?s.removeAttribute(t):s.setAttribute(t,!0===e?"""":e)})))}))},effect:function(t){var e=t.state,i={popper:{position:e.options.strategy,left:""0"",top:""0"",margin:""0""},arrow:{position:""absolute""},reference:{}};return Object.assign(e.elements.popper.style,i.popper),e.styles=i,e.elements.arrow&&Object.assign(e.elements.arrow.style,i.arrow),function(){Object.keys(e.elements).forEach((function(t){var n=e.elements[t],s=e.attributes[t]||{},o=Object.keys(e.styles.hasOwnProperty(t)?e.styles[t]:i[t]).reduce((function(t,e){return t[e]="""",t}),{});zt(n)&&Rt(n)&&(Object.assign(n.style,o),Object.keys(s).forEach((function(t){n.removeAttribute(t)})))}))}},requires:[""computeStyles""]};function Ut(t){return t.split(""-"")[0]}function Vt(t,e){var i=t.getBoundingClientRect();return{width:i.width/1,height:i.height/1,top:i.top/1,right:i.right/1,bottom:i.bottom/1,left:i.left/1,x:i.left/1,y:i.top/1}}function Kt(t){var e=Vt(t),i=t.offsetWidth,n=t.offsetHeight;return Math.abs(e.width-i)<=1&&(i=e.width),Math.abs(e.height-n)<=1&&(n=e.height),{x:t.offsetLeft,y:t.offsetTop,width:i,height:n}}function Xt(t,e){var i=e.getRootNode&&e.getRootNode();if(t.contains(e))return!0;if(i&&qt(i)){var n=e;do{if(n&&t.isSameNode(n))return!0;n=n.parentNode||n.host}while(n)}return!1}function Yt(t){return Wt(t).getComputedStyle(t)}function Qt(t){return[""table"",""td"",""th""].indexOf(Rt(t))>=0}function Gt(t){return(($t(t)?t.ownerDocument:t.document)||window.document).documentElement}function Zt(t){return""html""===Rt(t)?t:t.assignedSlot||t.parentNode||(qt(t)?t.host:null)||Gt(t)}function Jt(t){return zt(t)&&""fixed""!==Yt(t).position?t.offsetParent:null}function te(t){for(var e=Wt(t),i=Jt(t);i&&Qt(i)&&""static""===Yt(i).position;)i=Jt(i);return i&&(""html""===Rt(i)||""body""===Rt(i)&&""static""===Yt(i).position)?e:i||function(t){var e=-1!==navigator.userAgent.toLowerCase().indexOf(""firefox"");if(-1!==navigator.userAgent.indexOf(""Trident"")&&zt(t)&&""fixed""===Yt(t).position)return null;for(var i=Zt(t);zt(i)&&[""html"",""body""].indexOf(Rt(i))<0;){var n=Yt(i);if(""none""!==n.transform||""none""!==n.perspective||""paint""===n.contain||-1!==[""transform"",""perspective""].indexOf(n.willChange)||e&&""filter""===n.willChange||e&&n.filter&&""none""!==n.filter)return i;i=i.parentNode}return null}(t)||e}function ee(t){return[""top"",""bottom""].indexOf(t)>=0?""x"":""y""}var ie=Math.max,ne=Math.min,se=Math.round;function oe(t,e,i){return ie(t,ne(e,i))}function re(t){return Object.assign({},{top:0,right:0,bottom:0,left:0},t)}function ae(t,e){return e.reduce((function(e,i){return e[i]=t,e}),{})}const le={name:""arrow"",enabled:!0,phase:""main"",fn:function(t){var e,i=t.state,n=t.name,s=t.options,o=i.elements.arrow,r=i.modifiersData.popperOffsets,a=Ut(i.placement),l=ee(a),c=[bt,_t].indexOf(a)>=0?""height"":""width"";if(o&&r){var h=function(t,e){return re(""number""!=typeof(t=""function""==typeof t?t(Object.assign({},e.rects,{placement:e.placement})):t)?t:ae(t,yt))}(s.padding,i),d=Kt(o),u=""y""===l?mt:bt,f=""y""===l?gt:_t,p=i.rects.reference[c]+i.rects.reference[l]-r[l]-i.rects.popper[c],m=r[l]-i.rects.reference[l],g=te(o),_=g?""y""===l?g.clientHeight||0:g.clientWidth||0:0,b=p/2-m/2,v=h[u],y=_-d[c]-h[f],w=_/2-d[c]/2+b,E=oe(v,w,y),A=l;i.modifiersData[n]=((e={})[A]=E,e.centerOffset=E-w,e)}},effect:function(t){var e=t.state,i=t.options.element,n=void 0===i?""[data-popper-arrow]"":i;null!=n&&(""string""!=typeof n||(n=e.elements.popper.querySelector(n)))&&Xt(e.elements.popper,n)&&(e.elements.arrow=n)},requires:[""popperOffsets""],requiresIfExists:[""preventOverflow""]};function ce(t){return t.split(""-"")[1]}var he={top:""auto"",right:""auto"",bottom:""auto"",left:""auto""};function de(t){var e,i=t.popper,n=t.popperRect,s=t.placement,o=t.variation,r=t.offsets,a=t.position,l=t.gpuAcceleration,c=t.adaptive,h=t.roundOffsets,d=!0===h?function(t){var e=t.x,i=t.y,n=window.devicePixelRatio||1;return{x:se(se(e*n)/n)||0,y:se(se(i*n)/n)||0}}(r):""function""==typeof h?h(r):r,u=d.x,f=void 0===u?0:u,p=d.y,m=void 0===p?0:p,g=r.hasOwnProperty(""x""),_=r.hasOwnProperty(""y""),b=bt,v=mt,y=window;if(c){var w=te(i),E=""clientHeight"",A=""clientWidth"";w===Wt(i)&&""static""!==Yt(w=Gt(i)).position&&""absolute""===a&&(E=""scrollHeight"",A=""scrollWidth""),w=w,s!==mt&&(s!==bt&&s!==_t||o!==Et)||(v=gt,m-=w[E]-n.height,m*=l?1:-1),s!==bt&&(s!==mt&&s!==gt||o!==Et)||(b=_t,f-=w[A]-n.width,f*=l?1:-1)}var T,O=Object.assign({position:a},c&&he);return l?Object.assign({},O,((T={})[v]=_?""0"":"""",T[b]=g?""0"":"""",T.transform=(y.devicePixelRatio||1)<=1?""translate(""+f+""px, ""+m+""px)"":""translate3d(""+f+""px, ""+m+""px, 0)"",T)):Object.assign({},O,((e={})[v]=_?m+""px"":"""",e[b]=g?f+""px"":"""",e.transform="""",e))}const ue={name:""computeStyles"",enabled:!0,phase:""beforeWrite"",fn:function(t){var e=t.state,i=t.options,n=i.gpuAcceleration,s=void 0===n||n,o=i.adaptive,r=void 0===o||o,a=i.roundOffsets,l=void 0===a||a,c={placement:Ut(e.placement),variation:ce(e.placement),popper:e.elements.popper,popperRect:e.rects.popper,gpuAcceleration:s};null!=e.modifiersData.popperOffsets&&(e.styles.popper=Object.assign({},e.styles.popper,de(Object.assign({},c,{offsets:e.modifiersData.popperOffsets,position:e.options.strategy,adaptive:r,roundOffsets:l})))),null!=e.modifiersData.arrow&&(e.styles.arrow=Object.assign({},e.styles.arrow,de(Object.assign({},c,{offsets:e.modifiersData.arrow,position:""absolute"",adaptive:!1,roundOffsets:l})))),e.attributes.popper=Object.assign({},e.attributes.popper,{""data-popper-placement"":e.placement})},data:{}};var fe={passive:!0};const pe={name:""eventListeners"",enabled:!0,phase:""write"",fn:function(){},effect:function(t){var e=t.state,i=t.instance,n=t.options,s=n.scroll,o=void 0===s||s,r=n.resize,a=void 0===r||r,l=Wt(e.elements.popper),c=[].concat(e.scrollParents.reference,e.scrollParents.popper);return o&&c.forEach((function(t){t.addEventListener(""scroll"",i.update,fe)})),a&&l.addEventListener(""resize"",i.update,fe),function(){o&&c.forEach((function(t){t.removeEventListener(""scroll"",i.update,fe)})),a&&l.removeEventListener(""resize"",i.update,fe)}},data:{}};var me={left:""right"",right:""left"",bottom:""top"",top:""bottom""};function ge(t){return t.replace(/left|right|bottom|top/g,(function(t){return me[t]}))}var _e={start:""end"",end:""start""};function be(t){return t.replace(/start|end/g,(function(t){return _e[t]}))}function ve(t){var e=Wt(t);return{scrollLeft:e.pageXOffset,scrollTop:e.pageYOffset}}function ye(t){return Vt(Gt(t)).left+ve(t).scrollLeft}function we(t){var e=Yt(t),i=e.overflow,n=e.overflowX,s=e.overflowY;return/auto|scroll|overlay|hidden/.test(i+s+n)}function Ee(t){return[""html"",""body"",""#document""].indexOf(Rt(t))>=0?t.ownerDocument.body:zt(t)&&we(t)?t:Ee(Zt(t))}function Ae(t,e){var i;void 0===e&&(e=[]);var n=Ee(t),s=n===(null==(i=t.ownerDocument)?void 0:i.body),o=Wt(n),r=s?[o].concat(o.visualViewport||[],we(n)?n:[]):n,a=e.concat(r);return s?a:a.concat(Ae(Zt(r)))}function Te(t){return Object.assign({},t,{left:t.x,top:t.y,right:t.x+t.width,bottom:t.y+t.height})}function Oe(t,e){return e===Tt?Te(function(t){var e=Wt(t),i=Gt(t),n=e.visualViewport,s=i.clientWidth,o=i.clientHeight,r=0,a=0;return n&&(s=n.width,o=n.height,/^((?!chrome|android).)*safari/i.test(navigator.userAgent)||(r=n.offsetLeft,a=n.offsetTop)),{width:s,height:o,x:r+ye(t),y:a}}(t)):zt(e)?function(t){var e=Vt(t);return e.top=e.top+t.clientTop,e.left=e.left+t.clientLeft,e.bottom=e.top+t.clientHeight,e.right=e.left+t.clientWidth,e.width=t.clientWidth,e.height=t.clientHeight,e.x=e.left,e.y=e.top,e}(e):Te(function(t){var e,i=Gt(t),n=ve(t),s=null==(e=t.ownerDocument)?void 0:e.body,o=ie(i.scrollWidth,i.clientWidth,s?s.scrollWidth:0,s?s.clientWidth:0),r=ie(i.scrollHeight,i.clientHeight,s?s.scrollHeight:0,s?s.clientHeight:0),a=-n.scrollLeft+ye(t),l=-n.scrollTop;return""rtl""===Yt(s||i).direction&&(a+=ie(i.clientWidth,s?s.clientWidth:0)-o),{width:o,height:r,x:a,y:l}}(Gt(t)))}function Ce(t){var e,i=t.reference,n=t.element,s=t.placement,o=s?Ut(s):null,r=s?ce(s):null,a=i.x+i.width/2-n.width/2,l=i.y+i.height/2-n.height/2;switch(o){case mt:e={x:a,y:i.y-n.height};break;case gt:e={x:a,y:i.y+i.height};break;case _t:e={x:i.x+i.width,y:l};break;case bt:e={x:i.x-n.width,y:l};break;default:e={x:i.x,y:i.y}}var c=o?ee(o):null;if(null!=c){var h=""y""===c?""height"":""width"";switch(r){case wt:e[c]=e[c]-(i[h]/2-n[h]/2);break;case Et:e[c]=e[c]+(i[h]/2-n[h]/2)}}return e}function ke(t,e){void 0===e&&(e={});var i=e,n=i.placement,s=void 0===n?t.placement:n,o=i.boundary,r=void 0===o?At:o,a=i.rootBoundary,l=void 0===a?Tt:a,c=i.elementContext,h=void 0===c?Ot:c,d=i.altBoundary,u=void 0!==d&&d,f=i.padding,p=void 0===f?0:f,m=re(""number""!=typeof p?p:ae(p,yt)),g=h===Ot?Ct:Ot,_=t.rects.popper,b=t.elements[u?g:h],v=function(t,e,i){var n=""clippingParents""===e?function(t){var e=Ae(Zt(t)),i=[""absolute"",""fixed""].indexOf(Yt(t).position)>=0&&zt(t)?te(t):t;return $t(i)?e.filter((function(t){return $t(t)&&Xt(t,i)&&""body""!==Rt(t)})):[]}(t):[].concat(e),s=[].concat(n,[i]),o=s[0],r=s.reduce((function(e,i){var n=Oe(t,i);return e.top=ie(n.top,e.top),e.right=ne(n.right,e.right),e.bottom=ne(n.bottom,e.bottom),e.left=ie(n.left,e.left),e}),Oe(t,o));return r.width=r.right-r.left,r.height=r.bottom-r.top,r.x=r.left,r.y=r.top,r}($t(b)?b:b.contextElement||Gt(t.elements.popper),r,l),y=Vt(t.elements.reference),w=Ce({reference:y,element:_,strategy:""absolute"",placement:s}),E=Te(Object.assign({},_,w)),A=h===Ot?E:y,T={top:v.top-A.top+m.top,bottom:A.bottom-v.bottom+m.bottom,left:v.left-A.left+m.left,right:A.right-v.right+m.right},O=t.modifiersData.offset;if(h===Ot&&O){var C=O[s];Object.keys(T).forEach((function(t){var e=[_t,gt].indexOf(t)>=0?1:-1,i=[mt,gt].indexOf(t)>=0?""y"":""x"";T[t]+=C[i]*e}))}return T}function Le(t,e){void 0===e&&(e={});var i=e,n=i.placement,s=i.boundary,o=i.rootBoundary,r=i.padding,a=i.flipVariations,l=i.allowedAutoPlacements,c=void 0===l?Lt:l,h=ce(n),d=h?a?kt:kt.filter((function(t){return ce(t)===h})):yt,u=d.filter((function(t){return c.indexOf(t)>=0}));0===u.length&&(u=d);var f=u.reduce((function(e,i){return e[i]=ke(t,{placement:i,boundary:s,rootBoundary:o,padding:r})[Ut(i)],e}),{});return Object.keys(f).sort((function(t,e){return f[t]-f[e]}))}const xe={name:""flip"",enabled:!0,phase:""main"",fn:function(t){var e=t.state,i=t.options,n=t.name;if(!e.modifiersData[n]._skip){for(var s=i.mainAxis,o=void 0===s||s,r=i.altAxis,a=void 0===r||r,l=i.fallbackPlacements,c=i.padding,h=i.boundary,d=i.rootBoundary,u=i.altBoundary,f=i.flipVariations,p=void 0===f||f,m=i.allowedAutoPlacements,g=e.options.placement,_=Ut(g),b=l||(_!==g&&p?function(t){if(Ut(t)===vt)return[];var e=ge(t);return[be(t),e,be(e)]}(g):[ge(g)]),v=[g].concat(b).reduce((function(t,i){return t.concat(Ut(i)===vt?Le(e,{placement:i,boundary:h,rootBoundary:d,padding:c,flipVariations:p,allowedAutoPlacements:m}):i)}),[]),y=e.rects.reference,w=e.rects.popper,E=new Map,A=!0,T=v[0],O=0;O<v.length;O++){var C=v[O],k=Ut(C),L=ce(C)===wt,x=[mt,gt].indexOf(k)>=0,D=x?""width"":""height"",S=ke(e,{placement:C,boundary:h,rootBoundary:d,altBoundary:u,padding:c}),N=x?L?_t:bt:L?gt:mt;y[D]>w[D]&&(N=ge(N));var I=ge(N),P=[];if(o&&P.push(S[k]<=0),a&&P.push(S[N]<=0,S[I]<=0),P.every((function(t){return t}))){T=C,A=!1;break}E.set(C,P)}if(A)for(var j=function(t){var e=v.find((function(e){var i=E.get(e);if(i)return i.slice(0,t).every((function(t){return t}))}));if(e)return T=e,""break""},M=p?3:1;M>0&&""break""!==j(M);M--);e.placement!==T&&(e.modifiersData[n]._skip=!0,e.placement=T,e.reset=!0)}},requiresIfExists:[""offset""],data:{_skip:!1}};function De(t,e,i){return void 0===i&&(i={x:0,y:0}),{top:t.top-e.height-i.y,right:t.right-e.width+i.x,bottom:t.bottom-e.height+i.y,left:t.left-e.width-i.x}}function Se(t){return[mt,_t,gt,bt].some((function(e){return t[e]>=0}))}const Ne={name:""hide"",enabled:!0,phase:""main"",requiresIfExists:[""preventOverflow""],fn:function(t){var e=t.state,i=t.name,n=e.rects.reference,s=e.rects.popper,o=e.modifiersData.preventOverflow,r=ke(e,{elementContext:""reference""}),a=ke(e,{altBoundary:!0}),l=De(r,n),c=De(a,s,o),h=Se(l),d=Se(c);e.modifiersData[i]={referenceClippingOffsets:l,popperEscapeOffsets:c,isReferenceHidden:h,hasPopperEscaped:d},e.attributes.popper=Object.assign({},e.attributes.popper,{""data-popper-reference-hidden"":h,""data-popper-escaped"":d})}},Ie={name:""offset"",enabled:!0,phase:""main"",requires:[""popperOffsets""],fn:function(t){var e=t.state,i=t.options,n=t.name,s=i.offset,o=void 0===s?[0,0]:s,r=Lt.reduce((function(t,i){return t[i]=function(t,e,i){var n=Ut(t),s=[bt,mt].indexOf(n)>=0?-1:1,o=""function""==typeof i?i(Object.assign({},e,{placement:t})):i,r=o[0],a=o[1];return r=r||0,a=(a||0)*s,[bt,_t].indexOf(n)>=0?{x:a,y:r}:{x:r,y:a}}(i,e.rects,o),t}),{}),a=r[e.placement],l=a.x,c=a.y;null!=e.modifiersData.popperOffsets&&(e.modifiersData.popperOffsets.x+=l,e.modifiersData.popperOffsets.y+=c),e.modifiersData[n]=r}},Pe={name:""popperOffsets"",enabled:!0,phase:""read"",fn:function(t){var e=t.state,i=t.name;e.modifiersData[i]=Ce({reference:e.rects.reference,element:e.rects.popper,strategy:""absolute"",placement:e.placement})},data:{}},je={name:""preventOverflow"",enabled:!0,phase:""main"",fn:function(t){var e=t.state,i=t.options,n=t.name,s=i.mainAxis,o=void 0===s||s,r=i.altAxis,a=void 0!==r&&r,l=i.boundary,c=i.rootBoundary,h=i.altBoundary,d=i.padding,u=i.tether,f=void 0===u||u,p=i.tetherOffset,m=void 0===p?0:p,g=ke(e,{boundary:l,rootBoundary:c,padding:d,altBoundary:h}),_=Ut(e.placement),b=ce(e.placement),v=!b,y=ee(_),w=""x""===y?""y"":""x"",E=e.modifiersData.popperOffsets,A=e.rects.reference,T=e.rects.popper,O=""function""==typeof m?m(Object.assign({},e.rects,{placement:e.placement})):m,C={x:0,y:0};if(E){if(o||a){var k=""y""===y?mt:bt,L=""y""===y?gt:_t,x=""y""===y?""height"":""width"",D=E[y],S=E[y]+g[k],N=E[y]-g[L],I=f?-T[x]/2:0,P=b===wt?A[x]:T[x],j=b===wt?-T[x]:-A[x],M=e.elements.arrow,H=f&&M?Kt(M):{width:0,height:0},B=e.modifiersData[""arrow#persistent""]?e.modifiersData[""arrow#persistent""].padding:{top:0,right:0,bottom:0,left:0},R=B[k],W=B[L],$=oe(0,A[x],H[x]),z=v?A[x]/2-I-$-R-O:P-$-R-O,q=v?-A[x]/2+I+$+W+O:j+$+W+O,F=e.elements.arrow&&te(e.elements.arrow),U=F?""y""===y?F.clientTop||0:F.clientLeft||0:0,V=e.modifiersData.offset?e.modifiersData.offset[e.placement][y]:0,K=E[y]+z-V-U,X=E[y]+q-V;if(o){var Y=oe(f?ne(S,K):S,D,f?ie(N,X):N);E[y]=Y,C[y]=Y-D}if(a){var Q=""x""===y?mt:bt,G=""x""===y?gt:_t,Z=E[w],J=Z+g[Q],tt=Z-g[G],et=oe(f?ne(J,K):J,Z,f?ie(tt,X):tt);E[w]=et,C[w]=et-Z}}e.modifiersData[n]=C}},requiresIfExists:[""offset""]};function Me(t,e,i){void 0===i&&(i=!1);var n=zt(e);zt(e)&&function(t){var e=t.getBoundingClientRect();e.width,t.offsetWidth,e.height,t.offsetHeight}(e);var s,o,r=Gt(e),a=Vt(t),l={scrollLeft:0,scrollTop:0},c={x:0,y:0};return(n||!n&&!i)&&((""body""!==Rt(e)||we(r))&&(l=(s=e)!==Wt(s)&&zt(s)?{scrollLeft:(o=s).scrollLeft,scrollTop:o.scrollTop}:ve(s)),zt(e)?((c=Vt(e)).x+=e.clientLeft,c.y+=e.clientTop):r&&(c.x=ye(r))),{x:a.left+l.scrollLeft-c.x,y:a.top+l.scrollTop-c.y,width:a.width,height:a.height}}function He(t){var e=new Map,i=new Set,n=[];function s(t){i.add(t.name),[].concat(t.requires||[],t.requiresIfExists||[]).forEach((function(t){if(!i.has(t)){var n=e.get(t);n&&s(n)}})),n.push(t)}return t.forEach((function(t){e.set(t.name,t)})),t.forEach((function(t){i.has(t.name)||s(t)})),n}var Be={placement:""bottom"",modifiers:[],strategy:""absolute""};function Re(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];return!e.some((function(t){return!(t&&""function""==typeof t.getBoundingClientRect)}))}function We(t){void 0===t&&(t={});var e=t,i=e.defaultModifiers,n=void 0===i?[]:i,s=e.defaultOptions,o=void 0===s?Be:s;return function(t,e,i){void 0===i&&(i=o);var s,r,a={placement:""bottom"",orderedModifiers:[],options:Object.assign({},Be,o),modifiersData:{},elements:{reference:t,popper:e},attributes:{},styles:{}},l=[],c=!1,h={state:a,setOptions:function(i){var s=""function""==typeof i?i(a.options):i;d(),a.options=Object.assign({},o,a.options,s),a.scrollParents={reference:$t(t)?Ae(t):t.contextElement?Ae(t.contextElement):[],popper:Ae(e)};var r,c,u=function(t){var e=He(t);return Bt.reduce((function(t,i){return t.concat(e.filter((function(t){return t.phase===i})))}),[])}((r=[].concat(n,a.options.modifiers),c=r.reduce((function(t,e){var i=t[e.name];return t[e.name]=i?Object.assign({},i,e,{options:Object.assign({},i.options,e.options),data:Object.assign({},i.data,e.data)}):e,t}),{}),Object.keys(c).map((function(t){return c[t]}))));return a.orderedModifiers=u.filter((function(t){return t.enabled})),a.orderedModifiers.forEach((function(t){var e=t.name,i=t.options,n=void 0===i?{}:i,s=t.effect;if(""function""==typeof s){var o=s({state:a,name:e,instance:h,options:n});l.push(o||function(){})}})),h.update()},forceUpdate:function(){if(!c){var t=a.elements,e=t.reference,i=t.popper;if(Re(e,i)){a.rects={reference:Me(e,te(i),""fixed""===a.options.strategy),popper:Kt(i)},a.reset=!1,a.placement=a.options.placement,a.orderedModifiers.forEach((function(t){return a.modifiersData[t.name]=Object.assign({},t.data)}));for(var n=0;n<a.orderedModifiers.length;n++)if(!0!==a.reset){var s=a.orderedModifiers[n],o=s.fn,r=s.options,l=void 0===r?{}:r,d=s.name;""function""==typeof o&&(a=o({state:a,options:l,name:d,instance:h})||a)}else a.reset=!1,n=-1}}},update:(s=function(){return new Promise((function(t){h.forceUpdate(),t(a)}))},function(){return r||(r=new Promise((function(t){Promise.resolve().then((function(){r=void 0,t(s())}))}))),r}),destroy:function(){d(),c=!0}};if(!Re(t,e))return h;function d(){l.forEach((function(t){return t()})),l=[]}return h.setOptions(i).then((function(t){!c&&i.onFirstUpdate&&i.onFirstUpdate(t)})),h}}var $e=We(),ze=We({defaultModifiers:[pe,Pe,ue,Ft]}),qe=We({defaultModifiers:[pe,Pe,ue,Ft,Ie,xe,je,le,Ne]});const Fe=Object.freeze({__proto__:null,popperGenerator:We,detectOverflow:ke,createPopperBase:$e,createPopper:qe,createPopperLite:ze,top:mt,bottom:gt,right:_t,left:bt,auto:vt,basePlacements:yt,start:wt,end:Et,clippingParents:At,viewport:Tt,popper:Ot,reference:Ct,variationPlacements:kt,placements:Lt,beforeRead:xt,read:Dt,afterRead:St,beforeMain:Nt,main:It,afterMain:Pt,beforeWrite:jt,write:Mt,afterWrite:Ht,modifierPhases:Bt,applyStyles:Ft,arrow:le,computeStyles:ue,eventListeners:pe,flip:xe,hide:Ne,offset:Ie,popperOffsets:Pe,preventOverflow:je}),Ue=""dropdown"",Ve=""Escape"",Ke=""Space"",Xe=""ArrowUp"",Ye=""ArrowDown"",Qe=new RegExp(""ArrowUp|ArrowDown|Escape""),Ge=""click.bs.dropdown.data-api"",Ze=""keydown.bs.dropdown.data-api"",Je=""show"",ti='[data-bs-toggle=""dropdown""]',ei="".dropdown-menu"",ii=m()?""top-end"":""top-start"",ni=m()?""top-start"":""top-end"",si=m()?""bottom-end"":""bottom-start"",oi=m()?""bottom-start"":""bottom-end"",ri=m()?""left-start"":""right-start"",ai=m()?""right-start"":""left-start"",li={offset:[0,2],boundary:""clippingParents"",reference:""toggle"",display:""dynamic"",popperConfig:null,autoClose:!0},ci={offset:""(array|string|function)"",boundary:""(string|element)"",reference:""(string|element|object)"",display:""string"",popperConfig:""(null|object|function)"",autoClose:""(boolean|string)""};class hi extends B{constructor(t,e){super(t),this._popper=null,this._config=this._getConfig(e),this._menu=this._getMenuElement(),this._inNavbar=this._detectNavbar()}static get Default(){return li}static get DefaultType(){return ci}static get NAME(){return Ue}toggle(){return this._isShown()?this.hide():this.show()}show(){if(c(this._element)||this._isShown(this._menu))return;const t={relatedTarget:this._element};if(j.trigger(this._element,""show.bs.dropdown"",t).defaultPrevented)return;const e=hi.getParentFromElement(this._element);this._inNavbar?U.setDataAttribute(this._menu,""popper"",""none""):this._createPopper(e),""ontouchstart""in document.documentElement&&!e.closest("".navbar-nav"")&&[].concat(...document.body.children).forEach((t=>j.on(t,""mouseover"",d))),this._element.focus(),this._element.setAttribute(""aria-expanded"",!0),this._menu.classList.add(Je),this._element.classList.add(Je),j.trigger(this._element,""shown.bs.dropdown"",t)}hide(){if(c(this._element)||!this._isShown(this._menu))return;const t={relatedTarget:this._element};this._completeHide(t)}dispose(){this._popper&&this._popper.destroy(),super.dispose()}update(){this._inNavbar=this._detectNavbar(),this._popper&&this._popper.update()}_completeHide(t){j.trigger(this._element,""hide.bs.dropdown"",t).defaultPrevented||(""ontouchstart""in document.documentElement&&[].concat(...document.body.children).forEach((t=>j.off(t,""mouseover"",d))),this._popper&&this._popper.destroy(),this._menu.classList.remove(Je),this._element.classList.remove(Je),this._element.setAttribute(""aria-expanded"",""false""),U.removeDataAttribute(this._menu,""popper""),j.trigger(this._element,""hidden.bs.dropdown"",t))}_getConfig(t){if(t={...this.constructor.Default,...U.getDataAttributes(this._element),...t},a(Ue,t,this.constructor.DefaultType),""object""==typeof t.reference&&!o(t.reference)&&""function""!=typeof t.reference.getBoundingClientRect)throw new TypeError(`${Ue.toUpperCase()}: Option ""reference"" provided type ""object"" without a required ""getBoundingClientRect"" method.`);return t}_createPopper(t){if(void 0===Fe)throw new TypeError(""Bootstrap's dropdowns require Popper (https://popper.js.org)"");let e=this._element;""parent""===this._config.reference?e=t:o(this._config.reference)?e=r(this._config.reference):""object""==typeof this._config.reference&&(e=this._config.reference);const i=this._getPopperConfig(),n=i.modifiers.find((t=>""applyStyles""===t.name&&!1===t.enabled));this._popper=qe(e,this._menu,i),n&&U.setDataAttribute(this._menu,""popper"",""static"")}_isShown(t=this._element){return t.classList.contains(Je)}_getMenuElement(){return V.next(this._element,ei)[0]}_getPlacement(){const t=this._element.parentNode;if(t.classList.contains(""dropend""))return ri;if(t.classList.contains(""dropstart""))return ai;const e=""end""===getComputedStyle(this._menu).getPropertyValue(""--bs-position"").trim();return t.classList.contains(""dropup"")?e?ni:ii:e?oi:si}_detectNavbar(){return null!==this._element.closest("".navbar"")}_getOffset(){const{offset:t}=this._config;return""string""==typeof t?t.split("","").map((t=>Number.parseInt(t,10))):""function""==typeof t?e=>t(e,this._element):t}_getPopperConfig(){const t={placement:this._getPlacement(),modifiers:[{name:""preventOverflow"",options:{boundary:this._config.boundary}},{name:""offset"",options:{offset:this._getOffset()}}]};return""static""===this._config.display&&(t.modifiers=[{name:""applyStyles"",enabled:!1}]),{...t,...""function""==typeof this._config.popperConfig?this._config.popperConfig(t):this._config.popperConfig}}_selectMenuItem({key:t,target:e}){const i=V.find("".dropdown-menu .dropdown-item:not(.disabled):not(:disabled)"",this._menu).filter(l);i.length&&v(i,e,t===Ye,!i.includes(e)).focus()}static jQueryInterface(t){return this.each((function(){const e=hi.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}static clearMenus(t){if(t&&(2===t.button||""keyup""===t.type&&""Tab""!==t.key))return;const e=V.find(ti);for(let i=0,n=e.length;i<n;i++){const n=hi.getInstance(e[i]);if(!n||!1===n._config.autoClose)continue;if(!n._isShown())continue;const s={relatedTarget:n._element};if(t){const e=t.composedPath(),i=e.includes(n._menu);if(e.includes(n._element)||""inside""===n._config.autoClose&&!i||""outside""===n._config.autoClose&&i)continue;if(n._menu.contains(t.target)&&(""keyup""===t.type&&""Tab""===t.key||/input|select|option|textarea|form/i.test(t.target.tagName)))continue;""click""===t.type&&(s.clickEvent=t)}n._completeHide(s)}}static getParentFromElement(t){return n(t)||t.parentNode}static dataApiKeydownHandler(t){if(/input|textarea/i.test(t.target.tagName)?t.key===Ke||t.key!==Ve&&(t.key!==Ye&&t.key!==Xe||t.target.closest(ei)):!Qe.test(t.key))return;const e=this.classList.contains(Je);if(!e&&t.key===Ve)return;if(t.preventDefault(),t.stopPropagation(),c(this))return;const i=this.matches(ti)?this:V.prev(this,ti)[0],n=hi.getOrCreateInstance(i);if(t.key!==Ve)return t.key===Xe||t.key===Ye?(e||n.show(),void n._selectMenuItem(t)):void(e&&t.key!==Ke||hi.clearMenus());n.hide()}}j.on(document,Ze,ti,hi.dataApiKeydownHandler),j.on(document,Ze,ei,hi.dataApiKeydownHandler),j.on(document,Ge,hi.clearMenus),j.on(document,""keyup.bs.dropdown.data-api"",hi.clearMenus),j.on(document,Ge,ti,(function(t){t.preventDefault(),hi.getOrCreateInstance(this).toggle()})),g(hi);const di="".fixed-top, .fixed-bottom, .is-fixed, .sticky-top"",ui="".sticky-top"";class fi{constructor(){this._element=document.body}getWidth(){const t=document.documentElement.clientWidth;return Math.abs(window.innerWidth-t)}hide(){const t=this.getWidth();this._disableOverFlow(),this._setElementAttributes(this._element,""paddingRight"",(e=>e+t)),this._setElementAttributes(di,""paddingRight"",(e=>e+t)),this._setElementAttributes(ui,""marginRight"",(e=>e-t))}_disableOverFlow(){this._saveInitialAttribute(this._element,""overflow""),this._element.style.overflow=""hidden""}_setElementAttributes(t,e,i){const n=this.getWidth();this._applyManipulationCallback(t,(t=>{if(t!==this._element&&window.innerWidth>t.clientWidth+n)return;this._saveInitialAttribute(t,e);const s=window.getComputedStyle(t)[e];t.style[e]=`${i(Number.parseFloat(s))}px`}))}reset(){this._resetElementAttributes(this._element,""overflow""),this._resetElementAttributes(this._element,""paddingRight""),this._resetElementAttributes(di,""paddingRight""),this._resetElementAttributes(ui,""marginRight"")}_saveInitialAttribute(t,e){const i=t.style[e];i&&U.setDataAttribute(t,e,i)}_resetElementAttributes(t,e){this._applyManipulationCallback(t,(t=>{const i=U.getDataAttribute(t,e);void 0===i?t.style.removeProperty(e):(U.removeDataAttribute(t,e),t.style[e]=i)}))}_applyManipulationCallback(t,e){o(t)?e(t):V.find(t,this._element).forEach(e)}isOverflowing(){return this.getWidth()>0}}const pi={className:""modal-backdrop"",isVisible:!0,isAnimated:!1,rootElement:""body"",clickCallback:null},mi={className:""string"",isVisible:""boolean"",isAnimated:""boolean"",rootElement:""(element|string)"",clickCallback:""(function|null)""},gi=""show"",_i=""mousedown.bs.backdrop"";class bi{constructor(t){this._config=this._getConfig(t),this._isAppended=!1,this._element=null}show(t){this._config.isVisible?(this._append(),this._config.isAnimated&&u(this._getElement()),this._getElement().classList.add(gi),this._emulateAnimation((()=>{_(t)}))):_(t)}hide(t){this._config.isVisible?(this._getElement().classList.remove(gi),this._emulateAnimation((()=>{this.dispose(),_(t)}))):_(t)}_getElement(){if(!this._element){const t=document.createElement(""div"");t.className=this._config.className,this._config.isAnimated&&t.classList.add(""fade""),this._element=t}return this._element}_getConfig(t){return(t={...pi,...""object""==typeof t?t:{}}).rootElement=r(t.rootElement),a(""backdrop"",t,mi),t}_append(){this._isAppended||(this._config.rootElement.append(this._getElement()),j.on(this._getElement(),_i,(()=>{_(this._config.clickCallback)})),this._isAppended=!0)}dispose(){this._isAppended&&(j.off(this._element,_i),this._element.remove(),this._isAppended=!1)}_emulateAnimation(t){b(t,this._getElement(),this._config.isAnimated)}}const vi={trapElement:null,autofocus:!0},yi={trapElement:""element"",autofocus:""boolean""},wi="".bs.focustrap"",Ei=""backward"";class Ai{constructor(t){this._config=this._getConfig(t),this._isActive=!1,this._lastTabNavDirection=null}activate(){const{trapElement:t,autofocus:e}=this._config;this._isActive||(e&&t.focus(),j.off(document,wi),j.on(document,""focusin.bs.focustrap"",(t=>this._handleFocusin(t))),j.on(document,""keydown.tab.bs.focustrap"",(t=>this._handleKeydown(t))),this._isActive=!0)}deactivate(){this._isActive&&(this._isActive=!1,j.off(document,wi))}_handleFocusin(t){const{target:e}=t,{trapElement:i}=this._config;if(e===document||e===i||i.contains(e))return;const n=V.focusableChildren(i);0===n.length?i.focus():this._lastTabNavDirection===Ei?n[n.length-1].focus():n[0].focus()}_handleKeydown(t){""Tab""===t.key&&(this._lastTabNavDirection=t.shiftKey?Ei:""forward"")}_getConfig(t){return t={...vi,...""object""==typeof t?t:{}},a(""focustrap"",t,yi),t}}const Ti=""modal"",Oi=""Escape"",Ci={backdrop:!0,keyboard:!0,focus:!0},ki={backdrop:""(boolean|string)"",keyboard:""boolean"",focus:""boolean""},Li=""hidden.bs.modal"",xi=""show.bs.modal"",Di=""resize.bs.modal"",Si=""click.dismiss.bs.modal"",Ni=""keydown.dismiss.bs.modal"",Ii=""mousedown.dismiss.bs.modal"",Pi=""modal-open"",ji=""show"",Mi=""modal-static"";class Hi extends B{constructor(t,e){super(t),this._config=this._getConfig(e),this._dialog=V.findOne("".modal-dialog"",this._element),this._backdrop=this._initializeBackDrop(),this._focustrap=this._initializeFocusTrap(),this._isShown=!1,this._ignoreBackdropClick=!1,this._isTransitioning=!1,this._scrollBar=new fi}static get Default(){return Ci}static get NAME(){return Ti}toggle(t){return this._isShown?this.hide():this.show(t)}show(t){this._isShown||this._isTransitioning||j.trigger(this._element,xi,{relatedTarget:t}).defaultPrevented||(this._isShown=!0,this._isAnimated()&&(this._isTransitioning=!0),this._scrollBar.hide(),document.body.classList.add(Pi),this._adjustDialog(),this._setEscapeEvent(),this._setResizeEvent(),j.on(this._dialog,Ii,(()=>{j.one(this._element,""mouseup.dismiss.bs.modal"",(t=>{t.target===this._element&&(this._ignoreBackdropClick=!0)}))})),this._showBackdrop((()=>this._showElement(t))))}hide(){if(!this._isShown||this._isTransitioning)return;if(j.trigger(this._element,""hide.bs.modal"").defaultPrevented)return;this._isShown=!1;const t=this._isAnimated();t&&(this._isTransitioning=!0),this._setEscapeEvent(),this._setResizeEvent(),this._focustrap.deactivate(),this._element.classList.remove(ji),j.off(this._element,Si),j.off(this._dialog,Ii),this._queueCallback((()=>this._hideModal()),this._element,t)}dispose(){[window,this._dialog].forEach((t=>j.off(t,"".bs.modal""))),this._backdrop.dispose(),this._focustrap.deactivate(),super.dispose()}handleUpdate(){this._adjustDialog()}_initializeBackDrop(){return new bi({isVisible:Boolean(this._config.backdrop),isAnimated:this._isAnimated()})}_initializeFocusTrap(){return new Ai({trapElement:this._element})}_getConfig(t){return t={...Ci,...U.getDataAttributes(this._element),...""object""==typeof t?t:{}},a(Ti,t,ki),t}_showElement(t){const e=this._isAnimated(),i=V.findOne("".modal-body"",this._dialog);this._element.parentNode&&this._element.parentNode.nodeType===Node.ELEMENT_NODE||document.body.append(this._element),this._element.style.display=""block"",this._element.removeAttribute(""aria-hidden""),this._element.setAttribute(""aria-modal"",!0),this._element.setAttribute(""role"",""dialog""),this._element.scrollTop=0,i&&(i.scrollTop=0),e&&u(this._element),this._element.classList.add(ji),this._queueCallback((()=>{this._config.focus&&this._focustrap.activate(),this._isTransitioning=!1,j.trigger(this._element,""shown.bs.modal"",{relatedTarget:t})}),this._dialog,e)}_setEscapeEvent(){this._isShown?j.on(this._element,Ni,(t=>{this._config.keyboard&&t.key===Oi?(t.preventDefault(),this.hide()):this._config.keyboard||t.key!==Oi||this._triggerBackdropTransition()})):j.off(this._element,Ni)}_setResizeEvent(){this._isShown?j.on(window,Di,(()=>this._adjustDialog())):j.off(window,Di)}_hideModal(){this._element.style.display=""none"",this._element.setAttribute(""aria-hidden"",!0),this._element.removeAttribute(""aria-modal""),this._element.removeAttribute(""role""),this._isTransitioning=!1,this._backdrop.hide((()=>{document.body.classList.remove(Pi),this._resetAdjustments(),this._scrollBar.reset(),j.trigger(this._element,Li)}))}_showBackdrop(t){j.on(this._element,Si,(t=>{this._ignoreBackdropClick?this._ignoreBackdropClick=!1:t.target===t.currentTarget&&(!0===this._config.backdrop?this.hide():""static""===this._config.backdrop&&this._triggerBackdropTransition())})),this._backdrop.show(t)}_isAnimated(){return this._element.classList.contains(""fade"")}_triggerBackdropTransition(){if(j.trigger(this._element,""hidePrevented.bs.modal"").defaultPrevented)return;const{classList:t,scrollHeight:e,style:i}=this._element,n=e>document.documentElement.clientHeight;!n&&""hidden""===i.overflowY||t.contains(Mi)||(n||(i.overflowY=""hidden""),t.add(Mi),this._queueCallback((()=>{t.remove(Mi),n||this._queueCallback((()=>{i.overflowY=""""}),this._dialog)}),this._dialog),this._element.focus())}_adjustDialog(){const t=this._element.scrollHeight>document.documentElement.clientHeight,e=this._scrollBar.getWidth(),i=e>0;(!i&&t&&!m()||i&&!t&&m())&&(this._element.style.paddingLeft=`${e}px`),(i&&!t&&!m()||!i&&t&&m())&&(this._element.style.paddingRight=`${e}px`)}_resetAdjustments(){this._element.style.paddingLeft="""",this._element.style.paddingRight=""""}static jQueryInterface(t,e){return this.each((function(){const i=Hi.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===i[t])throw new TypeError(`No method named ""${t}""`);i[t](e)}}))}}j.on(document,""click.bs.modal.data-api"",'[data-bs-toggle=""modal""]',(function(t){const e=n(this);[""A"",""AREA""].includes(this.tagName)&&t.preventDefault(),j.one(e,xi,(t=>{t.defaultPrevented||j.one(e,Li,(()=>{l(this)&&this.focus()}))}));const i=V.findOne("".modal.show"");i&&Hi.getInstance(i).hide(),Hi.getOrCreateInstance(e).toggle(this)})),R(Hi),g(Hi);const Bi=""offcanvas"",Ri={backdrop:!0,keyboard:!0,scroll:!1},Wi={backdrop:""boolean"",keyboard:""boolean"",scroll:""boolean""},$i=""show"",zi="".offcanvas.show"",qi=""hidden.bs.offcanvas"";class Fi extends B{constructor(t,e){super(t),this._config=this._getConfig(e),this._isShown=!1,this._backdrop=this._initializeBackDrop(),this._focustrap=this._initializeFocusTrap(),this._addEventListeners()}static get NAME(){return Bi}static get Default(){return Ri}toggle(t){return this._isShown?this.hide():this.show(t)}show(t){this._isShown||j.trigger(this._element,""show.bs.offcanvas"",{relatedTarget:t}).defaultPrevented||(this._isShown=!0,this._element.style.visibility=""visible"",this._backdrop.show(),this._config.scroll||(new fi).hide(),this._element.removeAttribute(""aria-hidden""),this._element.setAttribute(""aria-modal"",!0),this._element.setAttribute(""role"",""dialog""),this._element.classList.add($i),this._queueCallback((()=>{this._config.scroll||this._focustrap.activate(),j.trigger(this._element,""shown.bs.offcanvas"",{relatedTarget:t})}),this._element,!0))}hide(){this._isShown&&(j.trigger(this._element,""hide.bs.offcanvas"").defaultPrevented||(this._focustrap.deactivate(),this._element.blur(),this._isShown=!1,this._element.classList.remove($i),this._backdrop.hide(),this._queueCallback((()=>{this._element.setAttribute(""aria-hidden"",!0),this._element.removeAttribute(""aria-modal""),this._element.removeAttribute(""role""),this._element.style.visibility=""hidden"",this._config.scroll||(new fi).reset(),j.trigger(this._element,qi)}),this._element,!0)))}dispose(){this._backdrop.dispose(),this._focustrap.deactivate(),super.dispose()}_getConfig(t){return t={...Ri,...U.getDataAttributes(this._element),...""object""==typeof t?t:{}},a(Bi,t,Wi),t}_initializeBackDrop(){return new bi({className:""offcanvas-backdrop"",isVisible:this._config.backdrop,isAnimated:!0,rootElement:this._element.parentNode,clickCallback:()=>this.hide()})}_initializeFocusTrap(){return new Ai({trapElement:this._element})}_addEventListeners(){j.on(this._element,""keydown.dismiss.bs.offcanvas"",(t=>{this._config.keyboard&&""Escape""===t.key&&this.hide()}))}static jQueryInterface(t){return this.each((function(){const e=Fi.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t]||t.startsWith(""_"")||""constructor""===t)throw new TypeError(`No method named ""${t}""`);e[t](this)}}))}}j.on(document,""click.bs.offcanvas.data-api"",'[data-bs-toggle=""offcanvas""]',(function(t){const e=n(this);if([""A"",""AREA""].includes(this.tagName)&&t.preventDefault(),c(this))return;j.one(e,qi,(()=>{l(this)&&this.focus()}));const i=V.findOne(zi);i&&i!==e&&Fi.getInstance(i).hide(),Fi.getOrCreateInstance(e).toggle(this)})),j.on(window,""load.bs.offcanvas.data-api"",(()=>V.find(zi).forEach((t=>Fi.getOrCreateInstance(t).show())))),R(Fi),g(Fi);const Ui=new Set([""background"",""cite"",""href"",""itemtype"",""longdesc"",""poster"",""src"",""xlink:href""]),Vi=/^(?:(?:https?|mailto|ftp|tel|file|sms):|[^#&/:?]*(?:[#/?]|$))/i,Ki=/^data:(?:image\/(?:bmp|gif|jpeg|jpg|png|tiff|webp)|video\/(?:mpeg|mp4|ogg|webm)|audio\/(?:mp3|oga|ogg|opus));base64,[\d+/a-z]+=*$/i,Xi=(t,e)=>{const i=t.nodeName.toLowerCase();if(e.includes(i))return!Ui.has(i)||Boolean(Vi.test(t.nodeValue)||Ki.test(t.nodeValue));const n=e.filter((t=>t instanceof RegExp));for(let t=0,e=n.length;t<e;t++)if(n[t].test(i))return!0;return!1};function Yi(t,e,i){if(!t.length)return t;if(i&&""function""==typeof i)return i(t);const n=(new window.DOMParser).parseFromString(t,""text/html""),s=[].concat(...n.body.querySelectorAll(""*""));for(let t=0,i=s.length;t<i;t++){const i=s[t],n=i.nodeName.toLowerCase();if(!Object.keys(e).includes(n)){i.remove();continue}const o=[].concat(...i.attributes),r=[].concat(e[""*""]||[],e[n]||[]);o.forEach((t=>{Xi(t,r)||i.removeAttribute(t.nodeName)}))}return n.body.innerHTML}const Qi=""tooltip"",Gi=new Set([""sanitize"",""allowList"",""sanitizeFn""]),Zi={animation:""boolean"",template:""string"",title:""(string|element|function)"",trigger:""string"",delay:""(number|object)"",html:""boolean"",selector:""(string|boolean)"",placement:""(string|function)"",offset:""(array|string|function)"",container:""(string|element|boolean)"",fallbackPlacements:""array"",boundary:""(string|element)"",customClass:""(string|function)"",sanitize:""boolean"",sanitizeFn:""(null|function)"",allowList:""object"",popperConfig:""(null|object|function)""},Ji={AUTO:""auto"",TOP:""top"",RIGHT:m()?""left"":""right"",BOTTOM:""bottom"",LEFT:m()?""right"":""left""},tn={animation:!0,template:'<div class=""tooltip"" role=""tooltip""><div class=""tooltip-arrow""></div><div class=""tooltip-inner""></div></div>',trigger:""hover focus"",title:"""",delay:0,html:!1,selector:!1,placement:""top"",offset:[0,0],container:!1,fallbackPlacements:[""top"",""right"",""bottom"",""left""],boundary:""clippingParents"",customClass:"""",sanitize:!0,sanitizeFn:null,allowList:{""*"":[""class"",""dir"",""id"",""lang"",""role"",/^aria-[\w-]*$/i],a:[""target"",""href"",""title"",""rel""],area:[],b:[],br:[],col:[],code:[],div:[],em:[],hr:[],h1:[],h2:[],h3:[],h4:[],h5:[],h6:[],i:[],img:[""src"",""srcset"",""alt"",""title"",""width"",""height""],li:[],ol:[],p:[],pre:[],s:[],small:[],span:[],sub:[],sup:[],strong:[],u:[],ul:[]},popperConfig:null},en={HIDE:""hide.bs.tooltip"",HIDDEN:""hidden.bs.tooltip"",SHOW:""show.bs.tooltip"",SHOWN:""shown.bs.tooltip"",INSERTED:""inserted.bs.tooltip"",CLICK:""click.bs.tooltip"",FOCUSIN:""focusin.bs.tooltip"",FOCUSOUT:""focusout.bs.tooltip"",MOUSEENTER:""mouseenter.bs.tooltip"",MOUSELEAVE:""mouseleave.bs.tooltip""},nn=""fade"",sn=""show"",on=""show"",rn=""out"",an="".tooltip-inner"",ln="".modal"",cn=""hide.bs.modal"",hn=""hover"",dn=""focus"";class un extends B{constructor(t,e){if(void 0===Fe)throw new TypeError(""Bootstrap's tooltips require Popper (https://popper.js.org)"");super(t),this._isEnabled=!0,this._timeout=0,this._hoverState="""",this._activeTrigger={},this._popper=null,this._config=this._getConfig(e),this.tip=null,this._setListeners()}static get Default(){return tn}static get NAME(){return Qi}static get Event(){return en}static get DefaultType(){return Zi}enable(){this._isEnabled=!0}disable(){this._isEnabled=!1}toggleEnabled(){this._isEnabled=!this._isEnabled}toggle(t){if(this._isEnabled)if(t){const e=this._initializeOnDelegatedTarget(t);e._activeTrigger.click=!e._activeTrigger.click,e._isWithActiveTrigger()?e._enter(null,e):e._leave(null,e)}else{if(this.getTipElement().classList.contains(sn))return void this._leave(null,this);this._enter(null,this)}}dispose(){clearTimeout(this._timeout),j.off(this._element.closest(ln),cn,this._hideModalHandler),this.tip&&this.tip.remove(),this._disposePopper(),super.dispose()}show(){if(""none""===this._element.style.display)throw new Error(""Please use show on visible elements"");if(!this.isWithContent()||!this._isEnabled)return;const t=j.trigger(this._element,this.constructor.Event.SHOW),e=h(this._element),i=null===e?this._element.ownerDocument.documentElement.contains(this._element):e.contains(this._element);if(t.defaultPrevented||!i)return;""tooltip""===this.constructor.NAME&&this.tip&&this.getTitle()!==this.tip.querySelector(an).innerHTML&&(this._disposePopper(),this.tip.remove(),this.tip=null);const n=this.getTipElement(),s=(t=>{do{t+=Math.floor(1e6*Math.random())}while(document.getElementById(t));return t})(this.constructor.NAME);n.setAttribute(""id"",s),this._element.setAttribute(""aria-describedby"",s),this._config.animation&&n.classList.add(nn);const o=""function""==typeof this._config.placement?this._config.placement.call(this,n,this._element):this._config.placement,r=this._getAttachment(o);this._addAttachmentClass(r);const{container:a}=this._config;H.set(n,this.constructor.DATA_KEY,this),this._element.ownerDocument.documentElement.contains(this.tip)||(a.append(n),j.trigger(this._element,this.constructor.Event.INSERTED)),this._popper?this._popper.update():this._popper=qe(this._element,n,this._getPopperConfig(r)),n.classList.add(sn);const l=this._resolvePossibleFunction(this._config.customClass);l&&n.classList.add(...l.split("" "")),""ontouchstart""in document.documentElement&&[].concat(...document.body.children).forEach((t=>{j.on(t,""mouseover"",d)}));const c=this.tip.classList.contains(nn);this._queueCallback((()=>{const t=this._hoverState;this._hoverState=null,j.trigger(this._element,this.constructor.Event.SHOWN),t===rn&&this._leave(null,this)}),this.tip,c)}hide(){if(!this._popper)return;const t=this.getTipElement();if(j.trigger(this._element,this.constructor.Event.HIDE).defaultPrevented)return;t.classList.remove(sn),""ontouchstart""in document.documentElement&&[].concat(...document.body.children).forEach((t=>j.off(t,""mouseover"",d))),this._activeTrigger.click=!1,this._activeTrigger.focus=!1,this._activeTrigger.hover=!1;const e=this.tip.classList.contains(nn);this._queueCallback((()=>{this._isWithActiveTrigger()||(this._hoverState!==on&&t.remove(),this._cleanTipClass(),this._element.removeAttribute(""aria-describedby""),j.trigger(this._element,this.constructor.Event.HIDDEN),this._disposePopper())}),this.tip,e),this._hoverState=""""}update(){null!==this._popper&&this._popper.update()}isWithContent(){return Boolean(this.getTitle())}getTipElement(){if(this.tip)return this.tip;const t=document.createElement(""div"");t.innerHTML=this._config.template;const e=t.children[0];return this.setContent(e),e.classList.remove(nn,sn),this.tip=e,this.tip}setContent(t){this._sanitizeAndSetContent(t,this.getTitle(),an)}_sanitizeAndSetContent(t,e,i){const n=V.findOne(i,t);e||!n?this.setElementContent(n,e):n.remove()}setElementContent(t,e){if(null!==t)return o(e)?(e=r(e),void(this._config.html?e.parentNode!==t&&(t.innerHTML="""",t.append(e)):t.textContent=e.textContent)):void(this._config.html?(this._config.sanitize&&(e=Yi(e,this._config.allowList,this._config.sanitizeFn)),t.innerHTML=e):t.textContent=e)}getTitle(){const t=this._element.getAttribute(""data-bs-original-title"")||this._config.title;return this._resolvePossibleFunction(t)}updateAttachment(t){return""right""===t?""end"":""left""===t?""start"":t}_initializeOnDelegatedTarget(t,e){return e||this.constructor.getOrCreateInstance(t.delegateTarget,this._getDelegateConfig())}_getOffset(){const{offset:t}=this._config;return""string""==typeof t?t.split("","").map((t=>Number.parseInt(t,10))):""function""==typeof t?e=>t(e,this._element):t}_resolvePossibleFunction(t){return""function""==typeof t?t.call(this._element):t}_getPopperConfig(t){const e={placement:t,modifiers:[{name:""flip"",options:{fallbackPlacements:this._config.fallbackPlacements}},{name:""offset"",options:{offset:this._getOffset()}},{name:""preventOverflow"",options:{boundary:this._config.boundary}},{name:""arrow"",options:{element:`.${this.constructor.NAME}-arrow`}},{name:""onChange"",enabled:!0,phase:""afterWrite"",fn:t=>this._handlePopperPlacementChange(t)}],onFirstUpdate:t=>{t.options.placement!==t.placement&&this._handlePopperPlacementChange(t)}};return{...e,...""function""==typeof this._config.popperConfig?this._config.popperConfig(e):this._config.popperConfig}}_addAttachmentClass(t){this.getTipElement().classList.add(`${this._getBasicClassPrefix()}-${this.updateAttachment(t)}`)}_getAttachment(t){return Ji[t.toUpperCase()]}_setListeners(){this._config.trigger.split("" "").forEach((t=>{if(""click""===t)j.on(this._element,this.constructor.Event.CLICK,this._config.selector,(t=>this.toggle(t)));else if(""manual""!==t){const e=t===hn?this.constructor.Event.MOUSEENTER:this.constructor.Event.FOCUSIN,i=t===hn?this.constructor.Event.MOUSELEAVE:this.constructor.Event.FOCUSOUT;j.on(this._element,e,this._config.selector,(t=>this._enter(t))),j.on(this._element,i,this._config.selector,(t=>this._leave(t)))}})),this._hideModalHandler=()=>{this._element&&this.hide()},j.on(this._element.closest(ln),cn,this._hideModalHandler),this._config.selector?this._config={...this._config,trigger:""manual"",selector:""""}:this._fixTitle()}_fixTitle(){const t=this._element.getAttribute(""title""),e=typeof this._element.getAttribute(""data-bs-original-title"");(t||""string""!==e)&&(this._element.setAttribute(""data-bs-original-title"",t||""""),!t||this._element.getAttribute(""aria-label"")||this._element.textContent||this._element.setAttribute(""aria-label"",t),this._element.setAttribute(""title"",""""))}_enter(t,e){e=this._initializeOnDelegatedTarget(t,e),t&&(e._activeTrigger[""focusin""===t.type?dn:hn]=!0),e.getTipElement().classList.contains(sn)||e._hoverState===on?e._hoverState=on:(clearTimeout(e._timeout),e._hoverState=on,e._config.delay&&e._config.delay.show?e._timeout=setTimeout((()=>{e._hoverState===on&&e.show()}),e._config.delay.show):e.show())}_leave(t,e){e=this._initializeOnDelegatedTarget(t,e),t&&(e._activeTrigger[""focusout""===t.type?dn:hn]=e._element.contains(t.relatedTarget)),e._isWithActiveTrigger()||(clearTimeout(e._timeout),e._hoverState=rn,e._config.delay&&e._config.delay.hide?e._timeout=setTimeout((()=>{e._hoverState===rn&&e.hide()}),e._config.delay.hide):e.hide())}_isWithActiveTrigger(){for(const t in this._activeTrigger)if(this._activeTrigger[t])return!0;return!1}_getConfig(t){const e=U.getDataAttributes(this._element);return Object.keys(e).forEach((t=>{Gi.has(t)&&delete e[t]})),(t={...this.constructor.Default,...e,...""object""==typeof t&&t?t:{}}).container=!1===t.container?document.body:r(t.container),""number""==typeof t.delay&&(t.delay={show:t.delay,hide:t.delay}),""number""==typeof t.title&&(t.title=t.title.toString()),""number""==typeof t.content&&(t.content=t.content.toString()),a(Qi,t,this.constructor.DefaultType),t.sanitize&&(t.template=Yi(t.template,t.allowList,t.sanitizeFn)),t}_getDelegateConfig(){const t={};for(const e in this._config)this.constructor.Default[e]!==this._config[e]&&(t[e]=this._config[e]);return t}_cleanTipClass(){const t=this.getTipElement(),e=new RegExp(`(^|\\s)${this._getBasicClassPrefix()}\\S+`,""g""),i=t.getAttribute(""class"").match(e);null!==i&&i.length>0&&i.map((t=>t.trim())).forEach((e=>t.classList.remove(e)))}_getBasicClassPrefix(){return""bs-tooltip""}_handlePopperPlacementChange(t){const{state:e}=t;e&&(this.tip=e.elements.popper,this._cleanTipClass(),this._addAttachmentClass(this._getAttachment(e.placement)))}_disposePopper(){this._popper&&(this._popper.destroy(),this._popper=null)}static jQueryInterface(t){return this.each((function(){const e=un.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}g(un);const fn={...un.Default,placement:""right"",offset:[0,8],trigger:""click"",content:"""",template:'<div class=""popover"" role=""tooltip""><div class=""popover-arrow""></div><h3 class=""popover-header""></h3><div class=""popover-body""></div></div>'},pn={...un.DefaultType,content:""(string|element|function)""},mn={HIDE:""hide.bs.popover"",HIDDEN:""hidden.bs.popover"",SHOW:""show.bs.popover"",SHOWN:""shown.bs.popover"",INSERTED:""inserted.bs.popover"",CLICK:""click.bs.popover"",FOCUSIN:""focusin.bs.popover"",FOCUSOUT:""focusout.bs.popover"",MOUSEENTER:""mouseenter.bs.popover"",MOUSELEAVE:""mouseleave.bs.popover""};class gn extends un{static get Default(){return fn}static get NAME(){return""popover""}static get Event(){return mn}static get DefaultType(){return pn}isWithContent(){return this.getTitle()||this._getContent()}setContent(t){this._sanitizeAndSetContent(t,this.getTitle(),"".popover-header""),this._sanitizeAndSetContent(t,this._getContent(),"".popover-body"")}_getContent(){return this._resolvePossibleFunction(this._config.content)}_getBasicClassPrefix(){return""bs-popover""}static jQueryInterface(t){return this.each((function(){const e=gn.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}g(gn);const _n=""scrollspy"",bn={offset:10,method:""auto"",target:""""},vn={offset:""number"",method:""string"",target:""(string|element)""},yn=""active"",wn="".nav-link, .list-group-item, .dropdown-item"",En=""position"";class An extends B{constructor(t,e){super(t),this._scrollElement=""BODY""===this._element.tagName?window:this._element,this._config=this._getConfig(e),this._offsets=[],this._targets=[],this._activeTarget=null,this._scrollHeight=0,j.on(this._scrollElement,""scroll.bs.scrollspy"",(()=>this._process())),this.refresh(),this._process()}static get Default(){return bn}static get NAME(){return _n}refresh(){const t=this._scrollElement===this._scrollElement.window?""offset"":En,e=""auto""===this._config.method?t:this._config.method,n=e===En?this._getScrollTop():0;this._offsets=[],this._targets=[],this._scrollHeight=this._getScrollHeight(),V.find(wn,this._config.target).map((t=>{const s=i(t),o=s?V.findOne(s):null;if(o){const t=o.getBoundingClientRect();if(t.width||t.height)return[U[e](o).top+n,s]}return null})).filter((t=>t)).sort(((t,e)=>t[0]-e[0])).forEach((t=>{this._offsets.push(t[0]),this._targets.push(t[1])}))}dispose(){j.off(this._scrollElement,"".bs.scrollspy""),super.dispose()}_getConfig(t){return(t={...bn,...U.getDataAttributes(this._element),...""object""==typeof t&&t?t:{}}).target=r(t.target)||document.documentElement,a(_n,t,vn),t}_getScrollTop(){return this._scrollElement===window?this._scrollElement.pageYOffset:this._scrollElement.scrollTop}_getScrollHeight(){return this._scrollElement.scrollHeight||Math.max(document.body.scrollHeight,document.documentElement.scrollHeight)}_getOffsetHeight(){return this._scrollElement===window?window.innerHeight:this._scrollElement.getBoundingClientRect().height}_process(){const t=this._getScrollTop()+this._config.offset,e=this._getScrollHeight(),i=this._config.offset+e-this._getOffsetHeight();if(this._scrollHeight!==e&&this.refresh(),t>=i){const t=this._targets[this._targets.length-1];this._activeTarget!==t&&this._activate(t)}else{if(this._activeTarget&&t<this._offsets[0]&&this._offsets[0]>0)return this._activeTarget=null,void this._clear();for(let e=this._offsets.length;e--;)this._activeTarget!==this._targets[e]&&t>=this._offsets[e]&&(void 0===this._offsets[e+1]||t<this._offsets[e+1])&&this._activate(this._targets[e])}}_activate(t){this._activeTarget=t,this._clear();const e=wn.split("","").map((e=>`${e}[data-bs-target=""${t}""],${e}[href=""${t}""]`)),i=V.findOne(e.join("",""),this._config.target);i.classList.add(yn),i.classList.contains(""dropdown-item"")?V.findOne("".dropdown-toggle"",i.closest("".dropdown"")).classList.add(yn):V.parents(i,"".nav, .list-group"").forEach((t=>{V.prev(t,"".nav-link, .list-group-item"").forEach((t=>t.classList.add(yn))),V.prev(t,"".nav-item"").forEach((t=>{V.children(t,"".nav-link"").forEach((t=>t.classList.add(yn)))}))})),j.trigger(this._scrollElement,""activate.bs.scrollspy"",{relatedTarget:t})}_clear(){V.find(wn,this._config.target).filter((t=>t.classList.contains(yn))).forEach((t=>t.classList.remove(yn)))}static jQueryInterface(t){return this.each((function(){const e=An.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}j.on(window,""load.bs.scrollspy.data-api"",(()=>{V.find('[data-bs-spy=""scroll""]').forEach((t=>new An(t)))})),g(An);const Tn=""active"",On=""fade"",Cn=""show"",kn="".active"",Ln="":scope > li > .active"";class xn extends B{static get NAME(){return""tab""}show(){if(this._element.parentNode&&this._element.parentNode.nodeType===Node.ELEMENT_NODE&&this._element.classList.contains(Tn))return;let t;const e=n(this._element),i=this._element.closest("".nav, .list-group"");if(i){const e=""UL""===i.nodeName||""OL""===i.nodeName?Ln:kn;t=V.find(e,i),t=t[t.length-1]}const s=t?j.trigger(t,""hide.bs.tab"",{relatedTarget:this._element}):null;if(j.trigger(this._element,""show.bs.tab"",{relatedTarget:t}).defaultPrevented||null!==s&&s.defaultPrevented)return;this._activate(this._element,i);const o=()=>{j.trigger(t,""hidden.bs.tab"",{relatedTarget:this._element}),j.trigger(this._element,""shown.bs.tab"",{relatedTarget:t})};e?this._activate(e,e.parentNode,o):o()}_activate(t,e,i){const n=(!e||""UL""!==e.nodeName&&""OL""!==e.nodeName?V.children(e,kn):V.find(Ln,e))[0],s=i&&n&&n.classList.contains(On),o=()=>this._transitionComplete(t,n,i);n&&s?(n.classList.remove(Cn),this._queueCallback(o,t,!0)):o()}_transitionComplete(t,e,i){if(e){e.classList.remove(Tn);const t=V.findOne("":scope > .dropdown-menu .active"",e.parentNode);t&&t.classList.remove(Tn),""tab""===e.getAttribute(""role"")&&e.setAttribute(""aria-selected"",!1)}t.classList.add(Tn),""tab""===t.getAttribute(""role"")&&t.setAttribute(""aria-selected"",!0),u(t),t.classList.contains(On)&&t.classList.add(Cn);let n=t.parentNode;if(n&&""LI""===n.nodeName&&(n=n.parentNode),n&&n.classList.contains(""dropdown-menu"")){const e=t.closest("".dropdown"");e&&V.find("".dropdown-toggle"",e).forEach((t=>t.classList.add(Tn))),t.setAttribute(""aria-expanded"",!0)}i&&i()}static jQueryInterface(t){return this.each((function(){const e=xn.getOrCreateInstance(this);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}j.on(document,""click.bs.tab.data-api"",'[data-bs-toggle=""tab""], [data-bs-toggle=""pill""], [data-bs-toggle=""list""]',(function(t){[""A"",""AREA""].includes(this.tagName)&&t.preventDefault(),c(this)||xn.getOrCreateInstance(this).show()})),g(xn);const Dn=""toast"",Sn=""hide"",Nn=""show"",In=""showing"",Pn={animation:""boolean"",autohide:""boolean"",delay:""number""},jn={animation:!0,autohide:!0,delay:5e3};class Mn extends B{constructor(t,e){super(t),this._config=this._getConfig(e),this._timeout=null,this._hasMouseInteraction=!1,this._hasKeyboardInteraction=!1,this._setListeners()}static get DefaultType(){return Pn}static get Default(){return jn}static get NAME(){return Dn}show(){j.trigger(this._element,""show.bs.toast"").defaultPrevented||(this._clearTimeout(),this._config.animation&&this._element.classList.add(""fade""),this._element.classList.remove(Sn),u(this._element),this._element.classList.add(Nn),this._element.classList.add(In),this._queueCallback((()=>{this._element.classList.remove(In),j.trigger(this._element,""shown.bs.toast""),this._maybeScheduleHide()}),this._element,this._config.animation))}hide(){this._element.classList.contains(Nn)&&(j.trigger(this._element,""hide.bs.toast"").defaultPrevented||(this._element.classList.add(In),this._queueCallback((()=>{this._element.classList.add(Sn),this._element.classList.remove(In),this._element.classList.remove(Nn),j.trigger(this._element,""hidden.bs.toast"")}),this._element,this._config.animation)))}dispose(){this._clearTimeout(),this._element.classList.contains(Nn)&&this._element.classList.remove(Nn),super.dispose()}_getConfig(t){return t={...jn,...U.getDataAttributes(this._element),...""object""==typeof t&&t?t:{}},a(Dn,t,this.constructor.DefaultType),t}_maybeScheduleHide(){this._config.autohide&&(this._hasMouseInteraction||this._hasKeyboardInteraction||(this._timeout=setTimeout((()=>{this.hide()}),this._config.delay)))}_onInteraction(t,e){switch(t.type){case""mouseover"":case""mouseout"":this._hasMouseInteraction=e;break;case""focusin"":case""focusout"":this._hasKeyboardInteraction=e}if(e)return void this._clearTimeout();const i=t.relatedTarget;this._element===i||this._element.contains(i)||this._maybeScheduleHide()}_setListeners(){j.on(this._element,""mouseover.bs.toast"",(t=>this._onInteraction(t,!0))),j.on(this._element,""mouseout.bs.toast"",(t=>this._onInteraction(t,!1))),j.on(this._element,""focusin.bs.toast"",(t=>this._onInteraction(t,!0))),j.on(this._element,""focusout.bs.toast"",(t=>this._onInteraction(t,!1)))}_clearTimeout(){clearTimeout(this._timeout),this._timeout=null}static jQueryInterface(t){return this.each((function(){const e=Mn.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t](this)}}))}}return R(Mn),g(Mn),{Alert:W,Button:z,Carousel:st,Collapse:pt,Dropdown:hi,Modal:Hi,Offcanvas:Fi,Popover:gn,ScrollSpy:An,Tab:xn,Toast:Mn,Tooltip:un}}));
-//# sourceMappingURL=bootstrap.bundle.min.js.map
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/basics/libs/clipboard/clipboard.min.js---
@@ -1,7 +0,0 @@
-/*!
- * clipboard.js v2.0.10
- * https://clipboardjs.com/
- *
- * Licensed MIT © Zeno Rocha
- */
-!function(t,e){""object""==typeof exports&&""object""==typeof module?module.exports=e():""function""==typeof define&&define.amd?define([],e):""object""==typeof exports?exports.ClipboardJS=e():t.ClipboardJS=e()}(this,function(){return n={686:function(t,e,n){""use strict"";n.d(e,{default:function(){return o}});var e=n(279),i=n.n(e),e=n(370),u=n.n(e),e=n(817),c=n.n(e);function a(t){try{return document.execCommand(t)}catch(t){return}}var f=function(t){t=c()(t);return a(""cut""),t};var l=function(t){var e,n,o,r=1<arguments.length&&void 0!==arguments[1]?arguments[1]:{container:document.body},i="""";return""string""==typeof t?(e=t,n=""rtl""===document.documentElement.getAttribute(""dir""),(o=document.createElement(""textarea"")).style.fontSize=""12pt"",o.style.border=""0"",o.style.padding=""0"",o.style.margin=""0"",o.style.position=""absolute"",o.style[n?""right"":""left""]=""-9999px"",n=window.pageYOffset||document.documentElement.scrollTop,o.style.top="""".concat(n,""px""),o.setAttribute(""readonly"",""""),o.value=e,o=o,r.container.appendChild(o),i=c()(o),a(""copy""),o.remove()):(i=c()(t),a(""copy"")),i};function r(t){return(r=""function""==typeof Symbol&&""symbol""==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&""function""==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?""symbol"":typeof t})(t)}var s=function(){var t=0<arguments.length&&void 0!==arguments[0]?arguments[0]:{},e=t.action,n=void 0===e?""copy"":e,o=t.container,e=t.target,t=t.text;if(""copy""!==n&&""cut""!==n)throw new Error('Invalid ""action"" value, use either ""copy"" or ""cut""');if(void 0!==e){if(!e||""object""!==r(e)||1!==e.nodeType)throw new Error('Invalid ""target"" value, use a valid Element');if(""copy""===n&&e.hasAttribute(""disabled""))throw new Error('Invalid ""target"" attribute. Please use ""readonly"" instead of ""disabled"" attribute');if(""cut""===n&&(e.hasAttribute(""readonly"")||e.hasAttribute(""disabled"")))throw new Error('Invalid ""target"" attribute. You can\'t cut text from elements with ""readonly"" or ""disabled"" attributes')}return t?l(t,{container:o}):e?""cut""===n?f(e):l(e,{container:o}):void 0};function p(t){return(p=""function""==typeof Symbol&&""symbol""==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&""function""==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?""symbol"":typeof t})(t)}function d(t,e){for(var n=0;n<e.length;n++){var o=e[n];o.enumerable=o.enumerable||!1,o.configurable=!0,""value""in o&&(o.writable=!0),Object.defineProperty(t,o.key,o)}}function y(t,e){return(y=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function h(n){var o=function(){if(""undefined""==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(""function""==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],function(){})),!0}catch(t){return!1}}();return function(){var t,e=m(n);return t=o?(t=m(this).constructor,Reflect.construct(e,arguments,t)):e.apply(this,arguments),e=this,!(t=t)||""object""!==p(t)&&""function""!=typeof t?function(t){if(void 0!==t)return t;throw new ReferenceError(""this hasn't been initialised - super() hasn't been called"")}(e):t}}function m(t){return(m=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}function v(t,e){t=""data-clipboard-"".concat(t);if(e.hasAttribute(t))return e.getAttribute(t)}var o=function(){!function(t,e){if(""function""!=typeof e&&null!==e)throw new TypeError(""Super expression must either be null or a function"");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&y(t,e)}(r,i());var t,e,n,o=h(r);function r(t,e){var n;return function(t){if(!(t instanceof r))throw new TypeError(""Cannot call a class as a function"")}(this),(n=o.call(this)).resolveOptions(e),n.listenClick(t),n}return t=r,n=[{key:""copy"",value:function(t){var e=1<arguments.length&&void 0!==arguments[1]?arguments[1]:{container:document.body};return l(t,e)}},{key:""cut"",value:function(t){return f(t)}},{key:""isSupported"",value:function(){var t=0<arguments.length&&void 0!==arguments[0]?arguments[0]:[""copy"",""cut""],t=""string""==typeof t?[t]:t,e=!!document.queryCommandSupported;return t.forEach(function(t){e=e&&!!document.queryCommandSupported(t)}),e}}],(e=[{key:""resolveOptions"",value:function(){var t=0<arguments.length&&void 0!==arguments[0]?arguments[0]:{};this.action=""function""==typeof t.action?t.action:this.defaultAction,this.target=""function""==typeof t.target?t.target:this.defaultTarget,this.text=""function""==typeof t.text?t.text:this.defaultText,this.container=""object""===p(t.container)?t.container:document.body}},{key:""listenClick"",value:function(t){var e=this;this.listener=u()(t,""click"",function(t){return e.onClick(t)})}},{key:""onClick"",value:function(t){var e=t.delegateTarget||t.currentTarget,n=this.action(e)||""copy"",t=s({action:n,container:this.container,target:this.target(e),text:this.text(e)});this.emit(t?""success"":""error"",{action:n,text:t,trigger:e,clearSelection:function(){e&&e.focus(),document.activeElement.blur(),window.getSelection().removeAllRanges()}})}},{key:""defaultAction"",value:function(t){return v(""action"",t)}},{key:""defaultTarget"",value:function(t){t=v(""target"",t);if(t)return document.querySelector(t)}},{key:""defaultText"",value:function(t){return v(""text"",t)}},{key:""destroy"",value:function(){this.listener.destroy()}}])&&d(t.prototype,e),n&&d(t,n),r}()},828:function(t){var e;""undefined""==typeof Element||Element.prototype.matches||((e=Element.prototype).matches=e.matchesSelector||e.mozMatchesSelector||e.msMatchesSelector||e.oMatchesSelector||e.webkitMatchesSelector),t.exports=function(t,e){for(;t&&9!==t.nodeType;){if(""function""==typeof t.matches&&t.matches(e))return t;t=t.parentNode}}},438:function(t,e,n){var u=n(828);function i(t,e,n,o,r){var i=function(e,n,t,o){return function(t){t.delegateTarget=u(t.target,n),t.delegateTarget&&o.call(e,t)}}.apply(this,arguments);return t.addEventListener(n,i,r),{destroy:function(){t.removeEventListener(n,i,r)}}}t.exports=function(t,e,n,o,r){return""function""==typeof t.addEventListener?i.apply(null,arguments):""function""==typeof n?i.bind(null,document).apply(null,arguments):(""string""==typeof t&&(t=document.querySelectorAll(t)),Array.prototype.map.call(t,function(t){return i(t,e,n,o,r)}))}},879:function(t,n){n.node=function(t){return void 0!==t&&t instanceof HTMLElement&&1===t.nodeType},n.nodeList=function(t){var e=Object.prototype.toString.call(t);return void 0!==t&&(""[object NodeList]""===e||""[object HTMLCollection]""===e)&&""length""in t&&(0===t.length||n.node(t[0]))},n.string=function(t){return""string""==typeof t||t instanceof String},n.fn=function(t){return""[object Function]""===Object.prototype.toString.call(t)}},370:function(t,e,n){var f=n(879),l=n(438);t.exports=function(t,e,n){if(!t&&!e&&!n)throw new Error(""Missing required arguments"");if(!f.string(e))throw new TypeError(""Second argument must be a String"");if(!f.fn(n))throw new TypeError(""Third argument must be a Function"");if(f.node(t))return c=e,a=n,(u=t).addEventListener(c,a),{destroy:function(){u.removeEventListener(c,a)}};if(f.nodeList(t))return o=t,r=e,i=n,Array.prototype.forEach.call(o,function(t){t.addEventListener(r,i)}),{destroy:function(){Array.prototype.forEach.call(o,function(t){t.removeEventListener(r,i)})}};if(f.string(t))return t=t,e=e,n=n,l(document.body,t,e,n);throw new TypeError(""First argument must be a String, HTMLElement, HTMLCollection, or NodeList"");var o,r,i,u,c,a}},817:function(t){t.exports=function(t){var e,n=""SELECT""===t.nodeName?(t.focus(),t.value):""INPUT""===t.nodeName||""TEXTAREA""===t.nodeName?((e=t.hasAttribute(""readonly""))||t.setAttribute(""readonly"",""""),t.select(),t.setSelectionRange(0,t.value.length),e||t.removeAttribute(""readonly""),t.value):(t.hasAttribute(""contenteditable"")&&t.focus(),n=window.getSelection(),(e=document.createRange()).selectNodeContents(t),n.removeAllRanges(),n.addRange(e),n.toString());return n}},279:function(t){function e(){}e.prototype={on:function(t,e,n){var o=this.e||(this.e={});return(o[t]||(o[t]=[])).push({fn:e,ctx:n}),this},once:function(t,e,n){var o=this;function r(){o.off(t,r),e.apply(n,arguments)}return r._=e,this.on(t,r,n)},emit:function(t){for(var e=[].slice.call(arguments,1),n=((this.e||(this.e={}))[t]||[]).slice(),o=0,r=n.length;o<r;o++)n[o].fn.apply(n[o].ctx,e);return this},off:function(t,e){var n=this.e||(this.e={}),o=n[t],r=[];if(o&&e)for(var i=0,u=o.length;i<u;i++)o[i].fn!==e&&o[i].fn._!==e&&r.push(o[i]);return r.length?n[t]=r:delete n[t],this}},t.exports=e,t.exports.TinyEmitter=e}},r={},o.n=function(t){var e=t&&t.__esModule?function(){return t.default}:function(){return t};return o.d(e,{a:e}),e},o.d=function(t,e){for(var n in e)o.o(e,n)&&!o.o(t,n)&&Object.defineProperty(t,n,{enumerable:!0,get:e[n]})},o.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)},o(686).default;function o(t){if(r[t])return r[t].exports;var e=r[t]={exports:{}};return n[t](e,e.exports,o),e.exports}var n,r});
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/basics/libs/quarto-html/anchor.min.js---
@@ -1,9 +0,0 @@
-// @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&dn=expat.txt Expat
-//
-// AnchorJS - v4.3.1 - 2021-04-17
-// https://www.bryanbraun.com/anchorjs/
-// Copyright (c) 2021 Bryan Braun; Licensed MIT
-//
-// @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&dn=expat.txt Expat
-!function(A,e){""use strict"";""function""==typeof define&&define.amd?define([],e):""object""==typeof module&&module.exports?module.exports=e():(A.AnchorJS=e(),A.anchors=new A.AnchorJS)}(this,function(){""use strict"";return function(A){function d(A){A.icon=Object.prototype.hasOwnProperty.call(A,""icon"")?A.icon:"""",A.visible=Object.prototype.hasOwnProperty.call(A,""visible"")?A.visible:""hover"",A.placement=Object.prototype.hasOwnProperty.call(A,""placement"")?A.placement:""right"",A.ariaLabel=Object.prototype.hasOwnProperty.call(A,""ariaLabel"")?A.ariaLabel:""Anchor"",A.class=Object.prototype.hasOwnProperty.call(A,""class"")?A.class:"""",A.base=Object.prototype.hasOwnProperty.call(A,""base"")?A.base:"""",A.truncate=Object.prototype.hasOwnProperty.call(A,""truncate"")?Math.floor(A.truncate):64,A.titleText=Object.prototype.hasOwnProperty.call(A,""titleText"")?A.titleText:""""}function w(A){var e;if(""string""==typeof A||A instanceof String)e=[].slice.call(document.querySelectorAll(A));else{if(!(Array.isArray(A)||A instanceof NodeList))throw new TypeError(""The selector provided to AnchorJS was invalid."");e=[].slice.call(A)}return e}this.options=A||{},this.elements=[],d(this.options),this.isTouchDevice=function(){return Boolean(""ontouchstart""in window||window.TouchEvent||window.DocumentTouch&&document instanceof DocumentTouch)},this.add=function(A){var e,t,o,i,n,s,a,c,r,l,h,u,p=[];if(d(this.options),""touch""===(l=this.options.visible)&&(l=this.isTouchDevice()?""always"":""hover""),0===(e=w(A=A||""h2, h3, h4, h5, h6"")).length)return this;for(null===document.head.querySelector(""style.anchorjs"")&&((u=document.createElement(""style"")).className=""anchorjs"",u.appendChild(document.createTextNode("""")),void 0===(A=document.head.querySelector('[rel=""stylesheet""],style'))?document.head.appendChild(u):document.head.insertBefore(u,A),u.sheet.insertRule("".anchorjs-link{opacity:0;text-decoration:none;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}"",u.sheet.cssRules.length),u.sheet.insertRule("":hover>.anchorjs-link,.anchorjs-link:focus{opacity:1}"",u.sheet.cssRules.length),u.sheet.insertRule(""[data-anchorjs-icon]::after{content:attr(data-anchorjs-icon)}"",u.sheet.cssRules.length),u.sheet.insertRule('@font-face{font-family:anchorjs-icons;src:url(data:n/a;base64,AAEAAAALAIAAAwAwT1MvMg8yG2cAAAE4AAAAYGNtYXDp3gC3AAABpAAAAExnYXNwAAAAEAAAA9wAAAAIZ2x5ZlQCcfwAAAH4AAABCGhlYWQHFvHyAAAAvAAAADZoaGVhBnACFwAAAPQAAAAkaG10eASAADEAAAGYAAAADGxvY2EACACEAAAB8AAAAAhtYXhwAAYAVwAAARgAAAAgbmFtZQGOH9cAAAMAAAAAunBvc3QAAwAAAAADvAAAACAAAQAAAAEAAHzE2p9fDzz1AAkEAAAAAADRecUWAAAAANQA6R8AAAAAAoACwAAAAAgAAgAAAAAAAAABAAADwP/AAAACgAAA/9MCrQABAAAAAAAAAAAAAAAAAAAAAwABAAAAAwBVAAIAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAMCQAGQAAUAAAKZAswAAACPApkCzAAAAesAMwEJAAAAAAAAAAAAAAAAAAAAARAAAAAAAAAAAAAAAAAAAAAAQAAg//0DwP/AAEADwABAAAAAAQAAAAAAAAAAAAAAIAAAAAAAAAIAAAACgAAxAAAAAwAAAAMAAAAcAAEAAwAAABwAAwABAAAAHAAEADAAAAAIAAgAAgAAACDpy//9//8AAAAg6cv//f///+EWNwADAAEAAAAAAAAAAAAAAAAACACEAAEAAAAAAAAAAAAAAAAxAAACAAQARAKAAsAAKwBUAAABIiYnJjQ3NzY2MzIWFxYUBwcGIicmNDc3NjQnJiYjIgYHBwYUFxYUBwYGIwciJicmNDc3NjIXFhQHBwYUFxYWMzI2Nzc2NCcmNDc2MhcWFAcHBgYjARQGDAUtLXoWOR8fORYtLTgKGwoKCjgaGg0gEhIgDXoaGgkJBQwHdR85Fi0tOAobCgoKOBoaDSASEiANehoaCQkKGwotLXoWOR8BMwUFLYEuehYXFxYugC44CQkKGwo4GkoaDQ0NDXoaShoKGwoFBe8XFi6ALjgJCQobCjgaShoNDQ0NehpKGgobCgoKLYEuehYXAAAADACWAAEAAAAAAAEACAAAAAEAAAAAAAIAAwAIAAEAAAAAAAMACAAAAAEAAAAAAAQACAAAAAEAAAAAAAUAAQALAAEAAAAAAAYACAAAAAMAAQQJAAEAEAAMAAMAAQQJAAIABgAcAAMAAQQJAAMAEAAMAAMAAQQJAAQAEAAMAAMAAQQJAAUAAgAiAAMAAQQJAAYAEAAMYW5jaG9yanM0MDBAAGEAbgBjAGgAbwByAGoAcwA0ADAAMABAAAAAAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAH//wAP) format(""truetype"")}',u.sheet.cssRules.length)),u=document.querySelectorAll(""[id]""),t=[].map.call(u,function(A){return A.id}),i=0;i<e.length;i++)if(this.hasAnchorJSLink(e[i]))p.push(i);else{if(e[i].hasAttribute(""id""))o=e[i].getAttribute(""id"");else if(e[i].hasAttribute(""data-anchor-id""))o=e[i].getAttribute(""data-anchor-id"");else{for(c=a=this.urlify(e[i].textContent),s=0;n=t.indexOf(c=void 0!==n?a+""-""+s:c),s+=1,-1!==n;);n=void 0,t.push(c),e[i].setAttribute(""id"",c),o=c}(r=document.createElement(""a"")).className=""anchorjs-link ""+this.options.class,r.setAttribute(""aria-label"",this.options.ariaLabel),r.setAttribute(""data-anchorjs-icon"",this.options.icon),this.options.titleText&&(r.title=this.options.titleText),h=document.querySelector(""base"")?window.location.pathname+window.location.search:"""",h=this.options.base||h,r.href=h+""#""+o,""always""===l&&(r.style.opacity=""1""),""""===this.options.icon&&(r.style.font=""1em/1 anchorjs-icons"",""left""===this.options.placement&&(r.style.lineHeight=""inherit"")),""left""===this.options.placement?(r.style.position=""absolute"",r.style.marginLeft=""-1em"",r.style.paddingRight="".5em"",e[i].insertBefore(r,e[i].firstChild)):(r.style.paddingLeft="".375em"",e[i].appendChild(r))}for(i=0;i<p.length;i++)e.splice(p[i]-i,1);return this.elements=this.elements.concat(e),this},this.remove=function(A){for(var e,t,o=w(A),i=0;i<o.length;i++)(t=o[i].querySelector("".anchorjs-link""))&&(-1!==(e=this.elements.indexOf(o[i]))&&this.elements.splice(e,1),o[i].removeChild(t));return this},this.removeAll=function(){this.remove(this.elements)},this.urlify=function(A){var e=document.createElement(""textarea"");return e.innerHTML=A,A=e.value,this.options.truncate||d(this.options),A.trim().replace(/'/gi,"""").replace(/[& +$,:;=?@""#{}|^~[`%!'<>\]./()*\\\n\t\b\v\u00A0]/g,""-"").replace(/-{2,}/g,""-"").substring(0,this.options.truncate).replace(/^-+|-+$/gm,"""").toLowerCase()},this.hasAnchorJSLink=function(A){var e=A.firstChild&&-1<("" ""+A.firstChild.className+"" "").indexOf("" anchorjs-link ""),A=A.lastChild&&-1<("" ""+A.lastChild.className+"" "").indexOf("" anchorjs-link "");return e||A||!1}}});
-// @license-end
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/basics/libs/quarto-html/popper.min.js---
@@ -1,6 +0,0 @@
-/**
- * @popperjs/core v2.11.4 - MIT License
- */
-
-!function(e,t){""object""==typeof exports&&""undefined""!=typeof module?t(exports):""function""==typeof define&&define.amd?define([""exports""],t):t((e=""undefined""!=typeof globalThis?globalThis:e||self).Popper={})}(this,(function(e){""use strict"";function t(e){if(null==e)return window;if(""[object Window]""!==e.toString()){var t=e.ownerDocument;return t&&t.defaultView||window}return e}function n(e){return e instanceof t(e).Element||e instanceof Element}function r(e){return e instanceof t(e).HTMLElement||e instanceof HTMLElement}function o(e){return""undefined""!=typeof ShadowRoot&&(e instanceof t(e).ShadowRoot||e instanceof ShadowRoot)}var i=Math.max,a=Math.min,s=Math.round;function f(e,t){void 0===t&&(t=!1);var n=e.getBoundingClientRect(),o=1,i=1;if(r(e)&&t){var a=e.offsetHeight,f=e.offsetWidth;f>0&&(o=s(n.width)/f||1),a>0&&(i=s(n.height)/a||1)}return{width:n.width/o,height:n.height/i,top:n.top/i,right:n.right/o,bottom:n.bottom/i,left:n.left/o,x:n.left/o,y:n.top/i}}function c(e){var n=t(e);return{scrollLeft:n.pageXOffset,scrollTop:n.pageYOffset}}function p(e){return e?(e.nodeName||"""").toLowerCase():null}function u(e){return((n(e)?e.ownerDocument:e.document)||window.document).documentElement}function l(e){return f(u(e)).left+c(e).scrollLeft}function d(e){return t(e).getComputedStyle(e)}function h(e){var t=d(e),n=t.overflow,r=t.overflowX,o=t.overflowY;return/auto|scroll|overlay|hidden/.test(n+o+r)}function m(e,n,o){void 0===o&&(o=!1);var i,a,d=r(n),m=r(n)&&function(e){var t=e.getBoundingClientRect(),n=s(t.width)/e.offsetWidth||1,r=s(t.height)/e.offsetHeight||1;return 1!==n||1!==r}(n),v=u(n),g=f(e,m),y={scrollLeft:0,scrollTop:0},b={x:0,y:0};return(d||!d&&!o)&&((""body""!==p(n)||h(v))&&(y=(i=n)!==t(i)&&r(i)?{scrollLeft:(a=i).scrollLeft,scrollTop:a.scrollTop}:c(i)),r(n)?((b=f(n,!0)).x+=n.clientLeft,b.y+=n.clientTop):v&&(b.x=l(v))),{x:g.left+y.scrollLeft-b.x,y:g.top+y.scrollTop-b.y,width:g.width,height:g.height}}function v(e){var t=f(e),n=e.offsetWidth,r=e.offsetHeight;return Math.abs(t.width-n)<=1&&(n=t.width),Math.abs(t.height-r)<=1&&(r=t.height),{x:e.offsetLeft,y:e.offsetTop,width:n,height:r}}function g(e){return""html""===p(e)?e:e.assignedSlot||e.parentNode||(o(e)?e.host:null)||u(e)}function y(e){return[""html"",""body"",""#document""].indexOf(p(e))>=0?e.ownerDocument.body:r(e)&&h(e)?e:y(g(e))}function b(e,n){var r;void 0===n&&(n=[]);var o=y(e),i=o===(null==(r=e.ownerDocument)?void 0:r.body),a=t(o),s=i?[a].concat(a.visualViewport||[],h(o)?o:[]):o,f=n.concat(s);return i?f:f.concat(b(g(s)))}function x(e){return[""table"",""td"",""th""].indexOf(p(e))>=0}function w(e){return r(e)&&""fixed""!==d(e).position?e.offsetParent:null}function O(e){for(var n=t(e),i=w(e);i&&x(i)&&""static""===d(i).position;)i=w(i);return i&&(""html""===p(i)||""body""===p(i)&&""static""===d(i).position)?n:i||function(e){var t=-1!==navigator.userAgent.toLowerCase().indexOf(""firefox"");if(-1!==navigator.userAgent.indexOf(""Trident"")&&r(e)&&""fixed""===d(e).position)return null;var n=g(e);for(o(n)&&(n=n.host);r(n)&&[""html"",""body""].indexOf(p(n))<0;){var i=d(n);if(""none""!==i.transform||""none""!==i.perspective||""paint""===i.contain||-1!==[""transform"",""perspective""].indexOf(i.willChange)||t&&""filter""===i.willChange||t&&i.filter&&""none""!==i.filter)return n;n=n.parentNode}return null}(e)||n}var j=""top"",E=""bottom"",D=""right"",A=""left"",L=""auto"",P=[j,E,D,A],M=""start"",k=""end"",W=""viewport"",B=""popper"",H=P.reduce((function(e,t){return e.concat([t+""-""+M,t+""-""+k])}),[]),T=[].concat(P,[L]).reduce((function(e,t){return e.concat([t,t+""-""+M,t+""-""+k])}),[]),R=[""beforeRead"",""read"",""afterRead"",""beforeMain"",""main"",""afterMain"",""beforeWrite"",""write"",""afterWrite""];function S(e){var t=new Map,n=new Set,r=[];function o(e){n.add(e.name),[].concat(e.requires||[],e.requiresIfExists||[]).forEach((function(e){if(!n.has(e)){var r=t.get(e);r&&o(r)}})),r.push(e)}return e.forEach((function(e){t.set(e.name,e)})),e.forEach((function(e){n.has(e.name)||o(e)})),r}function C(e){return e.split(""-"")[0]}function q(e,t){var n=t.getRootNode&&t.getRootNode();if(e.contains(t))return!0;if(n&&o(n)){var r=t;do{if(r&&e.isSameNode(r))return!0;r=r.parentNode||r.host}while(r)}return!1}function V(e){return Object.assign({},e,{left:e.x,top:e.y,right:e.x+e.width,bottom:e.y+e.height})}function N(e,r){return r===W?V(function(e){var n=t(e),r=u(e),o=n.visualViewport,i=r.clientWidth,a=r.clientHeight,s=0,f=0;return o&&(i=o.width,a=o.height,/^((?!chrome|android).)*safari/i.test(navigator.userAgent)||(s=o.offsetLeft,f=o.offsetTop)),{width:i,height:a,x:s+l(e),y:f}}(e)):n(r)?function(e){var t=f(e);return t.top=t.top+e.clientTop,t.left=t.left+e.clientLeft,t.bottom=t.top+e.clientHeight,t.right=t.left+e.clientWidth,t.width=e.clientWidth,t.height=e.clientHeight,t.x=t.left,t.y=t.top,t}(r):V(function(e){var t,n=u(e),r=c(e),o=null==(t=e.ownerDocument)?void 0:t.body,a=i(n.scrollWidth,n.clientWidth,o?o.scrollWidth:0,o?o.clientWidth:0),s=i(n.scrollHeight,n.clientHeight,o?o.scrollHeight:0,o?o.clientHeight:0),f=-r.scrollLeft+l(e),p=-r.scrollTop;return""rtl""===d(o||n).direction&&(f+=i(n.clientWidth,o?o.clientWidth:0)-a),{width:a,height:s,x:f,y:p}}(u(e)))}function I(e,t,o){var s=""clippingParents""===t?function(e){var t=b(g(e)),o=[""absolute"",""fixed""].indexOf(d(e).position)>=0&&r(e)?O(e):e;return n(o)?t.filter((function(e){return n(e)&&q(e,o)&&""body""!==p(e)})):[]}(e):[].concat(t),f=[].concat(s,[o]),c=f[0],u=f.reduce((function(t,n){var r=N(e,n);return t.top=i(r.top,t.top),t.right=a(r.right,t.right),t.bottom=a(r.bottom,t.bottom),t.left=i(r.left,t.left),t}),N(e,c));return u.width=u.right-u.left,u.height=u.bottom-u.top,u.x=u.left,u.y=u.top,u}function _(e){return e.split(""-"")[1]}function F(e){return[""top"",""bottom""].indexOf(e)>=0?""x"":""y""}function U(e){var t,n=e.reference,r=e.element,o=e.placement,i=o?C(o):null,a=o?_(o):null,s=n.x+n.width/2-r.width/2,f=n.y+n.height/2-r.height/2;switch(i){case j:t={x:s,y:n.y-r.height};break;case E:t={x:s,y:n.y+n.height};break;case D:t={x:n.x+n.width,y:f};break;case A:t={x:n.x-r.width,y:f};break;default:t={x:n.x,y:n.y}}var c=i?F(i):null;if(null!=c){var p=""y""===c?""height"":""width"";switch(a){case M:t[c]=t[c]-(n[p]/2-r[p]/2);break;case k:t[c]=t[c]+(n[p]/2-r[p]/2)}}return t}function z(e){return Object.assign({},{top:0,right:0,bottom:0,left:0},e)}function X(e,t){return t.reduce((function(t,n){return t[n]=e,t}),{})}function Y(e,t){void 0===t&&(t={});var r=t,o=r.placement,i=void 0===o?e.placement:o,a=r.boundary,s=void 0===a?""clippingParents"":a,c=r.rootBoundary,p=void 0===c?W:c,l=r.elementContext,d=void 0===l?B:l,h=r.altBoundary,m=void 0!==h&&h,v=r.padding,g=void 0===v?0:v,y=z(""number""!=typeof g?g:X(g,P)),b=d===B?""reference"":B,x=e.rects.popper,w=e.elements[m?b:d],O=I(n(w)?w:w.contextElement||u(e.elements.popper),s,p),A=f(e.elements.reference),L=U({reference:A,element:x,strategy:""absolute"",placement:i}),M=V(Object.assign({},x,L)),k=d===B?M:A,H={top:O.top-k.top+y.top,bottom:k.bottom-O.bottom+y.bottom,left:O.left-k.left+y.left,right:k.right-O.right+y.right},T=e.modifiersData.offset;if(d===B&&T){var R=T[i];Object.keys(H).forEach((function(e){var t=[D,E].indexOf(e)>=0?1:-1,n=[j,E].indexOf(e)>=0?""y"":""x"";H[e]+=R[n]*t}))}return H}var G={placement:""bottom"",modifiers:[],strategy:""absolute""};function J(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return!t.some((function(e){return!(e&&""function""==typeof e.getBoundingClientRect)}))}function K(e){void 0===e&&(e={});var t=e,r=t.defaultModifiers,o=void 0===r?[]:r,i=t.defaultOptions,a=void 0===i?G:i;return function(e,t,r){void 0===r&&(r=a);var i,s,f={placement:""bottom"",orderedModifiers:[],options:Object.assign({},G,a),modifiersData:{},elements:{reference:e,popper:t},attributes:{},styles:{}},c=[],p=!1,u={state:f,setOptions:function(r){var i=""function""==typeof r?r(f.options):r;l(),f.options=Object.assign({},a,f.options,i),f.scrollParents={reference:n(e)?b(e):e.contextElement?b(e.contextElement):[],popper:b(t)};var s,p,d=function(e){var t=S(e);return R.reduce((function(e,n){return e.concat(t.filter((function(e){return e.phase===n})))}),[])}((s=[].concat(o,f.options.modifiers),p=s.reduce((function(e,t){var n=e[t.name];return e[t.name]=n?Object.assign({},n,t,{options:Object.assign({},n.options,t.options),data:Object.assign({},n.data,t.data)}):t,e}),{}),Object.keys(p).map((function(e){return p[e]}))));return f.orderedModifiers=d.filter((function(e){return e.enabled})),f.orderedModifiers.forEach((function(e){var t=e.name,n=e.options,r=void 0===n?{}:n,o=e.effect;if(""function""==typeof o){var i=o({state:f,name:t,instance:u,options:r}),a=function(){};c.push(i||a)}})),u.update()},forceUpdate:function(){if(!p){var e=f.elements,t=e.reference,n=e.popper;if(J(t,n)){f.rects={reference:m(t,O(n),""fixed""===f.options.strategy),popper:v(n)},f.reset=!1,f.placement=f.options.placement,f.orderedModifiers.forEach((function(e){return f.modifiersData[e.name]=Object.assign({},e.data)}));for(var r=0;r<f.orderedModifiers.length;r++)if(!0!==f.reset){var o=f.orderedModifiers[r],i=o.fn,a=o.options,s=void 0===a?{}:a,c=o.name;""function""==typeof i&&(f=i({state:f,options:s,name:c,instance:u})||f)}else f.reset=!1,r=-1}}},update:(i=function(){return new Promise((function(e){u.forceUpdate(),e(f)}))},function(){return s||(s=new Promise((function(e){Promise.resolve().then((function(){s=void 0,e(i())}))}))),s}),destroy:function(){l(),p=!0}};if(!J(e,t))return u;function l(){c.forEach((function(e){return e()})),c=[]}return u.setOptions(r).then((function(e){!p&&r.onFirstUpdate&&r.onFirstUpdate(e)})),u}}var Q={passive:!0};var Z={name:""eventListeners"",enabled:!0,phase:""write"",fn:function(){},effect:function(e){var n=e.state,r=e.instance,o=e.options,i=o.scroll,a=void 0===i||i,s=o.resize,f=void 0===s||s,c=t(n.elements.popper),p=[].concat(n.scrollParents.reference,n.scrollParents.popper);return a&&p.forEach((function(e){e.addEventListener(""scroll"",r.update,Q)})),f&&c.addEventListener(""resize"",r.update,Q),function(){a&&p.forEach((function(e){e.removeEventListener(""scroll"",r.update,Q)})),f&&c.removeEventListener(""resize"",r.update,Q)}},data:{}};var $={name:""popperOffsets"",enabled:!0,phase:""read"",fn:function(e){var t=e.state,n=e.name;t.modifiersData[n]=U({reference:t.rects.reference,element:t.rects.popper,strategy:""absolute"",placement:t.placement})},data:{}},ee={top:""auto"",right:""auto"",bottom:""auto"",left:""auto""};function te(e){var n,r=e.popper,o=e.popperRect,i=e.placement,a=e.variation,f=e.offsets,c=e.position,p=e.gpuAcceleration,l=e.adaptive,h=e.roundOffsets,m=e.isFixed,v=f.x,g=void 0===v?0:v,y=f.y,b=void 0===y?0:y,x=""function""==typeof h?h({x:g,y:b}):{x:g,y:b};g=x.x,b=x.y;var w=f.hasOwnProperty(""x""),L=f.hasOwnProperty(""y""),P=A,M=j,W=window;if(l){var B=O(r),H=""clientHeight"",T=""clientWidth"";if(B===t(r)&&""static""!==d(B=u(r)).position&&""absolute""===c&&(H=""scrollHeight"",T=""scrollWidth""),B=B,i===j||(i===A||i===D)&&a===k)M=E,b-=(m&&B===W&&W.visualViewport?W.visualViewport.height:B[H])-o.height,b*=p?1:-1;if(i===A||(i===j||i===E)&&a===k)P=D,g-=(m&&B===W&&W.visualViewport?W.visualViewport.width:B[T])-o.width,g*=p?1:-1}var R,S=Object.assign({position:c},l&&ee),C=!0===h?function(e){var t=e.x,n=e.y,r=window.devicePixelRatio||1;return{x:s(t*r)/r||0,y:s(n*r)/r||0}}({x:g,y:b}):{x:g,y:b};return g=C.x,b=C.y,p?Object.assign({},S,((R={})[M]=L?""0"":"""",R[P]=w?""0"":"""",R.transform=(W.devicePixelRatio||1)<=1?""translate(""+g+""px, ""+b+""px)"":""translate3d(""+g+""px, ""+b+""px, 0)"",R)):Object.assign({},S,((n={})[M]=L?b+""px"":"""",n[P]=w?g+""px"":"""",n.transform="""",n))}var ne={name:""computeStyles"",enabled:!0,phase:""beforeWrite"",fn:function(e){var t=e.state,n=e.options,r=n.gpuAcceleration,o=void 0===r||r,i=n.adaptive,a=void 0===i||i,s=n.roundOffsets,f=void 0===s||s,c={placement:C(t.placement),variation:_(t.placement),popper:t.elements.popper,popperRect:t.rects.popper,gpuAcceleration:o,isFixed:""fixed""===t.options.strategy};null!=t.modifiersData.popperOffsets&&(t.styles.popper=Object.assign({},t.styles.popper,te(Object.assign({},c,{offsets:t.modifiersData.popperOffsets,position:t.options.strategy,adaptive:a,roundOffsets:f})))),null!=t.modifiersData.arrow&&(t.styles.arrow=Object.assign({},t.styles.arrow,te(Object.assign({},c,{offsets:t.modifiersData.arrow,position:""absolute"",adaptive:!1,roundOffsets:f})))),t.attributes.popper=Object.assign({},t.attributes.popper,{""data-popper-placement"":t.placement})},data:{}};var re={name:""applyStyles"",enabled:!0,phase:""write"",fn:function(e){var t=e.state;Object.keys(t.elements).forEach((function(e){var n=t.styles[e]||{},o=t.attributes[e]||{},i=t.elements[e];r(i)&&p(i)&&(Object.assign(i.style,n),Object.keys(o).forEach((function(e){var t=o[e];!1===t?i.removeAttribute(e):i.setAttribute(e,!0===t?"""":t)})))}))},effect:function(e){var t=e.state,n={popper:{position:t.options.strategy,left:""0"",top:""0"",margin:""0""},arrow:{position:""absolute""},reference:{}};return Object.assign(t.elements.popper.style,n.popper),t.styles=n,t.elements.arrow&&Object.assign(t.elements.arrow.style,n.arrow),function(){Object.keys(t.elements).forEach((function(e){var o=t.elements[e],i=t.attributes[e]||{},a=Object.keys(t.styles.hasOwnProperty(e)?t.styles[e]:n[e]).reduce((function(e,t){return e[t]="""",e}),{});r(o)&&p(o)&&(Object.assign(o.style,a),Object.keys(i).forEach((function(e){o.removeAttribute(e)})))}))}},requires:[""computeStyles""]};var oe={name:""offset"",enabled:!0,phase:""main"",requires:[""popperOffsets""],fn:function(e){var t=e.state,n=e.options,r=e.name,o=n.offset,i=void 0===o?[0,0]:o,a=T.reduce((function(e,n){return e[n]=function(e,t,n){var r=C(e),o=[A,j].indexOf(r)>=0?-1:1,i=""function""==typeof n?n(Object.assign({},t,{placement:e})):n,a=i[0],s=i[1];return a=a||0,s=(s||0)*o,[A,D].indexOf(r)>=0?{x:s,y:a}:{x:a,y:s}}(n,t.rects,i),e}),{}),s=a[t.placement],f=s.x,c=s.y;null!=t.modifiersData.popperOffsets&&(t.modifiersData.popperOffsets.x+=f,t.modifiersData.popperOffsets.y+=c),t.modifiersData[r]=a}},ie={left:""right"",right:""left"",bottom:""top"",top:""bottom""};function ae(e){return e.replace(/left|right|bottom|top/g,(function(e){return ie[e]}))}var se={start:""end"",end:""start""};function fe(e){return e.replace(/start|end/g,(function(e){return se[e]}))}function ce(e,t){void 0===t&&(t={});var n=t,r=n.placement,o=n.boundary,i=n.rootBoundary,a=n.padding,s=n.flipVariations,f=n.allowedAutoPlacements,c=void 0===f?T:f,p=_(r),u=p?s?H:H.filter((function(e){return _(e)===p})):P,l=u.filter((function(e){return c.indexOf(e)>=0}));0===l.length&&(l=u);var d=l.reduce((function(t,n){return t[n]=Y(e,{placement:n,boundary:o,rootBoundary:i,padding:a})[C(n)],t}),{});return Object.keys(d).sort((function(e,t){return d[e]-d[t]}))}var pe={name:""flip"",enabled:!0,phase:""main"",fn:function(e){var t=e.state,n=e.options,r=e.name;if(!t.modifiersData[r]._skip){for(var o=n.mainAxis,i=void 0===o||o,a=n.altAxis,s=void 0===a||a,f=n.fallbackPlacements,c=n.padding,p=n.boundary,u=n.rootBoundary,l=n.altBoundary,d=n.flipVariations,h=void 0===d||d,m=n.allowedAutoPlacements,v=t.options.placement,g=C(v),y=f||(g===v||!h?[ae(v)]:function(e){if(C(e)===L)return[];var t=ae(e);return[fe(e),t,fe(t)]}(v)),b=[v].concat(y).reduce((function(e,n){return e.concat(C(n)===L?ce(t,{placement:n,boundary:p,rootBoundary:u,padding:c,flipVariations:h,allowedAutoPlacements:m}):n)}),[]),x=t.rects.reference,w=t.rects.popper,O=new Map,P=!0,k=b[0],W=0;W<b.length;W++){var B=b[W],H=C(B),T=_(B)===M,R=[j,E].indexOf(H)>=0,S=R?""width"":""height"",q=Y(t,{placement:B,boundary:p,rootBoundary:u,altBoundary:l,padding:c}),V=R?T?D:A:T?E:j;x[S]>w[S]&&(V=ae(V));var N=ae(V),I=[];if(i&&I.push(q[H]<=0),s&&I.push(q[V]<=0,q[N]<=0),I.every((function(e){return e}))){k=B,P=!1;break}O.set(B,I)}if(P)for(var F=function(e){var t=b.find((function(t){var n=O.get(t);if(n)return n.slice(0,e).every((function(e){return e}))}));if(t)return k=t,""break""},U=h?3:1;U>0;U--){if(""break""===F(U))break}t.placement!==k&&(t.modifiersData[r]._skip=!0,t.placement=k,t.reset=!0)}},requiresIfExists:[""offset""],data:{_skip:!1}};function ue(e,t,n){return i(e,a(t,n))}var le={name:""preventOverflow"",enabled:!0,phase:""main"",fn:function(e){var t=e.state,n=e.options,r=e.name,o=n.mainAxis,s=void 0===o||o,f=n.altAxis,c=void 0!==f&&f,p=n.boundary,u=n.rootBoundary,l=n.altBoundary,d=n.padding,h=n.tether,m=void 0===h||h,g=n.tetherOffset,y=void 0===g?0:g,b=Y(t,{boundary:p,rootBoundary:u,padding:d,altBoundary:l}),x=C(t.placement),w=_(t.placement),L=!w,P=F(x),k=""x""===P?""y"":""x"",W=t.modifiersData.popperOffsets,B=t.rects.reference,H=t.rects.popper,T=""function""==typeof y?y(Object.assign({},t.rects,{placement:t.placement})):y,R=""number""==typeof T?{mainAxis:T,altAxis:T}:Object.assign({mainAxis:0,altAxis:0},T),S=t.modifiersData.offset?t.modifiersData.offset[t.placement]:null,q={x:0,y:0};if(W){if(s){var V,N=""y""===P?j:A,I=""y""===P?E:D,U=""y""===P?""height"":""width"",z=W[P],X=z+b[N],G=z-b[I],J=m?-H[U]/2:0,K=w===M?B[U]:H[U],Q=w===M?-H[U]:-B[U],Z=t.elements.arrow,$=m&&Z?v(Z):{width:0,height:0},ee=t.modifiersData[""arrow#persistent""]?t.modifiersData[""arrow#persistent""].padding:{top:0,right:0,bottom:0,left:0},te=ee[N],ne=ee[I],re=ue(0,B[U],$[U]),oe=L?B[U]/2-J-re-te-R.mainAxis:K-re-te-R.mainAxis,ie=L?-B[U]/2+J+re+ne+R.mainAxis:Q+re+ne+R.mainAxis,ae=t.elements.arrow&&O(t.elements.arrow),se=ae?""y""===P?ae.clientTop||0:ae.clientLeft||0:0,fe=null!=(V=null==S?void 0:S[P])?V:0,ce=z+ie-fe,pe=ue(m?a(X,z+oe-fe-se):X,z,m?i(G,ce):G);W[P]=pe,q[P]=pe-z}if(c){var le,de=""x""===P?j:A,he=""x""===P?E:D,me=W[k],ve=""y""===k?""height"":""width"",ge=me+b[de],ye=me-b[he],be=-1!==[j,A].indexOf(x),xe=null!=(le=null==S?void 0:S[k])?le:0,we=be?ge:me-B[ve]-H[ve]-xe+R.altAxis,Oe=be?me+B[ve]+H[ve]-xe-R.altAxis:ye,je=m&&be?function(e,t,n){var r=ue(e,t,n);return r>n?n:r}(we,me,Oe):ue(m?we:ge,me,m?Oe:ye);W[k]=je,q[k]=je-me}t.modifiersData[r]=q}},requiresIfExists:[""offset""]};var de={name:""arrow"",enabled:!0,phase:""main"",fn:function(e){var t,n=e.state,r=e.name,o=e.options,i=n.elements.arrow,a=n.modifiersData.popperOffsets,s=C(n.placement),f=F(s),c=[A,D].indexOf(s)>=0?""height"":""width"";if(i&&a){var p=function(e,t){return z(""number""!=typeof(e=""function""==typeof e?e(Object.assign({},t.rects,{placement:t.placement})):e)?e:X(e,P))}(o.padding,n),u=v(i),l=""y""===f?j:A,d=""y""===f?E:D,h=n.rects.reference[c]+n.rects.reference[f]-a[f]-n.rects.popper[c],m=a[f]-n.rects.reference[f],g=O(i),y=g?""y""===f?g.clientHeight||0:g.clientWidth||0:0,b=h/2-m/2,x=p[l],w=y-u[c]-p[d],L=y/2-u[c]/2+b,M=ue(x,L,w),k=f;n.modifiersData[r]=((t={})[k]=M,t.centerOffset=M-L,t)}},effect:function(e){var t=e.state,n=e.options.element,r=void 0===n?""[data-popper-arrow]"":n;null!=r&&(""string""!=typeof r||(r=t.elements.popper.querySelector(r)))&&q(t.elements.popper,r)&&(t.elements.arrow=r)},requires:[""popperOffsets""],requiresIfExists:[""preventOverflow""]};function he(e,t,n){return void 0===n&&(n={x:0,y:0}),{top:e.top-t.height-n.y,right:e.right-t.width+n.x,bottom:e.bottom-t.height+n.y,left:e.left-t.width-n.x}}function me(e){return[j,D,E,A].some((function(t){return e[t]>=0}))}var ve={name:""hide"",enabled:!0,phase:""main"",requiresIfExists:[""preventOverflow""],fn:function(e){var t=e.state,n=e.name,r=t.rects.reference,o=t.rects.popper,i=t.modifiersData.preventOverflow,a=Y(t,{elementContext:""reference""}),s=Y(t,{altBoundary:!0}),f=he(a,r),c=he(s,o,i),p=me(f),u=me(c);t.modifiersData[n]={referenceClippingOffsets:f,popperEscapeOffsets:c,isReferenceHidden:p,hasPopperEscaped:u},t.attributes.popper=Object.assign({},t.attributes.popper,{""data-popper-reference-hidden"":p,""data-popper-escaped"":u})}},ge=K({defaultModifiers:[Z,$,ne,re]}),ye=[Z,$,ne,re,oe,pe,le,de,ve],be=K({defaultModifiers:ye});e.applyStyles=re,e.arrow=de,e.computeStyles=ne,e.createPopper=be,e.createPopperLite=ge,e.defaultModifiers=ye,e.detectOverflow=Y,e.eventListeners=Z,e.flip=pe,e.hide=ve,e.offset=oe,e.popperGenerator=K,e.popperOffsets=$,e.preventOverflow=le,Object.defineProperty(e,""__esModule"",{value:!0})}));
-

---FILE: _freeze/tensorflow/guide/basics/libs/quarto-html/quarto-syntax-highlighting.css---
@@ -1,167 +0,0 @@
-/* quarto syntax highlight colors */
-:root {
-  --quarto-hl-ot-color: #003B4F;
-  --quarto-hl-at-color: #657422;
-  --quarto-hl-ss-color: #20794D;
-  --quarto-hl-an-color: #5E5E5E;
-  --quarto-hl-fu-color: #4758AB;
-  --quarto-hl-st-color: #20794D;
-  --quarto-hl-cf-color: #003B4F;
-  --quarto-hl-op-color: #5E5E5E;
-  --quarto-hl-er-color: #AD0000;
-  --quarto-hl-bn-color: #AD0000;
-  --quarto-hl-al-color: #AD0000;
-  --quarto-hl-va-color: #111111;
-  --quarto-hl-bu-color: inherit;
-  --quarto-hl-ex-color: inherit;
-  --quarto-hl-pp-color: #AD0000;
-  --quarto-hl-in-color: #5E5E5E;
-  --quarto-hl-vs-color: #20794D;
-  --quarto-hl-wa-color: #5E5E5E;
-  --quarto-hl-do-color: #5E5E5E;
-  --quarto-hl-im-color: #00769E;
-  --quarto-hl-ch-color: #20794D;
-  --quarto-hl-dt-color: #AD0000;
-  --quarto-hl-fl-color: #AD0000;
-  --quarto-hl-co-color: #5E5E5E;
-  --quarto-hl-cv-color: #5E5E5E;
-  --quarto-hl-cn-color: #8f5902;
-  --quarto-hl-sc-color: #5E5E5E;
-  --quarto-hl-dv-color: #AD0000;
-  --quarto-hl-kw-color: #003B4F;
-}
-
-/* other quarto variables */
-:root {
-  --quarto-font-monospace: SFMono-Regular, Menlo, Monaco, Consolas, ""Liberation Mono"", ""Courier New"", monospace;
-}
-
-code span {
-  color: #003B4F;
-}
-
-code.sourceCode > span {
-  color: #003B4F;
-}
-
-div.sourceCode,
-div.sourceCode pre.sourceCode {
-  color: #003B4F;
-}
-
-code span.ot {
-  color: #003B4F;
-}
-
-code span.at {
-  color: #657422;
-}
-
-code span.ss {
-  color: #20794D;
-}
-
-code span.an {
-  color: #5E5E5E;
-}
-
-code span.fu {
-  color: #4758AB;
-}
-
-code span.st {
-  color: #20794D;
-}
-
-code span.cf {
-  color: #003B4F;
-}
-
-code span.op {
-  color: #5E5E5E;
-}
-
-code span.er {
-  color: #AD0000;
-}
-
-code span.bn {
-  color: #AD0000;
-}
-
-code span.al {
-  color: #AD0000;
-}
-
-code span.va {
-  color: #111111;
-}
-
-code span.pp {
-  color: #AD0000;
-}
-
-code span.in {
-  color: #5E5E5E;
-}
-
-code span.vs {
-  color: #20794D;
-}
-
-code span.wa {
-  color: #5E5E5E;
-  font-style: italic;
-}
-
-code span.do {
-  color: #5E5E5E;
-  font-style: italic;
-}
-
-code span.im {
-  color: #00769E;
-}
-
-code span.ch {
-  color: #20794D;
-}
-
-code span.dt {
-  color: #AD0000;
-}
-
-code span.fl {
-  color: #AD0000;
-}
-
-code span.co {
-  color: #5E5E5E;
-}
-
-code span.cv {
-  color: #5E5E5E;
-  font-style: italic;
-}
-
-code span.cn {
-  color: #8f5902;
-}
-
-code span.sc {
-  color: #5E5E5E;
-}
-
-code span.dv {
-  color: #AD0000;
-}
-
-code span.kw {
-  color: #003B4F;
-}
-
-.prevent-inlining {
-  content: ""</"";
-}
-
-/*# sourceMappingURL=debc5d5d77c3f9108843748ff7464032.css.map */

---FILE: _freeze/tensorflow/guide/basics/libs/quarto-html/quarto.js---
@@ -1,634 +0,0 @@
-const sectionChanged = new CustomEvent(""quarto-sectionChanged"", {
-  detail: {},
-  bubbles: true,
-  cancelable: false,
-  composed: false,
-});
-
-window.document.addEventListener(""DOMContentLoaded"", function (_event) {
-  const tocEl = window.document.querySelector('nav[role=""doc-toc""]');
-  const sidebarEl = window.document.getElementById(""quarto-sidebar"");
-  const leftTocEl = window.document.getElementById(""quarto-sidebar-toc-left"");
-  const marginSidebarEl = window.document.getElementById(
-    ""quarto-margin-sidebar""
-  );
-  // function to determine whether the element has a previous sibling that is active
-  const prevSiblingIsActiveLink = (el) => {
-    const sibling = el.previousElementSibling;
-    if (sibling && sibling.tagName === ""A"") {
-      return sibling.classList.contains(""active"");
-    } else {
-      return false;
-    }
-  };
-
-  // Track scrolling and mark TOC links as active
-  // get table of contents and sidebar (bail if we don't have at least one)
-  const tocLinks = tocEl
-    ? [...tocEl.querySelectorAll(""a[data-scroll-target]"")]
-    : [];
-  const makeActive = (link) => tocLinks[link].classList.add(""active"");
-  const removeActive = (link) => tocLinks[link].classList.remove(""active"");
-  const removeAllActive = () =>
-    [...Array(tocLinks.length).keys()].forEach((link) => removeActive(link));
-
-  // activate the anchor for a section associated with this TOC entry
-  tocLinks.forEach((link) => {
-    link.addEventListener(""click"", () => {
-      if (link.href.indexOf(""#"") !== -1) {
-        const anchor = link.href.split(""#"")[1];
-        const heading = window.document.querySelector(
-          `[data-anchor-id=${anchor}]`
-        );
-        if (heading) {
-          // Add the class
-          heading.classList.add(""reveal-anchorjs-link"");
-
-          // function to show the anchor
-          const handleMouseout = () => {
-            heading.classList.remove(""reveal-anchorjs-link"");
-            heading.removeEventListener(""mouseout"", handleMouseout);
-          };
-
-          // add a function to clear the anchor when the user mouses out of it
-          heading.addEventListener(""mouseout"", handleMouseout);
-        }
-      }
-    });
-  });
-
-  const sections = tocLinks.map((link) => {
-    const target = link.getAttribute(""data-scroll-target"");
-    if (target.startsWith(""#"")) {
-      return window.document.getElementById(decodeURI(`${target.slice(1)}`));
-    } else {
-      return window.document.querySelector(decodeURI(`${target}`));
-    }
-  });
-
-  const sectionMargin = 200;
-  let currentActive = 0;
-  // track whether we've initialized state the first time
-  let init = false;
-
-  const updateActiveLink = () => {
-    // The index from bottom to top (e.g. reversed list)
-    let sectionIndex = -1;
-    if (
-      window.innerHeight + window.pageYOffset >=
-      window.document.body.offsetHeight
-    ) {
-      sectionIndex = 0;
-    } else {
-      sectionIndex = [...sections].reverse().findIndex((section) => {
-        if (section) {
-          return window.pageYOffset >= section.offsetTop - sectionMargin;
-        } else {
-          return false;
-        }
-      });
-    }
-    if (sectionIndex > -1) {
-      const current = sections.length - sectionIndex - 1;
-      if (current !== currentActive) {
-        removeAllActive();
-        currentActive = current;
-        makeActive(current);
-        if (init) {
-          window.dispatchEvent(sectionChanged);
-        }
-        init = true;
-      }
-    }
-  };
-
-  const inHiddenRegion = (top, bottom, hiddenRegions) => {
-    for (const region of hiddenRegions) {
-      if (top <= region.bottom && bottom >= region.top) {
-        return true;
-      }
-    }
-    return false;
-  };
-
-  const categorySelector = ""header.quarto-title-block .quarto-category"";
-  const activateCategories = (href) => {
-    // Find any categories
-    // Surround them with a link pointing back to:
-    // #category=Authoring
-    try {
-      const categoryEls = window.document.querySelectorAll(categorySelector);
-      for (const categoryEl of categoryEls) {
-        const categoryText = categoryEl.textContent;
-        if (categoryText) {
-          const link = `${href}#category=${encodeURIComponent(categoryText)}`;
-          const linkEl = window.document.createElement(""a"");
-          linkEl.setAttribute(""href"", link);
-          for (const child of categoryEl.childNodes) {
-            linkEl.append(child);
-          }
-          categoryEl.appendChild(linkEl);
-        }
-      }
-    } catch {
-      // Ignore errors
-    }
-  };
-  function hasTitleCategories() {
-    return window.document.querySelector(categorySelector) !== null;
-  }
-
-  function offsetRelativeUrl(url) {
-    const offset = getMeta(""quarto:offset"");
-    return offset ? offset + url : url;
-  }
-
-  function offsetAbsoluteUrl(url) {
-    const offset = getMeta(""quarto:offset"");
-    const baseUrl = new URL(offset, window.location);
-    const projRelativeUrl = url.replace(baseUrl, """");
-    return ""/"" + projRelativeUrl;
-  }
-
-  // read a meta tag value
-  function getMeta(metaName) {
-    const metas = window.document.getElementsByTagName(""meta"");
-    for (let i = 0; i < metas.length; i++) {
-      if (metas[i].getAttribute(""name"") === metaName) {
-        return metas[i].getAttribute(""content"");
-      }
-    }
-    return """";
-  }
-
-  async function findAndActivateCategories() {
-    const thisPath = window.location.pathname;
-    const response = await fetch(offsetRelativeUrl(""listings.json""));
-    if (response.status == 200) {
-      return response.json().then(function (listingPaths) {
-        const listingHrefs = [];
-        for (const listingPath of listingPaths) {
-          for (const item of listingPath.items) {
-            if (item === thisPath || item === thisPath + ""index.html"") {
-              listingHrefs.push(listingPath.listing);
-              break;
-            }
-          }
-        }
-
-        // Look up the tree for a nearby linting and use that if we find one
-        const nearestListing = findNearestParentListing(
-          offsetAbsoluteUrl(window.location.pathname),
-          listingHrefs
-        );
-        if (nearestListing) {
-          activateCategories(nearestListing);
-        } else {
-          // See if the referrer is a listing page for this item
-          const referredRelativePath = offsetAbsoluteUrl(document.referrer);
-          const referrerListing = listingHrefs.find((listingHref) => {
-            const isListingReferrer =
-              listingHref === referredRelativePath ||
-              listingHref === referredRelativePath + ""index.html"";
-            return isListingReferrer;
-          });
-
-          if (referrerListing) {
-            // Try to use the referrer if possible
-            activateCategories(referrerListing);
-          } else if (listingHrefs.length > 0) {
-            // Otherwise, just fall back to the first listing
-            activateCategories(listingHrefs[0]);
-          }
-        }
-      });
-    }
-  }
-  if (hasTitleCategories()) {
-    findAndActivateCategories();
-  }
-
-  const findNearestParentListing = (href, listingHrefs) => {
-    if (!href || !listingHrefs) {
-      return undefined;
-    }
-    // Look up the tree for a nearby linting and use that if we find one
-    const relativeParts = href.substring(1).split(""/"");
-    while (relativeParts.length > 0) {
-      const path = relativeParts.join(""/"");
-      for (const listingHref of listingHrefs) {
-        if (listingHref.startsWith(path)) {
-          return listingHref;
-        }
-      }
-      relativeParts.pop();
-    }
-
-    return undefined;
-  };
-
-  const manageSidebarVisiblity = (el, placeholderDescriptor) => {
-    let isVisible = true;
-
-    return (hiddenRegions) => {
-      if (el === null) {
-        return;
-      }
-
-      // Find the last element of the TOC
-      const lastChildEl = el.lastElementChild;
-
-      if (lastChildEl) {
-        // Find the top and bottom o the element that is being managed
-        const elTop = el.offsetTop;
-        const elBottom =
-          elTop + lastChildEl.offsetTop + lastChildEl.offsetHeight;
-
-        // Converts the sidebar to a menu
-        const convertToMenu = () => {
-          for (const child of el.children) {
-            child.style.opacity = 0;
-          }
-
-          const toggleContainer = window.document.createElement(""div"");
-          toggleContainer.style.width = ""100%"";
-          toggleContainer.classList.add(""zindex-over-content"");
-          toggleContainer.classList.add(""quarto-sidebar-toggle"");
-          toggleContainer.classList.add(""headroom-target""); // Marks this to be managed by headeroom
-          toggleContainer.id = placeholderDescriptor.id;
-          toggleContainer.style.position = ""fixed"";
-
-          const toggleIcon = window.document.createElement(""i"");
-          toggleIcon.classList.add(""quarto-sidebar-toggle-icon"");
-          toggleIcon.classList.add(""bi"");
-          toggleIcon.classList.add(""bi-caret-down-fill"");
-
-          const toggleTitle = window.document.createElement(""div"");
-          const titleEl = window.document.body.querySelector(
-            placeholderDescriptor.titleSelector
-          );
-          if (titleEl) {
-            toggleTitle.append(titleEl.innerText, toggleIcon);
-          }
-          toggleTitle.classList.add(""zindex-over-content"");
-          toggleTitle.classList.add(""quarto-sidebar-toggle-title"");
-          toggleContainer.append(toggleTitle);
-
-          const toggleContents = window.document.createElement(""div"");
-          toggleContents.classList = el.classList;
-          toggleContents.classList.add(""zindex-over-content"");
-          toggleContents.classList.add(""quarto-sidebar-toggle-contents"");
-          for (const child of el.children) {
-            if (child.id === ""toc-title"") {
-              continue;
-            }
-
-            const clone = child.cloneNode(true);
-            clone.style.opacity = 1;
-            toggleContents.append(clone);
-          }
-          toggleContents.style.height = ""0px"";
-          toggleContainer.append(toggleContents);
-          el.parentElement.prepend(toggleContainer);
-
-          // Process clicks
-          let tocShowing = false;
-          // Allow the caller to control whether this is dismissed
-          // when it is clicked (e.g. sidebar navigation supports
-          // opening and closing the nav tree, so don't dismiss on click)
-          const clickEl = placeholderDescriptor.dismissOnClick
-            ? toggleContainer
-            : toggleTitle;
-
-          const closeToggle = () => {
-            if (tocShowing) {
-              toggleContainer.classList.remove(""expanded"");
-              toggleContents.style.height = ""0px"";
-              tocShowing = false;
-            }
-          };
-
-          const positionToggle = () => {
-            // position the element (top left of parent, same width as parent)
-            const elRect = el.getBoundingClientRect();
-            toggleContainer.style.left = `${elRect.left}px`;
-            toggleContainer.style.top = `${elRect.top}px`;
-            toggleContainer.style.width = `${elRect.width}px`;
-          };
-
-          // Get rid of any expanded toggle if the user scrolls
-          window.document.addEventListener(
-            ""scroll"",
-            throttle(() => {
-              closeToggle();
-            }, 50)
-          );
-
-          // Handle positioning of the toggle
-          window.addEventListener(
-            ""resize"",
-            throttle(() => {
-              positionToggle();
-            }, 50)
-          );
-          positionToggle();
-
-          // Process the click
-          clickEl.onclick = () => {
-            if (!tocShowing) {
-              toggleContainer.classList.add(""expanded"");
-              toggleContents.style.height = null;
-              tocShowing = true;
-            } else {
-              closeToggle();
-            }
-          };
-        };
-
-        // Converts a sidebar from a menu back to a sidebar
-        const convertToSidebar = () => {
-          for (const child of el.children) {
-            child.style.opacity = 1;
-          }
-
-          const placeholderEl = window.document.getElementById(
-            placeholderDescriptor.id
-          );
-          if (placeholderEl) {
-            placeholderEl.remove();
-          }
-
-          el.classList.remove(""rollup"");
-        };
-
-        if (isReaderMode()) {
-          convertToMenu();
-          isVisible = false;
-        } else {
-          if (!isVisible) {
-            // If the element is current not visible reveal if there are
-            // no conflicts with overlay regions
-            if (!inHiddenRegion(elTop, elBottom, hiddenRegions)) {
-              convertToSidebar();
-              isVisible = true;
-            }
-          } else {
-            // If the element is visible, hide it if it conflicts with overlay regions
-            // and insert a placeholder toggle (or if we're in reader mode)
-            if (inHiddenRegion(elTop, elBottom, hiddenRegions)) {
-              convertToMenu();
-              isVisible = false;
-            }
-          }
-        }
-      }
-    };
-  };
-
-  // Find any conflicting margin elements and add margins to the
-  // top to prevent overlap
-  const marginChildren = window.document.querySelectorAll(
-    "".column-margin.column-container > * ""
-  );
-  let lastBottom = 0;
-  for (const marginChild of marginChildren) {
-    const top = marginChild.getBoundingClientRect().top;
-    if (top < lastBottom) {
-      const margin = lastBottom - top;
-      marginChild.style.marginTop = `${margin}px`;
-    }
-    lastBottom = top + marginChild.getBoundingClientRect().height;
-  }
-
-  // Manage the visibility of the toc and the sidebar
-  const marginScrollVisibility = manageSidebarVisiblity(marginSidebarEl, {
-    id: ""quarto-toc-toggle"",
-    titleSelector: ""#toc-title"",
-    dismissOnClick: true,
-  });
-  const sidebarScrollVisiblity = manageSidebarVisiblity(sidebarEl, {
-    id: ""quarto-sidebarnav-toggle"",
-    titleSelector: "".title"",
-    dismissOnClick: false,
-  });
-  let tocLeftScrollVisibility;
-  if (leftTocEl) {
-    tocLeftScrollVisibility = manageSidebarVisiblity(leftTocEl, {
-      id: ""quarto-lefttoc-toggle"",
-      titleSelector: ""#toc-title"",
-      dismissOnClick: true,
-    });
-  }
-
-  // Find the first element that uses formatting in special columns
-  const conflictingEls = window.document.body.querySelectorAll(
-    '[class^=""column-""], [class*="" column-""], aside, [class*=""margin-caption""], [class*="" margin-caption""], [class*=""margin-ref""], [class*="" margin-ref""]'
-  );
-
-  // Filter all the possibly conflicting elements into ones
-  // the do conflict on the left or ride side
-  const arrConflictingEls = Array.from(conflictingEls);
-  const leftSideConflictEls = arrConflictingEls.filter((el) => {
-    if (el.tagName === ""ASIDE"") {
-      return false;
-    }
-    return Array.from(el.classList).find((className) => {
-      return (
-        className !== ""column-body"" &&
-        className.startsWith(""column-"") &&
-        !className.endsWith(""right"") &&
-        !className.endsWith(""container"") &&
-        className !== ""column-margin""
-      );
-    });
-  });
-  const rightSideConflictEls = arrConflictingEls.filter((el) => {
-    if (el.tagName === ""ASIDE"") {
-      return true;
-    }
-
-    const hasMarginCaption = Array.from(el.classList).find((className) => {
-      return className == ""margin-caption"";
-    });
-    if (hasMarginCaption) {
-      return true;
-    }
-
-    return Array.from(el.classList).find((className) => {
-      return (
-        className !== ""column-body"" &&
-        !className.endsWith(""container"") &&
-        className.startsWith(""column-"") &&
-        !className.endsWith(""left"")
-      );
-    });
-  });
-
-  const kOverlapPaddingSize = 10;
-  function toRegions(els) {
-    return els.map((el) => {
-      const top =
-        el.getBoundingClientRect().top +
-        document.documentElement.scrollTop -
-        kOverlapPaddingSize;
-      return {
-        top,
-        bottom: top + el.scrollHeight + 2 * kOverlapPaddingSize,
-      };
-    });
-  }
-
-  const hideOverlappedSidebars = () => {
-    marginScrollVisibility(toRegions(rightSideConflictEls));
-    sidebarScrollVisiblity(toRegions(leftSideConflictEls));
-    if (tocLeftScrollVisibility) {
-      tocLeftScrollVisibility(toRegions(leftSideConflictEls));
-    }
-  };
-
-  window.quartoToggleReader = () => {
-    // Applies a slow class (or removes it)
-    // to update the transition speed
-    const slowTransition = (slow) => {
-      const manageTransition = (id, slow) => {
-        const el = document.getElementById(id);
-        if (el) {
-          if (slow) {
-            el.classList.add(""slow"");
-          } else {
-            el.classList.remove(""slow"");
-          }
-        }
-      };
-
-      manageTransition(""TOC"", slow);
-      manageTransition(""quarto-sidebar"", slow);
-    };
-
-    const readerMode = !isReaderMode();
-    setReaderModeValue(readerMode);
-
-    // If we're entering reader mode, slow the transition
-    if (readerMode) {
-      slowTransition(readerMode);
-    }
-    highlightReaderToggle(readerMode);
-    hideOverlappedSidebars();
-
-    // If we're exiting reader mode, restore the non-slow transition
-    if (!readerMode) {
-      slowTransition(!readerMode);
-    }
-  };
-
-  const highlightReaderToggle = (readerMode) => {
-    const els = document.querySelectorAll("".quarto-reader-toggle"");
-    if (els) {
-      els.forEach((el) => {
-        if (readerMode) {
-          el.classList.add(""reader"");
-        } else {
-          el.classList.remove(""reader"");
-        }
-      });
-    }
-  };
-
-  const setReaderModeValue = (val) => {
-    if (window.location.protocol !== ""file:"") {
-      window.localStorage.setItem(""quarto-reader-mode"", val);
-    } else {
-      localReaderMode = val;
-    }
-  };
-
-  const isReaderMode = () => {
-    if (window.location.protocol !== ""file:"") {
-      return window.localStorage.getItem(""quarto-reader-mode"") === ""true"";
-    } else {
-      return localReaderMode;
-    }
-  };
-  let localReaderMode = null;
-
-  // Walk the TOC and collapse/expand nodes
-  // Nodes are expanded if:
-  // - they are top level
-  // - they have children that are 'active' links
-  // - they are directly below an link that is 'active'
-  const walk = (el, depth) => {
-    // Tick depth when we enter a UL
-    if (el.tagName === ""UL"") {
-      depth = depth + 1;
-    }
-
-    // It this is active link
-    let isActiveNode = false;
-    if (el.tagName === ""A"" && el.classList.contains(""active"")) {
-      isActiveNode = true;
-    }
-
-    // See if there is an active child to this element
-    let hasActiveChild = false;
-    for (child of el.children) {
-      hasActiveChild = walk(child, depth) || hasActiveChild;
-    }
-
-    // Process the collapse state if this is an UL
-    if (el.tagName === ""UL"") {
-      if (depth === 1 || hasActiveChild || prevSiblingIsActiveLink(el)) {
-        el.classList.remove(""collapse"");
-      } else {
-        el.classList.add(""collapse"");
-      }
-
-      // untick depth when we leave a UL
-      depth = depth - 1;
-    }
-    return hasActiveChild || isActiveNode;
-  };
-
-  // walk the TOC and expand / collapse any items that should be shown
-
-  if (tocEl) {
-    walk(tocEl, 0);
-    updateActiveLink();
-  }
-
-  // Throttle the scroll event and walk peridiocally
-  window.document.addEventListener(
-    ""scroll"",
-    throttle(() => {
-      if (tocEl) {
-        updateActiveLink();
-        walk(tocEl, 0);
-      }
-      if (!isReaderMode()) {
-        hideOverlappedSidebars();
-      }
-    }, 5)
-  );
-  window.addEventListener(
-    ""resize"",
-    throttle(() => {
-      if (!isReaderMode()) {
-        hideOverlappedSidebars();
-      }
-    }, 10)
-  );
-  hideOverlappedSidebars();
-  highlightReaderToggle(isReaderMode());
-});
-
-function throttle(func, wait) {
-  let waiting = false;
-  return function () {
-    if (!waiting) {
-      func.apply(this, arguments);
-      waiting = true;
-      setTimeout(function () {
-        waiting = false;
-      }, wait);
-    }
-  };
-}

---FILE: _freeze/tensorflow/guide/basics/libs/quarto-html/tippy.css---
@@ -1 +0,0 @@
-.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{position:relative;background-color:#333;color:#fff;border-radius:4px;font-size:14px;line-height:1.4;white-space:normal;outline:0;transition-property:transform,visibility,opacity}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{bottom:-7px;left:0;border-width:8px 8px 0;border-top-color:initial;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{top:-7px;left:0;border-width:0 8px 8px;border-bottom-color:initial;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-width:8px 0 8px 8px;border-left-color:initial;right:-7px;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{left:-7px;border-width:8px 8px 8px 0;border-right-color:initial;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{width:16px;height:16px;color:#333}.tippy-arrow:before{content:"""";position:absolute;border-color:transparent;border-style:solid}.tippy-content{position:relative;padding:5px 9px;z-index:1}
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/basics/libs/quarto-html/tippy.umd.min.js---
@@ -1,2 +0,0 @@
-!function(e,t){""object""==typeof exports&&""undefined""!=typeof module?module.exports=t(require(""@popperjs/core"")):""function""==typeof define&&define.amd?define([""@popperjs/core""],t):(e=e||self).tippy=t(e.Popper)}(this,(function(e){""use strict"";var t={passive:!0,capture:!0},n=function(){return document.body};function r(e,t,n){if(Array.isArray(e)){var r=e[t];return null==r?Array.isArray(n)?n[t]:n:r}return e}function o(e,t){var n={}.toString.call(e);return 0===n.indexOf(""[object"")&&n.indexOf(t+""]"")>-1}function i(e,t){return""function""==typeof e?e.apply(void 0,t):e}function a(e,t){return 0===t?e:function(r){clearTimeout(n),n=setTimeout((function(){e(r)}),t)};var n}function s(e,t){var n=Object.assign({},e);return t.forEach((function(e){delete n[e]})),n}function u(e){return[].concat(e)}function c(e,t){-1===e.indexOf(t)&&e.push(t)}function p(e){return e.split(""-"")[0]}function f(e){return[].slice.call(e)}function l(e){return Object.keys(e).reduce((function(t,n){return void 0!==e[n]&&(t[n]=e[n]),t}),{})}function d(){return document.createElement(""div"")}function v(e){return[""Element"",""Fragment""].some((function(t){return o(e,t)}))}function m(e){return o(e,""MouseEvent"")}function g(e){return!(!e||!e._tippy||e._tippy.reference!==e)}function h(e){return v(e)?[e]:function(e){return o(e,""NodeList"")}(e)?f(e):Array.isArray(e)?e:f(document.querySelectorAll(e))}function b(e,t){e.forEach((function(e){e&&(e.style.transitionDuration=t+""ms"")}))}function y(e,t){e.forEach((function(e){e&&e.setAttribute(""data-state"",t)}))}function w(e){var t,n=u(e)[0];return null!=n&&null!=(t=n.ownerDocument)&&t.body?n.ownerDocument:document}function E(e,t,n){var r=t+""EventListener"";[""transitionend"",""webkitTransitionEnd""].forEach((function(t){e[r](t,n)}))}function O(e,t){for(var n=t;n;){var r;if(e.contains(n))return!0;n=null==n.getRootNode||null==(r=n.getRootNode())?void 0:r.host}return!1}var x={isTouch:!1},C=0;function T(){x.isTouch||(x.isTouch=!0,window.performance&&document.addEventListener(""mousemove"",A))}function A(){var e=performance.now();e-C<20&&(x.isTouch=!1,document.removeEventListener(""mousemove"",A)),C=e}function L(){var e=document.activeElement;if(g(e)){var t=e._tippy;e.blur&&!t.state.isVisible&&e.blur()}}var D=!!(""undefined""!=typeof window&&""undefined""!=typeof document)&&!!window.msCrypto,R=Object.assign({appendTo:n,aria:{content:""auto"",expanded:""auto""},delay:0,duration:[300,250],getReferenceClientRect:null,hideOnClick:!0,ignoreAttributes:!1,interactive:!1,interactiveBorder:2,interactiveDebounce:0,moveTransition:"""",offset:[0,10],onAfterUpdate:function(){},onBeforeUpdate:function(){},onCreate:function(){},onDestroy:function(){},onHidden:function(){},onHide:function(){},onMount:function(){},onShow:function(){},onShown:function(){},onTrigger:function(){},onUntrigger:function(){},onClickOutside:function(){},placement:""top"",plugins:[],popperOptions:{},render:null,showOnCreate:!1,touch:!0,trigger:""mouseenter focus"",triggerTarget:null},{animateFill:!1,followCursor:!1,inlinePositioning:!1,sticky:!1},{allowHTML:!1,animation:""fade"",arrow:!0,content:"""",inertia:!1,maxWidth:350,role:""tooltip"",theme:"""",zIndex:9999}),k=Object.keys(R);function P(e){var t=(e.plugins||[]).reduce((function(t,n){var r,o=n.name,i=n.defaultValue;o&&(t[o]=void 0!==e[o]?e[o]:null!=(r=R[o])?r:i);return t}),{});return Object.assign({},e,t)}function j(e,t){var n=Object.assign({},t,{content:i(t.content,[e])},t.ignoreAttributes?{}:function(e,t){return(t?Object.keys(P(Object.assign({},R,{plugins:t}))):k).reduce((function(t,n){var r=(e.getAttribute(""data-tippy-""+n)||"""").trim();if(!r)return t;if(""content""===n)t[n]=r;else try{t[n]=JSON.parse(r)}catch(e){t[n]=r}return t}),{})}(e,t.plugins));return n.aria=Object.assign({},R.aria,n.aria),n.aria={expanded:""auto""===n.aria.expanded?t.interactive:n.aria.expanded,content:""auto""===n.aria.content?t.interactive?null:""describedby"":n.aria.content},n}function M(e,t){e.innerHTML=t}function V(e){var t=d();return!0===e?t.className=""tippy-arrow"":(t.className=""tippy-svg-arrow"",v(e)?t.appendChild(e):M(t,e)),t}function I(e,t){v(t.content)?(M(e,""""),e.appendChild(t.content)):""function""!=typeof t.content&&(t.allowHTML?M(e,t.content):e.textContent=t.content)}function S(e){var t=e.firstElementChild,n=f(t.children);return{box:t,content:n.find((function(e){return e.classList.contains(""tippy-content"")})),arrow:n.find((function(e){return e.classList.contains(""tippy-arrow"")||e.classList.contains(""tippy-svg-arrow"")})),backdrop:n.find((function(e){return e.classList.contains(""tippy-backdrop"")}))}}function N(e){var t=d(),n=d();n.className=""tippy-box"",n.setAttribute(""data-state"",""hidden""),n.setAttribute(""tabindex"",""-1"");var r=d();function o(n,r){var o=S(t),i=o.box,a=o.content,s=o.arrow;r.theme?i.setAttribute(""data-theme"",r.theme):i.removeAttribute(""data-theme""),""string""==typeof r.animation?i.setAttribute(""data-animation"",r.animation):i.removeAttribute(""data-animation""),r.inertia?i.setAttribute(""data-inertia"",""""):i.removeAttribute(""data-inertia""),i.style.maxWidth=""number""==typeof r.maxWidth?r.maxWidth+""px"":r.maxWidth,r.role?i.setAttribute(""role"",r.role):i.removeAttribute(""role""),n.content===r.content&&n.allowHTML===r.allowHTML||I(a,e.props),r.arrow?s?n.arrow!==r.arrow&&(i.removeChild(s),i.appendChild(V(r.arrow))):i.appendChild(V(r.arrow)):s&&i.removeChild(s)}return r.className=""tippy-content"",r.setAttribute(""data-state"",""hidden""),I(r,e.props),t.appendChild(n),n.appendChild(r),o(e.props,e.props),{popper:t,onUpdate:o}}N.$$tippy=!0;var B=1,H=[],U=[];function _(o,s){var v,g,h,C,T,A,L,k,M=j(o,Object.assign({},R,P(l(s)))),V=!1,I=!1,N=!1,_=!1,F=[],W=a(we,M.interactiveDebounce),X=B++,Y=(k=M.plugins).filter((function(e,t){return k.indexOf(e)===t})),$={id:X,reference:o,popper:d(),popperInstance:null,props:M,state:{isEnabled:!0,isVisible:!1,isDestroyed:!1,isMounted:!1,isShown:!1},plugins:Y,clearDelayTimeouts:function(){clearTimeout(v),clearTimeout(g),cancelAnimationFrame(h)},setProps:function(e){if($.state.isDestroyed)return;ae(""onBeforeUpdate"",[$,e]),be();var t=$.props,n=j(o,Object.assign({},t,l(e),{ignoreAttributes:!0}));$.props=n,he(),t.interactiveDebounce!==n.interactiveDebounce&&(ce(),W=a(we,n.interactiveDebounce));t.triggerTarget&&!n.triggerTarget?u(t.triggerTarget).forEach((function(e){e.removeAttribute(""aria-expanded"")})):n.triggerTarget&&o.removeAttribute(""aria-expanded"");ue(),ie(),J&&J(t,n);$.popperInstance&&(Ce(),Ae().forEach((function(e){requestAnimationFrame(e._tippy.popperInstance.forceUpdate)})));ae(""onAfterUpdate"",[$,e])},setContent:function(e){$.setProps({content:e})},show:function(){var e=$.state.isVisible,t=$.state.isDestroyed,o=!$.state.isEnabled,a=x.isTouch&&!$.props.touch,s=r($.props.duration,0,R.duration);if(e||t||o||a)return;if(te().hasAttribute(""disabled""))return;if(ae(""onShow"",[$],!1),!1===$.props.onShow($))return;$.state.isVisible=!0,ee()&&(z.style.visibility=""visible"");ie(),de(),$.state.isMounted||(z.style.transition=""none"");if(ee()){var u=re(),p=u.box,f=u.content;b([p,f],0)}A=function(){var e;if($.state.isVisible&&!_){if(_=!0,z.offsetHeight,z.style.transition=$.props.moveTransition,ee()&&$.props.animation){var t=re(),n=t.box,r=t.content;b([n,r],s),y([n,r],""visible"")}se(),ue(),c(U,$),null==(e=$.popperInstance)||e.forceUpdate(),ae(""onMount"",[$]),$.props.animation&&ee()&&function(e,t){me(e,t)}(s,(function(){$.state.isShown=!0,ae(""onShown"",[$])}))}},function(){var e,t=$.props.appendTo,r=te();e=$.props.interactive&&t===n||""parent""===t?r.parentNode:i(t,[r]);e.contains(z)||e.appendChild(z);$.state.isMounted=!0,Ce()}()},hide:function(){var e=!$.state.isVisible,t=$.state.isDestroyed,n=!$.state.isEnabled,o=r($.props.duration,1,R.duration);if(e||t||n)return;if(ae(""onHide"",[$],!1),!1===$.props.onHide($))return;$.state.isVisible=!1,$.state.isShown=!1,_=!1,V=!1,ee()&&(z.style.visibility=""hidden"");if(ce(),ve(),ie(!0),ee()){var i=re(),a=i.box,s=i.content;$.props.animation&&(b([a,s],o),y([a,s],""hidden""))}se(),ue(),$.props.animation?ee()&&function(e,t){me(e,(function(){!$.state.isVisible&&z.parentNode&&z.parentNode.contains(z)&&t()}))}(o,$.unmount):$.unmount()},hideWithInteractivity:function(e){ne().addEventListener(""mousemove"",W),c(H,W),W(e)},enable:function(){$.state.isEnabled=!0},disable:function(){$.hide(),$.state.isEnabled=!1},unmount:function(){$.state.isVisible&&$.hide();if(!$.state.isMounted)return;Te(),Ae().forEach((function(e){e._tippy.unmount()})),z.parentNode&&z.parentNode.removeChild(z);U=U.filter((function(e){return e!==$})),$.state.isMounted=!1,ae(""onHidden"",[$])},destroy:function(){if($.state.isDestroyed)return;$.clearDelayTimeouts(),$.unmount(),be(),delete o._tippy,$.state.isDestroyed=!0,ae(""onDestroy"",[$])}};if(!M.render)return $;var q=M.render($),z=q.popper,J=q.onUpdate;z.setAttribute(""data-tippy-root"",""""),z.id=""tippy-""+$.id,$.popper=z,o._tippy=$,z._tippy=$;var G=Y.map((function(e){return e.fn($)})),K=o.hasAttribute(""aria-expanded"");return he(),ue(),ie(),ae(""onCreate"",[$]),M.showOnCreate&&Le(),z.addEventListener(""mouseenter"",(function(){$.props.interactive&&$.state.isVisible&&$.clearDelayTimeouts()})),z.addEventListener(""mouseleave"",(function(){$.props.interactive&&$.props.trigger.indexOf(""mouseenter"")>=0&&ne().addEventListener(""mousemove"",W)})),$;function Q(){var e=$.props.touch;return Array.isArray(e)?e:[e,0]}function Z(){return""hold""===Q()[0]}function ee(){var e;return!(null==(e=$.props.render)||!e.$$tippy)}function te(){return L||o}function ne(){var e=te().parentNode;return e?w(e):document}function re(){return S(z)}function oe(e){return $.state.isMounted&&!$.state.isVisible||x.isTouch||C&&""focus""===C.type?0:r($.props.delay,e?0:1,R.delay)}function ie(e){void 0===e&&(e=!1),z.style.pointerEvents=$.props.interactive&&!e?"""":""none"",z.style.zIndex=""""+$.props.zIndex}function ae(e,t,n){var r;(void 0===n&&(n=!0),G.forEach((function(n){n[e]&&n[e].apply(n,t)})),n)&&(r=$.props)[e].apply(r,t)}function se(){var e=$.props.aria;if(e.content){var t=""aria-""+e.content,n=z.id;u($.props.triggerTarget||o).forEach((function(e){var r=e.getAttribute(t);if($.state.isVisible)e.setAttribute(t,r?r+"" ""+n:n);else{var o=r&&r.replace(n,"""").trim();o?e.setAttribute(t,o):e.removeAttribute(t)}}))}}function ue(){!K&&$.props.aria.expanded&&u($.props.triggerTarget||o).forEach((function(e){$.props.interactive?e.setAttribute(""aria-expanded"",$.state.isVisible&&e===te()?""true"":""false""):e.removeAttribute(""aria-expanded"")}))}function ce(){ne().removeEventListener(""mousemove"",W),H=H.filter((function(e){return e!==W}))}function pe(e){if(!x.isTouch||!N&&""mousedown""!==e.type){var t=e.composedPath&&e.composedPath()[0]||e.target;if(!$.props.interactive||!O(z,t)){if(u($.props.triggerTarget||o).some((function(e){return O(e,t)}))){if(x.isTouch)return;if($.state.isVisible&&$.props.trigger.indexOf(""click"")>=0)return}else ae(""onClickOutside"",[$,e]);!0===$.props.hideOnClick&&($.clearDelayTimeouts(),$.hide(),I=!0,setTimeout((function(){I=!1})),$.state.isMounted||ve())}}}function fe(){N=!0}function le(){N=!1}function de(){var e=ne();e.addEventListener(""mousedown"",pe,!0),e.addEventListener(""touchend"",pe,t),e.addEventListener(""touchstart"",le,t),e.addEventListener(""touchmove"",fe,t)}function ve(){var e=ne();e.removeEventListener(""mousedown"",pe,!0),e.removeEventListener(""touchend"",pe,t),e.removeEventListener(""touchstart"",le,t),e.removeEventListener(""touchmove"",fe,t)}function me(e,t){var n=re().box;function r(e){e.target===n&&(E(n,""remove"",r),t())}if(0===e)return t();E(n,""remove"",T),E(n,""add"",r),T=r}function ge(e,t,n){void 0===n&&(n=!1),u($.props.triggerTarget||o).forEach((function(r){r.addEventListener(e,t,n),F.push({node:r,eventType:e,handler:t,options:n})}))}function he(){var e;Z()&&(ge(""touchstart"",ye,{passive:!0}),ge(""touchend"",Ee,{passive:!0})),(e=$.props.trigger,e.split(/\s+/).filter(Boolean)).forEach((function(e){if(""manual""!==e)switch(ge(e,ye),e){case""mouseenter"":ge(""mouseleave"",Ee);break;case""focus"":ge(D?""focusout"":""blur"",Oe);break;case""focusin"":ge(""focusout"",Oe)}}))}function be(){F.forEach((function(e){var t=e.node,n=e.eventType,r=e.handler,o=e.options;t.removeEventListener(n,r,o)})),F=[]}function ye(e){var t,n=!1;if($.state.isEnabled&&!xe(e)&&!I){var r=""focus""===(null==(t=C)?void 0:t.type);C=e,L=e.currentTarget,ue(),!$.state.isVisible&&m(e)&&H.forEach((function(t){return t(e)})),""click""===e.type&&($.props.trigger.indexOf(""mouseenter"")<0||V)&&!1!==$.props.hideOnClick&&$.state.isVisible?n=!0:Le(e),""click""===e.type&&(V=!n),n&&!r&&De(e)}}function we(e){var t=e.target,n=te().contains(t)||z.contains(t);""mousemove""===e.type&&n||function(e,t){var n=t.clientX,r=t.clientY;return e.every((function(e){var t=e.popperRect,o=e.popperState,i=e.props.interactiveBorder,a=p(o.placement),s=o.modifiersData.offset;if(!s)return!0;var u=""bottom""===a?s.top.y:0,c=""top""===a?s.bottom.y:0,f=""right""===a?s.left.x:0,l=""left""===a?s.right.x:0,d=t.top-r+u>i,v=r-t.bottom-c>i,m=t.left-n+f>i,g=n-t.right-l>i;return d||v||m||g}))}(Ae().concat(z).map((function(e){var t,n=null==(t=e._tippy.popperInstance)?void 0:t.state;return n?{popperRect:e.getBoundingClientRect(),popperState:n,props:M}:null})).filter(Boolean),e)&&(ce(),De(e))}function Ee(e){xe(e)||$.props.trigger.indexOf(""click"")>=0&&V||($.props.interactive?$.hideWithInteractivity(e):De(e))}function Oe(e){$.props.trigger.indexOf(""focusin"")<0&&e.target!==te()||$.props.interactive&&e.relatedTarget&&z.contains(e.relatedTarget)||De(e)}function xe(e){return!!x.isTouch&&Z()!==e.type.indexOf(""touch"")>=0}function Ce(){Te();var t=$.props,n=t.popperOptions,r=t.placement,i=t.offset,a=t.getReferenceClientRect,s=t.moveTransition,u=ee()?S(z).arrow:null,c=a?{getBoundingClientRect:a,contextElement:a.contextElement||te()}:o,p=[{name:""offset"",options:{offset:i}},{name:""preventOverflow"",options:{padding:{top:2,bottom:2,left:5,right:5}}},{name:""flip"",options:{padding:5}},{name:""computeStyles"",options:{adaptive:!s}},{name:""$$tippy"",enabled:!0,phase:""beforeWrite"",requires:[""computeStyles""],fn:function(e){var t=e.state;if(ee()){var n=re().box;[""placement"",""reference-hidden"",""escaped""].forEach((function(e){""placement""===e?n.setAttribute(""data-placement"",t.placement):t.attributes.popper[""data-popper-""+e]?n.setAttribute(""data-""+e,""""):n.removeAttribute(""data-""+e)})),t.attributes.popper={}}}}];ee()&&u&&p.push({name:""arrow"",options:{element:u,padding:3}}),p.push.apply(p,(null==n?void 0:n.modifiers)||[]),$.popperInstance=e.createPopper(c,z,Object.assign({},n,{placement:r,onFirstUpdate:A,modifiers:p}))}function Te(){$.popperInstance&&($.popperInstance.destroy(),$.popperInstance=null)}function Ae(){return f(z.querySelectorAll(""[data-tippy-root]""))}function Le(e){$.clearDelayTimeouts(),e&&ae(""onTrigger"",[$,e]),de();var t=oe(!0),n=Q(),r=n[0],o=n[1];x.isTouch&&""hold""===r&&o&&(t=o),t?v=setTimeout((function(){$.show()}),t):$.show()}function De(e){if($.clearDelayTimeouts(),ae(""onUntrigger"",[$,e]),$.state.isVisible){if(!($.props.trigger.indexOf(""mouseenter"")>=0&&$.props.trigger.indexOf(""click"")>=0&&[""mouseleave"",""mousemove""].indexOf(e.type)>=0&&V)){var t=oe(!1);t?g=setTimeout((function(){$.state.isVisible&&$.hide()}),t):h=requestAnimationFrame((function(){$.hide()}))}}else ve()}}function F(e,n){void 0===n&&(n={});var r=R.plugins.concat(n.plugins||[]);document.addEventListener(""touchstart"",T,t),window.addEventListener(""blur"",L);var o=Object.assign({},n,{plugins:r}),i=h(e).reduce((function(e,t){var n=t&&_(t,o);return n&&e.push(n),e}),[]);return v(e)?i[0]:i}F.defaultProps=R,F.setDefaultProps=function(e){Object.keys(e).forEach((function(t){R[t]=e[t]}))},F.currentInput=x;var W=Object.assign({},e.applyStyles,{effect:function(e){var t=e.state,n={popper:{position:t.options.strategy,left:""0"",top:""0"",margin:""0""},arrow:{position:""absolute""},reference:{}};Object.assign(t.elements.popper.style,n.popper),t.styles=n,t.elements.arrow&&Object.assign(t.elements.arrow.style,n.arrow)}}),X={mouseover:""mouseenter"",focusin:""focus"",click:""click""};var Y={name:""animateFill"",defaultValue:!1,fn:function(e){var t;if(null==(t=e.props.render)||!t.$$tippy)return{};var n=S(e.popper),r=n.box,o=n.content,i=e.props.animateFill?function(){var e=d();return e.className=""tippy-backdrop"",y([e],""hidden""),e}():null;return{onCreate:function(){i&&(r.insertBefore(i,r.firstElementChild),r.setAttribute(""data-animatefill"",""""),r.style.overflow=""hidden"",e.setProps({arrow:!1,animation:""shift-away""}))},onMount:function(){if(i){var e=r.style.transitionDuration,t=Number(e.replace(""ms"",""""));o.style.transitionDelay=Math.round(t/10)+""ms"",i.style.transitionDuration=e,y([i],""visible"")}},onShow:function(){i&&(i.style.transitionDuration=""0ms"")},onHide:function(){i&&y([i],""hidden"")}}}};var $={clientX:0,clientY:0},q=[];function z(e){var t=e.clientX,n=e.clientY;$={clientX:t,clientY:n}}var J={name:""followCursor"",defaultValue:!1,fn:function(e){var t=e.reference,n=w(e.props.triggerTarget||t),r=!1,o=!1,i=!0,a=e.props;function s(){return""initial""===e.props.followCursor&&e.state.isVisible}function u(){n.addEventListener(""mousemove"",f)}function c(){n.removeEventListener(""mousemove"",f)}function p(){r=!0,e.setProps({getReferenceClientRect:null}),r=!1}function f(n){var r=!n.target||t.contains(n.target),o=e.props.followCursor,i=n.clientX,a=n.clientY,s=t.getBoundingClientRect(),u=i-s.left,c=a-s.top;!r&&e.props.interactive||e.setProps({getReferenceClientRect:function(){var e=t.getBoundingClientRect(),n=i,r=a;""initial""===o&&(n=e.left+u,r=e.top+c);var s=""horizontal""===o?e.top:r,p=""vertical""===o?e.right:n,f=""horizontal""===o?e.bottom:r,l=""vertical""===o?e.left:n;return{width:p-l,height:f-s,top:s,right:p,bottom:f,left:l}}})}function l(){e.props.followCursor&&(q.push({instance:e,doc:n}),function(e){e.addEventListener(""mousemove"",z)}(n))}function d(){0===(q=q.filter((function(t){return t.instance!==e}))).filter((function(e){return e.doc===n})).length&&function(e){e.removeEventListener(""mousemove"",z)}(n)}return{onCreate:l,onDestroy:d,onBeforeUpdate:function(){a=e.props},onAfterUpdate:function(t,n){var i=n.followCursor;r||void 0!==i&&a.followCursor!==i&&(d(),i?(l(),!e.state.isMounted||o||s()||u()):(c(),p()))},onMount:function(){e.props.followCursor&&!o&&(i&&(f($),i=!1),s()||u())},onTrigger:function(e,t){m(t)&&($={clientX:t.clientX,clientY:t.clientY}),o=""focus""===t.type},onHidden:function(){e.props.followCursor&&(p(),c(),i=!0)}}}};var G={name:""inlinePositioning"",defaultValue:!1,fn:function(e){var t,n=e.reference;var r=-1,o=!1,i=[],a={name:""tippyInlinePositioning"",enabled:!0,phase:""afterWrite"",fn:function(o){var a=o.state;e.props.inlinePositioning&&(-1!==i.indexOf(a.placement)&&(i=[]),t!==a.placement&&-1===i.indexOf(a.placement)&&(i.push(a.placement),e.setProps({getReferenceClientRect:function(){return function(e){return function(e,t,n,r){if(n.length<2||null===e)return t;if(2===n.length&&r>=0&&n[0].left>n[1].right)return n[r]||t;switch(e){case""top"":case""bottom"":var o=n[0],i=n[n.length-1],a=""top""===e,s=o.top,u=i.bottom,c=a?o.left:i.left,p=a?o.right:i.right;return{top:s,bottom:u,left:c,right:p,width:p-c,height:u-s};case""left"":case""right"":var f=Math.min.apply(Math,n.map((function(e){return e.left}))),l=Math.max.apply(Math,n.map((function(e){return e.right}))),d=n.filter((function(t){return""left""===e?t.left===f:t.right===l})),v=d[0].top,m=d[d.length-1].bottom;return{top:v,bottom:m,left:f,right:l,width:l-f,height:m-v};default:return t}}(p(e),n.getBoundingClientRect(),f(n.getClientRects()),r)}(a.placement)}})),t=a.placement)}};function s(){var t;o||(t=function(e,t){var n;return{popperOptions:Object.assign({},e.popperOptions,{modifiers:[].concat(((null==(n=e.popperOptions)?void 0:n.modifiers)||[]).filter((function(e){return e.name!==t.name})),[t])})}}(e.props,a),o=!0,e.setProps(t),o=!1)}return{onCreate:s,onAfterUpdate:s,onTrigger:function(t,n){if(m(n)){var o=f(e.reference.getClientRects()),i=o.find((function(e){return e.left-2<=n.clientX&&e.right+2>=n.clientX&&e.top-2<=n.clientY&&e.bottom+2>=n.clientY})),a=o.indexOf(i);r=a>-1?a:r}},onHidden:function(){r=-1}}}};var K={name:""sticky"",defaultValue:!1,fn:function(e){var t=e.reference,n=e.popper;function r(t){return!0===e.props.sticky||e.props.sticky===t}var o=null,i=null;function a(){var s=r(""reference"")?(e.popperInstance?e.popperInstance.state.elements.reference:t).getBoundingClientRect():null,u=r(""popper"")?n.getBoundingClientRect():null;(s&&Q(o,s)||u&&Q(i,u))&&e.popperInstance&&e.popperInstance.update(),o=s,i=u,e.state.isMounted&&requestAnimationFrame(a)}return{onMount:function(){e.props.sticky&&a()}}}};function Q(e,t){return!e||!t||(e.top!==t.top||e.right!==t.right||e.bottom!==t.bottom||e.left!==t.left)}return F.setDefaultProps({plugins:[Y,J,G,K],render:N}),F.createSingleton=function(e,t){var n;void 0===t&&(t={});var r,o=e,i=[],a=[],c=t.overrides,p=[],f=!1;function l(){a=o.map((function(e){return u(e.props.triggerTarget||e.reference)})).reduce((function(e,t){return e.concat(t)}),[])}function v(){i=o.map((function(e){return e.reference}))}function m(e){o.forEach((function(t){e?t.enable():t.disable()}))}function g(e){return o.map((function(t){var n=t.setProps;return t.setProps=function(o){n(o),t.reference===r&&e.setProps(o)},function(){t.setProps=n}}))}function h(e,t){var n=a.indexOf(t);if(t!==r){r=t;var s=(c||[]).concat(""content"").reduce((function(e,t){return e[t]=o[n].props[t],e}),{});e.setProps(Object.assign({},s,{getReferenceClientRect:""function""==typeof s.getReferenceClientRect?s.getReferenceClientRect:function(){var e;return null==(e=i[n])?void 0:e.getBoundingClientRect()}}))}}m(!1),v(),l();var b={fn:function(){return{onDestroy:function(){m(!0)},onHidden:function(){r=null},onClickOutside:function(e){e.props.showOnCreate&&!f&&(f=!0,r=null)},onShow:function(e){e.props.showOnCreate&&!f&&(f=!0,h(e,i[0]))},onTrigger:function(e,t){h(e,t.currentTarget)}}}},y=F(d(),Object.assign({},s(t,[""overrides""]),{plugins:[b].concat(t.plugins||[]),triggerTarget:a,popperOptions:Object.assign({},t.popperOptions,{modifiers:[].concat((null==(n=t.popperOptions)?void 0:n.modifiers)||[],[W])})})),w=y.show;y.show=function(e){if(w(),!r&&null==e)return h(y,i[0]);if(!r||null!=e){if(""number""==typeof e)return i[e]&&h(y,i[e]);if(o.indexOf(e)>=0){var t=e.reference;return h(y,t)}return i.indexOf(e)>=0?h(y,e):void 0}},y.showNext=function(){var e=i[0];if(!r)return y.show(0);var t=i.indexOf(r);y.show(i[t+1]||e)},y.showPrevious=function(){var e=i[i.length-1];if(!r)return y.show(e);var t=i.indexOf(r),n=i[t-1]||e;y.show(n)};var E=y.setProps;return y.setProps=function(e){c=e.overrides||c,E(e)},y.setInstances=function(e){m(!0),p.forEach((function(e){return e()})),o=e,m(!1),v(),l(),p=g(y),y.setProps({triggerTarget:a})},p=g(y),y},F.delegate=function(e,n){var r=[],o=[],i=!1,a=n.target,c=s(n,[""target""]),p=Object.assign({},c,{trigger:""manual"",touch:!1}),f=Object.assign({touch:R.touch},c,{showOnCreate:!0}),l=F(e,p);function d(e){if(e.target&&!i){var t=e.target.closest(a);if(t){var r=t.getAttribute(""data-tippy-trigger"")||n.trigger||R.trigger;if(!t._tippy&&!(""touchstart""===e.type&&""boolean""==typeof f.touch||""touchstart""!==e.type&&r.indexOf(X[e.type])<0)){var s=F(t,f);s&&(o=o.concat(s))}}}}function v(e,t,n,o){void 0===o&&(o=!1),e.addEventListener(t,n,o),r.push({node:e,eventType:t,handler:n,options:o})}return u(l).forEach((function(e){var n=e.destroy,a=e.enable,s=e.disable;e.destroy=function(e){void 0===e&&(e=!0),e&&o.forEach((function(e){e.destroy()})),o=[],r.forEach((function(e){var t=e.node,n=e.eventType,r=e.handler,o=e.options;t.removeEventListener(n,r,o)})),r=[],n()},e.enable=function(){a(),o.forEach((function(e){return e.enable()})),i=!1},e.disable=function(){s(),o.forEach((function(e){return e.disable()})),i=!0},function(e){var n=e.reference;v(n,""touchstart"",d,t),v(n,""mouseover"",d),v(n,""focusin"",d),v(n,""click"",d)}(e)})),l},F.hideAll=function(e){var t=void 0===e?{}:e,n=t.exclude,r=t.duration;U.forEach((function(e){var t=!1;if(n&&(t=g(n)?e.reference===n:e.popper===n.popper),!t){var o=e.props.duration;e.setProps({duration:r}),e.hide(),e.state.isDestroyed||e.setProps({duration:o})}}))},F.roundArrow='<svg width=""16"" height=""6"" xmlns=""http://www.w3.org/2000/svg""><path d=""M0 6s1.796-.013 4.67-3.615C5.851.9 6.93.006 8 0c1.07-.006 2.148.887 3.343 2.385C14.233 6.005 16 6 16 6H0z""></svg>',F}));
-

---FILE: _freeze/tensorflow/guide/intro_to_graphs/execute-results/html.json---
@@ -1,8 +1,10 @@
 {
   ""hash"": ""22e7b4b740cbcd83383a705e4deb2bf5"",
   ""result"": {
-    ""markdown"": ""---\ntitle: Intro To_graphs\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#@title Licensed under the Apache License, Version 2.0 (the \""License\"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \""AS IS\"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\n# Introduction to graphs and tf_function\n\n## Overview\n\nThis guide goes beneath the surface of TensorFlow and Keras to\ndemonstrate how TensorFlow works. If you instead want to immediately get\nstarted with Keras, check out the [collection of Keras\nguides](https://www.tensorflow.org/guide/keras/).\n\nIn this guide, you'll learn how TensorFlow allows you to make simple\nchanges to your code to get graphs, how graphs are stored and\nrepresented, and how you can use them to accelerate your models.\n\nNote: For those of you who are only familiar with TensorFlow 1.x, this\nguide demonstrates a very different view of graphs.\n\n**This is a big-picture overview that covers how `tf_function()` allows\nyou to switch from eager execution to graph execution.** For a more\ncomplete specification of `tf_function()`, go to the [`tf_function()`\nguide](function.qmd).\n\n### What are graphs?\n\nIn the previous three guides, you ran TensorFlow **eagerly**. This means\nTensorFlow operations are executed by Python, operation by operation,\nand returning results back to Python.\n\nWhile eager execution has several unique advantages, graph execution\nenables portability outside Python and tends to offer better\nperformance. **Graph execution** means that tensor computations are\nexecuted as a *TensorFlow graph*, sometimes referred to as a `tf$Graph`\nor simply a \""graph.\""\n\n**Graphs are data structures that contain a set of `tf$Operation`\nobjects, which represent units of computation; and `tf$Tensor` objects,\nwhich represent the units of data that flow between operations.** They\nare defined in a `tf$Graph` context. Since these graphs are data\nstructures, they can be saved, run, and restored all without the\noriginal R code.\n\nThis is what a TensorFlow graph representing a two-layer neural network\nlooks like when visualized in TensorBoard.\n\n![A simple TensorFlow\ng](https://github.com/tensorflow/docs/blob/master/site/en/guide/images/intro_to_graphs/two-layer-network.png?raw=1)\n\n### The benefits of graphs\n\nWith a graph, you have a great deal of flexibility. You can use your\nTensorFlow graph in environments that don't have an R interpreter, like\nmobile applications, embedded devices, and backend servers. TensorFlow\nuses graphs as the format for [saved models](saved_model) when it\nexports them from R.\n\nGraphs are also easily optimized, allowing the compiler to do\ntransformations like:\n\n-   Statically infer the value of tensors by folding constant nodes in\n    your computation *(\""constant folding\"")*.\n-   Separate sub-parts of a computation that are independent and split\n    them between threads or devices.\n-   Simplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system,\n[Grappler](./graph_optimization.qmd), to perform this and other\nspeedups.\n\nIn short, graphs are extremely useful and let your TensorFlow run\n**fast**, run **in parallel**, and run efficiently **on multiple\ndevices**.\n\nHowever, you still want to define your machine learning models (or other\ncomputations) in Python for convenience, and then automatically\nconstruct graphs when you need them.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(magrittr, include.only = \""%>%\"")\n```\n:::\n\n\n## Taking advantage of graphs\n\nYou create and run a graph in TensorFlow by using `tf_function()`,\neither as a direct call or as a decorator. `tf_function()` takes a\nregular function as input and returns a `Function`. **A `Function` is a\ncallable that builds TensorFlow graphs from the R function. You use a\n`Function` in the same way as its R equivalent.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define an R function.\na_regular_function <- function(x, y, b) {\n  x %>%\n    tf$matmul(y) %>%\n    { . + b }\n}\n\n# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\na_function_that_uses_a_graph <- tf_function(a_regular_function)\n\n# Make some tensors.\nx1 <- as_tensor(1:2, \""float64\"", shape = c(1, 2))\ny1 <- as_tensor(2:3, \""float64\"", shape = c(2, 1))\nb1 <- as_tensor(4)\n\norig_value <- as.array(a_regular_function(x1, y1, b1))\n# Call a `Function` like a Python function.\n\ntf_function_value <- as.array(a_function_that_uses_a_graph(x1, y1, b1))\nstopifnot(orig_value == tf_function_value)\n```\n:::\n\n\nOn the outside, a `Function` looks like a regular function you write\nusing TensorFlow operations.\n[Underneath](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py),\nhowever, it is *very different*. A `Function` **encapsulates [several\n`tf$Graph`s behind one API](#polymorphism_one_function_many_graphs).**\nThat is how `Function` is able to give you the [benefits of graph\nexecution](#the_benefits_of_graphs), like speed and deployability.\n\n`tf_function` applies to a function *and all other functions it calls*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_function <- function(x, y, b) {\n  tf$matmul(x, y) + b\n}\n\nouter_function <- tf_function(function(x) {\n  y <- as_tensor(2:3, \""float64\"", shape = c(2, 1))\n  b <- as_tensor(4.0)\n\n  inner_function(x, y, b)\n})\n\n# Note that the callable will create a graph that\n# includes `inner_function` as well as `outer_function`.\nouter_function(as_tensor(1:2, \""float64\"", shape = c(1, 2))) #%>% as.array()\n```\n:::\n\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you\nneed to define a `Placeholder` or `tf$Session()`.\n\n### Converting Python functions to graphs\n\nAny function you write with TensorFlow will contain a mixture of\nbuilt-in TF operations and R control-flow logic, such as `if-then`\nclauses, loops, `break`, `return`, `next`, and more. While TensorFlow\noperations are easily captured by a `tf$Graph`, R-specific logic needs\nto undergo an extra step in order to become part of the graph.\n`tf_function()` uses a library called {tfautograph} to evaluate the R\ncode in a special way so that it generates a graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_relu <- function(x) {\n  if (tf$greater(x, 0))\n    x\n  else\n    as_tensor(0, x$dtype)\n}\n\n# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\ntf_simple_relu <- tf_function(simple_relu)\n\ncat(\n  \""First branch, with graph: \"", format(tf_simple_relu(as_tensor(1))), \""\\n\"",\n  \""Second branch, with graph: \"", format(tf_simple_relu(as_tensor(-1))), \""\\n\"",\n  sep = \""\""\n)\n```\n:::\n\n\nThough it is unlikely that you will need to view graphs directly, you\ncan inspect the outputs to check the exact results. These are not easy\nto read, so no need to look too carefully!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is the graph itself.\ntf_simple_relu$get_concrete_function(as_tensor(1))$graph$as_graph_def()\n```\n:::\n\n\nMost of the time, `tf_function()` will work without special\nconsiderations. However, there are some caveats, and the [tf_function\nguide](./function.qmd) can help here, as well as the [tfautograph\nGetting Started\nvignette](https://t-kalinowski.github.io/tfautograph/articles/tfautograph.html)\n\n### Polymorphism: one `Function`, many graphs\n\nA `tf$Graph` is specialized to a specific type of inputs (for example,\ntensors with a specific\n[`dtype`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) or\nobjects with the same\n[`id()`](https://docs.python.org/3/library/functions.html#id%5D)) (i.e,\nthe same memory address).\n\nEach time you invoke a `Function` with a set of arguments that can't be\nhandled by any of its existing graphs (such as arguments with new\n`dtypes` or incompatible shapes), `Function` creates a new `tf$Graph`\nspecialized to those new arguments. The type specification of a\n`tf$Graph`'s inputs is known as its **input signature** or just a\n**signature**. For more information regarding when a new `tf$Graph` is\ngenerated and how that can be controlled, see the [rules of\nretracing](https://www.tensorflow.org/guide/function#rules_of_tracing).\n\nThe `Function` stores the `tf$Graph` corresponding to that signature in\na `ConcreteFunction`. **A `ConcreteFunction` is a wrapper around a\n`tf$Graph`.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_relu <- tf_function(function(x) {\n  message(\""Tracing my_relu(x) with: \"", x)\n  tf$maximum(as_tensor(0), x)\n})\n\n# `my_relu` creates new graphs as it observes more signatures.\n\nmy_relu(as_tensor(5.5))\nmy_relu(c(1, -1))\nmy_relu(as_tensor(c(3, -3)))\n```\n:::\n\n\nIf the `Function` has already been called with that signature,\n`Function` does not create a new `tf$Graph`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# These two calls do *not* create new graphs.\nmy_relu(as_tensor(-2.5)) # Signature matches `as_tensor(5.5)`.\nmy_relu(as_tensor(c(-1., 1.))) # Signature matches `as_tensor(c(3., -3.))`.\n```\n:::\n\n\nBecause it's backed by multiple graphs, a `Function` is ^polymorphic^.\nThat enables it to support more input types than a single `tf$Graph`\ncould represent, as well as to optimize each `tf$Graph` for better\nperformance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n# The `ConcreteFunction` also knows the return type and shape!\ncat(my_relu$pretty_printed_concrete_signatures())\n```\n:::\n\n\n## Using `tf_function()`\n\nSo far, you've learned how to convert a Python function into a graph\nsimply by using `tf_function()` as function wrapper. But in practice,\ngetting `tf_function` to work correctly can be tricky! In the following\nsections, you'll learn how you can make your code work as expected with\n`tf_function()`.\n\n### Graph execution vs. eager execution\n\nThe code in a `Function` can be executed both eagerly and as a graph. By\ndefault, `Function` executes its code as a graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE <- tf_function(function(y_true, y_pred) {\n  # if y_true and y_pred are tensors, the R generics mean`, `^`, and `-`\n  # dispatch to tf$reduce_mean(), tf$math$pow(), and tf$math$subtract()\n  mean((y_true - y_pred) ^ 2)\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(y_true <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n(y_pred <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE(y_true, y_pred)\n```\n:::\n\n\nTo verify that your `Function`'s graph is doing the same computation as\nits equivalent Python function, you can make it execute eagerly with\n`tf$config$run_functions_eagerly(TRUE)`. This is a switch that **turns\noff `Function`'s ability to create and run graphs**, instead executing\nthe code normally.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$config$run_functions_eagerly(TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE(y_true, y_pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Don't forget to set it back when you are done.\ntf$config$run_functions_eagerly(FALSE)\n```\n:::\n\n\nHowever, `Function` can behave differently under graph and eager\nexecution. The R `print()` function is one example of how these two\nmodes differ. Let's check out what happens when you insert a `print`\nstatement to your function and call it repeatedly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE <- tf_function(function(y_true, y_pred) {\n  print(\""Calculating MSE!\"")\n  mean((y_true - y_pred) ^ 2)\n  })\n```\n:::\n\n\nObserve what is printed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n```\n:::\n\n\nIs the output surprising? **`get_MSE` only printed once even though it\nwas called *three* times.**\n\nTo explain, the `print` statement is executed when `Function` runs the\noriginal code in order to create the graph in a process known as\n[\""tracing\""](function.qmd#tracing). **Tracing captures the TensorFlow\noperations into a graph, and `print()` is not captured in the graph.**\nThat graph is then executed for all three calls **without ever running\nthe R code again**.\n\nAs a sanity check, let's turn off graph execution to compare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now, globally set everything to run eagerly to force eager execution.\ntf$config$run_functions_eagerly(TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observe what is printed below.\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$config$run_functions_eagerly(FALSE)\n```\n:::\n\n\n`print` is an *R side effect*, and there are other differences that you\nshould be aware of when converting a function into a `Function`. Learn\nmore in the *Limitations* section of the [Better performance with\ntf_function](./function.qmd#limitations) guide.\n\n::: callout-note\nNote: If you would like to print values in both eager and graph\nexecution, use `tf$print()` instead.\n:::\n\n### Non-strict execution\n\nGraph execution only executes the operations necessary to produce the\nobservable effects, which includes:\n\n-   The return value of the function\n-   Documented well-known side-effects such as:\n    -   Input/output operations, like `tf$print()`\n    -   Debugging operations, such as the assert functions in\n        `tf$debugging()` (also, `stopifnot()`)\n    -   Mutations of `tf$Variable()`\n\nThis behavior is usually known as \""Non-strict execution\"", and differs\nfrom eager execution, which steps through all of the program operations,\nneeded or not.\n\nIn particular, runtime error checking does not count as an observable\neffect. If an operation is skipped because it is unnecessary, it cannot\nraise any runtime errors.\n\nIn the following example, the \""unnecessary\"" operation `tf$gather()` is\nskipped during graph execution, so the runtime error\n`InvalidArgumentError` is not raised as it would be in eager execution.\nDo not rely on an error being raised while executing a graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunused_return_eager <- function(x) {\n  # tf$gather() will fail on a CPU device if the index is out of bounds\n  with(tf$device(\""CPU\""),\n       tf$gather(x, list(2L))) # unused\n  x\n}\n\ntry(unused_return_eager(as_tensor(0, shape = c(1))))\n# All operations are run during eager execution so an error is raised.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nunused_return_graph <- tf_function(function(x) {\n  with(tf$device(\""CPU\""),\n       tf$gather(x, list(2L))) # unused\n  x\n})\n\n# Only needed operations are run during graph exection. The error is not raised.\nunused_return_graph(as_tensor(0, shape = 1))\n```\n:::\n\n\n### `tf_function()` best practices\n\nIt may take some time to get used to the behavior of `Function`. To get\nstarted quickly, first-time users should play around with wrapping toy\nfunctions with `tf_function()` to get experience with going from eager\nto graph execution.\n\n*Designing for `tf_function`* may be your best bet for writing\ngraph-compatible TensorFlow programs. Here are some tips:\n\n-   Toggle between eager and graph execution early and often with\n    `tf$config$run_functions_eagerly()` to pinpoint if/when the two\n    modes diverge.\n\n-   Create `tf$Variable`s outside the Python function and modify them on\n    the inside. The same goes for objects that use `tf$Variable`, like\n    `keras$layers`, `keras$Model`s and `tf$optimizers`.\n\n-   Avoid writing functions that [depend on outer Python\n    variables](function#depending_on_python_global_and_free_variables),\n    excluding `tf$Variable`s and Keras objects.\n\n-   Prefer to write functions which take tensors and other TensorFlow\n    types as input. You can pass in other object types but [be\n    careful](function#depending_on_python_objects)!\n\n-   Include as much computation as possible under a `tf_function` to\n    maximize the performance gain. For example, wrap a whole training\n    step or the entire training loop.\n\n## Seeing the speed-up\n\n`tf_function` usually improves the performance of your code, but the\namount of speed-up depends on the kind of computation you run. Small\ncomputations can be dominated by the overhead of calling a graph. You\ncan measure the difference in performance like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$random$uniform(shape(10, 10),\n                       minval = -1L, maxval = 2L,\n                       dtype = tf$dtypes$int32)\n\npower <- function(x, y) {\n  result <- tf$eye(10L, dtype = tf$dtypes$int32)\n  for (. in seq_len(y))\n    result <- tf$matmul(x, result)\n  result\n}\npower_as_graph <- tf_function(power)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(bench::mark(\n  \""Eager execution\"" = power(x, 100),\n  \""Graph execution\"" = power_as_graph(x, 100)))\n```\n:::\n\n\n`tf_function` is commonly used to speed up training loops, and you can\nlearn more about it in [Writing a training loop from\nscratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction)\nwith Keras.\n\nNote: You can also try\n[`tf_function(jit_compile = TRUE)`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunctionjit_compiletrue)\nfor a more significant performance boost, especially if your code is\nheavy on TF control flow and uses many small tensors.\n\n### Performance and trade-offs\n\nGraphs can speed up your code, but the process of creating them has some\noverhead. For some functions, the creation of the graph takes more time\nthan the execution of the graph. **This investment is usually quickly\npaid back with the performance boost of subsequent executions, but it's\nimportant to be aware that the first few steps of any large model\ntraining can be slower due to tracing.**\n\nNo matter how large your model, you want to avoid tracing frequently.\nThe `tf_function()` guide discusses [how to set input specifications and\nuse tensor arguments](function#controlling_retracing) to avoid\nretracing. If you find you are getting unusually poor performance, it's\na good idea to check if you are retracing accidentally.\n\n## When is a `Function` tracing?\n\nTo figure out when your `Function` is tracing, add a `print` or\n`message()` statement to its code. As a rule of thumb, `Function` will\nexecute the `message` statement every time it traces.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na_function_with_r_side_effect <- tf_function(function(x) {\n  message(\""Tracing!\"") # An eager-only side effect.\n  (x * x) + 2\n})\n\n# This is traced the first time.\na_function_with_r_side_effect(as_tensor(2))\n\n# The second time through, you won't see the side effect.\na_function_with_r_side_effect(as_tensor(3))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This retraces each time the Python argument changes,\n# as a Python argument could be an epoch count or other\n# hyperparameter.\n\na_function_with_r_side_effect(2)\na_function_with_r_side_effect(3)\n```\n:::\n\n\nNew (non-tensor) R arguments always trigger the creation of a new graph,\nhence the extra tracing.\n\n## Next steps\n\nYou can learn more about `tf_function()` on the API reference page and\nby following the [Better performance with `tf_function`](function.qmd)\nguide.\n"",
-    ""supporting"": [],
+    ""markdown"": ""---\ntitle: Intro To_graphs\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#@title Licensed under the Apache License, Version 2.0 (the \""License\"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \""AS IS\"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\n# Introduction to graphs and tf_function\n\n## Overview\n\nThis guide goes beneath the surface of TensorFlow and Keras to\ndemonstrate how TensorFlow works. If you instead want to immediately get\nstarted with Keras, check out the [collection of Keras\nguides](https://www.tensorflow.org/guide/keras/).\n\nIn this guide, you'll learn how TensorFlow allows you to make simple\nchanges to your code to get graphs, how graphs are stored and\nrepresented, and how you can use them to accelerate your models.\n\nNote: For those of you who are only familiar with TensorFlow 1.x, this\nguide demonstrates a very different view of graphs.\n\n**This is a big-picture overview that covers how `tf_function()` allows\nyou to switch from eager execution to graph execution.** For a more\ncomplete specification of `tf_function()`, go to the [`tf_function()`\nguide](function.qmd).\n\n### What are graphs?\n\nIn the previous three guides, you ran TensorFlow **eagerly**. This means\nTensorFlow operations are executed by Python, operation by operation,\nand returning results back to Python.\n\nWhile eager execution has several unique advantages, graph execution\nenables portability outside Python and tends to offer better\nperformance. **Graph execution** means that tensor computations are\nexecuted as a *TensorFlow graph*, sometimes referred to as a `tf$Graph`\nor simply a \""graph.\""\n\n**Graphs are data structures that contain a set of `tf$Operation`\nobjects, which represent units of computation; and `tf$Tensor` objects,\nwhich represent the units of data that flow between operations.** They\nare defined in a `tf$Graph` context. Since these graphs are data\nstructures, they can be saved, run, and restored all without the\noriginal R code.\n\nThis is what a TensorFlow graph representing a two-layer neural network\nlooks like when visualized in TensorBoard.\n\n![A simple TensorFlow\ng](https://github.com/tensorflow/docs/blob/master/site/en/guide/images/intro_to_graphs/two-layer-network.png?raw=1)\n\n### The benefits of graphs\n\nWith a graph, you have a great deal of flexibility. You can use your\nTensorFlow graph in environments that don't have an R interpreter, like\nmobile applications, embedded devices, and backend servers. TensorFlow\nuses graphs as the format for [saved models](saved_model) when it\nexports them from R.\n\nGraphs are also easily optimized, allowing the compiler to do\ntransformations like:\n\n-   Statically infer the value of tensors by folding constant nodes in\n    your computation *(\""constant folding\"")*.\n-   Separate sub-parts of a computation that are independent and split\n    them between threads or devices.\n-   Simplify arithmetic operations by eliminating common subexpressions.\n\nThere is an entire optimization system,\n[Grappler](./graph_optimization.qmd), to perform this and other\nspeedups.\n\nIn short, graphs are extremely useful and let your TensorFlow run\n**fast**, run **in parallel**, and run efficiently **on multiple\ndevices**.\n\nHowever, you still want to define your machine learning models (or other\ncomputations) in Python for convenience, and then automatically\nconstruct graphs when you need them.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(magrittr, include.only = \""%>%\"")\n```\n:::\n\n\n## Taking advantage of graphs\n\nYou create and run a graph in TensorFlow by using `tf_function()`,\neither as a direct call or as a decorator. `tf_function()` takes a\nregular function as input and returns a `Function`. **A `Function` is a\ncallable that builds TensorFlow graphs from the R function. You use a\n`Function` in the same way as its R equivalent.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define an R function.\na_regular_function <- function(x, y, b) {\n  x %>%\n    tf$matmul(y) %>%\n    { . + b }\n}\n\n# `a_function_that_uses_a_graph` is a TensorFlow `Function`.\na_function_that_uses_a_graph <- tf_function(a_regular_function)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n# Make some tensors.\nx1 <- as_tensor(1:2, \""float64\"", shape = c(1, 2))\ny1 <- as_tensor(2:3, \""float64\"", shape = c(2, 1))\nb1 <- as_tensor(4)\n\norig_value <- as.array(a_regular_function(x1, y1, b1))\n# Call a `Function` like a Python function.\n\ntf_function_value <- as.array(a_function_that_uses_a_graph(x1, y1, b1))\nstopifnot(orig_value == tf_function_value)\n```\n:::\n\n\nOn the outside, a `Function` looks like a regular function you write\nusing TensorFlow operations.\n[Underneath](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py),\nhowever, it is *very different*. A `Function` **encapsulates [several\n`tf$Graph`s behind one API](#polymorphism_one_function_many_graphs).**\nThat is how `Function` is able to give you the [benefits of graph\nexecution](#the_benefits_of_graphs), like speed and deployability.\n\n`tf_function` applies to a function *and all other functions it calls*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_function <- function(x, y, b) {\n  tf$matmul(x, y) + b\n}\n\nouter_function <- tf_function(function(x) {\n  y <- as_tensor(2:3, \""float64\"", shape = c(2, 1))\n  b <- as_tensor(4.0)\n\n  inner_function(x, y, b)\n})\n\n# Note that the callable will create a graph that\n# includes `inner_function` as well as `outer_function`.\nouter_function(as_tensor(1:2, \""float64\"", shape = c(1, 2))) #%>% as.array()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([[12.]], shape=(1, 1), dtype=float64)\n```\n:::\n:::\n\n\nIf you have used TensorFlow 1.x, you will notice that at no time did you\nneed to define a `Placeholder` or `tf$Session()`.\n\n### Converting Python functions to graphs\n\nAny function you write with TensorFlow will contain a mixture of\nbuilt-in TF operations and R control-flow logic, such as `if-then`\nclauses, loops, `break`, `return`, `next`, and more. While TensorFlow\noperations are easily captured by a `tf$Graph`, R-specific logic needs\nto undergo an extra step in order to become part of the graph.\n`tf_function()` uses a library called {tfautograph} to evaluate the R\ncode in a special way so that it generates a graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_relu <- function(x) {\n  if (tf$greater(x, 0))\n    x\n  else\n    as_tensor(0, x$dtype)\n}\n\n# `tf_simple_relu` is a TensorFlow `Function` that wraps `simple_relu`.\ntf_simple_relu <- tf_function(simple_relu)\n\ncat(\n  \""First branch, with graph: \"", format(tf_simple_relu(as_tensor(1))), \""\\n\"",\n  \""Second branch, with graph: \"", format(tf_simple_relu(as_tensor(-1))), \""\\n\"",\n  sep = \""\""\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst branch, with graph: tf.Tensor(1.0, shape=(), dtype=float64)\nSecond branch, with graph: tf.Tensor(0.0, shape=(), dtype=float64)\n```\n:::\n:::\n\n\nThough it is unlikely that you will need to view graphs directly, you\ncan inspect the outputs to check the exact results. These are not easy\nto read, so no need to look too carefully!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This is the graph itself.\ntf_simple_relu$get_concrete_function(as_tensor(1))$graph$as_graph_def()\n```\n:::\n\n\nMost of the time, `tf_function()` will work without special\nconsiderations. However, there are some caveats, and the [tf_function\nguide](./function.qmd) can help here, as well as the [tfautograph\nGetting Started\nvignette](https://t-kalinowski.github.io/tfautograph/articles/tfautograph.html)\n\n### Polymorphism: one `Function`, many graphs\n\nA `tf$Graph` is specialized to a specific type of inputs (for example,\ntensors with a specific\n[`dtype`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) or\nobjects with the same\n[`id()`](https://docs.python.org/3/library/functions.html#id%5D)) (i.e,\nthe same memory address).\n\nEach time you invoke a `Function` with a set of arguments that can't be\nhandled by any of its existing graphs (such as arguments with new\n`dtypes` or incompatible shapes), `Function` creates a new `tf$Graph`\nspecialized to those new arguments. The type specification of a\n`tf$Graph`'s inputs is known as its **input signature** or just a\n**signature**. For more information regarding when a new `tf$Graph` is\ngenerated and how that can be controlled, see the [rules of\nretracing](https://www.tensorflow.org/guide/function#rules_of_tracing).\n\nThe `Function` stores the `tf$Graph` corresponding to that signature in\na `ConcreteFunction`. **A `ConcreteFunction` is a wrapper around a\n`tf$Graph`.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_relu <- tf_function(function(x) {\n  message(\""Tracing my_relu(x) with: \"", x)\n  tf$maximum(as_tensor(0), x)\n})\n\n# `my_relu` creates new graphs as it observes more signatures.\n\nmy_relu(as_tensor(5.5))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing my_relu(x) with: Tensor(\""x:0\"", shape=(), dtype=float64)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(5.5, shape=(), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nmy_relu(c(1, -1))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing my_relu(x) with: 1-1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1. 0.], shape=(2), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nmy_relu(as_tensor(c(3, -3)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing my_relu(x) with: Tensor(\""x:0\"", shape=(2,), dtype=float64)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([3. 0.], shape=(2), dtype=float64)\n```\n:::\n:::\n\n\nIf the `Function` has already been called with that signature,\n`Function` does not create a new `tf$Graph`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# These two calls do *not* create new graphs.\nmy_relu(as_tensor(-2.5)) # Signature matches `as_tensor(5.5)`.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(0.0, shape=(), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nmy_relu(as_tensor(c(-1., 1.))) # Signature matches `as_tensor(c(3., -3.))`.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([0. 1.], shape=(2), dtype=float64)\n```\n:::\n:::\n\n\nBecause it's backed by multiple graphs, a `Function` is ^polymorphic^.\nThat enables it to support more input types than a single `tf$Graph`\ncould represent, as well as to optimize each `tf$Graph` for better\nperformance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n# The `ConcreteFunction` also knows the return type and shape!\ncat(my_relu$pretty_printed_concrete_signatures())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfn(x)\n  Args:\n    x: float64 Tensor, shape=()\n  Returns:\n    float64 Tensor, shape=()\n\nfn(x=[1.0, -1.0])\n  Returns:\n    float64 Tensor, shape=(2,)\n\nfn(x)\n  Args:\n    x: float64 Tensor, shape=(2,)\n  Returns:\n    float64 Tensor, shape=(2,)\n```\n:::\n:::\n\n\n## Using `tf_function()`\n\nSo far, you've learned how to convert a Python function into a graph\nsimply by using `tf_function()` as function wrapper. But in practice,\ngetting `tf_function` to work correctly can be tricky! In the following\nsections, you'll learn how you can make your code work as expected with\n`tf_function()`.\n\n### Graph execution vs. eager execution\n\nThe code in a `Function` can be executed both eagerly and as a graph. By\ndefault, `Function` executes its code as a graph:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE <- tf_function(function(y_true, y_pred) {\n  # if y_true and y_pred are tensors, the R generics mean`, `^`, and `-`\n  # dispatch to tf$reduce_mean(), tf$math$pow(), and tf$math$subtract()\n  mean((y_true - y_pred) ^ 2)\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(y_true <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([5 3 4 7 6], shape=(5), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\n(y_pred <- tf$random$uniform(shape(5), maxval = 10L, dtype = tf$int32))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([9 4 6 3 7], shape=(5), dtype=int32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE(y_true, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(7, shape=(), dtype=int32)\n```\n:::\n:::\n\n\nTo verify that your `Function`'s graph is doing the same computation as\nits equivalent Python function, you can make it execute eagerly with\n`tf$config$run_functions_eagerly(TRUE)`. This is a switch that **turns\noff `Function`'s ability to create and run graphs**, instead executing\nthe code normally.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$config$run_functions_eagerly(TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE(y_true, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(7, shape=(), dtype=int32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Don't forget to set it back when you are done.\ntf$config$run_functions_eagerly(FALSE)\n```\n:::\n\n\nHowever, `Function` can behave differently under graph and eager\nexecution. The R `print()` function is one example of how these two\nmodes differ. Let's check out what happens when you insert a `print`\nstatement to your function and call it repeatedly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_MSE <- tf_function(function(y_true, y_pred) {\n  print(\""Calculating MSE!\"")\n  mean((y_true - y_pred) ^ 2)\n  })\n```\n:::\n\n\nObserve what is printed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror <- get_MSE(y_true, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \""Calculating MSE!\""\n```\n:::\n\n```{.r .cell-code}\nerror <- get_MSE(y_true, y_pred)\nerror <- get_MSE(y_true, y_pred)\n```\n:::\n\n\nIs the output surprising? **`get_MSE` only printed once even though it\nwas called *three* times.**\n\nTo explain, the `print` statement is executed when `Function` runs the\noriginal code in order to create the graph in a process known as\n[\""tracing\""](function.qmd#tracing). **Tracing captures the TensorFlow\noperations into a graph, and `print()` is not captured in the graph.**\nThat graph is then executed for all three calls **without ever running\nthe R code again**.\n\nAs a sanity check, let's turn off graph execution to compare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now, globally set everything to run eagerly to force eager execution.\ntf$config$run_functions_eagerly(TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observe what is printed below.\nerror <- get_MSE(y_true, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \""Calculating MSE!\""\n```\n:::\n\n```{.r .cell-code}\nerror <- get_MSE(y_true, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \""Calculating MSE!\""\n```\n:::\n\n```{.r .cell-code}\nerror <- get_MSE(y_true, y_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \""Calculating MSE!\""\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$config$run_functions_eagerly(FALSE)\n```\n:::\n\n\n`print` is an *R side effect*, and there are other differences that you\nshould be aware of when converting a function into a `Function`. Learn\nmore in the *Limitations* section of the [Better performance with\ntf_function](./function.qmd#limitations) guide.\n\n::: callout-note\nNote: If you would like to print values in both eager and graph\nexecution, use `tf$print()` instead.\n:::\n\n### Non-strict execution\n\nGraph execution only executes the operations necessary to produce the\nobservable effects, which includes:\n\n-   The return value of the function\n-   Documented well-known side-effects such as:\n    -   Input/output operations, like `tf$print()`\n    -   Debugging operations, such as the assert functions in\n        `tf$debugging()` (also, `stopifnot()`)\n    -   Mutations of `tf$Variable()`\n\nThis behavior is usually known as \""Non-strict execution\"", and differs\nfrom eager execution, which steps through all of the program operations,\nneeded or not.\n\nIn particular, runtime error checking does not count as an observable\neffect. If an operation is skipped because it is unnecessary, it cannot\nraise any runtime errors.\n\nIn the following example, the \""unnecessary\"" operation `tf$gather()` is\nskipped during graph execution, so the runtime error\n`InvalidArgumentError` is not raised as it would be in eager execution.\nDo not rely on an error being raised while executing a graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunused_return_eager <- function(x) {\n  # tf$gather() will fail on a CPU device if the index is out of bounds\n  with(tf$device(\""CPU\""),\n       tf$gather(x, list(2L))) # unused\n  x\n}\n\ntry(unused_return_eager(as_tensor(0, shape = c(1))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0] = 2 is not in [0, 1) [Op:GatherV2]\n```\n:::\n\n```{.r .cell-code}\n# All operations are run during eager execution so an error is raised.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nunused_return_graph <- tf_function(function(x) {\n  with(tf$device(\""CPU\""),\n       tf$gather(x, list(2L))) # unused\n  x\n})\n\n# Only needed operations are run during graph exection. The error is not raised.\nunused_return_graph(as_tensor(0, shape = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([0.], shape=(1), dtype=float64)\n```\n:::\n:::\n\n\n### `tf_function()` best practices\n\nIt may take some time to get used to the behavior of `Function`. To get\nstarted quickly, first-time users should play around with wrapping toy\nfunctions with `tf_function()` to get experience with going from eager\nto graph execution.\n\n*Designing for `tf_function`* may be your best bet for writing\ngraph-compatible TensorFlow programs. Here are some tips:\n\n-   Toggle between eager and graph execution early and often with\n    `tf$config$run_functions_eagerly()` to pinpoint if/when the two\n    modes diverge.\n\n-   Create `tf$Variable`s outside the Python function and modify them on\n    the inside. The same goes for objects that use `tf$Variable`, like\n    `keras$layers`, `keras$Model`s and `tf$optimizers`.\n\n-   Avoid writing functions that [depend on outer Python\n    variables](function#depending_on_python_global_and_free_variables),\n    excluding `tf$Variable`s and Keras objects.\n\n-   Prefer to write functions which take tensors and other TensorFlow\n    types as input. You can pass in other object types but [be\n    careful](function#depending_on_python_objects)!\n\n-   Include as much computation as possible under a `tf_function` to\n    maximize the performance gain. For example, wrap a whole training\n    step or the entire training loop.\n\n## Seeing the speed-up\n\n`tf_function` usually improves the performance of your code, but the\namount of speed-up depends on the kind of computation you run. Small\ncomputations can be dominated by the overhead of calling a graph. You\ncan measure the difference in performance like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$random$uniform(shape(10, 10),\n                       minval = -1L, maxval = 2L,\n                       dtype = tf$dtypes$int32)\n\npower <- function(x, y) {\n  result <- tf$eye(10L, dtype = tf$dtypes$int32)\n  for (. in seq_len(y))\n    result <- tf$matmul(x, result)\n  result\n}\npower_as_graph <- tf_function(power)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(bench::mark(\n  \""Eager execution\"" = power(x, 100),\n  \""Graph execution\"" = power_as_graph(x, 100)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required namespace: tidyr\n```\n:::\n\n::: {.cell-output-display}\n![](intro_to_graphs_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n`tf_function` is commonly used to speed up training loops, and you can\nlearn more about it in [Writing a training loop from\nscratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction)\nwith Keras.\n\nNote: You can also try\n[`tf_function(jit_compile = TRUE)`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunctionjit_compiletrue)\nfor a more significant performance boost, especially if your code is\nheavy on TF control flow and uses many small tensors.\n\n### Performance and trade-offs\n\nGraphs can speed up your code, but the process of creating them has some\noverhead. For some functions, the creation of the graph takes more time\nthan the execution of the graph. **This investment is usually quickly\npaid back with the performance boost of subsequent executions, but it's\nimportant to be aware that the first few steps of any large model\ntraining can be slower due to tracing.**\n\nNo matter how large your model, you want to avoid tracing frequently.\nThe `tf_function()` guide discusses [how to set input specifications and\nuse tensor arguments](function#controlling_retracing) to avoid\nretracing. If you find you are getting unusually poor performance, it's\na good idea to check if you are retracing accidentally.\n\n## When is a `Function` tracing?\n\nTo figure out when your `Function` is tracing, add a `print` or\n`message()` statement to its code. As a rule of thumb, `Function` will\nexecute the `message` statement every time it traces.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na_function_with_r_side_effect <- tf_function(function(x) {\n  message(\""Tracing!\"") # An eager-only side effect.\n  (x * x) + 2\n})\n\n# This is traced the first time.\na_function_with_r_side_effect(as_tensor(2))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing!\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6.0, shape=(), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# The second time through, you won't see the side effect.\na_function_with_r_side_effect(as_tensor(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(11.0, shape=(), dtype=float64)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This retraces each time the Python argument changes,\n# as a Python argument could be an epoch count or other\n# hyperparameter.\n\na_function_with_r_side_effect(2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing!\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\na_function_with_r_side_effect(3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing!\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(11.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nNew (non-tensor) R arguments always trigger the creation of a new graph,\nhence the extra tracing.\n\n## Next steps\n\nYou can learn more about `tf_function()` on the API reference page and\nby following the [Better performance with `tf_function`](function.qmd)\nguide.\n"",
+    ""supporting"": [
+      ""intro_to_graphs_files""
+    ],
     ""filters"": [
       ""rmarkdown/pagebreak.lua""
     ],

---FILE: _freeze/tensorflow/guide/ragged_tensor/execute-results/html.json---
@@ -1,11 +0,0 @@
-{
-  ""hash"": ""1a343dafa8e9c9909bd0f98ee5c828d5"",
-  ""result"": {
-    ""markdown"": ""##### Copyright 2018 The TensorFlow Authors.\n\n\n```{r}\n#| cellView: form\n\n#@title Licensed under the Apache License, Version 2.0 (the \""License\"");\n\n# you may not use this file except in compliance with the License.\n\n# You may obtain a copy of the License at\n\n#\n# https://www$apache$org/licenses/LICENSE-2.0\n\n#\n# Unless required by applicable law or agreed to in writing, software\n\n# distributed under the License is distributed on an \""AS IS\"" BASIS,\n\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n\n# limitations under the License.\n\n```\n\n\n# Ragged tensors\n\n|                                                                                                                                  |                                                                                                                                                                                         |                                                                                                                                                                      |                                                                                                                                                                         |\n|----------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [![](https://www$tensorflow$org/images/tf_logo_32px$png)View on TensorFlow\\$org](https://www$tensorflow$org/guide/ragged_tensor) | [![](https://www$tensorflow$org/images/colab_logo_32px$png)Run in Google Colab](https://colab$research$google$com/github/tensorflow/docs/blob/master/site/en/guide/ragged_tensor$ipynb) | [![](https://www$tensorflow$org/images/GitHub-Mark-32px$png)View source on GitHub](https://github$com/tensorflow/docs/blob/master/site/en/guide/ragged_tensor$ipynb) | [![](https://www$tensorflow$org/images/download_logo_32px$png)Download notebook](https://storage$googleapis$com/tensorflow_docs/docs/site/en/guide/ragged_tensor$ipynb) |\n\n\\^API Documentation:\\^\n[`tf$RaggedTensor`](https://www$tensorflow$org/api_docs/python/tf/RaggedTensor)\n[`tf$ragged`](https://www$tensorflow$org/api_docs/python/tf/ragged)\n\n## Setup\n\n\n```{r}\nlibrary(tensorflow)\n```\n\n\n## Overview\n\nYour data comes in many shapes; your tensors should too. *Ragged\ntensors* are the TensorFlow equivalent of nested variable-length lists.\nThey make it easy to store and process data with non-uniform shapes,\nincluding:\n\n-   Variable-length features, such as the set of actors in a movie.\n-   Batches of variable-length sequential inputs, such as sentences or\n    video clips.\n-   Hierarchical inputs, such as text documents that are subdivided into\n    sections, paragraphs, sentences, and words.\n-   Individual fields in structured inputs, such as protocol buffers.\n\n### What you can do with a ragged tensor\n\nRagged tensors are supported by more than a hundred TensorFlow\noperations, including math operations (such as `tf$add` and\n`tf$reduce_mean`), array operations (such as `tf$concat` and `tf$tile`),\nstring manipulation ops (such as `tf$strings$substr`), control flow\noperations (such as `tf$while_loop` and `tf$map_fn`), and many others:\n\n\n```{r}\ndigits <- tf$ragged$constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nwords <- tf$ragged$constant([[\""So\"", \""long\""], [\""thanks\"", \""for\"", \""all\"", \""the\"", \""fish\""]])\nprint(tf$add(digits, 3))\nprint(tf$reduce_mean(digits, axis=1))\nprint(tf$concat([digits, [[5, 3]]], axis=0))\nprint(tf$tile(digits, [1, 2]))\nprint(tf$strings$substr(words, 0, 2))\nprint(tf$map_fn(tf$math$square, digits))\n```\n\n\nThere are also a number of methods and operations that are specific to\nragged tensors, including factory methods, conversion methods, and\nvalue-mapping operations. For a list of supported ops, see the\n\\^`tf$ragged` package documentation\\^.\n\nRagged tensors are supported by many TensorFlow APIs, including\n[Keras](https://www$tensorflow$org/guide/keras),\n[Datasets](https://www$tensorflow$org/guide/data),\n[tf_function](https://www$tensorflow$org/guide/function),\n[SavedModels](https://www$tensorflow$org/guide/saved_model), and\n\\[tf$Example](https://www$tensorflow\\$org/tutorials/load_data/tfrecord).\nFor more information, check the section on \\^TensorFlow APIs\\^ below.\n\nAs with normal tensors, you can use Python-style indexing to access\nspecific slices of a ragged tensor. For more information, refer to the\nsection on ^Indexing^ below.\n\n\n```{r}\nprint(digits[0])       # First row\n```\n\n```{r}\nprint(digits[:, :2])   # First two values in each row.\n```\n\n```{r}\nprint(digits[:, -2:])  # Last two values in each row.\n```\n\n\nAnd just like normal tensors, you can use Python arithmetic and\ncomparison operators to perform elementwise operations. For more\ninformation, check the section on \\^Overloaded operators\\^ below.\n\n\n```{r}\nprint(digits + 3)\n```\n\n```{r}\nprint(digits + tf$ragged$constant([[1, 2, 3, 4], [], [5, 6, 7], [8], []]))\n```\n\n\nIf you need to perform an elementwise transformation to the values of a\n`RaggedTensor`, you can use `tf$ragged$map_flat_values`, which takes a\nfunction plus one or more arguments, and applies the function to\ntransform the `RaggedTensor`'s values.\n\n\n```{r}\ntimes_two_plus_one <- lambda x: x * 2 + 1\nprint(tf$ragged$map_flat_values(times_two_plus_one, digits))\n```\n\n\nRagged tensors can be converted to nested Python `list`s and NumPy\n`array`s:\n\n\n```{r}\ndigits$to_list()\n```\n\n```{r}\ndigits$numpy()\n```\n\n\n### Constructing a ragged tensor\n\nThe simplest way to construct a ragged tensor is using\n`tf$ragged$constant`, which builds the `RaggedTensor` corresponding to a\ngiven nested Python `list` or NumPy `array`:\n\n\n```{r}\nsentences <- tf$ragged$constant([\n    [\""Let's\"", \""build\"", \""some\"", \""ragged\"", \""tensors\"", \""!\""],\n    [\""We\"", \""can\"", \""use\"", \""tf$ragged$constant\"", \"".\""]])\nprint(sentences)\n```\n\n```{r}\nparagraphs <- tf$ragged$constant([\n    [['I', 'have', 'a', 'cat'], ['His', 'name', 'is', 'Mat']],\n    [['Do', 'you', 'want', 'to', 'come', 'visit'], [\""I'm\"", 'free', 'tomorrow']],\n])\nprint(paragraphs)\n```\n\n\nRagged tensors can also be constructed by pairing flat *values* tensors\nwith *row-partitioning* tensors indicating how those values should be\ndivided into rows, using factory classmethods such as\n`tf$RaggedTensor$from_value_rowids`, `tf$RaggedTensor$from_row_lengths`,\nand `tf$RaggedTensor$from_row_splits`.\n\n#### `tf$RaggedTensor$from_value_rowids`\n\nIf you know which row each value belongs to, then you can build a\n`RaggedTensor` using a `value_rowids` row-partitioning tensor:\n\n![value_rowids row-partitioning\ntensor](https://www$tensorflow$org/images/ragged_tensors/value_rowids$png)\n\n\n```{r}\nprint(tf$RaggedTensor$from_value_rowids(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    value_rowids=[0, 0, 0, 0, 2, 2, 3]))\n```\n\n\n#### `tf$RaggedTensor$from_row_lengths`\n\nIf you know how long each row is, then you can use a `row_lengths`\nrow-partitioning tensor:\n\n![row_lengths row-partitioning\ntensor](https://www$tensorflow$org/images/ragged_tensors/row_lengths$png)\n\n\n```{r}\nprint(tf$RaggedTensor$from_row_lengths(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    row_lengths=[4, 0, 2, 1]))\n```\n\n\n#### `tf$RaggedTensor$from_row_splits`\n\nIf you know the index where each row starts and ends, then you can use a\n`row_splits` row-partitioning tensor:\n\n![row_splits row-partitioning\ntensor](https://www$tensorflow$org/images/ragged_tensors/row_splits$png)\n\n\n```{r}\nprint(tf$RaggedTensor$from_row_splits(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    row_splits=[0, 4, 4, 6, 7]))\n```\n\n\nSee the `tf$RaggedTensor` class documentation for a full list of factory\nmethods.\n\nNote: By default, these factory methods add assertions that the row\npartition tensor is well-formed and consistent with the number of\nvalues. The `validate=FALSE` parameter can be used to skip these checks\nif you can guarantee that the inputs are well-formed and consistent.\n\n### What you can store in a ragged tensor\n\nAs with normal `Tensor`s, the values in a `RaggedTensor` must all have\nthe same type; and the values must all be at the same nesting depth (the\n*rank* of the tensor):\n\n\n```{r}\nprint(tf$ragged$constant([[\""Hi\""], [\""How\"", \""are\"", \""you\""]]))  # ok: type=string, rank=2\n```\n\n```{r}\nprint(tf$ragged$constant([[[1, 2], [3]], [[4, 5]]]))        # ok: type=int32, rank=3\n```\n\n```{r}\ntry:\n  tf$ragged$constant([[\""one\"", \""two\""], [3, 4]])              # bad: multiple types\nexcept ValueError as exception:\n  print(exception)\n```\n\n```{r}\ntry:\n  tf$ragged$constant([\""A\"", [\""B\"", \""C\""]])                     # bad: multiple nesting depths\nexcept ValueError as exception:\n  print(exception)\n```\n\n\n## Example use case\n\nThe following example demonstrates how `RaggedTensor`s can be used to\nconstruct and combine unigram and bigram embeddings for a batch of\nvariable-length queries, using special markers for the beginning and end\nof each sentence. For more details on the ops used in this example,\ncheck the `tf$ragged` package documentation.\n\n\n```{r}\nqueries <- tf$ragged$constant([['Who', 'is', 'Dan', 'Smith'],\n                              ['Pause'],\n                              ['Will', 'it', 'rain', 'later', 'today']])\n\n# Create an embedding table.\n\nnum_buckets <- 1024\nembedding_size <- 4\nembedding_table <- tf$Variable(\n    tf$random$truncated_normal([num_buckets, embedding_size],\n                       stddev=1.0 / math$sqrt(embedding_size)))\n\n# Look up the embedding for each word.\n\nword_buckets <- tf$strings$to_hash_bucket_fast(queries, num_buckets)\nword_embeddings <- tf$nn$embedding_lookup(embedding_table, word_buckets)     # ①\n\n# Add markers to the beginning and end of each sentence.\n\nmarker <- tf$fill([queries$nrows(), 1], '#')\npadded <- tf$concat([marker, queries, marker], axis=1)                       # ②\n\n# Build word bigrams and look up embeddings.\n\nbigrams <- tf$strings$join([padded[:, :-1], padded[:, 1:]], separator='+')   # ③\n\nbigram_buckets <- tf$strings$to_hash_bucket_fast(bigrams, num_buckets)\nbigram_embeddings <- tf$nn$embedding_lookup(embedding_table, bigram_buckets) # ④\n\n# Find the average embedding for each sentence\n\nall_embeddings <- tf$concat([word_embeddings, bigram_embeddings], axis=1)    # ⑤\navg_embedding <- tf$reduce_mean(all_embeddings, axis=1)                      # ⑥\nprint(avg_embedding)\n```\n\n\n![Ragged tensor\nexample](https://www$tensorflow$org/images/ragged_tensors/ragged_example$png)\n\n## Ragged and uniform dimensions\n\nA \\^*ragged dimension\\^* is a dimension whose slices may have different\nlengths. For example, the inner (column) dimension of\n`rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]` is ragged, since the column\nslices (`rt[0, :]`, ..., `rt[4, :]`) have different lengths. Dimensions\nwhose slices all have the same length are called *uniform dimensions*.\n\nThe outermost dimension of a ragged tensor is always uniform, since it\nconsists of a single slice (and, therefore, there is no possibility for\ndiffering slice lengths). The remaining dimensions may be either ragged\nor uniform. For example, you may store the word embeddings for each word\nin a batch of sentences using a ragged tensor with shape\n`[num_sentences, (num_words), embedding_size]`, where the parentheses\naround `(num_words)` indicate that the dimension is ragged.\n\n![Word embeddings using a ragged\ntensor](https://www$tensorflow$org/images/ragged_tensors/sent_word_embed$png)\n\nRagged tensors may have multiple ragged dimensions. For example, you\ncould store a batch of structured text documents using a tensor with\nshape `[num_documents, (num_paragraphs), (num_sentences), (num_words)]`\n(where again parentheses are used to indicate ragged dimensions).\n\nAs with `tf$Tensor`, the \\^*rank\\^* of a ragged tensor is its total\nnumber of dimensions (including both ragged and uniform dimensions). A\n\\^*potentially ragged tensor\\^* is a value that might be either a\n`tf$Tensor` or a `tf$RaggedTensor`.\n\nWhen describing the shape of a RaggedTensor, ragged dimensions are\nconventionally indicated by enclosing them in parentheses. For example,\nas you saw above, the shape of a 3D RaggedTensor that stores word\nembeddings for each word in a batch of sentences can be written as\n`[num_sentences, (num_words), embedding_size]`.\n\nThe `RaggedTensor$shape` attribute returns a `tf$TensorShape` for a\nragged tensor where ragged dimensions have size `NULL`:\n\n\n```{r}\ntf$ragged$constant([[\""Hi\""], [\""How\"", \""are\"", \""you\""]]).shape\n```\n\n\nThe method `tf$RaggedTensor$bounding_shape` can be used to find a tight\nbounding shape for a given `RaggedTensor`:\n\n\n```{r}\nprint(tf$ragged$constant([[\""Hi\""], [\""How\"", \""are\"", \""you\""]]).bounding_shape())\n```\n\n\n## Ragged vs sparse\n\nA ragged tensor should *not* be thought of as a type of sparse tensor.\nIn particular, sparse tensors are *efficient encodings for `tf$Tensor`*\nthat model the same data in a compact format; but ragged tensor is an\n*extension to `tf$Tensor`* that models an expanded class of data. This\ndifference is crucial when defining operations:\n\n-   Applying an op to a sparse or dense tensor should always give the\n    same result.\n-   Applying an op to a ragged or sparse tensor may give different\n    results.\n\nAs an illustrative example, consider how array operations such as\n`concat`, `stack`, and `tile` are defined for ragged vs. sparse tensors.\nConcatenating ragged tensors joins each row to form a single row with\nthe combined length:\n\n![Concatenating ragged\ntensors](https://www$tensorflow$org/images/ragged_tensors/ragged_concat$png)\n\n\n```{r}\nragged_x <- tf$ragged$constant([[\""John\""], [\""a\"", \""big\"", \""dog\""], [\""my\"", \""cat\""]])\nragged_y <- tf$ragged$constant([[\""fell\"", \""asleep\""], [\""barked\""], [\""is\"", \""fuzzy\""]])\nprint(tf$concat([ragged_x, ragged_y], axis=1))\n```\n\n\nHowever, concatenating sparse tensors is equivalent to concatenating the\ncorresponding dense tensors, as illustrated by the following example\n(where Ø indicates missing values):\n\n![Concatenating sparse\ntensors](https://www$tensorflow$org/images/ragged_tensors/sparse_concat$png)\n\n\n```{r}\nsparse_x <- ragged_x$to_sparse()\nsparse_y <- ragged_y$to_sparse()\nsparse_result <- tf$sparse$concat(sp_inputs=[sparse_x, sparse_y], axis=1)\nprint(tf$sparse$to_dense(sparse_result, ''))\n```\n\n\nFor another example of why this distinction is important, consider the\ndefinition of \""the mean value of each row\"" for an op such as\n`tf$reduce_mean`. For a ragged tensor, the mean value for a row is the\nsum of the row's values divided by the row's width. But for a sparse\ntensor, the mean value for a row is the sum of the row's values divided\nby the sparse tensor's overall width (which is greater than or equal to\nthe width of the longest row).\n\n## TensorFlow APIs\n\n### Keras\n\n\\[tf$keras](https://www$tensorflow$org/guide/keras) is TensorFlow's high-level API for building and training deep learning models. Ragged tensors may be passed as inputs to a Keras model by setting `ragged=TRUE` on `tf$keras$Input` or `tf$keras$layers$InputLayer\\`.\nRagged tensors may also be passed between Keras layers, and returned by\nKeras models. The following example shows a toy LSTM model that is\ntrained using ragged tensors.\n\n\n```{r}\n# Task: predict whether each sentence is a question or not.\n\nsentences <- as_tensor(\n    ['What makes you think she is a witch?',\n     'She turned me into a newt.',\n     'A newt?',\n     'Well, I got better.'])\nis_question <- as_tensor([TRUE, FALSE, TRUE, FALSE])\n\n# Preprocess the input strings.\n\nhash_buckets <- 1000\nwords <- tf$strings$split(sentences, ' ')\nhashed_words <- tf$strings$to_hash_bucket_fast(words, hash_buckets)\n\n# Build the Keras model.\n\nkeras_model <- tf$keras_model_sequential([\n    tf$keras$layers$Input(shape=[NULL], dtype=tf$int64, ragged=TRUE),\n    tf$keras$layers$Embedding(hash_buckets, 16),\n    tf$keras$layers$LSTM(32, use_bias=FALSE),\n    tf$layer_dense(32),\n    tf$keras$layers$Activation(tf$nn$relu),\n    tf$layer_dense(1)\n])\n\nkeras_model %>% compile(loss='binary_crossentropy', optimizer='rmsprop')\nkeras_model %>% fit(hashed_words, is_question, epochs=5)\nprint(keras_model$predict(hashed_words))\n```\n\n\n### tf\\$Example\n\n\\[tf$Example](https://www$tensorflow$org/tutorials/load_data/tfrecord) is a standard [protobuf](https://developers$google$com/protocol-buffers/) encoding for TensorFlow data. Data encoded with `tf$Example`s often includes variable-length features.  For example, the following code defines a batch of four`tf\\$Example\\`\nmessages with different feature lengths:\n\n\n```{r}\nimport google$protobuf$text_format as pbtext\n\nbuild_tf_example <- function(s) {    }\n  return pbtext$Merge(s, tf$train$Example()).SerializeToString()\n\nexample_batch <- [\n  build_tf_example(r'''\n    features list(\n      feature {key: \""colors\"" value {bytes_list {value: [\""red\"", \""blue\""]) } }\n      feature list(key: \""lengths\"" value {int64_list {value: [7]) } } }'''),\n  build_tf_example(r'''\n    features list(\n      feature {key: \""colors\"" value {bytes_list {value: [\""orange\""]) } }\n      feature list(key: \""lengths\"" value {int64_list {value: []) } } }'''),\n  build_tf_example(r'''\n    features list(\n      feature {key: \""colors\"" value {bytes_list {value: [\""black\"", \""yellow\""]) } }\n      feature list(key: \""lengths\"" value {int64_list {value: [1, 3]) } } }'''),\n  build_tf_example(r'''\n    features list(\n      feature {key: \""colors\"" value {bytes_list {value: [\""green\""]) } }\n      feature list(key: \""lengths\"" value {int64_list {value: [3, 5, 2]) } } }''')]\n```\n\n\nYou can parse this encoded data using `tf$io$parse_example`, which takes\na tensor of serialized strings and a feature specification dictionary,\nand returns a dictionary mapping feature names to tensors. To read the\nvariable-length features into ragged tensors, you simply use\n`tf$io$RaggedFeature` in the feature specification dictionary:\n\n\n```{r}\nfeature_specification <- list(\n    'colors': tf$io$RaggedFeature(tf$string),\n    'lengths': tf$io$RaggedFeature(tf$int64),\n)\nfeature_tensors <- tf$io$parse_example(example_batch, feature_specification)\nfor name, value in feature_tensors$items():\n  print(\""list()=list()\"".format(name, value))\n```\n\n\n`tf$io$RaggedFeature` can also be used to read features with multiple\nragged dimensions. For details, refer to the [API\ndocumentation](https://www$tensorflow$org/api_docs/python/tf/io/RaggedFeature).\n\n### Datasets\n\n\\[tf$data](https://www$tensorflow$org/guide/data) is an API that enables you to build complex input pipelines from simple, reusable pieces. Its core data structure is `tf$data\\$Dataset\\`,\nwhich represents a sequence of elements, in which each element consists\nof one or more components.\n\n\n```{r}\n# Helper function used to print datasets in the examples below.\n\nprint_dictionary_dataset <- function(dataset) {    }\n  for i, element in enumerate(dataset):\n    print(\""Element list():\"".format(i))\n    for (feature_name, feature_value) in element$items():\n      print('list(:>14) = list()'.format(feature_name, feature_value))\n```\n\n\n#### Building Datasets with ragged tensors\n\nDatasets can be built from ragged tensors using the same methods that\nare used to build them from `tf$Tensor`s or NumPy `array`s, such as\n`Dataset$from_tensor_slices`:\n\n\n```{r}\ndataset <- tf$data$Dataset$from_tensor_slices(feature_tensors)\nprint_dictionary_dataset(dataset)\n```\n\n\nNote: `Dataset$from_generator` does not support ragged tensors yet, but\nsupport will be added soon.\n\n#### Batching and unbatching Datasets with ragged tensors\n\nDatasets with ragged tensors can be batched (which combines *n*\nconsecutive elements into a single elements) using the `Dataset$batch`\nmethod.\n\n\n```{r}\nbatched_dataset <- dataset$batch(2)\nprint_dictionary_dataset(batched_dataset)\n```\n\n\nConversely, a batched dataset can be transformed into a flat dataset\nusing `Dataset$unbatch`.\n\n\n```{r}\nunbatched_dataset <- batched_dataset$unbatch()\nprint_dictionary_dataset(unbatched_dataset)\n```\n\n\n#### Batching Datasets with variable-length non-ragged tensors\n\nIf you have a Dataset that contains non-ragged tensors, and tensor\nlengths vary across elements, then you can batch those non-ragged\ntensors into ragged tensors by applying the `dense_to_ragged_batch`\ntransformation:\n\n\n```{r}\nnon_ragged_dataset <- tf$data$Dataset$from_tensor_slices([1, 5, 3, 2, 8])\nnon_ragged_dataset <- non_ragged_dataset$map(tf$range)\nbatched_non_ragged_dataset <- non_ragged_dataset$apply(\n    tf$data$experimental$dense_to_ragged_batch(2))\nfor element in batched_non_ragged_dataset:\n  print(element)\n```\n\n\n#### Transforming Datasets with ragged tensors\n\nYou can also create or transform ragged tensors in Datasets using\n`Dataset$map`:\n\n\n```{r}\ntransform_lengths <- function(features) {    }\n  return list(\n      'mean_length': tf$math$reduce_mean(features['lengths']),\n      'length_ranges': tf$ragged$range(features['lengths']))\ntransformed_dataset <- dataset$map(transform_lengths)\nprint_dictionary_dataset(transformed_dataset)\n```\n\n\n### tf_function\n\n[tf_function](https://www$tensorflow$org/guide/function) is a decorator\nthat precomputes TensorFlow graphs for Python functions, which can\nsubstantially improve the performance of your TensorFlow code. Ragged\ntensors can be used transparently with `@tf_function`-decorated\nfunctions. For example, the following function works with both ragged\nand non-ragged tensors:\n\n\n```{r}\n@tf_function\nmake_palindrome <- function(x, axis) {    }\n  return tf$concat([x, tf$reverse(x, [axis])], axis)\n```\n\n```{r}\nmake_palindrome(as_tensor([[1, 2], [3, 4], [5, 6]]), axis=1)\n```\n\n```{r}\nmake_palindrome(tf$ragged$constant([[1, 2], [3], [4, 5, 6]]), axis=1)\n```\n\n\nIf you wish to explicitly specify the `input_signature` for the\n`tf_function`, then you can do so using `tf$RaggedTensorSpec`.\n\n\n```{r}\n@tf_function(\n    input_signature=[tf$RaggedTensorSpec(shape=[NULL, NULL], dtype=tf$int32)])\nmax_and_min <- function(rt) {    }\n  return (tf$math$reduce_max(rt, axis=-1), tf$math$reduce_min(rt, axis=-1))\n\nmax_and_min(tf$ragged$constant([[1, 2], [3], [4, 5, 6]]))\n```\n\n\n#### Concrete functions\n\n[Concrete\nfunctions](https://www$tensorflow$org/guide/function#obtaining_concrete_functions)\nencapsulate individual traced graphs that are built by `tf_function`.\nRagged tensors can be used transparently with concrete functions.\n\n\n```{r}\n@tf_function\nincrement <- function(x) {    }\n  return x + 1\n\nrt <- tf$ragged$constant([[1, 2], [3], [4, 5, 6]])\ncf <- increment$get_concrete_function(rt)\nprint(cf(rt))\n```\n\n\n### SavedModels\n\nA [SavedModel](https://www$tensorflow$org/guide/saved_model) is a\nserialized TensorFlow program, including both weights and computation.\nIt can be built from a Keras model or from a custom model. In either\ncase, ragged tensors can be used transparently with the functions and\nmethods defined by a SavedModel.\n\n#### Example: saving a Keras model\n\n\n```{r}\nimport tempfile\n\nkeras_module_path <- tempfile$mkdtemp()\ntf$saved_model$save(keras_model, keras_module_path)\nimported_model <- tf$saved_model$load(keras_module_path)\nimported_model(hashed_words)\n```\n\n\n#### Example: saving a custom model\n\n\n```{r}\nclass CustomModule(tf$Module):\n  initialize <- function(variable_value) {    }\n    super$initialize()\n    self$v <- tf$Variable(variable_value)\n\n  @tf_function\n  grow <- function(x) {    }\n    return x * self$v\n\nmodule <- CustomModule(100.0)\n\n# Before saving a custom model, you must ensure that concrete functions are\n\n# built for each input signature that you will need.\n\nmodule$grow$get_concrete_function(tf$RaggedTensorSpec(shape=[NULL, NULL],\n                                                      dtype=tf$float32))\n\ncustom_module_path <- tempfile$mkdtemp()\ntf$saved_model$save(module, custom_module_path)\nimported_model <- tf$saved_model$load(custom_module_path)\nimported_model$grow(tf$ragged$constant([[1.0, 4.0, 3.0], [2.0]]))\n```\n\n\nNote: SavedModel\n[signatures](https://www$tensorflow$org/guide/saved_model#specifying_signatures_during_export)\nare concrete functions. As discussed in the section on Concrete\nFunctions above, ragged tensors are only handled correctly by concrete\nfunctions starting with TensorFlow 2.3. If you need to use SavedModel\nsignatures in a previous version of TensorFlow, then it's recommended\nthat you decompose the ragged tensor into its component tensors.\n\n## Overloaded operators\n\nThe `RaggedTensor` class overloads the standard Python arithmetic and\ncomparison operators, making it easy to perform basic elementwise math:\n\n\n```{r}\nx <- tf$ragged$constant([[1, 2], [3], [4, 5, 6]])\ny <- tf$ragged$constant([[1, 1], [2], [3, 3, 3]])\nprint(x + y)\n```\n\n\nSince the overloaded operators perform elementwise computations, the\ninputs to all binary operations must have the same shape or be\nbroadcastable to the same shape. In the simplest broadcasting case, a\nsingle scalar is combined elementwise with each value in a ragged\ntensor:\n\n\n```{r}\nx <- tf$ragged$constant([[1, 2], [3], [4, 5, 6]])\nprint(x + 3)\n```\n\n\nFor a discussion of more advanced cases, check the section on\n^Broadcasting^.\n\nRagged tensors overload the same set of operators as normal `Tensor`s:\nthe unary operators `-`, `~`, and `abs()`; and the binary operators `+`,\n`-`, `*`, `/`, `//`, `%`, `^`, `&`, `|`, `^`, `==`, `<`, `<=`, `>`, and\n`>=`.\n\n## Indexing\n\nRagged tensors support Python-style indexing, including multidimensional\nindexing and slicing. The following examples demonstrate ragged tensor\nindexing with a 2D and a 3D ragged tensor.\n\n### Indexing examples: 2D ragged tensor\n\n\n```{r}\nqueries <- tf$ragged$constant(\n    [['Who', 'is', 'George', 'Washington'],\n     ['What', 'is', 'the', 'weather', 'tomorrow'],\n     ['Goodnight']])\n```\n\n```{r}\nprint(queries[1])                   # A single query\n```\n\n```{r}\nprint(queries[1, 2])                # A single word\n```\n\n```{r}\nprint(queries[1:])                  # Everything but the first row\n```\n\n```{r}\nprint(queries[:, :3])               # The first 3 words of each query\n```\n\n```{r}\nprint(queries[:, -2:])              # The last 2 words of each query\n```\n\n\n### Indexing examples: 3D ragged tensor\n\n\n```{r}\nrt <- tf$ragged$constant([[[1, 2, 3], [4]],\n                         [[5], [], [6]],\n                         [[7]],\n                         [[8, 9], [10]]])\n```\n\n```{r}\nprint(rt[1])                        # Second row (2D RaggedTensor)\n```\n\n```{r}\nprint(rt[3, 0])                     # First element of fourth row (1D Tensor)\n```\n\n```{r}\nprint(rt[:, 1:3])                   # Items 1-3 of each row (3D RaggedTensor)\n```\n\n```{r}\nprint(rt[:, -1:])                   # Last item of each row (3D RaggedTensor)\n```\n\n\n`RaggedTensor`s support multidimensional indexing and slicing with one\nrestriction: indexing into a ragged dimension is not allowed. This case\nis problematic because the indicated value may exist in some rows but\nnot others. In such cases, it's not obvious whether you should (1) raise\nan `IndexError`; (2) use a default value; or (3) skip that value and\nreturn a tensor with fewer rows than you started with. Following the\n[guiding principles of\nPython](https://www$python$org/dev/peps/pep-0020/) (\""In the face of\nambiguity, refuse the temptation to guess\""), this operation is currently\ndisallowed.\n\n## Tensor type conversion\n\nThe `RaggedTensor` class defines methods that can be used to convert\nbetween `RaggedTensor`s and `tf$Tensor`s or `tf$SparseTensors`:\n\n\n```{r}\nragged_sentences <- tf$ragged$constant([\n    ['Hi'], ['Welcome', 'to', 'the', 'fair'], ['Have', 'fun']])\n```\n\n```{r}\n# RaggedTensor -> Tensor\n\nprint(ragged_sentences$to_tensor(default_value='', shape=[NULL, 10]))\n```\n\n```{r}\n# Tensor -> RaggedTensor\n\nx <- [[1, 3, -1, -1], [2, -1, -1, -1], [4, 5, 8, 9]]\nprint(tf$RaggedTensor$from_tensor(x, padding=-1))\n```\n\n```{r}\n#RaggedTensor -> SparseTensor\n\nprint(ragged_sentences$to_sparse())\n```\n\n```{r}\n# SparseTensor -> RaggedTensor\n\nst <- tf$SparseTensor(indices=[[0, 0], [2, 0], [2, 1]],\n                     values=['a', 'b', 'c'],\n                     dense_shape=[3, 3])\nprint(tf$RaggedTensor$from_sparse(st))\n```\n\n\n## Evaluating ragged tensors\n\nTo access the values in a ragged tensor, you can:\n\n1.  Use `tf$RaggedTensor$to_list` to convert the ragged tensor to a\n    nested Python list.\n2.  Use `tf$RaggedTensor$numpy` to convert the ragged tensor to a NumPy\n    array whose values are nested NumPy arrays.\n3.  Decompose the ragged tensor into its components, using the\n    `tf$RaggedTensor$values` and `tf$RaggedTensor$row_splits`\n    properties, or row-paritioning methods such as\n    `tf$RaggedTensor$row_lengths` and `tf$RaggedTensor$value_rowids`.\n4.  Use Python indexing to select values from the ragged tensor.\n\n\n```{r}\nrt <- tf$ragged$constant([[1, 2], [3, 4, 5], [6], [], [7]])\nprint(\""Python list:\"", rt$to_list())\nprint(\""NumPy array:\"", rt$numpy())\nprint(\""Values:\"", rt$values$numpy())\nprint(\""Splits:\"", rt$row_splits$numpy())\nprint(\""Indexed value:\"", rt[1].numpy())\n```\n\n\n## Ragged Shapes\n\nThe shape of a tensor specifies the size of each axis. For example, the\nshape of `[[1, 2], [3, 4], [5, 6]]` is `[3, 2]`, since there are 3 rows\nand 2 columns. TensorFlow has two separate but related ways to describe\nshapes:\n\n-   \\^*static shape\\^*: Information about axis sizes that is known\n    statically (e.g., while tracing a `tf_function`). May be partially\n    specified.\n\n-   \\^*dynamic shape\\^*: Runtime information about the axis sizes.\n\n### Static shape\n\nA Tensor's static shape contains information about its axis sizes that\nis known at graph-construction time. For both `tf$Tensor` and\n`tf$RaggedTensor`, it is available using the `.shape` property, and is\nencoded using `tf$TensorShape`:\n\n\n```{r}\nx <- as_tensor([[1, 2], [3, 4], [5, 6]])\nx$shape  # shape of a tf$tensor\n```\n\n```{r}\nrt <- tf$ragged$constant([[1], [2, 3], [], [4]])\nrt$shape  # shape of a tf$RaggedTensor\n```\n\n\nThe static shape of a ragged dimension is always `NULL` (i\\$e.,\nunspecified). However, the inverse is not true -- if a `TensorShape`\ndimension is `NULL`, then that could indicate that the dimension is\nragged, *or* it could indicate that the dimension is uniform but that\nits size is not statically known.\n\n### Dynamic shape\n\nA tensor's dynamic shape contains information about its axis sizes that\nis known when the graph is run. It is constructed using the `tf$shape`\noperation. For `tf$Tensor`, `tf$shape` returns the shape as a 1D integer\n`Tensor`, where `tf$shape(x)[i]` is the size of axis `i`.\n\n\n```{r}\nx <- as_tensor([['a', 'b'], ['c', 'd'], ['e', 'f']])\ntf$shape(x)\n```\n\n\nHowever, a 1D `Tensor` is not expressive enough to describe the shape of\na `tf$RaggedTensor`. Instead, the dynamic shape for ragged tensors is\nencoded using a dedicated type, `tf$experimental$DynamicRaggedShape`. In\nthe following example, the `DynamicRaggedShape` returned by\n`tf$shape(rt)` indicates that the ragged tensor has 4 rows, with lengths\n1, 3, 0, and 2:\n\n\n```{r}\nrt <- tf$ragged$constant([[1], [2, 3, 4], [], [5, 6]])\nrt_shape <- tf$shape(rt)\nprint(rt_shape)\n```\n\n\n#### Dynamic shape: operations\n\n`DynamicRaggedShape`s can be used with most TensorFlow ops that expect\nshapes, including `tf$reshape`, `tf$zeros`, `tf$ones`. `tf$fill`,\n`tf$broadcast_dynamic_shape`, and `tf$broadcast_to`.\n\n\n```{r}\nprint(f\""tf$reshape(x, rt_shape) = list(tf$reshape(x, rt_shape))\"")\nprint(f\""tf$zeros(rt_shape) = list(tf$zeros(rt_shape))\"")\nprint(f\""tf$ones(rt_shape) = list(tf$ones(rt_shape))\"")\nprint(f\""tf$fill(rt_shape, 9) = list(tf$fill(rt_shape, 'x'))\"")\n```\n\n\n#### Dynamic shape: indexing and slicing\n\n`DynamicRaggedShape` can be also be indexed to get the sizes of uniform\ndimensions. For example, we can find the number of rows in a\nraggedtensor using `tf$shape(rt)[0]` (just as we would for a non-ragged\ntensor):\n\n\n```{r}\nrt_shape[0]\n```\n\n\nHowever, it is an error to use indexing to try to retrieve the size of a\nragged dimension, since it doesn't have a single size. (Since\n`RaggedTensor` keeps track of which axes are ragged, this error is only\nthrown during eager execution or when tracing a `tf_function`; it will\nnever be thrown when executing a concrete function.)\n\n\n```{r}\ntry:\n  rt_shape[1]\nexcept ValueError as e:\n  print(\""Got expected ValueError:\"", e)\n```\n\n\n`DynamicRaggedShape`s can also be sliced, as long as the slice either\nbegins with axis `0`, or contains only dense dimensions.\n\n\n```{r}\nrt_shape[:1]\n```\n\n\n#### Dynamic shape: encoding\n\n`DynamicRaggedShape` is encoded using two fields:\n\n-   `inner_shape`: An integer vector giving the shape of a dense\n    `tf$Tensor`.\n-   `row_partitions`: A list of `tf$experimental$RowPartition` objects,\n    describing how the outermost dimension of that inner shape should be\n    partitioned to add ragged axes.\n\nFor more information about row partitions, see the \""RaggedTensor\nencoding\"" section below, and the API docs for\n`tf$experimental$RowPartition`.\n\n#### Dynamic shape: construction\n\n`DynamicRaggedShape` is most often constructed by applying `tf$shape` to\na `RaggedTensor`, but it can also be constructed directly:\n\n\n```{r}\ntf$experimental$DynamicRaggedShape(\n    row_partitions=[tf$experimental$RowPartition$from_row_lengths([5, 3, 2])],\n    inner_shape=[10, 8])\n```\n\n\nIf the lengths of all rows are known statically,\n`DynamicRaggedShape$from_lengths` can also be used to construct a\ndynamic ragged shape. (This is mostly useful for testing and\ndemonstration code, since it's rare for the lengths of ragged dimensions\nto be known statically).\n\n\n```{r}\ntf$experimental$DynamicRaggedShape$from_lengths([4, (2, 1, 0, 8), 12])\n```\n\n\n### Broadcasting\n\nBroadcasting is the process of making tensors with different shapes have\ncompatible shapes for elementwise operations. For more background on\nbroadcasting, refer to:\n\n-   [NumPy:\n    Broadcasting](https://docs$scipy$org/doc/numpy/user/basics$broadcasting$html)\n-   `tf$broadcast_dynamic_shape`\n-   `tf$broadcast_to`\n\nThe basic steps for broadcasting two inputs `x` and `y` to have\ncompatible shapes are:\n\n1.  If `x` and `y` do not have the same number of dimensions, then add\n    outer dimensions (with size 1) until they do.\n\n2.  For each dimension where `x` and `y` have different sizes:\n\n-   If `x` or `y` have size `1` in dimension `d`, then repeat its values\n    across dimension `d` to match the other input's size.\n-   Otherwise, raise an exception (`x` and `y` are not broadcast\n    compatible).\n\nWhere the size of a tensor in a uniform dimension is a single number\n(the size of slices across that dimension); and the size of a tensor in\na ragged dimension is a list of slice lengths (for all slices across\nthat dimension).\n\n#### Broadcasting examples\n\n\n```{r}\n# x       (2D ragged):  2 x (num_rows)\n\n# y       (scalar)\n\n# result  (2D ragged):  2 x (num_rows)\n\nx <- tf$ragged$constant([[1, 2], [3]])\ny <- 3\nprint(x + y)\n```\n\n```{r}\n# x         (2d ragged):  3 x (num_rows)\n\n# y         (2d tensor):  3 x          1\n\n# Result    (2d ragged):  3 x (num_rows)\n\nx <- tf$ragged$constant(\n   [[10, 87, 12],\n    [19, 53],\n    [12, 32]])\ny <- [[1000], [2000], [3000]]\nprint(x + y)\n```\n\n```{r}\n# x      (3d ragged):  2 x (r1) x 2\n\n# y      (2d ragged):         1 x 1\n\n# Result (3d ragged):  2 x (r1) x 2\n\nx <- tf$ragged$constant(\n    [[[1, 2], [3, 4], [5, 6]],\n     [[7, 8]]],\n    ragged_rank=1)\ny <- as_tensor([[10]])\nprint(x + y)\n```\n\n```{r}\n# x      (3d ragged):  2 x (r1) x (r2) x 1\n\n# y      (1d tensor):                    3\n\n# Result (3d ragged):  2 x (r1) x (r2) x 3\n\nx <- tf$ragged$constant(\n    [\n        [\n            [[1], [2]],\n            [],\n            [[3]],\n            [[4]],\n        ],\n        [\n            [[5], [6]],\n            [[7]]\n        ]\n    ],\n    ragged_rank=2)\ny <- as_tensor([10, 20, 30])\nprint(x + y)\n```\n\n\nHere are some examples of shapes that do not broadcast:\n\n\n```{r}\n# x      (2d ragged): 3 x (r1)\n\n# y      (2d tensor): 3 x    4  # trailing dimensions do not match\n\nx <- tf$ragged$constant([[1, 2], [3, 4, 5, 6], [7]])\ny <- as_tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\ntry:\n  x + y\nexcept tf$errors$InvalidArgumentError as exception:\n  print(exception)\n```\n\n```{r}\n# x      (2d ragged): 3 x (r1)\n\n# y      (2d ragged): 3 x (r2)  # ragged dimensions do not match.\n\nx <- tf$ragged$constant([[1, 2, 3], [4], [5, 6]])\ny <- tf$ragged$constant([[10, 20], [30, 40], [50]])\ntry:\n  x + y\nexcept tf$errors$InvalidArgumentError as exception:\n  print(exception)\n```\n\n```{r}\n# x      (3d ragged): 3 x (r1) x 2\n\n# y      (3d ragged): 3 x (r1) x 3  # trailing dimensions do not match\n\nx <- tf$ragged$constant([[[1, 2], [3, 4], [5, 6]],\n                        [[7, 8], [9, 10]]])\ny <- tf$ragged$constant([[[1, 2, 0], [3, 4, 0], [5, 6, 0]],\n                        [[7, 8, 0], [9, 10, 0]]])\ntry:\n  x + y\nexcept tf$errors$InvalidArgumentError as exception:\n  print(exception)\n```\n\n\n## RaggedTensor encoding\n\nRagged tensors are encoded using the `RaggedTensor` class. Internally,\neach `RaggedTensor` consists of:\n\n-   A `values` tensor, which concatenates the variable-length rows into\n    a flattened list.\n-   A `row_partition`, which indicates how those flattened values are\n    divided into rows.\n\n![RaggedTensor\nencoding](https://www$tensorflow$org/images/ragged_tensors/ragged_encoding_2$png)\n\nThe `row_partition` can be stored using four different encodings:\n\n-   `row_splits` is an integer vector specifying the split points\n    between rows.\n-   `value_rowids` is an integer vector specifying the row index for\n    each value.\n-   `row_lengths` is an integer vector specifying the length of each\n    row.\n-   `uniform_row_length` is an integer scalar specifying a single length\n    for all rows.\n\n![row_partition\nencodings](https://www$tensorflow$org/images/ragged_tensors/partition_encodings$png)\n\nAn integer scalar `nrows` can also be included in the `row_partition`\nencoding to account for empty trailing rows with `value_rowids` or empty\nrows with `uniform_row_length`.\n\n\n```{r}\nrt <- tf$RaggedTensor$from_row_splits(\n    values=[3, 1, 4, 1, 5, 9, 2],\n    row_splits=[0, 4, 4, 6, 7])\nprint(rt)\n```\n\n\nThe choice of which encoding to use for row partitions is managed\ninternally by ragged tensors to improve efficiency in some contexts. In\nparticular, some of the advantages and disadvantages of the different\nrow-partitioning schemes are:\n\n-   \\^Efficient indexing\\^: The `row_splits` encoding enables\n    constant-time indexing and slicing into ragged tensors.\n-   \\^Efficient concatenation\\^: The `row_lengths` encoding is more\n    efficient when concatenating ragged tensors, since row lengths do\n    not change when two tensors are concatenated together.\n-   \\^Small encoding size\\^: The `value_rowids` encoding is more\n    efficient when storing ragged tensors that have a large number of\n    empty rows, since the size of the tensor depends only on the total\n    number of values. On the other hand, the `row_splits` and\n    `row_lengths` encodings are more efficient when storing ragged\n    tensors with longer rows, since they require only one scalar value\n    for each row.\n-   ^Compatibility^: The `value_rowids` scheme matches the\n    [segmentation](https://www$tensorflow$org/api_docs/python/tf/math#about_segmentation)\n    format used by operations, such as `tf$segment_sum`. The\n    `row_limits` scheme matches the format used by ops such as\n    `tf$sequence_mask`.\n-   \\^Uniform dimensions\\^: As discussed below, the `uniform_row_length`\n    encoding is used to encode ragged tensors with uniform dimensions.\n\n### Multiple ragged dimensions\n\nA ragged tensor with multiple ragged dimensions is encoded by using a\nnested `RaggedTensor` for the `values` tensor. Each nested\n`RaggedTensor` adds a single ragged dimension.\n\n![Encoding of a ragged tensor with multiple ragged dimensions (rank\n2)](https://www$tensorflow$org/images/ragged_tensors/ragged_rank_2$png)\n\n\n```{r}\nrt <- tf$RaggedTensor$from_row_splits(\n    values=tf$RaggedTensor$from_row_splits(\n        values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n        row_splits=[0, 3, 3, 5, 9, 10]),\n    row_splits=[0, 1, 1, 5])\nprint(rt)\nprint(\""Shape: list()\"".format(rt$shape))\nprint(\""Number of partitioned dimensions: list()\"".format(rt$ragged_rank))\n```\n\n\nThe factory function `tf$RaggedTensor$from_nested_row_splits` may be\nused to construct a RaggedTensor with multiple ragged dimensions\ndirectly by providing a list of `row_splits` tensors:\n\n\n```{r}\nrt <- tf$RaggedTensor$from_nested_row_splits(\n    flat_values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n    nested_row_splits=([0, 1, 1, 5], [0, 3, 3, 5, 9, 10]))\nprint(rt)\n```\n\n\n### Ragged rank and flat values\n\nA ragged tensor's \\^*ragged rank\\^* is the number of times that the\nunderlying `values` tensor has been partitioned (i\\$e. the nesting depth\nof `RaggedTensor` objects). The innermost `values` tensor is known as\nits \\^*flat_values\\^*. In the following example, `conversations` has\nragged_rank=3, and its `flat_values` is a 1D `Tensor` with 24 strings:\n\n\n```{r}\n# shape = [batch, (paragraph), (sentence), (word)]\n\nconversations <- tf$ragged$constant(\n    [[[[\""I\"", \""like\"", \""ragged\"", \""tensors.\""]],\n      [[\""Oh\"", \""yeah?\""], [\""What\"", \""can\"", \""you\"", \""use\"", \""them\"", \""for?\""]],\n      [[\""Processing\"", \""variable\"", \""length\"", \""data!\""]]],\n     [[[\""I\"", \""like\"", \""cheese.\""], [\""Do\"", \""you?\""]],\n      [[\""Yes.\""], [\""I\"", \""do.\""]]]])\nconversations$shape\n```\n\n```{r}\nassert conversations$ragged_rank == len(conversations$nested_row_splits)\nconversations$ragged_rank  # Number of partitioned dimensions.\n```\n\n```{r}\nconversations$flat_values$numpy()\n```\n\n\n### Uniform inner dimensions\n\nRagged tensors with uniform inner dimensions are encoded by using a\nmultidimensional `tf$Tensor` for the flat_values (i\\$e., the innermost\n`values`).\n\n![Encoding of ragged tensors with uniform inner\ndimensions](https://www$tensorflow$org/images/ragged_tensors/uniform_inner$png)\n\n\n```{r}\nrt <- tf$RaggedTensor$from_row_splits(\n    values=[[1, 3], [0, 0], [1, 3], [5, 3], [3, 3], [1, 2]],\n    row_splits=[0, 3, 4, 6])\nprint(rt)\nprint(\""Shape: list()\"".format(rt$shape))\nprint(\""Number of partitioned dimensions: list()\"".format(rt$ragged_rank))\nprint(\""Flat values shape: list()\"".format(rt$flat_values$shape))\nprint(\""Flat values:\\nlist()\"".format(rt$flat_values))\n```\n\n\n### Uniform non-inner dimensions\n\nRagged tensors with uniform non-inner dimensions are encoded by\npartitioning rows with `uniform_row_length`.\n\n![Encoding of ragged tensors with uniform non-inner\ndimensions](https://www$tensorflow$org/images/ragged_tensors/uniform_outer$png)\n\n\n```{r}\nrt <- tf$RaggedTensor$from_uniform_row_length(\n    values=tf$RaggedTensor$from_row_splits(\n        values=[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n        row_splits=[0, 3, 5, 9, 10]),\n    uniform_row_length=2)\nprint(rt)\nprint(\""Shape: list()\"".format(rt$shape))\nprint(\""Number of partitioned dimensions: list()\"".format(rt$ragged_rank))\n```\n\n"",
-    ""supporting"": [
-      ""ragged_tensor_files""
-    ],
-    ""filters"": [],
-    ""includes"": {}
-  }
-}
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/tensor/execute-results/html.json---
@@ -0,0 +1,14 @@
+{
+  ""hash"": ""a8758d0e13f79a1181a6b4ac5446fe1f"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: Introduction to Tensors\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\n\n::: {.cell cellView='form'}\n\n```{.r .cell-code}\n#@title Licensed under the Apache License, Version 2.0 (the \""License\"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \""AS IS\"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n```\n:::\n\n\nTensors are multi-dimensional arrays with a uniform type (called a\n`dtype`). You can see all supported `dtypes` with `names(tf$dtypes)`.\n\nIf you're familiar with R array or\n[NumPy](https://numpy.org/devdocs/user/quickstart.html), tensors are\n(kind of) like R or NumPy arrays.\n\nAll tensors are immutable: you can never\nupdate the contents of a tensor, only create a new one.\n\n## Basics\n\nLet's create some basic tensors.\n\nHere is a \""scalar\"" or \""rank-0\"" tensor . A scalar contains a single\nvalue, and no \""axes\"".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This will be an float64 tensor by default; see \""dtypes\"" below.\nrank_0_tensor <- as_tensor(4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\nprint(rank_0_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(4.0, shape=(), dtype=float64)\n```\n:::\n:::\n\n\nA \""vector\"" or \""rank-1\"" tensor is like a list of values. A vector has one\naxis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_1_tensor <- as_tensor(c(2, 3, 4))\nprint(rank_1_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 3. 4.], shape=(3), dtype=float64)\n```\n:::\n:::\n\n\nA \""matrix\"" or \""rank-2\"" tensor has two axes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If you want to be specific, you can set the dtype (see below) at creation time\nrank_2_tensor <- \n  as_tensor(rbind(c(1, 2), \n                  c(3, 4), \n                  c(5, 6)), \n            dtype=tf$float16)\nprint(rank_2_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2.]\n [3. 4.]\n [5. 6.]], shape=(3, 2), dtype=float16)\n```\n:::\n:::\n\n\n| A scalar, shape: `[]`                               | A vector, shape: `[3]`                                                               | A matrix, shape: `[3, 2]`                                                    |\n|------------------|----------------------------|--------------------------|\n| ![A scalar, the number 4](images/tensor/scalar.png) | ![The line with 3 sections, each one containing a number.](images/tensor/vector.png) | ![A 3x2 grid, with each cell containing a number.](images/tensor/matrix.png) |\n\nTensors may have more axes; here is a tensor with three axes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# There can be an arbitrary number of\n# axes (sometimes called \""dimensions\"")\n\nrank_3_tensor <- as_tensor(0:29, shape = c(3, 2, 5))\nrank_3_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n```\n:::\n:::\n\n\nThere are many ways you might visualize a tensor with more than two\naxes.\n\n| A 3-axis tensor, shape: `[3, 2, 5]`                                                                          |\n|------------------------------------------------------------------------|\n|                                                                                                              |\n| ![](images/tensor/3-axis_numpy.png)! ![](images/tensor/3-axis_numpy.png) ![](images/tensor/3-axis_block.png) |\n\nYou can convert a tensor to an R array using `as.array()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.array(rank_2_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n```\n:::\n:::\n\n\nTensors often contain floats and ints, but have many other types,\nincluding:\n\n-   complex numbers\n-   strings\n\nThe base `tf$Tensor` class requires tensors to be \""rectangular\""---that\nis, along each axis, every element is the same size. However, there are\nspecialized types of tensors that can handle different shapes:\n\n-   Ragged tensors (see [RaggedTensor](#ragged_tensors) below)\n-   Sparse tensors (see [SparseTensor](#sparse_tensors) below)\n\nYou can do basic math on tensors, including addition, element-wise\nmultiplication, and matrix multiplication.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- as_tensor(1:4, shape = c(2, 2)) \nb <- as_tensor(1L, shape = c(2, 2))\n\na + b # element-wise addition, same as tf$add(a, b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[2 3]\n [4 5]], shape=(2, 2), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\na * b # element-wise multiplication, same as tf$multiply(a, b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$matmul(a, b) # matrix multiplication\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[3 3]\n [7 7]], shape=(2, 2), dtype=int32)\n```\n:::\n:::\n\n\nTensors are used in all kinds of operations (ops).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(rbind(c(4, 5), c(10, 1)))\n\n# Find the largest value\n\n# Find the largest value\ntf$reduce_max(x) # can also just call max(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(10.0, shape=(), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# Find the index of the largest value\ntf$math$argmax(x) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1 0], shape=(2), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\ntf$nn$softmax(x) # Compute the softmax\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[2.68941421e-01 7.31058579e-01]\n [9.99876605e-01 1.23394576e-04]], shape=(2, 2), dtype=float64)\n```\n:::\n:::\n\n\n## About shapes\n\nTensors have shapes. Some vocabulary:\n\n-   **Shape**: The length (number of elements) of each of the axes of a\n    tensor.\n-   **Rank**: Number of tensor axes. A scalar has rank 0, a vector has\n    rank 1, a matrix is rank 2.\n-   **Axis** or **Dimension**: A particular dimension of a tensor.\n-   **Size**: The total number of items in the tensor, the product of\n    the shape vector's elements.\n\nNote: Although you may see reference to a \""tensor of two dimensions\"", a\nrank-2 tensor does not usually describe a 2D space.\n\nTensors and `tf$TensorShape` objects have convenient properties for\naccessing these:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_4_tensor <- tf$zeros(shape(3, 2, 4, 5))\n```\n:::\n\n\n| A rank-4 tensor, shape: `[3, 2, 4, 5]`                       |\n|--------------------------------------------------------------|\n| ![A tensor shape is like a vector.](images/tensor/shape.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessage(\""Type of every element: \"", rank_4_tensor$dtype)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nType of every element: <dtype: 'float32'>\n```\n:::\n\n```{.r .cell-code}\nmessage(\""Number of axes: \"", length(dim(rank_4_tensor)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNumber of axes: 4\n```\n:::\n\n```{.r .cell-code}\nmessage(\""Shape of tensor: \"", dim(rank_4_tensor)) # can also access via rank_4_tensor$shape\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nShape of tensor: 3245\n```\n:::\n\n```{.r .cell-code}\nmessage(\""Elements along axis 0 of tensor: \"", dim(rank_4_tensor)[1])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nElements along axis 0 of tensor: 3\n```\n:::\n\n```{.r .cell-code}\nmessage(\""Elements along the last axis of tensor: \"", dim(rank_4_tensor) |> tail(1)) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nElements along the last axis of tensor: 5\n```\n:::\n\n```{.r .cell-code}\nmessage(\""Total number of elements (3*2*4*5): \"", length(rank_4_tensor)) # can also call tf$size()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTotal number of elements (3*2*4*5): 120\n```\n:::\n:::\n\n\nWhile axes are often referred to by their indices, you should always\nkeep track of the meaning of each. Often axes are ordered from global to\nlocal: The batch axis first, followed by spatial dimensions, and\nfeatures for each location last. This way feature vectors are contiguous\nregions of memory.\n\n| Typical axis order                                                                                                     |\n|------------------------------------------------------------------------|\n| ![Keep track of what each axis is. A 4-axis tensor might be: Batch, Width, Height, Features](images/tensor/shape2.png) |\n\n## Indexing\n\n### Single-axis indexing\n\nSee `` ?`[.tensorflow.tensor` `` for details\n\n### Multi-axis indexing\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis\nindependently.\n\nRead the [tensor slicing\nguide](https://tensorflow.org/guide/tensor_slicing) to learn how you can\napply indexing to manipulate individual elements in your tensors.\n\n## Manipulating Shapes\n\nReshaping a tensor is of great utility.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shape returns a `TensorShape` object that shows the size along each axis\n\nx <- as_tensor(1:3, shape = c(1, -1)) \nx$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([1, 3])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can convert this object into an R vector too\nas.integer(x$shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 3\n```\n:::\n:::\n\n\nYou can reshape a tensor into a new shape. The `tf$reshape` operation is\nfast and cheap as the underlying data does not need to be duplicated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can reshape a tensor to a new shape.\n# Note that you're passing in integers\n\nreshaped <- tf$reshape(x, c(1L, 3L))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([1, 3])\n```\n:::\n\n```{.r .cell-code}\nreshaped$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([1, 3])\n```\n:::\n:::\n\n\nThe data maintains its layout in memory and a new tensor is created,\nwith the requested shape, pointing to the same data. TensorFlow uses\nC-style \""row-major\"" memory ordering, where incrementing the rightmost\nindex corresponds to a single step in memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_3_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n```\n:::\n:::\n\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A `-1` passed in the `shape` argument says \""Whatever fits\"".\ntf$reshape(rank_3_tensor, c(-1L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29], shape=(30), dtype=int32)\n```\n:::\n:::\n\n\nA typical and reasonable use of `tf$reshape` is to combine or split\nadjacent axes (or add/remove `1`s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both\nreasonable things to do, as the slices do not mix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$reshape(rank_3_tensor, as.integer(c(3*2, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]\n [20 21 22 23 24]\n [25 26 27 28 29]], shape=(6, 5), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$reshape(rank_3_tensor, as.integer(c(3L, -1L)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n```\n:::\n:::\n\n\n| Some good reshapes.                                                                                                                  |\n|------------------------------------------------------------------------|\n| ![A 3x2x5 tensor](images/tensor/reshape-before.png) ![3x10](images/tensor/reshape-good1.png) ![6x5](images/tensor/reshape-good2.png) |\n\nhttps://www.tensorflow.org/guide/images/tensor/reshape-before.png\nhttps://www.tensorflow.org/guide/\nhttps://www.tensorflow.org/guide/images/tensor/reshape-good2.png\n\nReshaping will \""work\"" for any new shape with the same total number of\nelements, but it will not do anything useful if you do not respect the\norder of the axes.\n\nSwapping axes in `tf$reshape` does not work; you need `tf$transpose` for\nthat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bad examples: don't do this\n\n# You can't reorder axes with reshape.\ntf$reshape(rank_3_tensor, as.integer(c(2, 3, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]\n  [10 11 12 13 14]]\n\n [[15 16 17 18 19]\n  [20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\n# This is a mess\ntf$reshape(rank_3_tensor, as.integer(c(5, 6)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]], shape=(5, 6), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\n# This doesn't work at all\ntry(tf$reshape(rank_3_tensor, as.integer(c(7, -1))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n```\n:::\n:::\n\n\n| Some bad reshapes.                                                                                                                                                                                                                                           |\n|------------------------------------------------------------------------|\n| ![You can't reorder axes, use tf\\$transpose for that](images/tensor/reshape-bad.png) ![Anything that mixes the slices of data together is probably wrong.](images/tensor/reshape-bad4.png) ![The new shape must fit exactly](images/tensor/reshape-bad2.png) |\n\nYou may run across not-fully-specified shapes. Either the shape contains\na `NULL` (an axis-length is unknown) or the whole shape is `NULL` (the\nrank of the tensor is unknown).\n\nExcept for [`tf$RaggedTensor`](#ragged_tensors), such shapes will only\noccur in the context of TensorFlow's symbolic, graph-building APIs:\n\n-   [tf_function](function.qmd)\n-   The [keras functional\n    API](https://www.tensorflow.org/guide/keras/functional).\n\n## More on `DTypes`\n\nTo inspect a `tf$Tensor`'s data type use the `Tensor$dtype` property.\n\nWhen creating a `tf$Tensor` from a Python object you may optionally\nspecify the datatype.\n\nIf you don't, TensorFlow chooses a datatype that can represent your\ndata. TensorFlow converts R integers to `tf$int32` and R floating point\nnumbers to `tf$float64`.\n\nYou can cast from type to type.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthe_f64_tensor <- as_tensor(c(2.2, 3.3, 4.4), dtype = tf$float64)\nthe_f16_tensor <- tf$cast(the_f64_tensor, dtype = tf$float16)\n# Now, cast to an uint8 and lose the decimal precision\n\nthe_u8_tensor <- tf$cast(the_f16_tensor, dtype = tf$uint8)\nthe_u8_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2 3 4], shape=(3), dtype=uint8)\n```\n:::\n:::\n\n\n## Broadcasting\n\nBroadcasting is a concept borrowed from the [equivalent feature in\nNumPy](https://numpy.org/doc/stable/user/basics.html). In short, under\ncertain conditions, smaller tensors are recycled automatically to fit\nlarger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add\na tensor to a scalar. In that case, the scalar is broadcast to be the\nsame shape as the other argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(c(1, 2, 3))\n\ny <- as_tensor(2)\nz <- as_tensor(c(2, 2, 2))\n\n# All of these are the same computation\ntf$multiply(x, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nx * y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nx * z\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n```\n:::\n:::\n\n\nLikewise, axes with length 1 can be stretched out to match the other\narguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to\nproduce a 3x4 matrix. Note how the leading 1 is optional: The shape of y\nis `[4]`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# These are the same computations\n(x <- tf$reshape(x, as.integer(c(3, 1))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1.]\n [2.]\n [3.]], shape=(3, 1), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n(y <- tf$range(1, 5,  dtype = \""float64\""))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1. 2. 3. 4.], shape=(4), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nx * y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n```\n:::\n:::\n\n\n| A broadcasted add: a `[3, 1]` times a `[1, 4]` gives a `[3,4]`                                   |\n|------------------------------------------------------------------------|\n| ![Adding a 3x1 matrix to a 4x1 matrix results in a 3x4 matrix](images/tensor/broadcasting.png)\\\\ |\n\nHere is the same operation without broadcasting:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_stretch <- as_tensor(rbind(c(1, 1, 1, 1),\n                             c(2, 2, 2, 2),\n                             c(3, 3, 3, 3)))\n\ny_stretch <- as_tensor(rbind(c(1, 2, 3, 4),\n                             c(1, 2, 3, 4),\n                             c(1, 2, 3, 4)))\n\nx_stretch * y_stretch  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n```\n:::\n:::\n\n\nMost of the time, broadcasting is both time and space efficient, as the\nbroadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using `tf$broadcast_to`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$broadcast_to(as_tensor(c(1, 2, 3)), c(3L, 3L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]], shape=(3, 3), dtype=float64)\n```\n:::\n:::\n\n\nUnlike a mathematical op, for example, `broadcast_to` does nothing\nspecial to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. [This\nsection](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)\nof Jake VanderPlas's book *Python Data Science Handbook* shows more\nbroadcasting tricks (again in NumPy).\n\n## `tf$convert_to_tensor`\n\nMost ops, like `tf$matmul` and `tf$reshape` take arguments of class\n`tf$Tensor`. However, you'll notice in the above case, objects shaped\nlike tensors are also accepted.\n\nMost, but not all, ops call `convert_to_tensor` on non-tensor arguments.\nThere is a registry of conversions, and most object classes like NumPy's\n`ndarray`, `TensorShape`, Python lists, and `tf$Variable` will all\nconvert automatically.\n\nSee `tf$register_tensor_conversion_function` for more details, and if\nyou have your own type you'd like to automatically convert to a tensor.\n\n## Ragged Tensors\n\nA tensor with variable numbers of elements along some axis is called\n\""ragged\"". Use `tf$ragged$RaggedTensor` for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\n| A `tf$RaggedTensor`, shape: `[4, NULL]`                                                    |\n|------------------------------------------------------------------------|\n| ![A 2-axis ragged tensor, each row can have a different length.](images/tensor/ragged.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\nragged_list <- list(list(0, 1, 2, 3),\n                    list(4, 5),\n                    list(6, 7, 8),\n                    list(9))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntry(tensor <- as_tensor(ragged_list))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Can't convert non-rectangular Python sequence to Tensor.\n```\n:::\n:::\n\n\nInstead create a `tf$RaggedTensor` using `tf$ragged$constant`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(ragged_tensor <- tf$ragged$constant(ragged_list))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.RaggedTensor [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0], [6.0, 7.0, 8.0], [9.0]]>\n```\n:::\n:::\n\n\nThe shape of a `tf$RaggedTensor` will contain some axes with unknown\nlengths:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(ragged_tensor$shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([4, None])\n```\n:::\n:::\n\n\n## String tensors\n\n`tf$string` is a `dtype`, which is to say you can represent data as\nstrings (variable-length byte arrays) in tensors.\n\nThe length of the string is not one of the axes of the tensor. See\n`tf$strings` for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tensors can be strings, too here is a scalar string.\n\n(scalar_string_tensor <- as_tensor(\""Gray wolf\""))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(b'Gray wolf', shape=(), dtype=string)\n```\n:::\n:::\n\n\nAnd a vector of strings:\n\n| A vector of strings, shape: `[3,]`                                               |\n|------------------------------------------------------------------------|\n| ![The string length is not one of the tensor's axes.](images/tensor/strings.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntensor_of_strings <- as_tensor(c(\""Gray wolf\"",\n                                 \""Quick brown fox\"",\n                                 \""Lazy dog\""))\n# Note that the shape is (3). The string length is not included.\n\ntensor_of_strings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3), dtype=string)\n```\n:::\n:::\n\n\nIn the above printout the `b` prefix indicates that `tf$string` dtype is\nnot a unicode string, but a byte-string. See the [Unicode\nTutorial](https://www.tensorflow.org/tutorials/load_data/unicode) for\nmore about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_tensor(\""🥳👍\"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(b'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d', shape=(), dtype=string)\n```\n:::\n:::\n\n\nSome basic functions with strings can be found in `tf$strings`,\nincluding `tf$strings$split`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can use split to split a string into a set of tensors\ntf$strings$split(scalar_string_tensor, sep=\"" \"")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'Gray' b'wolf'], shape=(2), dtype=string)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ...and it turns into a `RaggedTensor` if you split up a tensor of strings,\n# as each string might be split into a different number of parts.\ntf$strings$split(tensor_of_strings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>\n```\n:::\n:::\n\n\n| Three strings split, shape: `[3, NULL]`                                                  |\n|------------------------------------------------------------------------|\n| ![Splitting multiple strings returns a tf\\$RaggedTensor](images/tensor/string-split.png) |\n\nAnd `tf$string$to_number`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- as_tensor(\""1 10 100\"")\ntf$strings$to_number(tf$strings$split(text, \"" \""))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([  1.  10. 100.], shape=(3), dtype=float32)\n```\n:::\n:::\n\n\nAlthough you can't use `tf$cast` to turn a string tensor into numbers,\nyou can convert it into bytes, and then into numbers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbyte_strings <- tf$strings$bytes_split(as_tensor(\""Duck\""))\nbyte_ints <- tf$io$decode_raw(as_tensor(\""Duck\""), tf$uint8)\ncat(\""Byte strings: \""); print(byte_strings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nByte strings: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'D' b'u' b'c' b'k'], shape=(4), dtype=string)\n```\n:::\n\n```{.r .cell-code}\ncat(\""Bytes: \""); print(byte_ints)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBytes: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([ 68 117  99 107], shape=(4), dtype=uint8)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Or split it up as unicode and then decode it\nunicode_bytes <- as_tensor(\""アヒル 🦆\"")\nunicode_char_bytes <- tf$strings$unicode_split(unicode_bytes, \""UTF-8\"")\nunicode_values <- tf$strings$unicode_decode(unicode_bytes, \""UTF-8\"")\n\ncat(\""Unicode bytes: \""); unicode_bytes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnicode bytes: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(b'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86', shape=(), dtype=string)\n```\n:::\n\n```{.r .cell-code}\ncat(\""Unicode chars: \""); unicode_char_bytes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnicode chars: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'\\xe3\\x82\\xa2' b'\\xe3\\x83\\x92' b'\\xe3\\x83\\xab' b' ' b'\\xf0\\x9f\\xa6\\x86'], shape=(5), dtype=string)\n```\n:::\n\n```{.r .cell-code}\ncat(\""Unicode values: \""); unicode_values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnicode values: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([ 12450  12498  12523     32 129414], shape=(5), dtype=int32)\n```\n:::\n:::\n\n\nThe `tf$string` dtype is used for all raw bytes data in TensorFlow. The\n`tf$io` module contains functions for converting data to and from bytes,\nincluding decoding images and parsing csv.\n\n## Sparse tensors\n\nSometimes, your data is sparse, like a very wide embedding space.\nTensorFlow supports `tf$sparse$SparseTensor` and related operations to\nstore sparse data efficiently.\n\n| A `tf$SparseTensor`, shape: `[3, 4]`                                            |\n|------------------------------------------------------------------------|\n| ![An 3x4 grid, with values in only two of the cells.](images/tensor/sparse.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sparse tensors store values by index in a memory-efficient manner\nsparse_tensor <- tf$sparse$SparseTensor(\n  indices = rbind(c(0L, 0L),\n                  c(1L, 2L)),\n  values = c(1, 2),\n  dense_shape = as.integer(c(3, 4))\n)\n\nsparse_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSparseTensor(indices=tf.Tensor(\n[[0 0]\n [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2), dtype=int64))\n```\n:::\n\n```{.r .cell-code}\n# You can convert sparse tensors to dense\ntf$sparse$to_dense(sparse_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 0. 0. 0.]\n [0. 0. 2. 0.]\n [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)\n```\n:::\n:::\n"",
+    ""supporting"": [],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: _freeze/tensorflow/guide/variable/execute-results/html.json---
@@ -0,0 +1,14 @@
+{
+  ""hash"": ""fc0709f7108c4b352b25fa06038bb35b"",
+  ""result"": {
+    ""markdown"": ""---\ntitle: Introduction to Variables\n---\n\n\n##### Copyright 2020 The TensorFlow Authors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#@title Licensed under the Apache License, Version 2.0 (the \""License\"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \""AS IS\"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n```\n:::\n\n\nA TensorFlow **variable** is the recommended way to represent shared,\npersistent state your program manipulates. This guide covers how to\ncreate, update, and manage instances of `tf$Variable` in TensorFlow.\n\nVariables are created and tracked via the `tf$Variable` class. A\n`tf$Variable` represents a tensor whose value can be changed by running\nops on it. Specific ops allow you to read and modify the values of this\ntensor. Higher level libraries like `tf$keras` use `tf$Variable` to\nstore model parameters.\n\n## Setup\n\nThis notebook discusses variable placement. If you want to see on what\ndevice your variables are placed, uncomment this line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n\n# Uncomment to see where your variables get placed (see below)\n# tf$debugging$set_log_device_placement(TRUE)\n```\n:::\n\n\n## Create a variable\n\nTo create a variable, provide an initial value. The `tf$Variable` will\nhave the same `dtype` as the initialization value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_tensor <- as_tensor(1:4, \""float32\"", shape = c(2, 2))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.9.1\n```\n:::\n\n```{.r .cell-code}\n(my_variable <- tf$Variable(my_tensor))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\n# Variables can be all kinds of types, just like tensors\n\n(bool_variable <- tf$Variable(c(FALSE, FALSE, FALSE, TRUE)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(4,) dtype=bool, numpy=array([False, False, False,  True])>\n```\n:::\n\n```{.r .cell-code}\n(complex_variable <- tf$Variable(c(5 + 4i, 6 + 1i)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([5.+4.j, 6.+1.j])>\n```\n:::\n:::\n\n\nA variable looks and acts like a tensor, and, in fact, is a data\nstructure backed by a `tf$Tensor`. Like tensors, they have a `dtype` and\na shape, and can be exported to regular R arrays.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\""Shape: \""); my_variable$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([2, 2])\n```\n:::\n\n```{.r .cell-code}\ncat(\""DType: \""); my_variable$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDType: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.float32\n```\n:::\n\n```{.r .cell-code}\ncat(\""As R array: \""); str(as.array(my_variable))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAs R array: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n num [1:2, 1:2] 1 3 2 4\n```\n:::\n:::\n\n\nMost tensor operations work on variables as expected, although variables\ncannot be reshaped.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessage(\""A variable: \"")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nA variable: \n```\n:::\n\n```{.r .cell-code}\nmy_variable\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\narray([[1., 2.],\n       [3., 4.]], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\nmessage(\""Viewed as a tensor: \"")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nViewed as a tensor: \n```\n:::\n\n```{.r .cell-code}\nas_tensor(my_variable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nmessage(\""Index of highest value: \"")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nIndex of highest value: \n```\n:::\n\n```{.r .cell-code}\ntf$math$argmax(my_variable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1 1], shape=(2), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\n# This creates a new tensor; it does not reshape the variable.\nmessage(\""Copying and reshaping: \"") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCopying and reshaping: \n```\n:::\n\n```{.r .cell-code}\ntf$reshape(my_variable, c(1L, 4L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n```\n:::\n:::\n\n\nAs noted above, variables are backed by tensors. You can reassign the\ntensor using `tf$Variable$assign`. Calling `assign` does not (usually)\nallocate a new tensor; instead, the existing tensor's memory is reused.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- tf$Variable(c(2, 3))\n\n# assigning allowed, input is automatically \n# cast to the dtype of the Variable, float32\na$assign(as.integer(c(1, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\n# resize the variable is not allowed\ntry(a$assign(c(1.0, 2.0, 3.0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible.\n```\n:::\n:::\n\n\nIf you use a variable like a tensor in operations, you will usually\noperate on the backing tensor.\n\nCreating new variables from existing variables duplicates the backing\ntensors. Two variables will not share the same memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- tf$Variable(c(2, 3))\n# Create b based on the value of a\n\nb <- tf$Variable(a)\na$assign(c(5, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([5., 6.], dtype=float32)>\n```\n:::\n\n```{.r .cell-code}\n# a and b are different\n\nas.array(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5 6\n```\n:::\n\n```{.r .cell-code}\nas.array(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 3\n```\n:::\n\n```{.r .cell-code}\n# There are other versions of assign\n\nas.array(a$assign_add(c(2,3))) # c(7, 9)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7 9\n```\n:::\n\n```{.r .cell-code}\nas.array(a$assign_sub(c(7,9))) # c(0, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 0\n```\n:::\n:::\n\n\n## Lifecycles, naming, and watching\n\nIn TensorFlow, `tf$Variable` instance have the same lifecycle as other R\nobjects. When there are no references to a variable it is automatically\ndeallocated (garbage-collected).\n\nVariables can also be named which can help you track and debug them. You\ncan give two variables the same name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a and b; they will have the same name but will be backed by\n# different tensors.\n\na <- tf$Variable(my_tensor, name = \""Mark\"")\n# A new variable with the same name, but different value\n\n# Note that the scalar add `+` is broadcast\nb <- tf$Variable(my_tensor + 1, name = \""Mark\"")\n\n# These are elementwise-unequal, despite having the same name\nprint(a == b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[False False]\n [False False]], shape=(2, 2), dtype=bool)\n```\n:::\n:::\n\n\nVariable names are preserved when saving and loading models. By default,\nvariables in models will acquire unique variable names automatically, so\nyou don't need to assign them yourself unless you want to.\n\nAlthough variables are important for differentiation, some variables\nwill not need to be differentiated. You can turn off gradients for a\nvariable by setting `trainable` to false at creation. An example of a\nvariable that would not need gradients is a training step counter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(step_counter <- tf$Variable(1L, trainable = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>\n```\n:::\n:::\n\n\n## Placing variables and tensors\n\nFor better performance, TensorFlow will attempt to place tensors and\nvariables on the fastest device compatible with its `dtype`. This means\nmost variables are placed on a GPU if one is available.\n\nHowever, you can override this. In this snippet, place a float tensor\nand a variable on the CPU, even if a GPU is available. By turning on\ndevice placement logging (see above), you can see where the variable is\nplaced.\n\nNote: Although manual placement works, using [distribution\nstrategies](distributed_training.qmd) can be a more convenient and\nscalable way to optimize your computation.\n\nIf you run this notebook on different backends with and without a GPU\nyou will see different logging. *Note that logging device placement must\nbe turned on at the start of the session.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$device('CPU:0'), {\n  # Create some tensors\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \""float32\"")\n  b <- as_tensor(array(1:6, c(3, 2)), dtype = \""float32\"")\n  c <- tf$matmul(a, b)\n})\n\nc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[22. 49.]\n [28. 64.]], shape=(2, 2), dtype=float32)\n```\n:::\n:::\n\n\nIt's possible to set the location of a variable or tensor on one device\nand do the computation on another device. This will introduce delay, as\ndata needs to be copied between the devices.\n\nYou might do this, however, if you had multiple GPU workers but only\nwant one copy of the variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$device('CPU:0'), {\n  a <- tf$Variable(array(1:6, c(2, 3)), dtype = \""float32\"")\n  b <- tf$Variable(array(1:3, c(1, 3)), dtype = \""float32\"")\n})\n\nwith(tf$device('GPU:0'), {\n  # Element-wise multiply\n  k <- a * b\n})\n\nk\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.  6. 15.]\n [ 2.  8. 18.]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n\nNote: Because `tf$config$set_soft_device_placement()` is turned on by\ndefault, even if you run this code on a device without a GPU, it will\nstill run. The multiplication step will happen on the CPU.\n\nFor more on distributed training, refer to the\n[guide](distributed_training$qmd).\n\n## Next steps\n\nTo understand how variables are typically used, see our guide on\n[automatic differentiation](autodiff.qmd).\n"",
+    ""supporting"": [],
+    ""filters"": [
+      ""rmarkdown/pagebreak.lua""
+    ],
+    ""includes"": {},
+    ""engineDependencies"": {},
+    ""preserve"": {},
+    ""postProcess"": true
+  }
+}
\ No newline at end of file

---FILE: _quarto.yml---
@@ -1,5 +1,6 @@
 project:
   type: website
+  execute-dir: project
   render:
     - index.qmd
     - tensorflow/guide/*.qmd
@@ -9,43 +10,37 @@ project:
     browser: false
 
 execute:
-  freeze: false
-  eval: false
+  freeze: auto
 
 website:
   title: TensorFlow for R
   site-url: https://tensorflow.rstudio.com
+  repo-url: https://github.com/t-kalinowski/tf-site
   page-navigation: true
   reader-mode: true
   favicon: ""images/favicon/icon.png""
   navbar:
     search: true
-    background: light
-    collapse-below: lg
     logo: ""images/favicon/icon.png""
   sidebar:
-    - id: tensorflow
-      title: TensorFlow
-      align: left
-      collapse-level: 3
+    - title: TensorFlow
+      search: true
       contents:
         - text: Tensorflow Basics
-          contents:
-          - text: Overview
-            href: tensorflow/guide/basics.qmd
-          - text: Tensors
-            href: tensorflow/guide/tensor.qmd
-          - text: Variables
-            href: tensorflow/guide/variable.qmd
-          - text: Automatic differentiation
-            href: tensorflow/guide/autodiff.qmd
+        - text: Overview
+          href: tensorflow/guide/basics.qmd
+        - text: Tensors
+          href: tensorflow/guide/tensor.qmd
+        - text: Variables
+          href: tensorflow/guide/variable.qmd
+        - text: Automatic differentiation
+          href: tensorflow/guide/autodiff.qmd
+        - text: Graphs and functions
+          href: tensorflow/guide/intro_to_graphs.qmd
+
 
 format:
   html:
     toc: true
-    code-copy: true
     code-overflow: wrap
-    css: /css/styles.css
-    theme:
-      light: [cosmo, /css/theme.scss]
-      dark: [cosmo, /css/theme-dark.scss]
+    theme: united

---FILE: _site/css/styles.css---
@@ -1,111 +0,0 @@
-
-@media (min-width: 991.98px) {
-#quarto-header {
-  border-bottom: 1px solid #dee2e6;
-}
-}
-
-.navbar-brand > img {
-  max-height: 36px;
-}
-
-
-.platform-table td {
-  vertical-align: middle;
-}
-
-.platform-table td > div.sourceCode {
-  margin-top: 0.3rem;
-  margin-bottom: 0.3rem;
-}
-
-
-.document-example {
-  opacity: 0.9;
-  padding: 6px; 
-  font-weight: 500;
-  margin-bottom: 1rem;
-}
-
-.document-example div {
-  padding: 5px;
-}
-
-
-.document-example .citation {
-  color: blue;
-}
-
-.trademark {
-  font-size: 0.6rem;
-  display: inline-block;
-  margin-left: -3px;
-}
-
-.search-attribution {
-  margin-top: 20px;
-  padding-bottom: 20px;
-  height: 40px;
-}
-
-#download-button {
-  margin-top: 1em;
-}
-
-#download-table {
-  margin-bottom: 2em;  
-}
-
-#download-table p {
-  margin-bottom: 0;
-}
-
-#download-table .checksum {
-  color: var(--bs-primary);
-  font-size: .775em;
-  cursor: pointer;
-  padding-top: 4px;
-}
-
-#download-button {
-  display:flex;
-  justify-content: center;
-  padding-bottom: 10px;
-  padding-top: 10px;
-}
-
-#download-button .secondary {
-  font-size: .775em;
-  margin-bottom: 0;
-}
-
-#download-button .container {
-  display: flex;
-  padding-left: 10px;
-  padding-right: 40px;
-}
-
-#download-button .icon-container {
-  fill: white;
-  width: 30px;
-  margin-right: 15px;
-}
-
-.reveal-demo {
-  width: 100%;
-  height: 350px;
-  outline: none;
-}
-
-
-.slide-deck {
-  border: 3px solid #dee2e6;
-  width: 100%;
-  height: 475px;
-}
-
-@media only screen and (max-width: 600px) {
- .slide-deck {
-    height: 400px;
-  }
-}

---FILE: _site/index.html---
@@ -26,17 +26,17 @@
 <script src=""site_libs/quarto-search/quarto-search.js""></script>
 <meta name=""quarto:offset"" content=""./"">
 <link href=""./images/favicon/icon.png"" rel=""icon"" type=""image/png"">
+<script src=""site_libs/quarto-listing/list.min.js""></script>
+<script src=""site_libs/quarto-listing/quarto-listing.js""></script>
 <script src=""site_libs/quarto-html/quarto.js""></script>
 <script src=""site_libs/quarto-html/popper.min.js""></script>
 <script src=""site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""site_libs/quarto-html/anchor.min.js""></script>
 <link href=""site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link href=""site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" class=""quarto-color-scheme"" id=""quarto-text-highlighting-styles"">
-<link href=""site_libs/quarto-html/quarto-syntax-highlighting-dark.css"" rel=""prefetch"" class=""quarto-color-scheme quarto-color-alternate"" id=""quarto-text-highlighting-styles"">
+<link href=""site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
-<link href=""site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"" class=""quarto-color-scheme"">
-<link href=""site_libs/bootstrap/bootstrap-dark.min.css"" rel=""prefetch"" class=""quarto-color-scheme quarto-color-alternate"">
+<link href=""site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
   ""location"": ""navbar"",
   ""copy-button"": false,
@@ -56,9 +56,32 @@
     ""search-submit-button-title"": ""Submit""
   }
 }</script>
+<script>
 
+  window.document.addEventListener(""DOMContentLoaded"", function (_event) {
+    const listingTargetEl = window.document.querySelector('#listing-listing .list');
+    if (!listingTargetEl) {
+      // No listing discovered, do not attach.
+      return; 
+    }
+
+    const options = {
+      valueNames: ['listing-title','listing-author','listing-image','listing-description',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
+      
+      searchColumns: [""listing-title"",""listing-author"",""listing-image"",""listing-description""],
+    };
+
+    window['quarto-listings'] = window['quarto-listings'] || {};
+    window['quarto-listings']['listing-listing'] = new List('listing-listing', options);
+
+    if (window['quarto-listing-loaded']) {
+      window['quarto-listing-loaded']();
+    }
+  });
+  </script>
+
+  <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
 
-<link rel=""stylesheet"" href=""./css/styles.css"">
 </head>
 
 <body class=""nav-sidebar floating nav-fixed"">
@@ -72,7 +95,6 @@
     <span class=""navbar-title"">TensorFlow for R</span>
   </a>
           <div class=""quarto-toggle-container ms-auto"">
-              <a href="""" class=""quarto-color-scheme-toggle nav-link"" onclick=""window.quartoToggleColorScheme(); return false;"" title=""Toggle dark mode""><i class=""bi""></i></a>
               <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
   <div class=""quarto-reader-toggle-btn"">
   <i class=""bi""></i>
@@ -95,16 +117,13 @@ <h1 class=""quarto-secondary-nav-title"">R Interface to Tensorflow</h1>
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
   <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
     <div class=""sidebar-menu-container""> 
     <ul class=""list-unstyled mt-1"">
-        <li class=""sidebar-item sidebar-item-section"">
-    <div class=""sidebar-item-container""> 
-        <a class=""sidebar-item-text sidebar-link text-start"" data-bs-toggle=""collapse"" data-bs-target=""#"" aria-expanded=""true"">Tensorflow Basics</a>
-      <a class=""sidebar-item-toggle text-start"" data-bs-toggle=""collapse"" data-bs-target=""#"" aria-expanded=""true"">
-        <i class=""bi bi-chevron-right ms-2""></i>
-      </a>
-    </div>
-    <ul id="""" class=""collapse list-unstyled sidebar-section depth1 show"">  
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
   <a href=""./tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
@@ -125,8 +144,11 @@ <h1 class=""quarto-secondary-nav-title"">R Interface to Tensorflow</h1>
   <a href=""./tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
   </div>
 </li>
-    </ul>
-  </li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""./tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
     </ul>
     </div>
 </nav>
@@ -156,106 +178,374 @@ <h1 class=""title d-none d-lg-block"">R Interface to Tensorflow</h1>
 
 
 
-</main> <!-- /main -->
+
+<div class=""quarto-listing quarto-listing-container-default"" id=""listing-listing"">
+<div class=""list quarto-listing-default"">
+<div class=""quarto-post image-right"" data-index=""0"" data-listing-file-modified-sort=""1652207752000"" data-listing-reading-time-sort=""13.64"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/customizing_what_happens_in_fit.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/customizing_what_happens_in_fit.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/customizing_what_happens_in_fit.html"">
+<h3 class=""no-anchor listing-title"">
+Customizing what happens in <code>fit()</code>
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+When you’re doing supervised learning, you can use <code>fit()</code> and everything works smoothly.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/customizing_what_happens_in_fit.html"">
+<div class=""listing-author"">
+Francois Chollet, Tomasz Kalinowski
+</div>
+</a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""1"" data-listing-file-modified-sort=""1654201322478"" data-listing-reading-time-sort=""11.765"">
+<div class=""thumbnail"">
+<p><a href=""./tensorflow/guide/intro_to_graphs.html""> <img src=""https://github.com/tensorflow/docs/blob/master/site/en/guide/images/intro_to_graphs/two-layer-network.png?raw=1"" class=""thumnail-image""> </a></p>
+</div>
+<div class=""body"">
+<a href=""./tensorflow/guide/intro_to_graphs.html"">
+<h3 class=""no-anchor listing-title"">
+Intro To_graphs
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+This guide goes beneath the surface of TensorFlow and Keras to demonstrate how TensorFlow works. If you instead want to immediately get started with Keras, check out the collection of Keras guides.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./tensorflow/guide/intro_to_graphs.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""2"" data-listing-file-modified-sort=""1654532864585"" data-listing-reading-time-sort=""16.385"">
+<div class=""thumbnail"">
+<p><a href=""./tensorflow/guide/tensor.html""> <img src=""./tensorflow/guide/images/tensor/scalar.png"" class=""thumnail-image""> </a></p>
+</div>
+<div class=""body"">
+<a href=""./tensorflow/guide/tensor.html"">
+<h3 class=""no-anchor listing-title"">
+Introduction to Tensors
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+Tensors are multi-dimensional arrays with a uniform type (called a <code>dtype</code>). You can see all supported <code>dtypes</code> with <code>names(tf$dtypes)</code>.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./tensorflow/guide/tensor.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""3"" data-listing-file-modified-sort=""1654303611895"" data-listing-reading-time-sort=""4.37"">
+<div class=""thumbnail"">
+<a href=""./tensorflow/guide/variable.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./tensorflow/guide/variable.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./tensorflow/guide/variable.html"">
+<h3 class=""no-anchor listing-title"">
+Introduction to Variables
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+A TensorFlow <strong>variable</strong> is the recommended way to represent shared, persistent state your program manipulates. This guide covers how to create, update, and manage instances of <code>tf$Variable</code> in TensorFlow.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./tensorflow/guide/variable.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""4"" data-listing-file-modified-sort=""1654303611895"" data-listing-reading-time-sort=""10.865"">
+<div class=""thumbnail"">
+<a href=""./tensorflow/guide/autodiff.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./tensorflow/guide/autodiff.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./tensorflow/guide/autodiff.html"">
+<h3 class=""no-anchor listing-title"">
+Introduction to gradients and automatic differentiation
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./tensorflow/guide/autodiff.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""5"" data-listing-file-modified-sort=""1652364383814"" data-listing-reading-time-sort=""6.025"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/python_subclasses.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/python_subclasses.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/python_subclasses.html"">
+<h3 class=""no-anchor listing-title"">
+Python Subclasses
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+When using keras, a desire to create Python-based subclasses can arise in a number of ways. For example, when you want to:
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/python_subclasses.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""6"" data-listing-file-modified-sort=""1654303611895"" data-listing-reading-time-sort=""6.87"">
+<div class=""thumbnail"">
+<a href=""./tensorflow/guide/basics.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./tensorflow/guide/basics.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./tensorflow/guide/basics.html"">
+<h3 class=""no-anchor listing-title"">
+Tensorflow Basics
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+This guide provides a quick overview of <em>TensorFlow basics</em>. Each section of this doc is an overview of a larger topic—you can find links to full guides at the end of each section.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./tensorflow/guide/basics.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""7"" data-listing-file-modified-sort=""1654526382314"" data-listing-reading-time-sort=""21.235"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/functional_api.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/functional_api.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/functional_api.html"">
+<h3 class=""no-anchor listing-title"">
+The Functional API
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+<p>Complete guide to the functional API.</p>
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/functional_api.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""8"" data-listing-file-modified-sort=""1654528115465"" data-listing-reading-time-sort=""6.875"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/sequential_model.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/sequential_model.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/sequential_model.html"">
+<h3 class=""no-anchor listing-title"">
+The Sequential model
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+<p>Complete guide to the Sequential model.</p>
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/sequential_model.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""9"" data-listing-file-modified-sort=""1654532435811"" data-listing-reading-time-sort=""15.185"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/transfer_learning.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/transfer_learning.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/transfer_learning.html"">
+<h3 class=""no-anchor listing-title"">
+Transfer learning and fine-tuning
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+<strong>Transfer learning</strong> consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify skunks.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/transfer_learning.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""10"" data-listing-file-modified-sort=""1652364153678"" data-listing-reading-time-sort=""14.375"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/working_with_rnns.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/working_with_rnns.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/working_with_rnns.html"">
+<h3 class=""no-anchor listing-title"">
+Working with RNNs
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/working_with_rnns.html"">
+<div class=""listing-author"">
+Scott Zhu, Francois Chollet, Tomasz Kalinowski
+</div>
+</a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""11"" data-listing-file-modified-sort=""1652294900240"" data-listing-reading-time-sort=""13.535"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/preprocessing_layers.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/preprocessing_layers.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/preprocessing_layers.html"">
+<h3 class=""no-anchor listing-title"">
+Working with preprocessing layers
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+<p>Overview of how to leverage preprocessing layers to create end-to-end models.</p>
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/preprocessing_layers.html"">
+<div class=""listing-author"">
+Francois Chollet, Mark Omernick, Tomasz Kalinowski
+</div>
+</a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""12"" data-listing-file-modified-sort=""1652366065193"" data-listing-reading-time-sort=""22.5"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/making_new_layers_and_models_via_subclassing.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/making_new_layers_and_models_via_subclassing.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/making_new_layers_and_models_via_subclassing.html"">
+<h3 class=""no-anchor listing-title"">
+Writing <code>Layer</code> and <code>Model</code> objects from scratch.
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+One of the central abstractions in Keras is the <code>Layer</code> class. A layer encapsulates both a state (the layer’s “weights”) and a transformation from inputs to outputs (a “call”, the layer’s forward pass).
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/making_new_layers_and_models_via_subclassing.html""> </a>
+</div>
+</div>
+<div class=""quarto-post image-right"" data-index=""13"" data-listing-file-modified-sort=""1652364156758"" data-listing-reading-time-sort=""10.28"">
+<div class=""thumbnail"">
+<a href=""./keras/guides/writing_your_own_callbacks.html"">
+<div class=""thumbnail-image"">
+
+</div>
+</a><p><a href=""./keras/guides/writing_your_own_callbacks.html""></a></p>
+</div>
+<div class=""body"">
+<a href=""./keras/guides/writing_your_own_callbacks.html"">
+<h3 class=""no-anchor listing-title"">
+Writing your own callbacks
+</h3>
+<div class=""listing-subtitle"">
+
+</div>
+<div class=""listing-description"">
+A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Examples include <code>callback_tensorboard()</code> to visualize training progress and results with TensorBoard, or <code>callback_model_checkpoint()</code> to periodically save your model during training.
+</div>
+</a>
+</div>
+<div class=""metadata"">
+<a href=""./keras/guides/writing_your_own_callbacks.html"">
+<div class=""listing-author"">
+Rick Chao, Francois Chollet, Tomasz Kalinowski
+</div>
+</a>
+</div>
+</div>
+</div>
+<div class=""listing-no-matching d-none"">
+No matching items
+</div>
+</div></main> <!-- /main -->
 <script id=""quarto-html-after-body"" type=""application/javascript"">
 window.document.addEventListener(""DOMContentLoaded"", function (event) {
-  const disableStylesheet = (stylesheets) => {
-    for (let i=0; i < stylesheets.length; i++) {
-      const stylesheet = stylesheets[i];
-      stylesheet.rel = 'prefetch';
-    }
-  }
-  const enableStylesheet = (stylesheets) => {
-    for (let i=0; i < stylesheets.length; i++) {
-      const stylesheet = stylesheets[i];
-      stylesheet.rel = 'stylesheet';
-    }
-  }
-  const manageTransitions = (selector, allowTransitions) => {
-    const els = window.document.querySelectorAll(selector);
-    for (let i=0; i < els.length; i++) {
-      const el = els[i];
-      if (allowTransitions) {
-        el.classList.remove('notransition');
-      } else {
-        el.classList.add('notransition');
-      }
-    }
-  }
-  const toggleColorMode = (alternate) => {
-    // Switch the stylesheets
-    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
-    manageTransitions('#quarto-margin-sidebar .nav-link', false);
-    if (alternate) {
-      enableStylesheet(alternateStylesheets);
-    } else {
-      disableStylesheet(alternateStylesheets);
-    }
-    manageTransitions('#quarto-margin-sidebar .nav-link', true);
-    // Switch the toggles
-    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
-    for (let i=0; i < toggles.length; i++) {
-      const toggle = toggles[i];
-      if (toggle) {
-        if (alternate) {
-          toggle.classList.add(""alternate"");     
-        } else {
-          toggle.classList.remove(""alternate"");
-        }
-      }
-    }
-  }
-  const isFileUrl = () => { 
-    return window.location.protocol === 'file:';
-  }
-  const hasAlternateSentinel = () => {  
-    let styleSentinel = getColorSchemeSentinel();
-    if (styleSentinel !== null) {
-      return styleSentinel === ""alternate"";
-    } else {
-      return false;
-    }
-  }
-  const setStyleSentinel = (alternate) => {
-    const value = alternate ? ""alternate"" : ""default"";
-    if (!isFileUrl()) {
-      window.localStorage.setItem(""quarto-color-scheme"", value);
-    } else {
-      localAlternateSentinel = value;
-    }
-  }
-  const getColorSchemeSentinel = () => {
-    if (!isFileUrl()) {
-      const storageValue = window.localStorage.getItem(""quarto-color-scheme"");
-      return storageValue != null ? storageValue : localAlternateSentinel;
-    } else {
-      return localAlternateSentinel;
-    }
-  }
-  let localAlternateSentinel = 'default';
-  // Dark / light mode switch
-  window.quartoToggleColorScheme = () => {
-    // Read the current dark / light value 
-    let toAlternate = !hasAlternateSentinel();
-    toggleColorMode(toAlternate);
-    setStyleSentinel(toAlternate);
-  };
-  // Ensure there is a toggle, if there isn't float one in the top right
-  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
-    const a = window.document.createElement('a');
-    a.classList.add('top-right');
-    a.classList.add('quarto-color-scheme-toggle');
-    a.href = """";
-    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
-    const i = window.document.createElement(""i"");
-    i.classList.add('bi');
-    a.appendChild(i);
-    window.document.body.appendChild(a);
-  }
-  // Switch to dark mode if need be
-  if (hasAlternateSentinel()) {
-    toggleColorMode(true);
-  } 
   const icon = """";
   const anchorJS = new window.AnchorJS();
   anchorJS.options = {

---FILE: _site/keras/guides/customizing_what_happens_in_fit.html---
@@ -9,7 +9,7 @@
 <meta name=""author"" content=""Francois Chollet"">
 <meta name=""author"" content=""Tomasz Kalinowski"">
 
-<title>tf-site-title - Customizing what happens in fit()</title>
+<title>TensorFlow for R - Customizing what happens in fit()</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -82,27 +82,30 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <script src=""../../site_libs/quarto-nav/quarto-nav.js""></script>
+<script src=""../../site_libs/quarto-nav/headroom.min.js""></script>
 <script src=""../../site_libs/clipboard/clipboard.min.js""></script>
-<meta name=""quarto:offset"" content=""../../"">
 <script src=""../../site_libs/quarto-search/autocomplete.umd.js""></script>
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
+<meta name=""quarto:offset"" content=""../../"">
+<link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
 <script src=""../../site_libs/quarto-html/popper.min.js""></script>
 <script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""../../site_libs/quarto-html/anchor.min.js""></script>
 <link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link id=""quarto-text-highlighting-styles"" href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
 <link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
-  ""location"": ""sidebar"",
+  ""location"": ""navbar"",
   ""copy-button"": false,
   ""collapse-after"": 3,
-  ""panel-placement"": ""start"",
-  ""type"": ""textbox"",
+  ""panel-placement"": ""end"",
+  ""type"": ""overlay"",
   ""limit"": 20,
   ""language"": {
     ""search-no-results-text"": ""No results"",
@@ -120,12 +123,74 @@
 
 </head>
 
-<body>
+<body class=""nav-sidebar floating nav-fixed"">
 
 <div id=""quarto-search-results""></div>
+  <header id=""quarto-header"" class=""headroom fixed-top"">
+    <nav class=""navbar navbar-expand-lg navbar-dark "">
+      <div class=""navbar-container container-fluid"">
+      <a class=""navbar-brand"" href=""../../index.html"">
+    <img src=""../../images/favicon/icon.png"" alt="""">
+    <span class=""navbar-title"">TensorFlow for R</span>
+  </a>
+          <div class=""quarto-toggle-container ms-auto"">
+              <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
+  <div class=""quarto-reader-toggle-btn"">
+  <i class=""bi""></i>
+  </div>
+</a>
+          </div>
+          <div id=""quarto-search"" class="""" title=""Search""></div>
+      </div> <!-- /container-fluid -->
+    </nav>
+  <nav class=""quarto-secondary-nav"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+    <div class=""container-fluid d-flex justify-content-between"">
+      <h1 class=""quarto-secondary-nav-title"">Customizing what happens in <code>fit()</code></h1>
+      <button type=""button"" class=""quarto-btn-toggle btn"" aria-label=""Show secondary navigation"">
+        <i class=""bi bi-chevron-right""></i>
+      </button>
+    </div>
+  </nav>
+</header>
 <!-- content -->
-<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
+<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
+  <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
+    <div class=""sidebar-menu-container""> 
+    <ul class=""list-unstyled mt-1"">
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/tensor.html"" class=""sidebar-item-text sidebar-link"">Tensors</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/variable.html"" class=""sidebar-item-text sidebar-link"">Variables</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
+    </ul>
+    </div>
+</nav>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"">
@@ -147,13 +212,11 @@ <h2 id=""toc-title"">On this page</h2>
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
-<h1 class=""title"">Customizing what happens in <code>fit()</code></h1>
+<h1 class=""title d-none d-lg-block"">Customizing what happens in <code>fit()</code></h1>
 </div>
 
 
 
-
-
 <div class=""quarto-title-meta"">
 
     <div>
@@ -234,8 +297,8 @@ <h2 class=""anchored"" data-anchor-id=""a-first-simple-example"">A first simple exam
 <span id=""cb2-28""><a href=""#cb2-28"" aria-hidden=""true"" tabindex=""-1""></a>    results</span>
 <span id=""cb2-29""><a href=""#cb2-29"" aria-hidden=""true"" tabindex=""-1""></a>  }</span>
 <span id=""cb2-30""><a href=""#cb2-30"" aria-hidden=""true"" tabindex=""-1""></a>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stderr"">
-<pre><code>Loaded Tensorflow version 2.8.0</code></pre>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Loaded Tensorflow version 2.9.1</code></pre>
 </div>
 </div>
 <p>Let’s try this out:</p>
@@ -417,9 +480,9 @@ <h2 class=""anchored"" data-anchor-id=""providing-your-own-evaluation-step"">Providi
 <span id=""cb7-28""><a href=""#cb7-28"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> <span class=""fu"">k_random_uniform</span>(<span class=""fu"">c</span>(<span class=""dv"">1000</span>, <span class=""dv"">32</span>))</span>
 <span id=""cb7-29""><a href=""#cb7-29"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">k_random_uniform</span>(<span class=""fu"">c</span>(<span class=""dv"">1000</span>, <span class=""dv"">1</span>))</span>
 <span id=""cb7-30""><a href=""#cb7-30"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">evaluate</span>(x, y)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>     loss       mae 
-0.2720022 0.4182713 </code></pre>
+0.2331362 0.3878761 </code></pre>
 </div>
 </div>
 </section>

---FILE: _site/keras/guides/preprocessing_layers.html---
@@ -2,14 +2,14 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-0.9.377"">
+<meta name=""generator"" content=""quarto-99.9.9"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 <meta name=""author"" content=""Francois Chollet, Mark Omernick, Tomasz Kalinowski"">
 <meta name=""description"" content=""Overview of how to leverage preprocessing layers to create end-to-end models."">
 
-<title>tf-site-title - Working with preprocessing layers</title>
+<title>TensorFlow for R - Working with preprocessing layers</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -82,27 +82,30 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <script src=""../../site_libs/quarto-nav/quarto-nav.js""></script>
+<script src=""../../site_libs/quarto-nav/headroom.min.js""></script>
 <script src=""../../site_libs/clipboard/clipboard.min.js""></script>
-<meta name=""quarto:offset"" content=""../../"">
 <script src=""../../site_libs/quarto-search/autocomplete.umd.js""></script>
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
+<meta name=""quarto:offset"" content=""../../"">
+<link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
 <script src=""../../site_libs/quarto-html/popper.min.js""></script>
 <script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""../../site_libs/quarto-html/anchor.min.js""></script>
 <link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link id=""quarto-text-highlighting-styles"" href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
 <link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
-  ""location"": ""sidebar"",
+  ""location"": ""navbar"",
   ""copy-button"": false,
   ""collapse-after"": 3,
-  ""panel-placement"": ""start"",
-  ""type"": ""textbox"",
+  ""panel-placement"": ""end"",
+  ""type"": ""overlay"",
   ""limit"": 20,
   ""language"": {
     ""search-no-results-text"": ""No results"",
@@ -120,12 +123,74 @@
 
 </head>
 
-<body>
+<body class=""nav-sidebar floating nav-fixed"">
 
 <div id=""quarto-search-results""></div>
+  <header id=""quarto-header"" class=""headroom fixed-top"">
+    <nav class=""navbar navbar-expand-lg navbar-dark "">
+      <div class=""navbar-container container-fluid"">
+      <a class=""navbar-brand"" href=""../../index.html"">
+    <img src=""../../images/favicon/icon.png"" alt="""">
+    <span class=""navbar-title"">TensorFlow for R</span>
+  </a>
+          <div class=""quarto-toggle-container ms-auto"">
+              <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
+  <div class=""quarto-reader-toggle-btn"">
+  <i class=""bi""></i>
+  </div>
+</a>
+          </div>
+          <div id=""quarto-search"" class="""" title=""Search""></div>
+      </div> <!-- /container-fluid -->
+    </nav>
+  <nav class=""quarto-secondary-nav"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+    <div class=""container-fluid d-flex justify-content-between"">
+      <h1 class=""quarto-secondary-nav-title"">Working with preprocessing layers</h1>
+      <button type=""button"" class=""quarto-btn-toggle btn"" aria-label=""Show secondary navigation"">
+        <i class=""bi bi-chevron-right""></i>
+      </button>
+    </div>
+  </nav>
+</header>
 <!-- content -->
-<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
+<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
+  <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
+    <div class=""sidebar-menu-container""> 
+    <ul class=""list-unstyled mt-1"">
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/tensor.html"" class=""sidebar-item-text sidebar-link"">Tensors</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/variable.html"" class=""sidebar-item-text sidebar-link"">Variables</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
+    </ul>
+    </div>
+</nav>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"">
@@ -168,7 +233,7 @@ <h2 id=""toc-title"">On this page</h2>
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
-<h1 class=""title"">Working with preprocessing layers</h1>
+<h1 class=""title d-none d-lg-block"">Working with preprocessing layers</h1>
 </div>
 
 <div>
@@ -178,8 +243,6 @@ <h1 class=""title"">Working with preprocessing layers</h1>
 </div>
 
 
-
-
 <div class=""quarto-title-meta"">
 
     <div>
@@ -269,18 +332,18 @@ <h2 class=""anchored"" data-anchor-id=""the-adapt-function"">The <code>adapt()</code
 <span id=""cb2-2""><a href=""#cb2-2"" aria-hidden=""true"" tabindex=""-1""></a>              <span class=""fu"">c</span>(<span class=""fl"">0.8</span>, <span class=""fl"">0.9</span>, <span class=""fl"">1.0</span>),</span>
 <span id=""cb2-3""><a href=""#cb2-3"" aria-hidden=""true"" tabindex=""-1""></a>              <span class=""fu"">c</span>(<span class=""fl"">1.5</span>, <span class=""fl"">1.6</span>, <span class=""fl"">1.7</span>))</span>
 <span id=""cb2-4""><a href=""#cb2-4"" aria-hidden=""true"" tabindex=""-1""></a>layer <span class=""ot"">&lt;-</span> <span class=""fu"">layer_normalization</span>()</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stderr"">
-<pre><code>Loaded Tensorflow version 2.8.0</code></pre>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Loaded Tensorflow version 2.9.1</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb4""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb4-1""><a href=""#cb4-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">adapt</span>(layer, data)</span>
 <span id=""cb4-2""><a href=""#cb4-2"" aria-hidden=""true"" tabindex=""-1""></a>normalized_data <span class=""ot"">&lt;-</span> <span class=""fu"">as.array</span>(<span class=""fu"">layer</span>(data))</span>
 <span id=""cb4-3""><a href=""#cb4-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb4-4""><a href=""#cb4-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sprintf</span>(<span class=""st"">""Features mean: %.2f""</span>, <span class=""fu"">mean</span>(normalized_data))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>[1] ""Features mean: -0.00""</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb6""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb6-1""><a href=""#cb6-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sprintf</span>(<span class=""st"">""Features std: %.2f""</span>, <span class=""fu"">sd</span>(normalized_data))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>[1] ""Features std: 1.06""</code></pre>
 </div>
 </div>
@@ -303,7 +366,7 @@ <h2 class=""anchored"" data-anchor-id=""the-adapt-function"">The <code>adapt()</code
 <span id=""cb8-15""><a href=""#cb8-15"" aria-hidden=""true"" tabindex=""-1""></a>layer <span class=""sc"">%&gt;%</span> <span class=""fu"">adapt</span>(data)</span>
 <span id=""cb8-16""><a href=""#cb8-16"" aria-hidden=""true"" tabindex=""-1""></a>vectorized_text <span class=""ot"">&lt;-</span> <span class=""fu"">layer</span>(data)</span>
 <span id=""cb8-17""><a href=""#cb8-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(vectorized_text)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>tf.Tensor(
 [[31  0  0  0  0  0  0  0  0  0]
  [15 23  3 30  0  0  0  0  0  0]
@@ -326,7 +389,7 @@ <h2 class=""anchored"" data-anchor-id=""the-adapt-function"">The <code>adapt()</code
 <span id=""cb10-4""><a href=""#cb10-4"" aria-hidden=""true"" tabindex=""-1""></a>layer <span class=""ot"">&lt;-</span> <span class=""fu"">layer_string_lookup</span>(<span class=""at"">vocabulary=</span>vocab)</span>
 <span id=""cb10-5""><a href=""#cb10-5"" aria-hidden=""true"" tabindex=""-1""></a>vectorized_data <span class=""ot"">&lt;-</span> <span class=""fu"">layer</span>(data)</span>
 <span id=""cb10-6""><a href=""#cb10-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(vectorized_data)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>tf.Tensor(
 [[1 3 4]
  [4 0 2]], shape=(2, 3), dtype=int64)</code></pre>
@@ -478,7 +541,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-string-categorical-features-via-on
 <span id=""cb19-11""><a href=""#cb19-11"" aria-hidden=""true"" tabindex=""-1""></a>test_data <span class=""ot"">=</span> <span class=""fu"">as_tensor</span>(<span class=""fu"">matrix</span>(<span class=""fu"">c</span>(<span class=""st"">""a""</span>, <span class=""st"">""b""</span>, <span class=""st"">""c""</span>, <span class=""st"">""d""</span>, <span class=""st"">""e""</span>, <span class=""st"">""""</span>)))</span>
 <span id=""cb19-12""><a href=""#cb19-12"" aria-hidden=""true"" tabindex=""-1""></a>encoded_data <span class=""ot"">=</span> <span class=""fu"">lookup</span>(test_data)</span>
 <span id=""cb19-13""><a href=""#cb19-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(encoded_data)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>tf.Tensor(
 [[0. 0. 0. 1.]
  [0. 0. 1. 0.]
@@ -506,7 +569,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-integer-categorical-features-via-o
 <span id=""cb21-10""><a href=""#cb21-10"" aria-hidden=""true"" tabindex=""-1""></a>test_data <span class=""ot"">&lt;-</span> <span class=""fu"">as_tensor</span>(<span class=""fu"">matrix</span>(<span class=""fu"">c</span>(<span class=""dv"">10</span>, <span class=""dv"">10</span>, <span class=""dv"">20</span>, <span class=""dv"">50</span>, <span class=""dv"">60</span>, <span class=""dv"">0</span>)), <span class=""st"">""int32""</span>)</span>
 <span id=""cb21-11""><a href=""#cb21-11"" aria-hidden=""true"" tabindex=""-1""></a>encoded_data <span class=""ot"">&lt;-</span> <span class=""fu"">lookup</span>(test_data)</span>
 <span id=""cb21-12""><a href=""#cb21-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(encoded_data)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>tf.Tensor(
 [[0. 0. 1. 0. 0.]
  [0. 0. 1. 0. 0.]
@@ -533,7 +596,7 @@ <h3 class=""anchored"" data-anchor-id=""applying-the-hashing-trick-to-an-integer-ca
 <span id=""cb23-8""><a href=""#cb23-8"" aria-hidden=""true"" tabindex=""-1""></a>encoder <span class=""ot"">&lt;-</span> <span class=""fu"">layer_category_encoding</span>(<span class=""at"">num_tokens=</span><span class=""dv"">64</span>, <span class=""at"">output_mode=</span><span class=""st"">""multi_hot""</span>)</span>
 <span id=""cb23-9""><a href=""#cb23-9"" aria-hidden=""true"" tabindex=""-1""></a>encoded_data <span class=""ot"">&lt;-</span> <span class=""fu"">encoder</span>(<span class=""fu"">hasher</span>(data))</span>
 <span id=""cb23-10""><a href=""#cb23-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(encoded_data<span class=""sc"">$</span>shape)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>TensorShape([10000, 64])</code></pre>
 </div>
 </div>
@@ -562,7 +625,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-sequence-of-token-indice
 <span id=""cb25-18""><a href=""#cb25-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Try out the layer</span></span>
 <span id=""cb25-19""><a href=""#cb25-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Encoded text:</span><span class=""sc"">\n</span><span class=""st"">""</span>,</span>
 <span id=""cb25-20""><a href=""#cb25-20"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">as.array</span>(<span class=""fu"">text_vectorizer</span>(<span class=""st"">""The Brain is deeper than the sea""</span>)))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Encoded text:
  2 19 14 1 9 2 1</code></pre>
 </div>
@@ -590,7 +653,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-sequence-of-token-indice
 <span id=""cb27-22""><a href=""#cb27-22"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb27-23""><a href=""#cb27-23"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Train the model on the int sequences</span></span>
 <span id=""cb27-24""><a href=""#cb27-24"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Training model...</span><span class=""sc"">\n</span><span class=""st"">""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Training model...</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb29""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb29-1""><a href=""#cb29-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span></span>
@@ -607,14 +670,14 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-sequence-of-token-indice
 <span id=""cb29-12""><a href=""#cb29-12"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb29-13""><a href=""#cb29-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call the end-to-end model on test data (which includes unknown tokens)</span></span>
 <span id=""cb29-14""><a href=""#cb29-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Calling end-to-end model on test string...</span><span class=""sc"">\n</span><span class=""st"">""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Calling end-to-end model on test string...</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb31""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb31-1""><a href=""#cb31-1"" aria-hidden=""true"" tabindex=""-1""></a>test_data <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">constant</span>(<span class=""fu"">matrix</span>(<span class=""st"">""The one the other will absorb""</span>))</span>
 <span id=""cb31-2""><a href=""#cb31-2"" aria-hidden=""true"" tabindex=""-1""></a>test_output <span class=""ot"">&lt;-</span> <span class=""fu"">end_to_end_model</span>(test_data)</span>
 <span id=""cb31-3""><a href=""#cb31-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Model output:""</span>, <span class=""fu"">as.array</span>(test_output), <span class=""st"">""</span><span class=""sc"">\n</span><span class=""st"">""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
-<pre><code>Model output: 0.1437887 </code></pre>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model output: 0.1344098 </code></pre>
 </div>
 </div>
 <p>You can see the <code>layer_text_vectorization()</code> layer in action, combined with an <code>Embedding</code> mode, in the example <a href=""https://keras.io/examples/nlp/text_classification_from_scratch/"">text classification from scratch</a>.</p>
@@ -641,7 +704,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-dense-matrix-of-ngrams-w
 <span id=""cb33-15""><a href=""#cb33-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Try out the layer</span></span>
 <span id=""cb33-16""><a href=""#cb33-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Encoded text:</span><span class=""sc"">\n</span><span class=""st"">""</span>, </span>
 <span id=""cb33-17""><a href=""#cb33-17"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">as.array</span>(<span class=""fu"">text_vectorizer</span>(<span class=""st"">""The Brain is deeper than the sea""</span>)))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Encoded text:
  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0</code></pre>
 </div>
@@ -667,7 +730,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-dense-matrix-of-ngrams-w
 <span id=""cb35-20""><a href=""#cb35-20"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb35-21""><a href=""#cb35-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Train the model on the int sequences</span></span>
 <span id=""cb35-22""><a href=""#cb35-22"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Training model...</span><span class=""sc"">\n</span><span class=""st"">""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Training model...</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb37""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb37-1""><a href=""#cb37-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span></span>
@@ -685,17 +748,17 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-dense-matrix-of-ngrams-w
 <span id=""cb37-13""><a href=""#cb37-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb37-14""><a href=""#cb37-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call the end-to-end model on test data (which includes unknown tokens)</span></span>
 <span id=""cb37-15""><a href=""#cb37-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Calling end-to-end model on test string...</span><span class=""sc"">\n</span><span class=""st"">""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Calling end-to-end model on test string...</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb39""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb39-1""><a href=""#cb39-1"" aria-hidden=""true"" tabindex=""-1""></a>test_data <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">constant</span>(<span class=""fu"">matrix</span>(<span class=""st"">""The one the other will absorb""</span>))</span>
 <span id=""cb39-2""><a href=""#cb39-2"" aria-hidden=""true"" tabindex=""-1""></a>test_output <span class=""ot"">&lt;-</span> <span class=""fu"">end_to_end_model</span>(test_data)</span>
 <span id=""cb39-3""><a href=""#cb39-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Model output: ""</span>); <span class=""fu"">print</span>(test_output); <span class=""fu"">cat</span>(<span class=""st"">""</span><span class=""sc"">\n</span><span class=""st"">""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model output: </code></pre>
 </div>
-<div class=""cell-output-stdout"">
-<pre><code>tf.Tensor([[0.31572872]], shape=(1, 1), dtype=float32)</code></pre>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>tf.Tensor([[0.38880283]], shape=(1, 1), dtype=float32)</code></pre>
 </div>
 </div>
 </section>
@@ -725,7 +788,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-dense-matrix-of-ngrams-w
 <span id=""cb42-20""><a href=""#cb42-20"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Try out the layer</span></span>
 <span id=""cb42-21""><a href=""#cb42-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Encoded text:</span><span class=""sc"">\n</span><span class=""st"">""</span>, </span>
 <span id=""cb42-22""><a href=""#cb42-22"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""fu"">as.array</span>(<span class=""fu"">text_vectorizer</span>(<span class=""st"">""The Brain is deeper than the sea""</span>)))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Encoded text:
  5.461647 1.694596 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.098612 1.098612 1.098612 0 0 0 0 0 0 0 0 0 1.098612 0 0 0 0 0 0 0 1.098612 1.098612 0 0 0</code></pre>
 </div>
@@ -748,7 +811,7 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-dense-matrix-of-ngrams-w
 <span id=""cb44-17""><a href=""#cb44-17"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb44-18""><a href=""#cb44-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Train the model on the int sequences</span></span>
 <span id=""cb44-19""><a href=""#cb44-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Training model...""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Training model...</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb46""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb46-1""><a href=""#cb46-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span></span>
@@ -766,17 +829,17 @@ <h3 class=""anchored"" data-anchor-id=""encoding-text-as-a-dense-matrix-of-ngrams-w
 <span id=""cb46-13""><a href=""#cb46-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb46-14""><a href=""#cb46-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call the end-to-end model on test data (which includes unknown tokens)</span></span>
 <span id=""cb46-15""><a href=""#cb46-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Calling end-to-end model on test string...</span><span class=""sc"">\n</span><span class=""st"">""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Calling end-to-end model on test string...</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb48""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb48-1""><a href=""#cb48-1"" aria-hidden=""true"" tabindex=""-1""></a>test_data <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">constant</span>(<span class=""fu"">matrix</span>(<span class=""st"">""The one the other will absorb""</span>))</span>
 <span id=""cb48-2""><a href=""#cb48-2"" aria-hidden=""true"" tabindex=""-1""></a>test_output <span class=""ot"">&lt;-</span> <span class=""fu"">end_to_end_model</span>(test_data)</span>
 <span id=""cb48-3""><a href=""#cb48-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Model output: ""</span>); <span class=""fu"">print</span>(test_output)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model output: </code></pre>
 </div>
-<div class=""cell-output-stdout"">
-<pre><code>tf.Tensor([[0.09238248]], shape=(1, 1), dtype=float32)</code></pre>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>tf.Tensor([[-0.8505264]], shape=(1, 1), dtype=float32)</code></pre>
 </div>
 </div>
 </section>

---FILE: _site/keras/guides/python_subclasses.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-0.9.377"">
+<meta name=""generator"" content=""quarto-99.9.9"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>tf-site-title - Python Subclasses</title>
+<title>TensorFlow for R - Python Subclasses</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -80,27 +80,30 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <script src=""../../site_libs/quarto-nav/quarto-nav.js""></script>
+<script src=""../../site_libs/quarto-nav/headroom.min.js""></script>
 <script src=""../../site_libs/clipboard/clipboard.min.js""></script>
-<meta name=""quarto:offset"" content=""../../"">
 <script src=""../../site_libs/quarto-search/autocomplete.umd.js""></script>
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
+<meta name=""quarto:offset"" content=""../../"">
+<link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
 <script src=""../../site_libs/quarto-html/popper.min.js""></script>
 <script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""../../site_libs/quarto-html/anchor.min.js""></script>
 <link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link id=""quarto-text-highlighting-styles"" href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
 <link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
-  ""location"": ""sidebar"",
+  ""location"": ""navbar"",
   ""copy-button"": false,
   ""collapse-after"": 3,
-  ""panel-placement"": ""start"",
-  ""type"": ""textbox"",
+  ""panel-placement"": ""end"",
+  ""type"": ""overlay"",
   ""limit"": 20,
   ""language"": {
     ""search-no-results-text"": ""No results"",
@@ -118,12 +121,74 @@
 
 </head>
 
-<body>
+<body class=""nav-sidebar floating nav-fixed"">
 
 <div id=""quarto-search-results""></div>
+  <header id=""quarto-header"" class=""headroom fixed-top"">
+    <nav class=""navbar navbar-expand-lg navbar-dark "">
+      <div class=""navbar-container container-fluid"">
+      <a class=""navbar-brand"" href=""../../index.html"">
+    <img src=""../../images/favicon/icon.png"" alt="""">
+    <span class=""navbar-title"">TensorFlow for R</span>
+  </a>
+          <div class=""quarto-toggle-container ms-auto"">
+              <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
+  <div class=""quarto-reader-toggle-btn"">
+  <i class=""bi""></i>
+  </div>
+</a>
+          </div>
+          <div id=""quarto-search"" class="""" title=""Search""></div>
+      </div> <!-- /container-fluid -->
+    </nav>
+  <nav class=""quarto-secondary-nav"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+    <div class=""container-fluid d-flex justify-content-between"">
+      <h1 class=""quarto-secondary-nav-title"">Python Subclasses</h1>
+      <button type=""button"" class=""quarto-btn-toggle btn"" aria-label=""Show secondary navigation"">
+        <i class=""bi bi-chevron-right""></i>
+      </button>
+    </div>
+  </nav>
+</header>
 <!-- content -->
-<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
+<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
+  <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
+    <div class=""sidebar-menu-container""> 
+    <ul class=""list-unstyled mt-1"">
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/tensor.html"" class=""sidebar-item-text sidebar-link"">Tensors</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/variable.html"" class=""sidebar-item-text sidebar-link"">Variables</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
+    </ul>
+    </div>
+</nav>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"">
@@ -145,13 +210,11 @@ <h2 id=""toc-title"">On this page</h2>
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
-<h1 class=""title"">Python Subclasses</h1>
+<h1 class=""title d-none d-lg-block"">Python Subclasses</h1>
 </div>
 
 
 
-
-
 <div class=""quarto-title-meta"">
 
     
@@ -186,8 +249,8 @@ <h3 class=""anchored"" data-anchor-id=""a-custom-constraint-r6"">A custom constraint
 <span id=""cb1-7""><a href=""#cb1-7"" aria-hidden=""true"" tabindex=""-1""></a>  )</span>
 <span id=""cb1-8""><a href=""#cb1-8"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
 <span id=""cb1-9""><a href=""#cb1-9"" aria-hidden=""true"" tabindex=""-1""></a>NonNegative <span class=""ot"">&lt;-</span> <span class=""fu"">r_to_py</span>(NonNegative, <span class=""at"">convert=</span><span class=""cn"">TRUE</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stderr"">
-<pre><code>Loaded Tensorflow version 2.8.0</code></pre>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Loaded Tensorflow version 2.9.1</code></pre>
 </div>
 </div>
 <p>The <code>r_to_py</code> method will convert an R6 class generator into a Python class generator. After conversion, Python class generators will be different from R6 class generators in a few ways:</p>

---FILE: _site/keras/guides/sequential_model.html---
@@ -95,12 +95,10 @@
 <script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""../../site_libs/quarto-html/anchor.min.js""></script>
 <link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" class=""quarto-color-scheme"" id=""quarto-text-highlighting-styles"">
-<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css"" rel=""prefetch"" class=""quarto-color-scheme quarto-color-alternate"" id=""quarto-text-highlighting-styles"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
-<link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"" class=""quarto-color-scheme"">
-<link href=""../../site_libs/bootstrap/bootstrap-dark.min.css"" rel=""prefetch"" class=""quarto-color-scheme quarto-color-alternate"">
+<link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
   ""location"": ""navbar"",
   ""copy-button"": false,
@@ -122,7 +120,6 @@
 }</script>
 
 
-<link rel=""stylesheet"" href=""../../css/styles.css"">
 </head>
 
 <body class=""nav-sidebar floating nav-fixed"">
@@ -136,7 +133,6 @@
     <span class=""navbar-title"">TensorFlow for R</span>
   </a>
           <div class=""quarto-toggle-container ms-auto"">
-              <a href="""" class=""quarto-color-scheme-toggle nav-link"" onclick=""window.quartoToggleColorScheme(); return false;"" title=""Toggle dark mode""><i class=""bi""></i></a>
               <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
   <div class=""quarto-reader-toggle-btn"">
   <i class=""bi""></i>
@@ -159,16 +155,13 @@ <h1 class=""quarto-secondary-nav-title"">The Sequential model</h1>
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
   <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
     <div class=""sidebar-menu-container""> 
     <ul class=""list-unstyled mt-1"">
-        <li class=""sidebar-item sidebar-item-section"">
-    <div class=""sidebar-item-container""> 
-        <a class=""sidebar-item-text sidebar-link text-start"" data-bs-toggle=""collapse"" data-bs-target=""#"" aria-expanded=""true"">Tensorflow Basics</a>
-      <a class=""sidebar-item-toggle text-start"" data-bs-toggle=""collapse"" data-bs-target=""#"" aria-expanded=""true"">
-        <i class=""bi bi-chevron-right ms-2""></i>
-      </a>
-    </div>
-    <ul id="""" class=""collapse list-unstyled sidebar-section depth1 show"">  
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
   <a href=""../../tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
@@ -189,8 +182,11 @@ <h1 class=""quarto-secondary-nav-title"">The Sequential model</h1>
   <a href=""../../tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
   </div>
 </li>
-    </ul>
-  </li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
     </ul>
     </div>
 </nav>
@@ -252,22 +248,24 @@ <h2 class=""anchored"" data-anchor-id=""when-to-use-a-sequential-model"">When to use
 <span id=""cb2-2""><a href=""#cb2-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span> </span>
 <span id=""cb2-3""><a href=""#cb2-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer1""</span>) <span class=""sc"">%&gt;%</span> </span>
 <span id=""cb2-4""><a href=""#cb2-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer2""</span>) <span class=""sc"">%&gt;%</span> </span>
-<span id=""cb2-5""><a href=""#cb2-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>, <span class=""at"">name =</span> <span class=""st"">""layer3""</span>)</span>
-<span id=""cb2-6""><a href=""#cb2-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb2-7""><a href=""#cb2-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call model on a test input</span></span>
-<span id=""cb2-8""><a href=""#cb2-8"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">3</span>, <span class=""dv"">3</span>))</span>
-<span id=""cb2-9""><a href=""#cb2-9"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">model</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<span id=""cb2-5""><a href=""#cb2-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>, <span class=""at"">name =</span> <span class=""st"">""layer3""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Loaded Tensorflow version 2.9.1</code></pre>
+</div>
+<div class=""sourceCode cell-code"" id=""cb4""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb4-1""><a href=""#cb4-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call model on a test input</span></span>
+<span id=""cb4-2""><a href=""#cb4-2"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">3</span>, <span class=""dv"">3</span>))</span>
+<span id=""cb4-3""><a href=""#cb4-3"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">model</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>is equivalent to this function:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb3""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb3-1""><a href=""#cb3-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Create 3 layers</span></span>
-<span id=""cb3-2""><a href=""#cb3-2"" aria-hidden=""true"" tabindex=""-1""></a>layer1 <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer1""</span>)</span>
-<span id=""cb3-3""><a href=""#cb3-3"" aria-hidden=""true"" tabindex=""-1""></a>layer2 <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer2""</span>)</span>
-<span id=""cb3-4""><a href=""#cb3-4"" aria-hidden=""true"" tabindex=""-1""></a>layer3 <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">4</span>, <span class=""at"">name =</span> <span class=""st"">""layer3""</span>)</span>
-<span id=""cb3-5""><a href=""#cb3-5"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-6""><a href=""#cb3-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call layers on a test input</span></span>
-<span id=""cb3-7""><a href=""#cb3-7"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">3</span>, <span class=""dv"">3</span>))</span>
-<span id=""cb3-8""><a href=""#cb3-8"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">layer3</span>(<span class=""fu"">layer2</span>(<span class=""fu"">layer1</span>(x)))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb5""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb5-1""><a href=""#cb5-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Create 3 layers</span></span>
+<span id=""cb5-2""><a href=""#cb5-2"" aria-hidden=""true"" tabindex=""-1""></a>layer1 <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer1""</span>)</span>
+<span id=""cb5-3""><a href=""#cb5-3"" aria-hidden=""true"" tabindex=""-1""></a>layer2 <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer2""</span>)</span>
+<span id=""cb5-4""><a href=""#cb5-4"" aria-hidden=""true"" tabindex=""-1""></a>layer3 <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">4</span>, <span class=""at"">name =</span> <span class=""st"">""layer3""</span>)</span>
+<span id=""cb5-5""><a href=""#cb5-5"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb5-6""><a href=""#cb5-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call layers on a test input</span></span>
+<span id=""cb5-7""><a href=""#cb5-7"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">3</span>, <span class=""dv"">3</span>))</span>
+<span id=""cb5-8""><a href=""#cb5-8"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">layer3</span>(<span class=""fu"">layer2</span>(<span class=""fu"">layer1</span>(x)))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>A Sequential model is <strong>not appropriate</strong> when:</p>
 <ul>
@@ -281,81 +279,140 @@ <h2 class=""anchored"" data-anchor-id=""when-to-use-a-sequential-model"">When to use
 <h2 class=""anchored"" data-anchor-id=""creating-a-sequential-model"">Creating a Sequential model</h2>
 <p>You can create a Sequential model by piping a model through a series layers.</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb4""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb4-1""><a href=""#cb4-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span></span>
-<span id=""cb4-2""><a href=""#cb4-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb4-3""><a href=""#cb4-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb4-4""><a href=""#cb4-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb6""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb6-1""><a href=""#cb6-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span></span>
+<span id=""cb6-2""><a href=""#cb6-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb6-3""><a href=""#cb6-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb6-4""><a href=""#cb6-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Its layers are accessible via the <code>layers</code> attribute:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb5""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb5-1""><a href=""#cb5-1"" aria-hidden=""true"" tabindex=""-1""></a>model<span class=""sc"">$</span>layers</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb7""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb7-1""><a href=""#cb7-1"" aria-hidden=""true"" tabindex=""-1""></a>model<span class=""sc"">$</span>layers</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>[[1]]
+&lt;keras.layers.core.dense.Dense object at 0x7f28d0358ee0&gt;
+
+[[2]]
+&lt;keras.layers.core.dense.Dense object at 0x7f28d47f3670&gt;
+
+[[3]]
+&lt;keras.layers.core.dense.Dense object at 0x7f28d47f3580&gt;</code></pre>
+</div>
 </div>
 <p>You can also create a Sequential model incrementally:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb6""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb6-1""><a href=""#cb6-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>()</span>
-<span id=""cb6-2""><a href=""#cb6-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
-<span id=""cb6-3""><a href=""#cb6-3"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
-<span id=""cb6-4""><a href=""#cb6-4"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb9""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb9-1""><a href=""#cb9-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>()</span>
+<span id=""cb9-2""><a href=""#cb9-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
+<span id=""cb9-3""><a href=""#cb9-3"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
+<span id=""cb9-4""><a href=""#cb9-4"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Note that there’s also a corresponding <code>pop()</code> method to remove layers: a Sequential model behaves very much like a stack of layers.</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb7""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb7-1""><a href=""#cb7-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">pop_layer</span>()</span>
-<span id=""cb7-2""><a href=""#cb7-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">length</span>(model<span class=""sc"">$</span>layers)  <span class=""co""># 2</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb10""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb10-1""><a href=""#cb10-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">pop_layer</span>()</span>
+<span id=""cb10-2""><a href=""#cb10-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">length</span>(model<span class=""sc"">$</span>layers)  <span class=""co""># 2</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>[1] 2</code></pre>
+</div>
 </div>
 <p>Also note that the Sequential constructor accepts a <code>name</code> argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb8""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb8-1""><a href=""#cb8-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">name =</span> <span class=""st"">""my_sequential""</span>)</span>
-<span id=""cb8-2""><a href=""#cb8-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer1""</span>)</span>
-<span id=""cb8-3""><a href=""#cb8-3"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer2""</span>)</span>
-<span id=""cb8-4""><a href=""#cb8-4"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>, <span class=""at"">name =</span> <span class=""st"">""layer3""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb12""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb12-1""><a href=""#cb12-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">name =</span> <span class=""st"">""my_sequential""</span>)</span>
+<span id=""cb12-2""><a href=""#cb12-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer1""</span>)</span>
+<span id=""cb12-3""><a href=""#cb12-3"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""layer2""</span>)</span>
+<span id=""cb12-4""><a href=""#cb12-4"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>, <span class=""at"">name =</span> <span class=""st"">""layer3""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 </section>
 <section id=""specifying-the-input-shape-in-advance"" class=""level2"">
 <h2 class=""anchored"" data-anchor-id=""specifying-the-input-shape-in-advance"">Specifying the input shape in advance</h2>
 <p>Generally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb9""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb9-1""><a href=""#cb9-1"" aria-hidden=""true"" tabindex=""-1""></a>layer <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">3</span>)</span>
-<span id=""cb9-2""><a href=""#cb9-2"" aria-hidden=""true"" tabindex=""-1""></a>layer<span class=""sc"">$</span>weights  <span class=""co""># Empty</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb13""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb13-1""><a href=""#cb13-1"" aria-hidden=""true"" tabindex=""-1""></a>layer <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">3</span>)</span>
+<span id=""cb13-2""><a href=""#cb13-2"" aria-hidden=""true"" tabindex=""-1""></a>layer<span class=""sc"">$</span>weights  <span class=""co""># Empty</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>list()</code></pre>
+</div>
 </div>
 <p>It creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb10""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb10-1""><a href=""#cb10-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call layer on a test input</span></span>
-<span id=""cb10-2""><a href=""#cb10-2"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">4</span>))</span>
-<span id=""cb10-3""><a href=""#cb10-3"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">layer</span>(x)</span>
-<span id=""cb10-4""><a href=""#cb10-4"" aria-hidden=""true"" tabindex=""-1""></a>layer<span class=""sc"">$</span>weights  <span class=""co""># Now it has weights, of shape (4, 3) and (3,)</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb15""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb15-1""><a href=""#cb15-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call layer on a test input</span></span>
+<span id=""cb15-2""><a href=""#cb15-2"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">4</span>))</span>
+<span id=""cb15-3""><a href=""#cb15-3"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">layer</span>(x)</span>
+<span id=""cb15-4""><a href=""#cb15-4"" aria-hidden=""true"" tabindex=""-1""></a>layer<span class=""sc"">$</span>weights  <span class=""co""># Now it has weights, of shape (4, 3) and (3,)</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>[[1]]
+&lt;tf.Variable 'dense_6/kernel:0' shape=(4, 3) dtype=float32, numpy=
+array([[-0.0464139 ,  0.05605704, -0.50734866],
+       [ 0.27502584,  0.02782226, -0.81499064],
+       [-0.64200175, -0.55023456,  0.5515981 ],
+       [ 0.56866205, -0.89289516,  0.56033444]], dtype=float32)&gt;
+
+[[2]]
+&lt;tf.Variable 'dense_6/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)&gt;</code></pre>
+</div>
 </div>
 <p>Naturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn’t “built”: it has no weights (and calling <code>model$weights</code> results in an error stating just this). The weights are created when the model first sees some input data:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb11""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb11-1""><a href=""#cb11-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span> </span>
-<span id=""cb11-2""><a href=""#cb11-2"" aria-hidden=""true"" tabindex=""-1""></a>        <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span> </span>
-<span id=""cb11-3""><a href=""#cb11-3"" aria-hidden=""true"" tabindex=""-1""></a>        <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span> </span>
-<span id=""cb11-4""><a href=""#cb11-4"" aria-hidden=""true"" tabindex=""-1""></a>        <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>)</span>
-<span id=""cb11-5""><a href=""#cb11-5"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-6""><a href=""#cb11-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># No weights at this stage!</span></span>
-<span id=""cb11-7""><a href=""#cb11-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># At this point, you can't do this:</span></span>
-<span id=""cb11-8""><a href=""#cb11-8"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-9""><a href=""#cb11-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">try</span>(model<span class=""sc"">$</span>weights)</span>
-<span id=""cb11-10""><a href=""#cb11-10"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-11""><a href=""#cb11-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-12""><a href=""#cb11-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># The model summary is also not available:</span></span>
-<span id=""cb11-13""><a href=""#cb11-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(model)</span>
-<span id=""cb11-14""><a href=""#cb11-14"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-15""><a href=""#cb11-15"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-16""><a href=""#cb11-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call the model on a test input</span></span>
-<span id=""cb11-17""><a href=""#cb11-17"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">4</span>))</span>
-<span id=""cb11-18""><a href=""#cb11-18"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">model</span>(x)</span>
-<span id=""cb11-19""><a href=""#cb11-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Number of weights after calling the model:""</span>, <span class=""fu"">length</span>(model<span class=""sc"">$</span>weights), <span class=""st"">""</span><span class=""sc"">\n</span><span class=""st"">""</span>)  <span class=""co""># 6</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb17""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb17-1""><a href=""#cb17-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb17-2""><a href=""#cb17-2"" aria-hidden=""true"" tabindex=""-1""></a>        <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb17-3""><a href=""#cb17-3"" aria-hidden=""true"" tabindex=""-1""></a>        <span class=""fu"">layer_dense</span>(<span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb17-4""><a href=""#cb17-4"" aria-hidden=""true"" tabindex=""-1""></a>        <span class=""fu"">layer_dense</span>(<span class=""dv"">4</span>)</span>
+<span id=""cb17-5""><a href=""#cb17-5"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb17-6""><a href=""#cb17-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># No weights at this stage!</span></span>
+<span id=""cb17-7""><a href=""#cb17-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># At this point, you can't do this:</span></span>
+<span id=""cb17-8""><a href=""#cb17-8"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb17-9""><a href=""#cb17-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">try</span>(model<span class=""sc"">$</span>weights)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Error in py_get_attr_impl(x, name, silent) : 
+  ValueError: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.</code></pre>
+</div>
+<div class=""sourceCode cell-code"" id=""cb19""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb19-1""><a href=""#cb19-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># The model summary is also not available:</span></span>
+<span id=""cb19-2""><a href=""#cb19-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(model)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: &lt;no summary available, model was not built&gt;</code></pre>
+</div>
+<div class=""sourceCode cell-code"" id=""cb21""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb21-1""><a href=""#cb21-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call the model on a test input</span></span>
+<span id=""cb21-2""><a href=""#cb21-2"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">4</span>))</span>
+<span id=""cb21-3""><a href=""#cb21-3"" aria-hidden=""true"" tabindex=""-1""></a>y <span class=""ot"">&lt;-</span> <span class=""fu"">model</span>(x)</span>
+<span id=""cb21-4""><a href=""#cb21-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cat</span>(<span class=""st"">""Number of weights after calling the model:""</span>, <span class=""fu"">length</span>(model<span class=""sc"">$</span>weights), <span class=""st"">""</span><span class=""sc"">\n</span><span class=""st"">""</span>)  <span class=""co""># 6</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Number of weights after calling the model: 6 </code></pre>
+</div>
 </div>
 <p>Once a model is “built”, you can call its <code>summary()</code> method to display its contents (the <code>summary()</code> method is also called by the default <code>print()</code> method:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb12""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb12-1""><a href=""#cb12-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(model)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb23""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb23-1""><a href=""#cb23-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(model)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: ""sequential_3""
+____________________________________________________________________________
+ Layer (type)                     Output Shape                  Param #     
+============================================================================
+ dense_9 (Dense)                  (1, 2)                        10          
+ dense_8 (Dense)                  (1, 3)                        9           
+ dense_7 (Dense)                  (1, 4)                        16          
+============================================================================
+Total params: 35
+Trainable params: 35
+Non-trainable params: 0
+____________________________________________________________________________</code></pre>
+</div>
 </div>
 <p>However, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an <code>input_shape</code> argument to your model, so that it knows its input shape from the start:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb13""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb13-1""><a href=""#cb13-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">4</span>))</span>
-<span id=""cb13-2""><a href=""#cb13-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
-<span id=""cb13-3""><a href=""#cb13-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb13-4""><a href=""#cb13-4"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb25""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb25-1""><a href=""#cb25-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">4</span>))</span>
+<span id=""cb25-2""><a href=""#cb25-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
+<span id=""cb25-3""><a href=""#cb25-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb25-4""><a href=""#cb25-4"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: ""sequential_4""
+____________________________________________________________________________
+ Layer (type)                     Output Shape                  Param #     
+============================================================================
+ dense_10 (Dense)                 (None, 2)                     10          
+============================================================================
+Total params: 10
+Trainable params: 10
+Non-trainable params: 0
+____________________________________________________________________________</code></pre>
+</div>
 </div>
 <p>Models built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.</p>
 <p>In general, it’s a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.</p>
@@ -364,34 +421,66 @@ <h2 class=""anchored"" data-anchor-id=""specifying-the-input-shape-in-advance"">Spec
 <h2 class=""anchored"" data-anchor-id=""a-common-debugging-workflow-summary"">A common debugging workflow: <code>%&gt;%</code> + <code>summary()</code></h2>
 <p>When building a new Sequential architecture, it’s useful to incrementally stack layers and print model summaries. For instance, this enables you to monitor how a stack of <code>Conv2D</code> and <code>MaxPooling2D</code> layers is downsampling image feature maps:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb14""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb14-1""><a href=""#cb14-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>)) <span class=""co""># 250x250 RGB images</span></span>
-<span id=""cb14-2""><a href=""#cb14-2"" aria-hidden=""true"" tabindex=""-1""></a>  </span>
-<span id=""cb14-3""><a href=""#cb14-3"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> </span>
-<span id=""cb14-4""><a href=""#cb14-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">5</span>, <span class=""at"">strides =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-5""><a href=""#cb14-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-6""><a href=""#cb14-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_max_pooling_2d</span>(<span class=""dv"">3</span>) </span>
-<span id=""cb14-7""><a href=""#cb14-7"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb14-8""><a href=""#cb14-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Can you guess what the current output shape is at this point? Probably not.</span></span>
-<span id=""cb14-9""><a href=""#cb14-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Let's just print it:</span></span>
-<span id=""cb14-10""><a href=""#cb14-10"" aria-hidden=""true"" tabindex=""-1""></a>model</span>
-<span id=""cb14-11""><a href=""#cb14-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb14-12""><a href=""#cb14-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># The answer was: (40, 40, 32), so we can keep downsampling...</span></span>
-<span id=""cb14-13""><a href=""#cb14-13"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-14""><a href=""#cb14-14"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-15""><a href=""#cb14-15"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-16""><a href=""#cb14-16"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_max_pooling_2d</span>(<span class=""dv"">3</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-17""><a href=""#cb14-17"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-18""><a href=""#cb14-18"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb14-19""><a href=""#cb14-19"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_max_pooling_2d</span>(<span class=""dv"">2</span>) </span>
-<span id=""cb14-20""><a href=""#cb14-20"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb14-21""><a href=""#cb14-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># And now?</span></span>
-<span id=""cb14-22""><a href=""#cb14-22"" aria-hidden=""true"" tabindex=""-1""></a>model</span>
-<span id=""cb14-23""><a href=""#cb14-23"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb14-24""><a href=""#cb14-24"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Now that we have 4x4 feature maps, time to apply global max pooling.</span></span>
-<span id=""cb14-25""><a href=""#cb14-25"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_global_max_pooling_2d</span>()</span>
-<span id=""cb14-26""><a href=""#cb14-26"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb14-27""><a href=""#cb14-27"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Finally, we add a classification layer.</span></span>
-<span id=""cb14-28""><a href=""#cb14-28"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">10</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb27""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb27-1""><a href=""#cb27-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>)) <span class=""co""># 250x250 RGB images</span></span>
+<span id=""cb27-2""><a href=""#cb27-2"" aria-hidden=""true"" tabindex=""-1""></a>  </span>
+<span id=""cb27-3""><a href=""#cb27-3"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> </span>
+<span id=""cb27-4""><a href=""#cb27-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">5</span>, <span class=""at"">strides =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb27-5""><a href=""#cb27-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb27-6""><a href=""#cb27-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_max_pooling_2d</span>(<span class=""dv"">3</span>) </span>
+<span id=""cb27-7""><a href=""#cb27-7"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb27-8""><a href=""#cb27-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Can you guess what the current output shape is at this point? Probably not.</span></span>
+<span id=""cb27-9""><a href=""#cb27-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Let's just print it:</span></span>
+<span id=""cb27-10""><a href=""#cb27-10"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: ""sequential_5""
+____________________________________________________________________________
+ Layer (type)                     Output Shape                  Param #     
+============================================================================
+ conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        
+ conv2d (Conv2D)                  (None, 121, 121, 32)          9248        
+ max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           
+============================================================================
+Total params: 11,680
+Trainable params: 11,680
+Non-trainable params: 0
+____________________________________________________________________________</code></pre>
+</div>
+<div class=""sourceCode cell-code"" id=""cb29""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb29-1""><a href=""#cb29-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># The answer was: (40, 40, 32), so we can keep downsampling...</span></span>
+<span id=""cb29-2""><a href=""#cb29-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span></span>
+<span id=""cb29-3""><a href=""#cb29-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb29-4""><a href=""#cb29-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb29-5""><a href=""#cb29-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_max_pooling_2d</span>(<span class=""dv"">3</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb29-6""><a href=""#cb29-6"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb29-7""><a href=""#cb29-7"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb29-8""><a href=""#cb29-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_max_pooling_2d</span>(<span class=""dv"">2</span>) </span>
+<span id=""cb29-9""><a href=""#cb29-9"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb29-10""><a href=""#cb29-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># And now?</span></span>
+<span id=""cb29-11""><a href=""#cb29-11"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""cell-output cell-output-stdout"">
+<pre><code>Model: ""sequential_5""
+____________________________________________________________________________
+ Layer (type)                     Output Shape                  Param #     
+============================================================================
+ conv2d_1 (Conv2D)                (None, 123, 123, 32)          2432        
+ conv2d (Conv2D)                  (None, 121, 121, 32)          9248        
+ max_pooling2d (MaxPooling2D)     (None, 40, 40, 32)            0           
+ conv2d_5 (Conv2D)                (None, 38, 38, 32)            9248        
+ conv2d_4 (Conv2D)                (None, 36, 36, 32)            9248        
+ max_pooling2d_2 (MaxPooling2D)   (None, 12, 12, 32)            0           
+ conv2d_3 (Conv2D)                (None, 10, 10, 32)            9248        
+ conv2d_2 (Conv2D)                (None, 8, 8, 32)              9248        
+ max_pooling2d_1 (MaxPooling2D)   (None, 4, 4, 32)              0           
+============================================================================
+Total params: 48,672
+Trainable params: 48,672
+Non-trainable params: 0
+____________________________________________________________________________</code></pre>
+</div>
+<div class=""sourceCode cell-code"" id=""cb31""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb31-1""><a href=""#cb31-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Now that we have 4x4 feature maps, time to apply global max pooling.</span></span>
+<span id=""cb31-2""><a href=""#cb31-2"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_global_max_pooling_2d</span>()</span>
+<span id=""cb31-3""><a href=""#cb31-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb31-4""><a href=""#cb31-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Finally, we add a classification layer.</span></span>
+<span id=""cb31-5""><a href=""#cb31-5"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">layer_dense</span>(<span class=""dv"">10</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Very practical, right?</p>
 </section>
@@ -408,38 +497,38 @@ <h2 class=""anchored"" data-anchor-id=""what-to-do-once-you-have-a-model"">What to d
 <h2 class=""anchored"" data-anchor-id=""feature-extraction-with-a-sequential-model"">Feature extraction with a Sequential model</h2>
 <p>Once a Sequential model has been built, it behaves like a <a href=""../../guides/functional_api/"">Functional API model</a>. This means that every layer has an <code>input</code> and <code>output</code> attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb15""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb15-1""><a href=""#cb15-1"" aria-hidden=""true"" tabindex=""-1""></a>initial_model <span class=""ot"">&lt;-</span></span>
-<span id=""cb15-2""><a href=""#cb15-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb15-3""><a href=""#cb15-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">5</span>, <span class=""at"">strides =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb15-4""><a href=""#cb15-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb15-5""><a href=""#cb15-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
-<span id=""cb15-6""><a href=""#cb15-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb15-7""><a href=""#cb15-7"" aria-hidden=""true"" tabindex=""-1""></a>feature_extractor <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model</span>(</span>
-<span id=""cb15-8""><a href=""#cb15-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">inputs =</span> initial_model<span class=""sc"">$</span>inputs,</span>
-<span id=""cb15-9""><a href=""#cb15-9"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">outputs =</span> <span class=""fu"">lapply</span>(initial_model<span class=""sc"">$</span>layers, \(layer) layer<span class=""sc"">$</span>output)</span>
-<span id=""cb15-10""><a href=""#cb15-10"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
-<span id=""cb15-11""><a href=""#cb15-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb15-12""><a href=""#cb15-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call feature extractor on test input.</span></span>
-<span id=""cb15-13""><a href=""#cb15-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb15-14""><a href=""#cb15-14"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>))</span>
-<span id=""cb15-15""><a href=""#cb15-15"" aria-hidden=""true"" tabindex=""-1""></a>features <span class=""ot"">&lt;-</span> <span class=""fu"">feature_extractor</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb32""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb32-1""><a href=""#cb32-1"" aria-hidden=""true"" tabindex=""-1""></a>initial_model <span class=""ot"">&lt;-</span></span>
+<span id=""cb32-2""><a href=""#cb32-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>)) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb32-3""><a href=""#cb32-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">5</span>, <span class=""at"">strides =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb32-4""><a href=""#cb32-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb32-5""><a href=""#cb32-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
+<span id=""cb32-6""><a href=""#cb32-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb32-7""><a href=""#cb32-7"" aria-hidden=""true"" tabindex=""-1""></a>feature_extractor <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model</span>(</span>
+<span id=""cb32-8""><a href=""#cb32-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">inputs =</span> initial_model<span class=""sc"">$</span>inputs,</span>
+<span id=""cb32-9""><a href=""#cb32-9"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">outputs =</span> <span class=""fu"">lapply</span>(initial_model<span class=""sc"">$</span>layers, \(layer) layer<span class=""sc"">$</span>output)</span>
+<span id=""cb32-10""><a href=""#cb32-10"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
+<span id=""cb32-11""><a href=""#cb32-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb32-12""><a href=""#cb32-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call feature extractor on test input.</span></span>
+<span id=""cb32-13""><a href=""#cb32-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb32-14""><a href=""#cb32-14"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>))</span>
+<span id=""cb32-15""><a href=""#cb32-15"" aria-hidden=""true"" tabindex=""-1""></a>features <span class=""ot"">&lt;-</span> <span class=""fu"">feature_extractor</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Here’s a similar example that only extract features from one layer:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb16""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb16-1""><a href=""#cb16-1"" aria-hidden=""true"" tabindex=""-1""></a>initial_model <span class=""ot"">&lt;-</span></span>
-<span id=""cb16-2""><a href=""#cb16-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb16-3""><a href=""#cb16-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">5</span>, <span class=""at"">strides =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb16-4""><a href=""#cb16-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""my_intermediate_layer""</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb16-5""><a href=""#cb16-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
-<span id=""cb16-6""><a href=""#cb16-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb16-7""><a href=""#cb16-7"" aria-hidden=""true"" tabindex=""-1""></a>feature_extractor <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model</span>(</span>
-<span id=""cb16-8""><a href=""#cb16-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">inputs =</span> initial_model<span class=""sc"">$</span>inputs,</span>
-<span id=""cb16-9""><a href=""#cb16-9"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">outputs =</span>  <span class=""fu"">get_layer</span>(initial_model, <span class=""at"">name =</span> <span class=""st"">""my_intermediate_layer""</span>)<span class=""sc"">$</span>output</span>
-<span id=""cb16-10""><a href=""#cb16-10"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
-<span id=""cb16-11""><a href=""#cb16-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb16-12""><a href=""#cb16-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call feature extractor on test input.</span></span>
-<span id=""cb16-13""><a href=""#cb16-13"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>))</span>
-<span id=""cb16-14""><a href=""#cb16-14"" aria-hidden=""true"" tabindex=""-1""></a>features <span class=""ot"">&lt;-</span> <span class=""fu"">feature_extractor</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb33""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb33-1""><a href=""#cb33-1"" aria-hidden=""true"" tabindex=""-1""></a>initial_model <span class=""ot"">&lt;-</span></span>
+<span id=""cb33-2""><a href=""#cb33-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>)) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb33-3""><a href=""#cb33-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">5</span>, <span class=""at"">strides =</span> <span class=""dv"">2</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb33-4""><a href=""#cb33-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>, <span class=""at"">name =</span> <span class=""st"">""my_intermediate_layer""</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb33-5""><a href=""#cb33-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_conv_2d</span>(<span class=""dv"">32</span>, <span class=""dv"">3</span>, <span class=""at"">activation =</span> <span class=""st"">""relu""</span>)</span>
+<span id=""cb33-6""><a href=""#cb33-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb33-7""><a href=""#cb33-7"" aria-hidden=""true"" tabindex=""-1""></a>feature_extractor <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model</span>(</span>
+<span id=""cb33-8""><a href=""#cb33-8"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">inputs =</span> initial_model<span class=""sc"">$</span>inputs,</span>
+<span id=""cb33-9""><a href=""#cb33-9"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">outputs =</span>  <span class=""fu"">get_layer</span>(initial_model, <span class=""at"">name =</span> <span class=""st"">""my_intermediate_layer""</span>)<span class=""sc"">$</span>output</span>
+<span id=""cb33-10""><a href=""#cb33-10"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
+<span id=""cb33-11""><a href=""#cb33-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb33-12""><a href=""#cb33-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Call feature extractor on test input.</span></span>
+<span id=""cb33-13""><a href=""#cb33-13"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> tf<span class=""sc"">$</span><span class=""fu"">ones</span>(<span class=""fu"">shape</span>(<span class=""dv"">1</span>, <span class=""dv"">250</span>, <span class=""dv"">250</span>, <span class=""dv"">3</span>))</span>
+<span id=""cb33-14""><a href=""#cb33-14"" aria-hidden=""true"" tabindex=""-1""></a>features <span class=""ot"">&lt;-</span> <span class=""fu"">feature_extractor</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 </section>
 <section id=""transfer-learning-with-a-sequential-model"" class=""level2"">
@@ -448,47 +537,47 @@ <h2 class=""anchored"" data-anchor-id=""transfer-learning-with-a-sequential-model"">
 <p>Here are two common transfer learning blueprint involving Sequential models.</p>
 <p>First, let’s say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over <code>model$layers</code> and set <code>layer$trainable = FALSE</code> on each layer, except the last one. Like this:</p>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb17""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb17-1""><a href=""#cb17-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">784</span>)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb17-2""><a href=""#cb17-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">32</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb17-3""><a href=""#cb17-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">32</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb17-4""><a href=""#cb17-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">32</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb17-5""><a href=""#cb17-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">10</span>)</span>
-<span id=""cb17-6""><a href=""#cb17-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb17-7""><a href=""#cb17-7"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb17-8""><a href=""#cb17-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Presumably you would want to first load pre-trained weights.</span></span>
-<span id=""cb17-9""><a href=""#cb17-9"" aria-hidden=""true"" tabindex=""-1""></a>model<span class=""sc"">$</span><span class=""fu"">load_weights</span>(...)</span>
-<span id=""cb17-10""><a href=""#cb17-10"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb17-11""><a href=""#cb17-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Freeze all layers except the last one.</span></span>
-<span id=""cb17-12""><a href=""#cb17-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""cf"">for</span> (layer <span class=""cf"">in</span> <span class=""fu"">head</span>(model<span class=""sc"">$</span>layers, <span class=""sc"">-</span><span class=""dv"">1</span>))</span>
-<span id=""cb17-13""><a href=""#cb17-13"" aria-hidden=""true"" tabindex=""-1""></a>  layer<span class=""sc"">$</span>trainable <span class=""ot"">&lt;-</span> <span class=""cn"">FALSE</span></span>
-<span id=""cb17-14""><a href=""#cb17-14"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb17-15""><a href=""#cb17-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># can also just call: freeze_weights(model, to = -2)</span></span>
-<span id=""cb17-16""><a href=""#cb17-16"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb17-17""><a href=""#cb17-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Recompile and train (this will only update the weights of the last layer).</span></span>
-<span id=""cb17-18""><a href=""#cb17-18"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">compile</span>(...)</span>
-<span id=""cb17-19""><a href=""#cb17-19"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(...)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb34""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb34-1""><a href=""#cb34-1"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>(<span class=""at"">input_shape =</span> <span class=""fu"">c</span>(<span class=""dv"">784</span>)) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb34-2""><a href=""#cb34-2"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">32</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb34-3""><a href=""#cb34-3"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">32</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb34-4""><a href=""#cb34-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">32</span>, <span class=""at"">activation =</span> <span class=""st"">'relu'</span>) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb34-5""><a href=""#cb34-5"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">10</span>)</span>
+<span id=""cb34-6""><a href=""#cb34-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb34-7""><a href=""#cb34-7"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb34-8""><a href=""#cb34-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Presumably you would want to first load pre-trained weights.</span></span>
+<span id=""cb34-9""><a href=""#cb34-9"" aria-hidden=""true"" tabindex=""-1""></a>model<span class=""sc"">$</span><span class=""fu"">load_weights</span>(...)</span>
+<span id=""cb34-10""><a href=""#cb34-10"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb34-11""><a href=""#cb34-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Freeze all layers except the last one.</span></span>
+<span id=""cb34-12""><a href=""#cb34-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""cf"">for</span> (layer <span class=""cf"">in</span> <span class=""fu"">head</span>(model<span class=""sc"">$</span>layers, <span class=""sc"">-</span><span class=""dv"">1</span>))</span>
+<span id=""cb34-13""><a href=""#cb34-13"" aria-hidden=""true"" tabindex=""-1""></a>  layer<span class=""sc"">$</span>trainable <span class=""ot"">&lt;-</span> <span class=""cn"">FALSE</span></span>
+<span id=""cb34-14""><a href=""#cb34-14"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb34-15""><a href=""#cb34-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># can also just call: freeze_weights(model, to = -2)</span></span>
+<span id=""cb34-16""><a href=""#cb34-16"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb34-17""><a href=""#cb34-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Recompile and train (this will only update the weights of the last layer).</span></span>
+<span id=""cb34-18""><a href=""#cb34-18"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">compile</span>(...)</span>
+<span id=""cb34-19""><a href=""#cb34-19"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(...)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Another common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:</p>
 </section>
 <section id=""load-a-convolutional-base-with-pre-trained-weights"" class=""level1"">
 <h1>Load a convolutional base with pre-trained weights</h1>
 <div class=""cell"">
-<div class=""sourceCode cell-code"" id=""cb18""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb18-1""><a href=""#cb18-1"" aria-hidden=""true"" tabindex=""-1""></a>base_model <span class=""ot"">&lt;-</span> <span class=""fu"">application_xception</span>(</span>
-<span id=""cb18-2""><a href=""#cb18-2"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">weights =</span> <span class=""st"">'imagenet'</span>,</span>
-<span id=""cb18-3""><a href=""#cb18-3"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">include_top =</span> <span class=""cn"">FALSE</span>,</span>
-<span id=""cb18-4""><a href=""#cb18-4"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">pooling =</span> <span class=""st"">'avg'</span>)</span>
-<span id=""cb18-5""><a href=""#cb18-5"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb18-6""><a href=""#cb18-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Freeze the base model</span></span>
-<span id=""cb18-7""><a href=""#cb18-7"" aria-hidden=""true"" tabindex=""-1""></a>base_model<span class=""sc"">$</span>trainable <span class=""ot"">&lt;-</span> <span class=""cn"">FALSE</span></span>
-<span id=""cb18-8""><a href=""#cb18-8"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb18-9""><a href=""#cb18-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Use a Sequential model to add a trainable classifier on top</span></span>
-<span id=""cb18-10""><a href=""#cb18-10"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span></span>
-<span id=""cb18-11""><a href=""#cb18-11"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">base_model</span>() <span class=""sc"">%&gt;%</span></span>
-<span id=""cb18-12""><a href=""#cb18-12"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">1000</span>)</span>
-<span id=""cb18-13""><a href=""#cb18-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb18-14""><a href=""#cb18-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Compile &amp; train</span></span>
-<span id=""cb18-15""><a href=""#cb18-15"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">compile</span>(...)</span>
-<span id=""cb18-16""><a href=""#cb18-16"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(...)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<div class=""sourceCode cell-code"" id=""cb35""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb35-1""><a href=""#cb35-1"" aria-hidden=""true"" tabindex=""-1""></a>base_model <span class=""ot"">&lt;-</span> <span class=""fu"">application_xception</span>(</span>
+<span id=""cb35-2""><a href=""#cb35-2"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">weights =</span> <span class=""st"">'imagenet'</span>,</span>
+<span id=""cb35-3""><a href=""#cb35-3"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">include_top =</span> <span class=""cn"">FALSE</span>,</span>
+<span id=""cb35-4""><a href=""#cb35-4"" aria-hidden=""true"" tabindex=""-1""></a>    <span class=""at"">pooling =</span> <span class=""st"">'avg'</span>)</span>
+<span id=""cb35-5""><a href=""#cb35-5"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb35-6""><a href=""#cb35-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Freeze the base model</span></span>
+<span id=""cb35-7""><a href=""#cb35-7"" aria-hidden=""true"" tabindex=""-1""></a>base_model<span class=""sc"">$</span>trainable <span class=""ot"">&lt;-</span> <span class=""cn"">FALSE</span></span>
+<span id=""cb35-8""><a href=""#cb35-8"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb35-9""><a href=""#cb35-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Use a Sequential model to add a trainable classifier on top</span></span>
+<span id=""cb35-10""><a href=""#cb35-10"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model_sequential</span>() <span class=""sc"">%&gt;%</span></span>
+<span id=""cb35-11""><a href=""#cb35-11"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">base_model</span>() <span class=""sc"">%&gt;%</span></span>
+<span id=""cb35-12""><a href=""#cb35-12"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">1000</span>)</span>
+<span id=""cb35-13""><a href=""#cb35-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb35-14""><a href=""#cb35-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Compile &amp; train</span></span>
+<span id=""cb35-15""><a href=""#cb35-15"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">compile</span>(...)</span>
+<span id=""cb35-16""><a href=""#cb35-16"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(...)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>If you do transfer learning, you will probably find yourself frequently using these two patterns.</p>
 <p>That’s about all you need to know about Sequential models!</p>
@@ -504,103 +593,6 @@ <h1>Load a convolutional base with pre-trained weights</h1>
 </main> <!-- /main -->
 <script id=""quarto-html-after-body"" type=""application/javascript"">
 window.document.addEventListener(""DOMContentLoaded"", function (event) {
-  const disableStylesheet = (stylesheets) => {
-    for (let i=0; i < stylesheets.length; i++) {
-      const stylesheet = stylesheets[i];
-      stylesheet.rel = 'prefetch';
-    }
-  }
-  const enableStylesheet = (stylesheets) => {
-    for (let i=0; i < stylesheets.length; i++) {
-      const stylesheet = stylesheets[i];
-      stylesheet.rel = 'stylesheet';
-    }
-  }
-  const manageTransitions = (selector, allowTransitions) => {
-    const els = window.document.querySelectorAll(selector);
-    for (let i=0; i < els.length; i++) {
-      const el = els[i];
-      if (allowTransitions) {
-        el.classList.remove('notransition');
-      } else {
-        el.classList.add('notransition');
-      }
-    }
-  }
-  const toggleColorMode = (alternate) => {
-    // Switch the stylesheets
-    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
-    manageTransitions('#quarto-margin-sidebar .nav-link', false);
-    if (alternate) {
-      enableStylesheet(alternateStylesheets);
-    } else {
-      disableStylesheet(alternateStylesheets);
-    }
-    manageTransitions('#quarto-margin-sidebar .nav-link', true);
-    // Switch the toggles
-    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
-    for (let i=0; i < toggles.length; i++) {
-      const toggle = toggles[i];
-      if (toggle) {
-        if (alternate) {
-          toggle.classList.add(""alternate"");     
-        } else {
-          toggle.classList.remove(""alternate"");
-        }
-      }
-    }
-  }
-  const isFileUrl = () => { 
-    return window.location.protocol === 'file:';
-  }
-  const hasAlternateSentinel = () => {  
-    let styleSentinel = getColorSchemeSentinel();
-    if (styleSentinel !== null) {
-      return styleSentinel === ""alternate"";
-    } else {
-      return false;
-    }
-  }
-  const setStyleSentinel = (alternate) => {
-    const value = alternate ? ""alternate"" : ""default"";
-    if (!isFileUrl()) {
-      window.localStorage.setItem(""quarto-color-scheme"", value);
-    } else {
-      localAlternateSentinel = value;
-    }
-  }
-  const getColorSchemeSentinel = () => {
-    if (!isFileUrl()) {
-      const storageValue = window.localStorage.getItem(""quarto-color-scheme"");
-      return storageValue != null ? storageValue : localAlternateSentinel;
-    } else {
-      return localAlternateSentinel;
-    }
-  }
-  let localAlternateSentinel = 'default';
-  // Dark / light mode switch
-  window.quartoToggleColorScheme = () => {
-    // Read the current dark / light value 
-    let toAlternate = !hasAlternateSentinel();
-    toggleColorMode(toAlternate);
-    setStyleSentinel(toAlternate);
-  };
-  // Ensure there is a toggle, if there isn't float one in the top right
-  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
-    const a = window.document.createElement('a');
-    a.classList.add('top-right');
-    a.classList.add('quarto-color-scheme-toggle');
-    a.href = """";
-    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
-    const i = window.document.createElement(""i"");
-    i.classList.add('bi');
-    a.appendChild(i);
-    window.document.body.appendChild(a);
-  }
-  // Switch to dark mode if need be
-  if (hasAlternateSentinel()) {
-    toggleColorMode(true);
-  } 
   const icon = """";
   const anchorJS = new window.AnchorJS();
   anchorJS.options = {

---FILE: _site/keras/guides/transfer_learning.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-0.9.377"">
+<meta name=""generator"" content=""quarto-99.9.9"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>tf-site-title - Transfer learning and fine-tuning</title>
+<title>TensorFlow for R - Transfer learning and fine-tuning</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -80,27 +80,30 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <script src=""../../site_libs/quarto-nav/quarto-nav.js""></script>
+<script src=""../../site_libs/quarto-nav/headroom.min.js""></script>
 <script src=""../../site_libs/clipboard/clipboard.min.js""></script>
-<meta name=""quarto:offset"" content=""../../"">
 <script src=""../../site_libs/quarto-search/autocomplete.umd.js""></script>
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
+<meta name=""quarto:offset"" content=""../../"">
+<link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
 <script src=""../../site_libs/quarto-html/popper.min.js""></script>
 <script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""../../site_libs/quarto-html/anchor.min.js""></script>
 <link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link id=""quarto-text-highlighting-styles"" href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
 <link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
-  ""location"": ""sidebar"",
+  ""location"": ""navbar"",
   ""copy-button"": false,
   ""collapse-after"": 3,
-  ""panel-placement"": ""start"",
-  ""type"": ""textbox"",
+  ""panel-placement"": ""end"",
+  ""type"": ""overlay"",
   ""limit"": 20,
   ""language"": {
     ""search-no-results-text"": ""No results"",
@@ -118,12 +121,74 @@
 
 </head>
 
-<body>
+<body class=""nav-sidebar floating nav-fixed"">
 
 <div id=""quarto-search-results""></div>
+  <header id=""quarto-header"" class=""headroom fixed-top"">
+    <nav class=""navbar navbar-expand-lg navbar-dark "">
+      <div class=""navbar-container container-fluid"">
+      <a class=""navbar-brand"" href=""../../index.html"">
+    <img src=""../../images/favicon/icon.png"" alt="""">
+    <span class=""navbar-title"">TensorFlow for R</span>
+  </a>
+          <div class=""quarto-toggle-container ms-auto"">
+              <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
+  <div class=""quarto-reader-toggle-btn"">
+  <i class=""bi""></i>
+  </div>
+</a>
+          </div>
+          <div id=""quarto-search"" class="""" title=""Search""></div>
+      </div> <!-- /container-fluid -->
+    </nav>
+  <nav class=""quarto-secondary-nav"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+    <div class=""container-fluid d-flex justify-content-between"">
+      <h1 class=""quarto-secondary-nav-title"">Transfer learning and fine-tuning</h1>
+      <button type=""button"" class=""quarto-btn-toggle btn"" aria-label=""Show secondary navigation"">
+        <i class=""bi bi-chevron-right""></i>
+      </button>
+    </div>
+  </nav>
+</header>
 <!-- content -->
-<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
+<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
+  <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
+    <div class=""sidebar-menu-container""> 
+    <ul class=""list-unstyled mt-1"">
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/tensor.html"" class=""sidebar-item-text sidebar-link"">Tensors</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/variable.html"" class=""sidebar-item-text sidebar-link"">Variables</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
+    </ul>
+    </div>
+</nav>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"">
@@ -154,13 +219,11 @@ <h2 id=""toc-title"">On this page</h2>
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
-<h1 class=""title"">Transfer learning and fine-tuning</h1>
+<h1 class=""title d-none d-lg-block"">Transfer learning and fine-tuning</h1>
 </div>
 
 
 
-
-
 <div class=""quarto-title-meta"">
 
     
@@ -205,21 +268,21 @@ <h2 class=""anchored"" data-anchor-id=""freezing-layers-understanding-the-trainable
 <p><strong>Example: the <code>Dense</code> layer has 2 trainable weights (kernel and bias)</strong></p>
 <div class=""cell"" data-hold=""true"">
 <div class=""sourceCode cell-code"" id=""cb2""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb2-1""><a href=""#cb2-1"" aria-hidden=""true"" tabindex=""-1""></a>layer <span class=""ot"">&lt;-</span> <span class=""fu"">layer_dense</span>(<span class=""at"">units =</span> <span class=""dv"">3</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stderr"">
-<pre><code>Loaded Tensorflow version 2.8.0</code></pre>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Loaded Tensorflow version 2.9.1</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb4""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb4-1""><a href=""#cb4-1"" aria-hidden=""true"" tabindex=""-1""></a>layer<span class=""sc"">$</span><span class=""fu"">build</span>(<span class=""fu"">shape</span>(<span class=""cn"">NULL</span>, <span class=""dv"">4</span>))</span>
 <span id=""cb4-2""><a href=""#cb4-2"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb4-3""><a href=""#cb4-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>weights: 2</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb6""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb6-1""><a href=""#cb6-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""trainable_weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>trainable_weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>trainable_weights: 2</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb8""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb8-1""><a href=""#cb8-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""non_trainable_weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>non_trainable_weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>non_trainable_weights: 0</code></pre>
 </div>
 </div>
@@ -230,15 +293,15 @@ <h2 class=""anchored"" data-anchor-id=""freezing-layers-understanding-the-trainable
 <span id=""cb10-2""><a href=""#cb10-2"" aria-hidden=""true"" tabindex=""-1""></a>layer<span class=""sc"">$</span><span class=""fu"">build</span>(<span class=""fu"">shape</span>(<span class=""cn"">NULL</span>, <span class=""dv"">4</span>))</span>
 <span id=""cb10-3""><a href=""#cb10-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb10-4""><a href=""#cb10-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>weights: 4</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb12""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb12-1""><a href=""#cb12-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""trainable_weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>trainable_weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>trainable_weights: 2</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb14""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb14-1""><a href=""#cb14-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""non_trainable_weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>non_trainable_weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>non_trainable_weights: 2</code></pre>
 </div>
 </div>
@@ -250,15 +313,15 @@ <h2 class=""anchored"" data-anchor-id=""freezing-layers-understanding-the-trainable
 <span id=""cb16-3""><a href=""#cb16-3"" aria-hidden=""true"" tabindex=""-1""></a>layer<span class=""sc"">$</span>trainable <span class=""ot"">&lt;-</span> <span class=""cn"">FALSE</span>     <span class=""co""># Freeze the layer</span></span>
 <span id=""cb16-4""><a href=""#cb16-4"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb16-5""><a href=""#cb16-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>weights: 2</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb18""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb18-1""><a href=""#cb18-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""trainable_weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>trainable_weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>trainable_weights: 0</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb20""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb20-1""><a href=""#cb20-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""non_trainable_weights: %s""</span>, <span class=""fu"">length</span>(layer<span class=""sc"">$</span>non_trainable_weights))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>non_trainable_weights: 2</code></pre>
 </div>
 </div>
@@ -466,15 +529,15 @@ <h3 class=""anchored"" data-anchor-id=""getting-the-data"">Getting the data</h3>
 <span id=""cb30-9""><a href=""#cb30-9"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
 <span id=""cb30-10""><a href=""#cb30-10"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb30-11""><a href=""#cb30-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""Number of training samples: %d""</span>, <span class=""fu"">length</span>(train_ds))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Number of training samples: 9305</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb32""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb32-1""><a href=""#cb32-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""Number of validation samples: %d""</span>, <span class=""fu"">length</span>(validation_ds) )</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Number of validation samples: 2326</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb34""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb34-1""><a href=""#cb34-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">printf</span>(<span class=""st"">""Number of test samples: %d""</span>, <span class=""fu"">length</span>(test_ds))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Number of test samples: 2326</code></pre>
 </div>
 </div>
@@ -589,7 +652,7 @@ <h2 class=""anchored"" data-anchor-id=""build-a-model"">Build a model</h2>
 <span id=""cb41-8""><a href=""#cb41-8"" aria-hidden=""true"" tabindex=""-1""></a>base_model<span class=""sc"">$</span>trainable <span class=""ot"">&lt;-</span> <span class=""cn"">FALSE</span></span>
 <span id=""cb41-9""><a href=""#cb41-9"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb41-10""><a href=""#cb41-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Create new model on top</span></span>
-<span id=""cb41-11""><a href=""#cb41-11"" aria-hidden=""true"" tabindex=""-1""></a>inputs <span class=""ot"">=</span> <span class=""fu"">layer_input</span>(<span class=""at"">shape =</span> <span class=""fu"">c</span>(<span class=""dv"">150</span>, <span class=""dv"">150</span>, <span class=""dv"">3</span>))</span>
+<span id=""cb41-11""><a href=""#cb41-11"" aria-hidden=""true"" tabindex=""-1""></a>inputs <span class=""ot"">&lt;-</span> <span class=""fu"">layer_input</span>(<span class=""at"">shape =</span> <span class=""fu"">c</span>(<span class=""dv"">150</span>, <span class=""dv"">150</span>, <span class=""dv"">3</span>))</span>
 <span id=""cb41-12""><a href=""#cb41-12"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb41-13""><a href=""#cb41-13"" aria-hidden=""true"" tabindex=""-1""></a>outputs <span class=""ot"">&lt;-</span> inputs <span class=""sc"">%&gt;%</span></span>
 <span id=""cb41-14""><a href=""#cb41-14"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">data_augmentation</span>() <span class=""sc"">%&gt;%</span>   <span class=""co""># Apply random data augmentation</span></span>
@@ -609,7 +672,7 @@ <h2 class=""anchored"" data-anchor-id=""build-a-model"">Build a model</h2>
 <span id=""cb41-28""><a href=""#cb41-28"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb41-29""><a href=""#cb41-29"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model</span>(inputs, outputs)</span>
 <span id=""cb41-30""><a href=""#cb41-30"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model: ""model_1""
 ____________________________________________________________________________
  Layer (type)                Output Shape              Param #   Trainable  
@@ -639,7 +702,7 @@ <h2 class=""anchored"" data-anchor-id=""train-the-top-layer"">Train the top layer</h
 <span id=""cb43-4""><a href=""#cb43-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">metrics =</span> <span class=""fu"">metric_binary_accuracy</span>()</span>
 <span id=""cb43-5""><a href=""#cb43-5"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
 <span id=""cb43-6""><a href=""#cb43-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb43-7""><a href=""#cb43-7"" aria-hidden=""true"" tabindex=""-1""></a>epochs <span class=""ot"">&lt;-</span> <span class=""dv"">20</span></span>
+<span id=""cb43-7""><a href=""#cb43-7"" aria-hidden=""true"" tabindex=""-1""></a>epochs <span class=""ot"">&lt;-</span> <span class=""dv"">2</span></span>
 <span id=""cb43-8""><a href=""#cb43-8"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(train_ds, <span class=""at"">epochs =</span> epochs, <span class=""at"">validation_data =</span> validation_ds)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 </section>
@@ -655,7 +718,7 @@ <h2 class=""anchored"" data-anchor-id=""do-a-round-of-fine-tuning-of-the-entire-mod
 <span id=""cb44-5""><a href=""#cb44-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># we've done so far.</span></span>
 <span id=""cb44-6""><a href=""#cb44-6"" aria-hidden=""true"" tabindex=""-1""></a>base_model<span class=""sc"">$</span>trainable <span class=""ot"">&lt;-</span> <span class=""cn"">TRUE</span></span>
 <span id=""cb44-7""><a href=""#cb44-7"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model: ""model_1""
 ____________________________________________________________________________
  Layer (type)                Output Shape              Param #   Trainable  
@@ -680,7 +743,7 @@ <h2 class=""anchored"" data-anchor-id=""do-a-round-of-fine-tuning-of-the-entire-mod
 <span id=""cb46-4""><a href=""#cb46-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""at"">metrics =</span> <span class=""fu"">metric_binary_accuracy</span>()</span>
 <span id=""cb46-5""><a href=""#cb46-5"" aria-hidden=""true"" tabindex=""-1""></a>)</span>
 <span id=""cb46-6""><a href=""#cb46-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb46-7""><a href=""#cb46-7"" aria-hidden=""true"" tabindex=""-1""></a>epochs <span class=""ot"">&lt;-</span> <span class=""dv"">10</span></span>
+<span id=""cb46-7""><a href=""#cb46-7"" aria-hidden=""true"" tabindex=""-1""></a>epochs <span class=""ot"">&lt;-</span> <span class=""dv"">1</span></span>
 <span id=""cb46-8""><a href=""#cb46-8"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""sc"">%&gt;%</span> <span class=""fu"">fit</span>(train_ds, <span class=""at"">epochs =</span> epochs, <span class=""at"">validation_data =</span> validation_ds)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>After 10 epochs, fine-tuning gains us a nice improvement here.</p>

---FILE: _site/keras/guides/working_with_rnns.html---
@@ -2,13 +2,13 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-0.9.377"">
+<meta name=""generator"" content=""quarto-99.9.9"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 <meta name=""author"" content=""Scott Zhu, Francois Chollet, Tomasz Kalinowski"">
 
-<title>tf-site-title - Working with RNNs</title>
+<title>TensorFlow for R - Working with RNNs</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -81,27 +81,30 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <script src=""../../site_libs/quarto-nav/quarto-nav.js""></script>
+<script src=""../../site_libs/quarto-nav/headroom.min.js""></script>
 <script src=""../../site_libs/clipboard/clipboard.min.js""></script>
-<meta name=""quarto:offset"" content=""../../"">
 <script src=""../../site_libs/quarto-search/autocomplete.umd.js""></script>
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
+<meta name=""quarto:offset"" content=""../../"">
+<link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
 <script src=""../../site_libs/quarto-html/popper.min.js""></script>
 <script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""../../site_libs/quarto-html/anchor.min.js""></script>
 <link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link id=""quarto-text-highlighting-styles"" href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
 <link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
-  ""location"": ""sidebar"",
+  ""location"": ""navbar"",
   ""copy-button"": false,
   ""collapse-after"": 3,
-  ""panel-placement"": ""start"",
-  ""type"": ""textbox"",
+  ""panel-placement"": ""end"",
+  ""type"": ""overlay"",
   ""limit"": 20,
   ""language"": {
     ""search-no-results-text"": ""No results"",
@@ -119,12 +122,74 @@
 
 </head>
 
-<body>
+<body class=""nav-sidebar floating nav-fixed"">
 
 <div id=""quarto-search-results""></div>
+  <header id=""quarto-header"" class=""headroom fixed-top"">
+    <nav class=""navbar navbar-expand-lg navbar-dark "">
+      <div class=""navbar-container container-fluid"">
+      <a class=""navbar-brand"" href=""../../index.html"">
+    <img src=""../../images/favicon/icon.png"" alt="""">
+    <span class=""navbar-title"">TensorFlow for R</span>
+  </a>
+          <div class=""quarto-toggle-container ms-auto"">
+              <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
+  <div class=""quarto-reader-toggle-btn"">
+  <i class=""bi""></i>
+  </div>
+</a>
+          </div>
+          <div id=""quarto-search"" class="""" title=""Search""></div>
+      </div> <!-- /container-fluid -->
+    </nav>
+  <nav class=""quarto-secondary-nav"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+    <div class=""container-fluid d-flex justify-content-between"">
+      <h1 class=""quarto-secondary-nav-title"">Working with RNNs</h1>
+      <button type=""button"" class=""quarto-btn-toggle btn"" aria-label=""Show secondary navigation"">
+        <i class=""bi bi-chevron-right""></i>
+      </button>
+    </div>
+  </nav>
+</header>
 <!-- content -->
-<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
+<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
+  <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
+    <div class=""sidebar-menu-container""> 
+    <ul class=""list-unstyled mt-1"">
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/tensor.html"" class=""sidebar-item-text sidebar-link"">Tensors</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/variable.html"" class=""sidebar-item-text sidebar-link"">Variables</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
+    </ul>
+    </div>
+</nav>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"">
@@ -159,13 +224,11 @@ <h2 id=""toc-title"">On this page</h2>
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
-<h1 class=""title"">Working with RNNs</h1>
+<h1 class=""title d-none d-lg-block"">Working with RNNs</h1>
 </div>
 
 
 
-
-
 <div class=""quarto-title-meta"">
 
     <div>
@@ -219,11 +282,11 @@ <h2 class=""anchored"" data-anchor-id=""built-in-rnn-layers-a-simple-example"">Built
 <span id=""cb2-9""><a href=""#cb2-9"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb2-10""><a href=""#cb2-10"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""co""># Add a Dense layer with 10 units.</span></span>
 <span id=""cb2-11""><a href=""#cb2-11"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">10</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stderr"">
-<pre><code>Loaded Tensorflow version 2.8.0</code></pre>
+<div class=""cell-output cell-output-stderr"">
+<pre><code>Loaded Tensorflow version 2.9.1</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb4""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb4-1""><a href=""#cb4-1"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model: ""sequential""
 ____________________________________________________________________________
  Layer (type)                     Output Shape                  Param #     
@@ -264,7 +327,7 @@ <h2 class=""anchored"" data-anchor-id=""outputs-and-states"">Outputs and states</h2>
 <span id=""cb6-10""><a href=""#cb6-10"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">10</span>)</span>
 <span id=""cb6-11""><a href=""#cb6-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb6-12""><a href=""#cb6-12"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model: ""sequential_1""
 ____________________________________________________________________________
  Layer (type)                     Output Shape                  Param #     
@@ -311,7 +374,7 @@ <h2 class=""anchored"" data-anchor-id=""outputs-and-states"">Outputs and states</h2>
 <span id=""cb8-25""><a href=""#cb8-25"" aria-hidden=""true"" tabindex=""-1""></a>model <span class=""ot"">&lt;-</span> <span class=""fu"">keras_model</span>(<span class=""at"">inputs =</span> <span class=""fu"">list</span>(encoder_input, decoder_input),</span>
 <span id=""cb8-26""><a href=""#cb8-26"" aria-hidden=""true"" tabindex=""-1""></a>                     <span class=""at"">outputs =</span> output)</span>
 <span id=""cb8-27""><a href=""#cb8-27"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model: ""model""
 ____________________________________________________________________________
  Layer (type)            Output Shape    Param #  Connected to              
@@ -419,7 +482,7 @@ <h2 class=""anchored"" data-anchor-id=""bidirectional-rnns"">Bidirectional RNNs</h2>
 <span id=""cb14-4""><a href=""#cb14-4"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">layer_dense</span>(<span class=""dv"">10</span>)</span>
 <span id=""cb14-5""><a href=""#cb14-5"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb14-6""><a href=""#cb14-6"" aria-hidden=""true"" tabindex=""-1""></a>model</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Model: ""sequential_2""
 ____________________________________________________________________________
  Layer (type)                     Output Shape                  Param #     
@@ -548,7 +611,7 @@ <h3 class=""anchored"" data-anchor-id=""using-cudnn-kernels-when-available"">Using C
 <span id=""cb20-15""><a href=""#cb20-15"" aria-hidden=""true"" tabindex=""-1""></a>      <span class=""fu"">t</span>() <span class=""sc"">%&gt;%</span>           <span class=""co""># rotate</span></span>
 <span id=""cb20-16""><a href=""#cb20-16"" aria-hidden=""true"" tabindex=""-1""></a>      <span class=""fu"">image</span>(<span class=""at"">axes =</span> <span class=""cn"">FALSE</span>, <span class=""at"">asp =</span> <span class=""dv"">1</span>, <span class=""at"">col =</span> <span class=""fu"">grey</span>(<span class=""fu"">seq</span>(<span class=""dv"">0</span>, <span class=""dv"">1</span>, <span class=""at"">length.out =</span> <span class=""dv"">256</span>)))</span>
 <span id=""cb20-17""><a href=""#cb20-17"" aria-hidden=""true"" tabindex=""-1""></a>})</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-stdout"">
+<div class=""cell-output cell-output-stdout"">
 <pre><code>Predicted result is: 3, target result is: 5</code></pre>
 </div>
 <div class=""cell-output-display"">

---FILE: _site/keras/guides/writing_your_own_callbacks.html---
@@ -2,13 +2,13 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-0.9.377"">
+<meta name=""generator"" content=""quarto-99.9.9"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 <meta name=""author"" content=""Rick Chao, Francois Chollet, Tomasz Kalinowski"">
 
-<title>tf-site-title - Writing your own callbacks</title>
+<title>TensorFlow for R - Writing your own callbacks</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -81,27 +81,30 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <script src=""../../site_libs/quarto-nav/quarto-nav.js""></script>
+<script src=""../../site_libs/quarto-nav/headroom.min.js""></script>
 <script src=""../../site_libs/clipboard/clipboard.min.js""></script>
-<meta name=""quarto:offset"" content=""../../"">
 <script src=""../../site_libs/quarto-search/autocomplete.umd.js""></script>
 <script src=""../../site_libs/quarto-search/fuse.min.js""></script>
 <script src=""../../site_libs/quarto-search/quarto-search.js""></script>
+<meta name=""quarto:offset"" content=""../../"">
+<link href=""../../images/favicon/icon.png"" rel=""icon"" type=""image/png"">
 <script src=""../../site_libs/quarto-html/quarto.js""></script>
 <script src=""../../site_libs/quarto-html/popper.min.js""></script>
 <script src=""../../site_libs/quarto-html/tippy.umd.min.js""></script>
 <script src=""../../site_libs/quarto-html/anchor.min.js""></script>
 <link href=""../../site_libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link id=""quarto-text-highlighting-styles"" href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"">
+<link href=""../../site_libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles"">
 <script src=""../../site_libs/bootstrap/bootstrap.min.js""></script>
 <link href=""../../site_libs/bootstrap/bootstrap-icons.css"" rel=""stylesheet"">
 <link href=""../../site_libs/bootstrap/bootstrap.min.css"" rel=""stylesheet"">
 <script id=""quarto-search-options"" type=""application/json"">{
-  ""location"": ""sidebar"",
+  ""location"": ""navbar"",
   ""copy-button"": false,
   ""collapse-after"": 3,
-  ""panel-placement"": ""start"",
-  ""type"": ""textbox"",
+  ""panel-placement"": ""end"",
+  ""type"": ""overlay"",
   ""limit"": 20,
   ""language"": {
     ""search-no-results-text"": ""No results"",
@@ -119,12 +122,74 @@
 
 </head>
 
-<body>
+<body class=""nav-sidebar floating nav-fixed"">
 
 <div id=""quarto-search-results""></div>
+  <header id=""quarto-header"" class=""headroom fixed-top"">
+    <nav class=""navbar navbar-expand-lg navbar-dark "">
+      <div class=""navbar-container container-fluid"">
+      <a class=""navbar-brand"" href=""../../index.html"">
+    <img src=""../../images/favicon/icon.png"" alt="""">
+    <span class=""navbar-title"">TensorFlow for R</span>
+  </a>
+          <div class=""quarto-toggle-container ms-auto"">
+              <a href="""" class=""quarto-reader-toggle nav-link"" onclick=""window.quartoToggleReader(); return false;"" title=""Toggle reader mode"">
+  <div class=""quarto-reader-toggle-btn"">
+  <i class=""bi""></i>
+  </div>
+</a>
+          </div>
+          <div id=""quarto-search"" class="""" title=""Search""></div>
+      </div> <!-- /container-fluid -->
+    </nav>
+  <nav class=""quarto-secondary-nav"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+    <div class=""container-fluid d-flex justify-content-between"">
+      <h1 class=""quarto-secondary-nav-title"">Writing your own callbacks</h1>
+      <button type=""button"" class=""quarto-btn-toggle btn"" aria-label=""Show secondary navigation"">
+        <i class=""bi bi-chevron-right""></i>
+      </button>
+    </div>
+  </nav>
+</header>
 <!-- content -->
-<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
+<div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article page-navbar"">
 <!-- sidebar -->
+  <nav id=""quarto-sidebar"" class=""sidebar collapse sidebar-navigation floating overflow-auto"">
+      <div class=""mt-2 flex-shrink-0 align-items-center"">
+        <div class=""sidebar-search"">
+        <div id=""quarto-search"" class="""" title=""Search""></div>
+        </div>
+      </div>
+    <div class=""sidebar-menu-container""> 
+    <ul class=""list-unstyled mt-1"">
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/basics.html"" class=""sidebar-item-text sidebar-link"">Overview</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/tensor.html"" class=""sidebar-item-text sidebar-link"">Tensors</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/variable.html"" class=""sidebar-item-text sidebar-link"">Variables</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/autodiff.html"" class=""sidebar-item-text sidebar-link"">Automatic differentiation</a>
+  </div>
+</li>
+        <li class=""sidebar-item"">
+  <div class=""sidebar-item-container""> 
+  <a href=""../../tensorflow/guide/intro_to_graphs.html"" class=""sidebar-item-text sidebar-link"">Graphs and functions</a>
+  </div>
+</li>
+    </ul>
+    </div>
+</nav>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"">
@@ -159,13 +224,11 @@ <h2 id=""toc-title"">On this page</h2>
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
-<h1 class=""title"">Writing your own callbacks</h1>
+<h1 class=""title d-none d-lg-block"">Writing your own callbacks</h1>
 </div>
 
 
 
-
-
 <div class=""quarto-title-meta"">
 
     <div>

---FILE: _site/listings.json---
@@ -2,15 +2,16 @@
   {
     ""listing"": ""/index.html"",
     ""items"": [
-      ""/tensorflow/guide/autodiff.html"",
       ""/keras/guides/customizing_what_happens_in_fit.html"",
       ""/tensorflow/guide/intro_to_graphs.html"",
-      ""/keras/guides/python_subclasses.html"",
       ""/tensorflow/guide/tensor.html"",
+      ""/tensorflow/guide/variable.html"",
+      ""/tensorflow/guide/autodiff.html"",
+      ""/keras/guides/python_subclasses.html"",
       ""/tensorflow/guide/basics.html"",
+      ""/keras/guides/functional_api.html"",
       ""/keras/guides/sequential_model.html"",
       ""/keras/guides/transfer_learning.html"",
-      ""/tensorflow/guide/variable.html"",
       ""/keras/guides/working_with_rnns.html"",
       ""/keras/guides/preprocessing_layers.html"",
       ""/keras/guides/making_new_layers_and_models_via_subclassing.html"","
