repo_owner,repo_name,commit_hash,author_name,author_email,author_date,committer_name,committer_email,committer_date,message,filenames,touches_rmd,touches_r,touches_r_or_rmd,is_merge,added,deleted,changed,diff
biol607,biol607.github.io,c868377f20450d78a0cd2f11efa17012cdfe6713,jebyrnes,jarrett.byrnes@umb.edu,2025-03-06T14:37:17Z,jebyrnes,jarrett.byrnes@umb.edu,2025-03-06T14:37:17Z,Fix typos in categorical model lecture,lectures/categorical_predictors.Rmd;lectures/categorical_predictors.html;lectures/categorical_predictors_files/figure-html/lizard_mean-1.png;lectures/categorical_predictors_files/figure-html/unnamed-chunk-1-1.png;lectures/categorical_predictors_files/figure-html/unnamed-chunk-2-1.png;lectures/categorical_predictors_files/figure-html/unnamed-chunk-3-1.png;lectures/categorical_predictors_files/figure-html/unnamed-chunk-4-1.png;lectures/categorical_predictors_files/figure-html/unnamed-chunk-7-1.png;lectures/categorical_predictors_files/figure-html/unnamed-chunk-8-1.png;lectures/categorical_predictors_files/figure-html/unnamed-chunk-9-1.png,True,False,True,False,45,26,71,"---FILE: lectures/categorical_predictors.Rmd---
@@ -173,7 +173,7 @@ background-size: cover
 ```{r lizard_load, warning=FALSE}
 library(readr)
 library(dplyr)
-lizards <- read_csv(""lectures/data/09/12e3HornedLizards.csv"",
+lizards <- read_csv(""data/09/12e3HornedLizards.csv"",
                     col_types = ""di"") %>%
   mutate(Status = ifelse(Survive==1, ""Living"", ""Dead"")) %>%
   filter(!is.na(`Squamosal horn length`))
@@ -283,7 +283,7 @@ $$Length_i = \beta_0 + \beta_1 Status_i + \epsilon_i$$
 
 --
 
-- We could even fit a model wit no $\beta_0$ and code Dead = 0 or 1 and Living = 0 or 1
+- We could even fit a model with no $\beta_0$ and code Dead = 0 or 1 and Living = 0 or 1
 
 --
 
@@ -331,7 +331,7 @@ check_heteroscedasticity(horn_mod) |> plot() +
 ]
 
 ```{r load_brains}
-brainGene <- read.csv(""lectures/data/19/15q06DisordersAndGeneExpression.csv"") %>%
+brainGene <- read.csv(""data/19/15q06DisordersAndGeneExpression.csv"") %>%
   mutate(group = forcats::fct_relevel(group, c(""control"", ""schizo"", ""bipolar"")))
 ```
 
@@ -426,7 +426,6 @@ Underlying linear model with control = intercept, dummy variable for schizo
 ---
 # Linear Dummy Variable (Fixed Effect) Model
 $$\large y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}, \qquad x_{i} = 0,1$$  
-$$\epsilon_{ij} \sim N(0, \sigma^{2})$$
 
 - i = replicate, j = group  
 
@@ -531,7 +530,7 @@ What does this mean?
 
 --
 
-- Intercept ($\beta_{0}$) = the average value associated with being in the control group
+- Intercept ( $\beta_{0}$ ) = the average value associated with being in the control group
 
 - Others = the average difference between control and each other group
 
@@ -565,7 +564,7 @@ Being in group j is associated with an average outcome of y.
 ---
 
 # What's the best way to see this?
-.center[ ![:scale 75%](./images/anova/barplot_viz.jpg)]
+.center[ ![:scale 65%](./images/anova/barplot_viz.jpg)]
 
 ---
 # Many Ways to Visualize
@@ -574,7 +573,8 @@ Being in group j is associated with an average outcome of y.
 ggplot(brainGene,
        aes(x = group, y = PLP1.expression)) +
   geom_jitter(color = ""lightgrey"") +
-  stat_summary(fun.data = ""mean_se"", color = ""red"")
+  stat_summary(fun.data = ""mean_se"", color = ""red"") +
+  labs(subtitle = ""Means, SE, and Raw Data"")
 ```
 
 ---
@@ -585,7 +585,7 @@ ggplot(brainGene,
        aes(y = group, x = PLP1.expression,
            fill = group)) +
  stat_dotsinterval(dotsize = 0.3, binwidth = 0.1) +
-  labs(y="""")
+  labs(y="""", subtitle = ""Quantiles of Data, Histogram"")
 ```
 
 ---
@@ -597,7 +597,7 @@ ggplot(brainGene,
        aes(y = group, x = PLP1.expression,
            fill = group)) +
   stat_halfeye() +
-  labs(y="""") +
+  labs(y="""", subtitle = ""Raincloud Plot"") +
   stat_dots(side = ""bottom"", dotsize = .17, binwidth = 0.1, color = NA)
 ```
 
@@ -649,14 +649,24 @@ Many mini-linear models with two means....multiple comparisons!
 
 - Each group has a mean and SE
 
+--
+
 - We can calculate a comparison for each 
 
+--
+
 - BUT, we lose precision as we keep resampling the model  
-  
+
+--
+
 - Remember, for every time we look at a system, we have some % of our CI not overlapping the true value  
 
+--
+
 - Each time we compare means, we have a chance of our CI not covering the true value  
 
+--
+
 - To minimize this possibility, we correct (widen) our CIs for this **Family-Wise Error Rate**
 
 

---FILE: lectures/categorical_predictors.html---
@@ -139,7 +139,7 @@
 
 ---
 # First, Recode the Data with Dummy Variables
-&lt;table class=""table"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:right;""&gt; Squamosal horn length &lt;/th&gt;
@@ -183,7 +183,7 @@
 
 ---
 # First, Recode the Data with Dummy Variables
-&lt;table class=""table"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:right;""&gt; Squamosal horn length &lt;/th&gt;
@@ -241,7 +241,7 @@
 
 ---
 # But with an Intercept, we don't need Two Dummy Variables
-&lt;table class=""table"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:right;""&gt; Squamosal horn length &lt;/th&gt;
@@ -323,7 +323,7 @@
 
 --
 
-- We could even fit a model wit no `\(\beta_0\)` and code Dead = 0 or 1 and Living = 0 or 1
+- We could even fit a model with no `\(\beta_0\)` and code Dead = 0 or 1 and Living = 0 or 1
 
 --
 
@@ -419,7 +419,6 @@
 ---
 # Linear Dummy Variable (Fixed Effect) Model
 `$$\large y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}, \qquad x_{i} = 0,1$$`  
-`$$\epsilon_{ij} \sim N(0, \sigma^{2})$$`
 
 - i = replicate, j = group  
 
@@ -484,7 +483,7 @@
 
 **Using Least Squares**
 
-```r
+``` r
 brain_lm &lt;- lm(PLP1.expression ~ group, data=brainGene)
 
 tidy(brain_lm) |&gt; 
@@ -493,7 +492,7 @@
   kableExtra::kable_styling()
 ```
 
-&lt;table class=""table"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
@@ -539,7 +538,7 @@
 # R Fits with Treatment Contrasts
 `$$y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}$$`
 
-&lt;table class=""table"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
@@ -572,7 +571,7 @@
 
 --
 
-- Intercept ($\beta_{0}$) = the average value associated with being in the control group
+- Intercept ( `\(\beta_{0}\)` ) = the average value associated with being in the control group
 
 - Others = the average difference between control and each other group
 
@@ -584,7 +583,7 @@
 `$$y_{ij} = \alpha_{j} + \epsilon_{ij}$$`
 
 
-&lt;table class=""table"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:left;""&gt; group &lt;/th&gt;
@@ -622,7 +621,7 @@
 ---
 
 # What's the best way to see this?
-.center[ ![:scale 75%](./images/anova/barplot_viz.jpg)]
+.center[ ![:scale 65%](./images/anova/barplot_viz.jpg)]
 
 ---
 # Many Ways to Visualize
@@ -689,14 +688,24 @@
 
 - Each group has a mean and SE
 
+--
+
 - We can calculate a comparison for each 
 
+--
+
 - BUT, we lose precision as we keep resampling the model  
-  
+
+--
+
 - Remember, for every time we look at a system, we have some % of our CI not overlapping the true value  
 
+--
+
 - Each time we compare means, we have a chance of our CI not covering the true value  
 
+--
+
 - To minimize this possibility, we correct (widen) our CIs for this **Family-Wise Error Rate**
 
 
@@ -720,7 +729,7 @@
 ---
 
 # No Correction: Least Square Differences
-&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table table-striped"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:left;""&gt; contrast &lt;/th&gt;
@@ -753,7 +762,7 @@
 
 ---
 # Bonferroni Corrections
-&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table table-striped"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:left;""&gt; contrast &lt;/th&gt;
@@ -787,7 +796,7 @@
 
 ---
 # Tukey's Honestly Significant Difference
-&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table table-striped"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:left;""&gt; contrast &lt;/th&gt;
@@ -824,7 +833,7 @@
 
 ---
 # Dunnett's Comparison to Controls
-&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+&lt;table class=""table table-striped"" style=""color: black; margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:left;""&gt; contrast &lt;/th&gt;"
biol607,biol607.github.io,a9b29c236ecf3833fe58575ce9a136c29fb030f6,jebyrnes,jarrett.byrnes@umb.edu,2025-03-04T18:15:54Z,jebyrnes,jarrett.byrnes@umb.edu,2025-03-04T18:15:54Z,Update schedule with a few fixes,schedule.Rmd;schedule.html,True,False,True,False,18,22,40,"---FILE: schedule.Rmd---
@@ -3,15 +3,14 @@
 ---  
 
 <!--
-
+NEXT TIME I TEACH 607 in 2026/7
+- Assumptions in linear regression lecture is a SLOG - make less boring
+  - separate assumptions about DGP and EGP
+  - Show clearer violations and fixes
+  - NO BIG TEXT SLIDES
+  - do not need to describe Bayes or maybe even likelihood
 
 NEXT TIME I TEACH 607 in 2024/5
-redo explaining least squares - it's not good in the lecture
-
-re-arrange the linear regression lab
-  - workflow
-  - transforms
-  - viz of prediction intervals and such
 
 Causal Intro is 2 lectures
   - End is week and flabby. Rework
@@ -123,7 +122,7 @@ __Homework:__ [Correlation and Linear Models](homework/06_correlation_regression
 __Lecture:__  [Many Predictors](./lectures/mlr.html),  [Categorical Predictors: Just Another Linear Model](lectures/categorical_predictors.html)   
 __Lab Topic:__ [Models with Categorical Variables](lab/multiple_predictors.html)  
 __Lab Data:__  [Multiple Files](lab/data/categorical_data.zip)   
-__Reading:__ [Feiberg  on Multiple Predictors through](https://statistics4ecologists-v3.netlify.app/03-multipleregression), [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/), [Feiberg ch 6](https://fw8051statistics4ecologists.netlify.app/multicol.html)   
+__Reading:__ [Feiberg  on Multiple Predictors through categorical variables](https://statistics4ecologists-v3.netlify.app/03-multipleregression), [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/)   
 __Optional Reading:__ [Analysis of variance with unbalanced data: an update for ecology & evolution](./readings/Hector_et_al-2010-Journal_of_Animal_Ecology.pdf), [Day and Quinn 1989 on Post-hocs](https://www.jstor.org/stable/1943075).   
 __Packages for The Week:__ `install.packages(c(""car"", ""emmeans"", ""multcompView"", ""contrast"", ""visreg""))`    
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-many-predictors-2025  
@@ -139,7 +138,7 @@ __Homework:__ None. Catch up!
 __Lectures:__  [Many categories or mixing categories and continuous predictors](./lectures/many_types_of_predictors.html), [When one predictor's effect depends on the other](lectures/interactions_lm.html)   
 __Lab Topic:__  [Models with many types of predictors - and interactions!](lab/complex_linear_models.html)  
 __Lab Data:__  [Multiple Files](lab/data_10.zip)  
-__Reading:__  [Feiberg  on Multiple Predictors through](https://statistics4ecologists-v3.netlify.app/03-multipleregression), [Feiberg on Multicollinearity](https://statistics4ecologists-v3.netlify.app/06-multicollinearity),  [Simple means to improve the interpretability of regression coefficients](./readings/Schielzeth_2010_MEE.pdf), [Understanding ‘it depends’ in ecology: a guide to hypothesising, visualising and interpreting statistical interactions](https://onlinelibrary.wiley.com/doi/full/10.1111/brv.12939)   
+__Reading:__  [Feiberg  on Multiple Predictors all](https://statistics4ecologists-v3.netlify.app/03-multipleregression), [Feiberg on Multicollinearity](https://statistics4ecologists-v3.netlify.app/06-multicollinearity),  [Simple means to improve the interpretability of regression coefficients](./readings/Schielzeth_2010_MEE.pdf), [Understanding ‘it depends’ in ecology: a guide to hypothesising, visualising and interpreting statistical interactions](https://onlinelibrary.wiley.com/doi/full/10.1111/brv.12939)   
 __Optional Readings:__ [Interactions in statistical models: Three things to know](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13714), [Centring in regression analyses: a strategy to prevent errors in statistical inference](./readings/kramer_blasey_centering.pdf)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-complex-linear-models-2025  
 __Homework:__ [Complex linear models](homework/complex_linear_models.html)    

---FILE: schedule.html---
@@ -13,7 +13,7 @@
 
 <title>Course Schedule and Readings</title>
 
-<script src=""site_libs/header-attrs-2.29/header-attrs.js""></script>
+<script src=""site_libs/header-attrs-2.27/header-attrs.js""></script>
 <script src=""site_libs/jquery-3.6.0/jquery-3.6.0.min.js""></script>
 <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
 <link href=""site_libs/bootstrap-3.3.5/css/lumen.min.css"" rel=""stylesheet"" />
@@ -290,15 +290,14 @@ <h1 class=""title toc-ignore"">Course Schedule and Readings</h1>
 
 
 <!--
-
+NEXT TIME I TEACH 607 in 2026/7
+- Assumptions in linear regression lecture is a SLOG - make less boring
+  - separate assumptions about DGP and EGP
+  - Show clearer violations and fixes
+  - NO BIG TEXT SLIDES
+  - do not need to describe Bayes or maybe even likelihood
 
 NEXT TIME I TEACH 607 in 2024/5
-redo explaining least squares - it's not good in the lecture
-
-re-arrange the linear regression lab
-  - workflow
-  - transforms
-  - viz of prediction intervals and such
 
 Causal Intro is 2 lectures
   - End is week and flabby. Rework
@@ -524,11 +523,9 @@ <h3>Week 6</h3>
 href=""lab/data/categorical_data.zip"">Multiple Files</a><br />
 <strong>Reading:</strong> <a
 href=""https://statistics4ecologists-v3.netlify.app/03-multipleregression"">Feiberg
-on Multiple Predictors through</a>, <a
+on Multiple Predictors through categorical variables</a>, <a
 href=""https://lindeloev.github.io/tests-as-linear/"">Common statistical
-tests are linear models</a>, <a
-href=""https://fw8051statistics4ecologists.netlify.app/multicol.html"">Feiberg
-ch 6</a><br />
+tests are linear models</a><br />
 <strong>Optional Reading:</strong> <a
 href=""./readings/Hector_et_al-2010-Journal_of_Animal_Ecology.pdf"">Analysis
 of variance with unbalanced data: an update for ecology &amp;
@@ -557,7 +554,7 @@ <h3>Week 7</h3>
 Files</a><br />
 <strong>Reading:</strong> <a
 href=""https://statistics4ecologists-v3.netlify.app/03-multipleregression"">Feiberg
-on Multiple Predictors through</a>, <a
+on Multiple Predictors all</a>, <a
 href=""https://statistics4ecologists-v3.netlify.app/06-multicollinearity"">Feiberg
 on Multicollinearity</a>, <a
 href=""./readings/Schielzeth_2010_MEE.pdf"">Simple means to improve the"
biol607,biol607.github.io,d3a16a867ae1e4bb95c90a140c2e065c4333eb16,jebyrnes,jarrett.byrnes@umb.edu,2025-03-04T18:14:39Z,jebyrnes,jarrett.byrnes@umb.edu,2025-03-04T18:15:28Z,Fix incorrect equation,lectures/mlr.Rmd;lectures/mlr.html;lectures/mlr_files/figure-html/venn_mlr-1.jpeg;lectures/mlr_files/figure-html/venn_mlr_coll-1.jpeg;lectures/mlr_files/figure-html/venn_mlr_x_y-1.jpeg,True,False,True,False,4,4,8,"---FILE: lectures/mlr.Rmd---
@@ -353,10 +353,10 @@ $$\epsilon_i \sim N(0, \sigma)$$
 
 --
 
-### With n predictors
+### With K predictors
 
 
-$$y_i = \beta_0 + \sum_{j = 1}^{K} \beta_1  x_{ij} + \epsilon_i$$
+$$y_i = \beta_0 + \sum_{j = 1}^{K} \beta_j  x_{ij} + \epsilon_i$$
 $$\epsilon_i \sim N(0, \sigma)$$
 
 

---FILE: lectures/mlr.html---
@@ -177,10 +177,10 @@
 
 --
 
-### With n predictors
+### With K predictors
 
 
-`$$y_i = \beta_0 + \sum_{j = 1}^{K} \beta_1  x_{ij} + \epsilon_i$$`
+`$$y_i = \beta_0 + \sum_{j = 1}^{K} \beta_j  x_{ij} + \epsilon_i$$`
 `$$\epsilon_i \sim N(0, \sigma)$$`
 
 "
biol607,biol607.github.io,b23c5122e82693090f47a07cfae9815695a528b1,jebyrnes,jarrett.byrnes@umb.edu,2025-03-04T18:14:39Z,jebyrnes,jarrett.byrnes@umb.edu,2025-03-04T18:14:39Z,Fix incorrect equation,lectures/mlr_files/figure-html/venn_mlr-1.jpeg;lectures/mlr_files/figure-html/venn_mlr_coll-1.jpeg;lectures/mlr_files/figure-html/venn_mlr_x_y-1.jpeg,False,False,False,False,0,0,0,
biol607,biol607.github.io,c9a0c56bbcb3361132c775f20fb7b36795993ad6,jebyrnes,jarrett.byrnes@umb.edu,2025-02-26T20:47:24Z,jebyrnes,jarrett.byrnes@umb.edu,2025-02-26T20:47:24Z,update linear regression fitting,lectures/linear_regression_details.Rmd;lectures/linear_regression_details.html;lectures/linear_regression_details_files/figure-html/PredictionRange3-1.png;lectures/linear_regression_details_files/figure-html/predRange2-1.png;lectures/linear_regression_details_files/figure-html/predRange4-1.png;lectures/linear_regression_details_files/figure-html/predictionRange1-1.png;lectures/linear_regression_details_files/figure-html/pufferout-1.png;lectures/linear_regression_details_files/figure-html/pufferout_cook-1.png;lectures/linear_regression_details_files/figure-html/pufferout_leverage-1.png;lectures/linear_regression_details_files/figure-html/pufferqq-1.png;lectures/linear_regression_details_files/figure-html/puffershow-1.png;lectures/linear_regression_details_files/figure-html/sims-1.png;lectures/linear_regression_details_files/figure-html/ssm-1.png;lectures/linear_regression_details_files/figure-html/ssr-1.png;lectures/linear_regression_details_files/figure-html/sst-1.png;lectures/linear_regression_details_files/figure-html/sst2-1.png;lectures/linear_regression_details_files/figure-html/unnamed-chunk-1-1.png;lectures/linear_regression_details_files/figure-html/unnamed-chunk-4-1.png;lectures/linear_regression_details_files/figure-html/wolf_scatterplot-1.png;lectures/linear_regression_details_files/figure-html/wolfqq-1.png;lectures/linear_regression_details_files/figure-html/wolfsims-1.png,True,False,True,False,151,805,956,"---FILE: lectures/linear_regression_details.Rmd---
@@ -45,14 +45,6 @@ puffer_lm <- lm(predators ~ resemblance, data=puffer)
 ```
 
 
----
-class: center, middle
-
-# Etherpad
-<br><br>
-<center><h3>https://etherpad.wikimedia.org/p/607-lm-2022</h3></center>
-
-
 ---
 
 
@@ -284,7 +276,7 @@ Does the model seem to fit the data? Are there any deviations? Can be hard to se
 #                lwd = 2, color = ""blue"") +
 #   labs(title = ""Distribution of Predator Visits in Our Data\nand as Predicted By Model"")
 # 
-check_posterior_predictions(puffer_lm)
+check_predictions(puffer_lm)
 ```
 
 Is anything off?
@@ -306,7 +298,7 @@ Is anything off?
 #                lwd = 2, color = ""blue"") +
 #   labs(title = ""Distribution of Pups in Our Data\nand as Predicted By Model"")
 
-check_posterior_predictions(wolf_mod)
+check_predictions(wolf_mod)
 ```
 
 ---
@@ -481,7 +473,6 @@ check_outliers(wolf_mod, method = ""cook"") |> plot()
 
 2. .red[How did we fit this model?]
 
-3. How do we draw inference from this model?
 
 ---
 # So, uh.... How would you fit a line here?
@@ -537,7 +528,9 @@ for(i in 1:3){
 
 # Basic Principles of Least Squares Regression
 
-$\widehat{Y} = \beta_0 + \beta_1 X + \epsilon$ where $\beta_0$ = intercept, $\beta_1$ = slope
+$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ where $\beta_0$ = intercept, $\beta_1$ = slope. 
+
+$\epsilon_i \sim \mathcal{N}(0, \sigma)$ - the residuals
 
 ```{r linefit}
 set.seed(697)
@@ -547,14 +540,103 @@ a<-lm(y~x)
 plot(x,y,pch=19, cex=1.5)
 abline(a, lwd=2)
 segments(x,fitted(a),x,y, col=""red"", lwd=2)
+dat <- tibble(x=x, y = y,
+              mean_resid = y - mean(y),
+              fit = fitted(lm(y~x)),
+              resid = residuals(lm(y~x)))
 ``` 
+---
+
+# Basic Principles of Least Squares Regression: Total Sums of Squares
+
+$$SST = \sum (Y_i - \bar{Y})^2$$
+```{r sst}
+
+ggplot(dat, aes(x=x, y=y)) +
+  geom_point() +
+  geom_hline(yintercept = mean(dat$y), color = ""purple"") +
+  labs(
+    subtitle = ""purple = mean of y"") +
+  ylim(c(0,12))
+```
+
+---
+# Basic Principles of Least Squares Regression: Total Sums of Squares
+
+$$SST = \sum (Y_i - \bar{Y})^2$$
+
+```{r sst2}
+
+ggplot(dat, aes(x=x, y=y)) +
+  geom_point() +
+  geom_hline(yintercept = mean(dat$y), color = ""purple"") +
+  geom_segment(aes(y = mean(y),
+                   yend =  mean(y) + mean_resid, 
+                   xend = x),
+               color = ""blue"")+
+  labs(
+    subtitle = ""purple = mean of y\nblue = distance to observed y"")+
+  ylim(c(0,12))
+```
+---
+class: center, middle, large
+# Sums of Squares of a Model
+
+SST = SS Model + SS Residuals
 
+--
+
+We want to minimize SS Residuals
+
+---
+
+# Basic Principles of Least Squares Regression: Sums of Squares of the Model
+
+$$SSM = \sum (\widehat{Y_i} - \bar{Y})^2$$
+
+
+```{r ssm}
+
+ggplot(dat, aes(x=x, y=y)) +
+  geom_hline(yintercept = mean(dat$y), color = ""purple"") +
+  geom_line(aes(y = fit)) +
+  geom_segment(aes(y = mean(y),
+                   yend =  fit, 
+                   xend = x),
+               color = ""orange"", lwd = 1.5)+
+    geom_point() +
+  labs(
+    subtitle = ""purple = mean of y\norange = distance to fit y"")+
+  ylim(c(0,12))
+```
+
+---
+# Basic Principles of Least Squares Regression: Sums of Squares of the Residuals
 Minimize Residuals defined as $SS_{residuals} = \sum(Y_{i} - \widehat{Y})^2$
 
+```{r ssr}
+
+ggplot(dat, aes(x=x, y=y)) +
+  geom_hline(yintercept = mean(dat$y), color = ""purple"") +
+  geom_line(aes(y = fit)) +
+  geom_segment(aes(y = mean(y),
+                   yend =  fit, 
+                   xend = x),
+               color = ""orange"", lwd = 1.5)+
+  geom_segment(aes(y = fit,
+                   yend =  y, 
+                   xend = x),
+               color = ""red"", lwd = 1.5)+
+    geom_point() +
+  labs(
+    subtitle = ""purple = mean of y\norange = distance to fit y\nred = residual distance"")+
+  ylim(c(0,12))
+```
+
 ---
 class: center, middle
 
-# Let's try it out!
+# Is there Another Way?
 
 ---
 
@@ -586,7 +668,7 @@ $\Large \beta_0 = \bar{Y} - \beta_1  \bar{X}$
 
 ---
 
-# Least Squares Visualized
+# Least Squares Visualized for Puffers
 
 ```{r}
 library(modelr)
@@ -613,9 +695,14 @@ ggplot(puffer_m,
 --
 $L = \prod p(Data|parmeters)$
 --
-$L(\theta | D) = \prod dnorm(y_i, \mu = \beta_0 + \beta_1 x_i, \sigma)$
+$L(\theta | D) = \prod dnorm(y_i, \mu = \beta_0 + \beta_1 x_i, \sigma)$  
+
+--
+
+Deviance = -2 * Log Likelihood  
+
 --
-Deviance = -2 * Log Likelihood
+This gives us the same answer as Least Squares
 
 ---
 
@@ -676,391 +763,4 @@ plot(p_chain, ""areas"")
 - Many ways to fit   
       - We will talk inference later
       - The key is looking at estimated values and their implications
-      - Look at precision - do you feel comfortable with inference?
-
----
-
-<!-- --- -->
-
-<!-- # Digging Deeper into Regression -->
-
-<!-- 1. Assumptions: Is our fit valid?  -->
-
-<!-- 2. How did we fit this model? -->
-
-<!-- 3. .red[How do we draw inference from this model?] -->
-
-<!-- --- -->
-<!-- # Inductive v. Deductive Reasoning -->
-
-<!-- <br><br> -->
-<!-- **Deductive Inference:** A larger theory is used to devise -->
-<!-- many small tests. -->
-
-
-<!-- **Inductive Inference:** Small pieces of evidence are used -->
-<!-- to shape a larger theory and degree of belief. -->
-<!-- --- -->
-
-<!-- # Applying Different Styles of Inference -->
-
-<!-- - **Null Hypothesis Testing**: What's the probability that things are not influencing our data? -->
-<!--       - Deductive -->
-
-<!-- - **Cross-Validation**: How good are you at predicting new data? -->
-<!--       - Deductive -->
-
-<!-- - **Model Comparison**: Comparison of alternate hypotheses -->
-<!--       - Deductive or Inductive -->
-
-<!-- - **Probabilistic Inference**: What's our degree of belief in a data? -->
-<!--       - Inductive -->
-
-<!-- --- -->
-<!-- # Null Hypothesis Testing is a Form of Deductive Inference -->
-
-<!-- .pull-left[ -->
-<!-- ![:scale 55%](./images/07/Karl_Popper_wikipedia.jpeg) -->
-
-<!-- Falsification of hypotheses is key! <br><br> -->
-
-<!-- A theory should be considered scientific if, and only if, it is falsifiable. -->
-
-<!-- ] -->
-
-<!-- -- -->
-<!-- .pull-right[ -->
-<!-- ![:scale 55%](./images/regression/Professor_Imre_Lakatos,_c1960s.jpg) -->
-
-<!-- Look at a whole research program and falsify auxilliary hypotheses -->
-<!-- ] -->
-
-
-<!-- --- -->
-<!-- # A Bigger View of Dedictive Inference -->
-
-<!-- ![](./images/regression/lakatos_structure.png) -->
-
-<!-- .small[https://plato.stanford.edu/entries/lakatos/#ImprPoppScie] -->
-
-<!-- --- -->
-
-<!-- # Reifying Refutation - What is the probability something is false? -->
-
-<!-- What if our hypothesis was that the resemblance-predator relationship was 2:1. We know our SE of our estimate is 0.57, so, we have a distribution of what we **could** observe. -->
-
-<!-- ```{r slopedist, fig.height = 5} -->
-<!-- dat_slope <- tibble(x = seq(-1,5,length.out = 200), -->
-<!--                     y = dnorm(x, 2, 0.57)) -->
-
-<!-- slopedist <- ggplot(dat_slope, -->
-<!--                     aes(x = x, y = y)) + -->
-<!--   geom_line() + -->
-<!--   labs(x = ""Hypothesized Slope"", y = ""Probability Density"") -->
-
-<!-- slopedist -->
-<!-- ``` -->
-
-<!-- --- -->
-<!-- # Reifying Refutation - What is the probability something is false? -->
-
-<!-- BUT - our estimated slope is 3. -->
-
-<!-- ```{r add_obs, fig.height = 5} -->
-<!-- slopedist + -->
-<!--   geom_vline(xintercept = 3, color = ""red"", lty = 2)  -->
-<!-- ``` -->
-
-<!-- --- -->
-<!-- # To falsify the 2:1 hypothesis, we need to know the probability of observing 3, or something GREATER than 3. -->
-
-<!-- We want to know if we did this experiment again and again, what's the probability of observing what we saw or worse (frequentist!) -->
-
-<!-- ```{r add_p, fig.height = 5} -->
-<!-- dat_obs <- tibble(x = seq(3,5,length.out = 200), -->
-<!--                     y = dnorm(x, 2, 0.57), -->
-<!--                   ymin = 0) -->
-
-<!-- slopedist + -->
-<!--   geom_vline(xintercept = 3, color = ""red"", lty = 2) + -->
-<!--   geom_ribbon(aes(ymin = ymin, ymax = y),  -->
-<!--               data = dat_obs, fill = ""red"", alpha = 0.5) -->
-<!-- ``` -->
-<!-- -- -->
-
-<!-- Probability = `r round(1-pnorm(3, 2, 0.57),3)` -->
-
-<!-- -- -->
-<!-- Note: We typically would multiply this by 2 to look at extremes in both tails. -->
-
-<!-- --- -->
-<!-- class: center, middle -->
-
-<!-- # Null hypothesis testing is asking what is the probability of our observation or more extreme observation given that some null expectation is true. -->
-
-<!-- ### (it is .red[**NOT**] the probability of any particular alternate hypothesis being true) -->
-
-<!-- --- -->
-<!-- # R.A. Fisher and The P-Value For Null Hypotheses -->
-
-<!-- .pull-left[ -->
-<!-- ![](./images/07/fisher2.jpeg) -->
-<!-- ] -->
-
-<!-- .pull-right[ -->
-<!-- P-value: The Probability of making an observation or more extreme -->
-<!-- observation given that the null hypothesis is true. -->
-<!-- ] -->
-
-<!-- --- -->
-<!-- # Applying Fisher: Evaluation of a Test Statistic -->
-
-<!-- We  use our data to calculate a **test statistic** that maps to a value -->
-<!-- of the null distribution.  -->
-
-<!-- We can then calculate the probability of observing our data, or of observing data even more extreme, given that the null hypothesis is true. -->
-
-
-<!-- $$\large P(X \leq Data | H_{0})$$ -->
-
-
-<!-- --- -->
-<!-- # Problems with P -->
-
-<!-- - Most people don't understand it. -->
-<!--      - See American Statistical Society' recent statements -->
-
-<!-- -- -->
-<!-- - Like SE, it gets smaller with sample size! -->
-<!-- -- -->
-<!-- - Neyman-Pearson Null Hypothesis Significance Testing -->
-<!--      - For Industrial Quality Control, NHST was introduced to establish cutoffs of reasonable p, called an $\alpha$ -->
-<!--      - This corresponds to Confidence intervals - 1-$\alpha$ = CI of interest -->
-<!--      - This has become weaponized so that $\alpha = 0.05$ has become a norm.... and often determines if something is worthy of being published? -->
-<!--      - Chilling effect on science -->
-
-<!-- -- -->
-<!-- - We don't know how to talk about it -->
-
-<!-- --- -->
-<!-- # How do you talk about results from a p-value? -->
-
-<!-- - Based on your experimental design, what is a reasonable range of p-values to expect if the null is false -->
-
-<!-- - Smaller p values indicate stronger support for rejection, larger ones weaker. Use that language. -->
-
-<!-- - Accumulate multiple lines of evidence so that the entire edifice of your research does not rest on a single p-value!!!! -->
-
-<!-- --- -->
-<!-- # For example, what does p = 0.061 mean? -->
-
-<!-- - There is a 6.1% chance of obtaining the observed data or more extreme data given that the null hypothesis is true. -->
-
-<!-- - If you choose to reject the null, you have a ~ 1 in 16 chance of being wrong -->
-
-<!-- - Are you comfortable with that?  -->
-
-<!-- - OR - What other evidence would you need to make you more or less comfortable? -->
-
-<!-- --- -->
-
-<!-- # Common Regression Test Statistics -->
-
-<!-- - Does my model explain variability in the data? -->
-<!--      - **Null Hypothesis**: The ratio of variability from your predictors versus noise is 1 -->
-<!--      - **Test Statistic**: F distribution (describes ratio of two variances) -->
-
-<!-- - Are my coefficients not 0? -->
-<!--     - **Null Hypothesis**: Coefficients are 0   -->
-<!--     - **Test Statistic**: T distribution (normal distribution modified for low sample size) -->
-
-<!-- --- -->
-<!-- # Does my model explain variability in the data? -->
-
-<!-- Ho = The model predicts no variation in the data.   -->
-
-<!-- Ha = The model predicts variation in the data. -->
-
-<!-- -- -->
-
-<!-- To evaluate these hypotheses, we need to have a measure of variation explained by data versus error - the sums of squares! -->
-
-<!-- -- -->
-<!-- $$SS_{Total} = SS_{Regression} + SS_{Error}$$ -->
-<!-- --- -->
-
-<!-- # Sums of Squares of Error, Visually -->
-<!-- ```{r linefit} -->
-<!-- ```  -->
-
-<!-- --- -->
-<!-- # Sums of Squares of Regression, Visually -->
-<!-- ```{r grandmean} -->
-<!-- set.seed(697) -->
-
-<!-- plot(x,y,pch=19, cex=0, cex.lab=1.5, cex.axis=1.1) -->
-<!-- abline(a, lwd=2) -->
-<!-- #segments(x,fitted(a),x,y, col=""red"", lwd=2) -->
-<!-- points(mean(x), mean(y), col=""blue"", pch=15) -->
-<!-- ```  -->
-
-<!-- --- -->
-<!-- # Sums of Squares of Regression, Visually -->
-<!-- ```{r ssr} -->
-<!-- set.seed(697) -->
-
-<!-- plot(x,y,pch=19, cex=0, cex.lab=1.5, cex.axis=1.1) -->
-<!-- abline(a, lwd=2) -->
-<!-- points(mean(x), mean(y), col=""blue"", pch=15) -->
-<!-- points(x, fitted(a), col=""blue"", pch=1) -->
-<!-- ```  -->
-
-<!-- Distance from $\hat{y}$ to $\bar{y}$ -->
-
-<!-- --- -->
-<!-- # Components of the Total Sums of Squares -->
-
-<!-- $SS_{R} = \sum(\hat{Y_{i}} - \bar{Y})^{2}$, df=1 -->
-
-<!-- $SS_{E} = \sum(Y_{i} - \hat{Y}_{i})^2$, df=n-2 -->
-
-
-<!-- -- -->
-<!-- To compare them, we need to correct for different DF. This is the Mean -->
-<!-- Square. -->
-
-<!-- MS=SS/DF -->
-
-<!-- e.g, $MS_{E} = \frac{SS_{E}}{n-2}$ -->
-
-<!-- --- -->
-
-<!-- # The F Distribution and Ratios of Variances -->
-
-<!-- $F = \frac{MS_{R}}{MS_{E}}$ with DF=1,n-2  -->
-
-<!-- ```{r f} -->
-
-<!-- x<-seq(0,6,.01) -->
-<!-- qplot(x,df(x,1,25), geom=""line"",  xlab=""Y"", ylab=""df(Y)"") +  -->
-<!--   theme_bw(base_size=17) -->
-
-<!-- ``` -->
-
-<!-- --- -->
-<!-- # F-Test and Pufferfish -->
-<!-- ```{r f-puffer} -->
-<!-- knitr::kable(anova(puffer_lm)) -->
-<!-- ``` -->
-
-<!-- <br><br> -->
-<!-- -- -->
-<!-- We  reject the null hypothesis that resemblance does not explain variability in predator approaches -->
-
-<!-- --- -->
-<!-- # Testing the Coefficients -->
-
-<!--  -  F-Tests evaluate whether elements of the model contribute to variability in the data -->
-<!--       - Are modeled predictors just noise? -->
-<!--       - What's the difference between a model with only an intercept and an intercept and slope? -->
-
-<!-- -- -->
-
-<!-- - T-tests evaluate whether coefficients are different from 0 -->
-
-<!-- -- -->
-
-<!-- - Often, F and T agree - but not always -->
-<!--     - T can be more sensitive with multiple predictors -->
-
-<!-- --- -->
-<!-- background-color: black -->
-<!-- class: center, middle, inverse -->
-
-<!-- ![:scale 90%](images/09/t_distribution.png) -->
-
-<!-- .small[xkcd] -->
-
-<!-- --- -->
-<!-- background-image: url(images/09/guiness_full.jpg) -->
-<!-- background-position: center -->
-<!-- background-size: contain -->
-
-<!-- --- -->
-<!-- background-image: url(images/09/gosset.jpg) -->
-<!-- background-position: center -->
-<!-- background-size: contain -->
-
-<!-- --- -->
-<!-- # T-Distributions are What You'd Expect Sampling a Standard Normal Population with a Small Sample Size -->
-
-<!-- - t = mean/SE, DF = n-1 -->
-<!-- - It assumes a normal population with mean of 0 and SD of 1 -->
-
-<!-- ```{r dist_shape_t, fig.height=5} -->
-<!-- x_dists <- data.frame(x=seq(-2.5, 2.5, 0.01)) %>% -->
-<!--   mutate(dn = dnorm(x), -->
-<!--          dt_1 = dt(x, 1), -->
-<!--          dt_2 = dt(x, 2), -->
-<!--          dt_3 = dt(x, 3) -->
-<!--   ) -->
-
-<!-- x_df <- data.frame(x=rnorm(100), x_unif=runif(100)) -->
-
-<!-- ggplot() + -->
-<!--   geom_line(data=x_dists, mapping=aes(x=x, y=dn)) + -->
-<!--   geom_line(data=x_dists, mapping=aes(x=x, y=dt_1), color=""red"") + -->
-<!--   geom_line(data=x_dists, mapping=aes(x=x, y=dt_2), color=""orange"") + -->
-<!--   geom_line(data=x_dists, mapping=aes(x=x, y=dt_3), color=""blue"") + -->
-<!--   theme_classic(base_size=14) + -->
-<!--   annotate(x=c(0.2,0.7,1.1,1.2), y=c(0.4, 0.3, 0.2, 0.1),  -->
-<!--              label=c(""Normal"",""3DF"", ""2DF"", ""1DF""), fill=""white"", -->
-<!--             fontface = ""bold"", geom=""label"") + -->
-<!--   ylab(""density"") -->
-<!-- ``` -->
-
-<!-- --- -->
-<!-- # Error in the Slope Estimate -->
-<!-- <br> -->
-
-
-<!-- $\Large SE_{b} = \sqrt{\frac{MS_{E}}{SS_{X}}}$ -->
-
-
-
-<!-- #### 95% CI = $b \pm t_{\alpha,df}SE_{b}$   -->
-
-<!-- (~ 1.96 when N is large) -->
-
-
-<!-- --- -->
-<!-- # Assessing the Slope with a T-Test -->
-<!-- <br> -->
-<!-- $$\Large t_{b} = \frac{b - \beta_{0}}{SE_{b}}$$  -->
-
-<!-- ##### DF=n-2 -->
-
-<!-- $H_0: \beta_{0} = 0$, but we can test other hypotheses -->
-
-<!-- --- -->
-<!-- # Slope of Puffer Relationship (DF = 1 for Parameter Tests) -->
-<!-- ```{r puffer_t} -->
-<!-- knitr::kable(coef(summary(puffer_lm))) -->
-<!-- ``` -->
-
-<!-- <Br> -->
-<!-- We reject the hypothesis of no slope for resemblance, but fail to reject it for the intercept. -->
-
-<!-- --- -->
-
-<!-- # So, what can we say? -->
-
-<!-- .pull-left[ -->
-<!-- - We reject that there is no relationship between resemblance and predator visits in our experiment.  -->
-<!-- - `r round(summary(puffer_lm)$r.squared, 2)` of the variability in predator visits is associated with resemblance.  -->
-<!-- ] -->
-
-<!-- .pull-right[ -->
-<!-- ```{r puffershow} -->
-<!-- ``` -->
-<!-- ] -->
\ No newline at end of file
+      - Look at precision - do you feel comfortable with inference?
\ No newline at end of file

---FILE: lectures/linear_regression_details.html---
@@ -24,14 +24,6 @@
 
 
 
----
-class: center, middle
-
-# Etherpad
-&lt;br&gt;&lt;br&gt;
-&lt;center&gt;&lt;h3&gt;https://etherpad.wikimedia.org/p/607-lm-2022&lt;/h3&gt;&lt;/center&gt;
-
-
 ---
 
 
@@ -344,7 +336,6 @@
 
 2. .red[How did we fit this model?]
 
-3. How do we draw inference from this model?
 
 ---
 # So, uh.... How would you fit a line here?
@@ -383,16 +374,53 @@
 
 # Basic Principles of Least Squares Regression
 
-`\(\widehat{Y} = \beta_0 + \beta_1 X + \epsilon\)` where `\(\beta_0\)` = intercept, `\(\beta_1\)` = slope
+`\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)` where `\(\beta_0\)` = intercept, `\(\beta_1\)` = slope. 
+
+`\(\epsilon_i \sim \mathcal{N}(0, \sigma)\)` - the residuals
 
 &lt;img src=""linear_regression_details_files/figure-html/linefit-1.png"" style=""display: block; margin: auto;"" /&gt;
+---
+
+# Basic Principles of Least Squares Regression: Total Sums of Squares
+
+`$$SST = \sum (Y_i - \bar{Y})^2$$`
+&lt;img src=""linear_regression_details_files/figure-html/sst-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Basic Principles of Least Squares Regression: Total Sums of Squares
+
+`$$SST = \sum (Y_i - \bar{Y})^2$$`
+
+&lt;img src=""linear_regression_details_files/figure-html/sst2-1.png"" style=""display: block; margin: auto;"" /&gt;
+---
+class: center, middle, large
+# Sums of Squares of a Model
+
+SST = SS Model + SS Residuals
+
+--
+
+We want to minimize SS Residuals
+
+---
 
+# Basic Principles of Least Squares Regression: Sums of Squares of the Model
+
+`$$SSM = \sum (\widehat{Y_i} - \bar{Y})^2$$`
+
+
+&lt;img src=""linear_regression_details_files/figure-html/ssm-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Basic Principles of Least Squares Regression: Sums of Squares of the Residuals
 Minimize Residuals defined as `\(SS_{residuals} = \sum(Y_{i} - \widehat{Y})^2\)`
 
+&lt;img src=""linear_regression_details_files/figure-html/ssr-1.png"" style=""display: block; margin: auto;"" /&gt;
+
 ---
 class: center, middle
 
-# Let's try it out!
+# Is there Another Way?
 
 ---
 
@@ -424,7 +452,7 @@
 
 ---
 
-# Least Squares Visualized
+# Least Squares Visualized for Puffers
 
 &lt;img src=""linear_regression_details_files/figure-html/unnamed-chunk-1-1.png"" style=""display: block; margin: auto;"" /&gt;
 
@@ -440,9 +468,14 @@
 --
 `\(L = \prod p(Data|parmeters)\)`
 --
-`\(L(\theta | D) = \prod dnorm(y_i, \mu = \beta_0 + \beta_1 x_i, \sigma)\)`
+`\(L(\theta | D) = \prod dnorm(y_i, \mu = \beta_0 + \beta_1 x_i, \sigma)\)`  
+
+--
+
+Deviance = -2 * Log Likelihood  
+
 --
-Deviance = -2 * Log Likelihood
+This gives us the same answer as Least Squares
 
 ---
 
@@ -497,393 +530,6 @@
       - We will talk inference later
       - The key is looking at estimated values and their implications
       - Look at precision - do you feel comfortable with inference?
-
----
-
-&lt;!-- --- --&gt;
-
-&lt;!-- # Digging Deeper into Regression --&gt;
-
-&lt;!-- 1. Assumptions: Is our fit valid?  --&gt;
-
-&lt;!-- 2. How did we fit this model? --&gt;
-
-&lt;!-- 3. .red[How do we draw inference from this model?] --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Inductive v. Deductive Reasoning --&gt;
-
-&lt;!-- &lt;br&gt;&lt;br&gt; --&gt;
-&lt;!-- **Deductive Inference:** A larger theory is used to devise --&gt;
-&lt;!-- many small tests. --&gt;
-
-
-&lt;!-- **Inductive Inference:** Small pieces of evidence are used --&gt;
-&lt;!-- to shape a larger theory and degree of belief. --&gt;
-&lt;!-- --- --&gt;
-
-&lt;!-- # Applying Different Styles of Inference --&gt;
-
-&lt;!-- - **Null Hypothesis Testing**: What's the probability that things are not influencing our data? --&gt;
-&lt;!--       - Deductive --&gt;
-
-&lt;!-- - **Cross-Validation**: How good are you at predicting new data? --&gt;
-&lt;!--       - Deductive --&gt;
-
-&lt;!-- - **Model Comparison**: Comparison of alternate hypotheses --&gt;
-&lt;!--       - Deductive or Inductive --&gt;
-
-&lt;!-- - **Probabilistic Inference**: What's our degree of belief in a data? --&gt;
-&lt;!--       - Inductive --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Null Hypothesis Testing is a Form of Deductive Inference --&gt;
-
-&lt;!-- .pull-left[ --&gt;
-&lt;!-- ![:scale 55%](./images/07/Karl_Popper_wikipedia.jpeg) --&gt;
-
-&lt;!-- Falsification of hypotheses is key! &lt;br&gt;&lt;br&gt; --&gt;
-
-&lt;!-- A theory should be considered scientific if, and only if, it is falsifiable. --&gt;
-
-&lt;!-- ] --&gt;
-
-&lt;!-- -- --&gt;
-&lt;!-- .pull-right[ --&gt;
-&lt;!-- ![:scale 55%](./images/regression/Professor_Imre_Lakatos,_c1960s.jpg) --&gt;
-
-&lt;!-- Look at a whole research program and falsify auxilliary hypotheses --&gt;
-&lt;!-- ] --&gt;
-
-
-&lt;!-- --- --&gt;
-&lt;!-- # A Bigger View of Dedictive Inference --&gt;
-
-&lt;!-- ![](./images/regression/lakatos_structure.png) --&gt;
-
-&lt;!-- .small[https://plato.stanford.edu/entries/lakatos/#ImprPoppScie] --&gt;
-
-&lt;!-- --- --&gt;
-
-&lt;!-- # Reifying Refutation - What is the probability something is false? --&gt;
-
-&lt;!-- What if our hypothesis was that the resemblance-predator relationship was 2:1. We know our SE of our estimate is 0.57, so, we have a distribution of what we **could** observe. --&gt;
-
-&lt;!-- ```{r slopedist, fig.height = 5} --&gt;
-&lt;!-- dat_slope &lt;- tibble(x = seq(-1,5,length.out = 200), --&gt;
-&lt;!--                     y = dnorm(x, 2, 0.57)) --&gt;
-
-&lt;!-- slopedist &lt;- ggplot(dat_slope, --&gt;
-&lt;!--                     aes(x = x, y = y)) + --&gt;
-&lt;!--   geom_line() + --&gt;
-&lt;!--   labs(x = ""Hypothesized Slope"", y = ""Probability Density"") --&gt;
-
-&lt;!-- slopedist --&gt;
-&lt;!-- ``` --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Reifying Refutation - What is the probability something is false? --&gt;
-
-&lt;!-- BUT - our estimated slope is 3. --&gt;
-
-&lt;!-- ```{r add_obs, fig.height = 5} --&gt;
-&lt;!-- slopedist + --&gt;
-&lt;!--   geom_vline(xintercept = 3, color = ""red"", lty = 2)  --&gt;
-&lt;!-- ``` --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # To falsify the 2:1 hypothesis, we need to know the probability of observing 3, or something GREATER than 3. --&gt;
-
-&lt;!-- We want to know if we did this experiment again and again, what's the probability of observing what we saw or worse (frequentist!) --&gt;
-
-&lt;!-- ```{r add_p, fig.height = 5} --&gt;
-&lt;!-- dat_obs &lt;- tibble(x = seq(3,5,length.out = 200), --&gt;
-&lt;!--                     y = dnorm(x, 2, 0.57), --&gt;
-&lt;!--                   ymin = 0) --&gt;
-
-&lt;!-- slopedist + --&gt;
-&lt;!--   geom_vline(xintercept = 3, color = ""red"", lty = 2) + --&gt;
-&lt;!--   geom_ribbon(aes(ymin = ymin, ymax = y),  --&gt;
-&lt;!--               data = dat_obs, fill = ""red"", alpha = 0.5) --&gt;
-&lt;!-- ``` --&gt;
-&lt;!-- -- --&gt;
-
-&lt;!-- Probability = 0.04 --&gt;
-
-&lt;!-- -- --&gt;
-&lt;!-- Note: We typically would multiply this by 2 to look at extremes in both tails. --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- class: center, middle --&gt;
-
-&lt;!-- # Null hypothesis testing is asking what is the probability of our observation or more extreme observation given that some null expectation is true. --&gt;
-
-&lt;!-- ### (it is .red[**NOT**] the probability of any particular alternate hypothesis being true) --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # R.A. Fisher and The P-Value For Null Hypotheses --&gt;
-
-&lt;!-- .pull-left[ --&gt;
-&lt;!-- ![](./images/07/fisher2.jpeg) --&gt;
-&lt;!-- ] --&gt;
-
-&lt;!-- .pull-right[ --&gt;
-&lt;!-- P-value: The Probability of making an observation or more extreme --&gt;
-&lt;!-- observation given that the null hypothesis is true. --&gt;
-&lt;!-- ] --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Applying Fisher: Evaluation of a Test Statistic --&gt;
-
-&lt;!-- We  use our data to calculate a **test statistic** that maps to a value --&gt;
-&lt;!-- of the null distribution.  --&gt;
-
-&lt;!-- We can then calculate the probability of observing our data, or of observing data even more extreme, given that the null hypothesis is true. --&gt;
-
-
-&lt;!-- `$$\large P(X \leq Data | H_{0})$$` --&gt;
-
-
-&lt;!-- --- --&gt;
-&lt;!-- # Problems with P --&gt;
-
-&lt;!-- - Most people don't understand it. --&gt;
-&lt;!--      - See American Statistical Society' recent statements --&gt;
-
-&lt;!-- -- --&gt;
-&lt;!-- - Like SE, it gets smaller with sample size! --&gt;
-&lt;!-- -- --&gt;
-&lt;!-- - Neyman-Pearson Null Hypothesis Significance Testing --&gt;
-&lt;!--      - For Industrial Quality Control, NHST was introduced to establish cutoffs of reasonable p, called an `\(\alpha\)` --&gt;
-&lt;!--      - This corresponds to Confidence intervals - 1-$\alpha$ = CI of interest --&gt;
-&lt;!--      - This has become weaponized so that `\(\alpha = 0.05\)` has become a norm.... and often determines if something is worthy of being published? --&gt;
-&lt;!--      - Chilling effect on science --&gt;
-
-&lt;!-- -- --&gt;
-&lt;!-- - We don't know how to talk about it --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # How do you talk about results from a p-value? --&gt;
-
-&lt;!-- - Based on your experimental design, what is a reasonable range of p-values to expect if the null is false --&gt;
-
-&lt;!-- - Smaller p values indicate stronger support for rejection, larger ones weaker. Use that language. --&gt;
-
-&lt;!-- - Accumulate multiple lines of evidence so that the entire edifice of your research does not rest on a single p-value!!!! --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # For example, what does p = 0.061 mean? --&gt;
-
-&lt;!-- - There is a 6.1% chance of obtaining the observed data or more extreme data given that the null hypothesis is true. --&gt;
-
-&lt;!-- - If you choose to reject the null, you have a ~ 1 in 16 chance of being wrong --&gt;
-
-&lt;!-- - Are you comfortable with that?  --&gt;
-
-&lt;!-- - OR - What other evidence would you need to make you more or less comfortable? --&gt;
-
-&lt;!-- --- --&gt;
-
-&lt;!-- # Common Regression Test Statistics --&gt;
-
-&lt;!-- - Does my model explain variability in the data? --&gt;
-&lt;!--      - **Null Hypothesis**: The ratio of variability from your predictors versus noise is 1 --&gt;
-&lt;!--      - **Test Statistic**: F distribution (describes ratio of two variances) --&gt;
-
-&lt;!-- - Are my coefficients not 0? --&gt;
-&lt;!--     - **Null Hypothesis**: Coefficients are 0   --&gt;
-&lt;!--     - **Test Statistic**: T distribution (normal distribution modified for low sample size) --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Does my model explain variability in the data? --&gt;
-
-&lt;!-- Ho = The model predicts no variation in the data.   --&gt;
-
-&lt;!-- Ha = The model predicts variation in the data. --&gt;
-
-&lt;!-- -- --&gt;
-
-&lt;!-- To evaluate these hypotheses, we need to have a measure of variation explained by data versus error - the sums of squares! --&gt;
-
-&lt;!-- -- --&gt;
-&lt;!-- `$$SS_{Total} = SS_{Regression} + SS_{Error}$$` --&gt;
-&lt;!-- --- --&gt;
-
-&lt;!-- # Sums of Squares of Error, Visually --&gt;
-&lt;!-- ```{r linefit} --&gt;
-&lt;!-- ```  --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Sums of Squares of Regression, Visually --&gt;
-&lt;!-- ```{r grandmean} --&gt;
-&lt;!-- set.seed(697) --&gt;
-
-&lt;!-- plot(x,y,pch=19, cex=0, cex.lab=1.5, cex.axis=1.1) --&gt;
-&lt;!-- abline(a, lwd=2) --&gt;
-&lt;!-- #segments(x,fitted(a),x,y, col=""red"", lwd=2) --&gt;
-&lt;!-- points(mean(x), mean(y), col=""blue"", pch=15) --&gt;
-&lt;!-- ```  --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Sums of Squares of Regression, Visually --&gt;
-&lt;!-- ```{r ssr} --&gt;
-&lt;!-- set.seed(697) --&gt;
-
-&lt;!-- plot(x,y,pch=19, cex=0, cex.lab=1.5, cex.axis=1.1) --&gt;
-&lt;!-- abline(a, lwd=2) --&gt;
-&lt;!-- points(mean(x), mean(y), col=""blue"", pch=15) --&gt;
-&lt;!-- points(x, fitted(a), col=""blue"", pch=1) --&gt;
-&lt;!-- ```  --&gt;
-
-&lt;!-- Distance from `\(\hat{y}\)` to `\(\bar{y}\)` --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Components of the Total Sums of Squares --&gt;
-
-&lt;!-- `\(SS_{R} = \sum(\hat{Y_{i}} - \bar{Y})^{2}\)`, df=1 --&gt;
-
-&lt;!-- `\(SS_{E} = \sum(Y_{i} - \hat{Y}_{i})^2\)`, df=n-2 --&gt;
-
-
-&lt;!-- -- --&gt;
-&lt;!-- To compare them, we need to correct for different DF. This is the Mean --&gt;
-&lt;!-- Square. --&gt;
-
-&lt;!-- MS=SS/DF --&gt;
-
-&lt;!-- e.g, `\(MS_{E} = \frac{SS_{E}}{n-2}\)` --&gt;
-
-&lt;!-- --- --&gt;
-
-&lt;!-- # The F Distribution and Ratios of Variances --&gt;
-
-&lt;!-- `\(F = \frac{MS_{R}}{MS_{E}}\)` with DF=1,n-2  --&gt;
-
-&lt;!-- ```{r f} --&gt;
-
-&lt;!-- x&lt;-seq(0,6,.01) --&gt;
-&lt;!-- qplot(x,df(x,1,25), geom=""line"",  xlab=""Y"", ylab=""df(Y)"") +  --&gt;
-&lt;!--   theme_bw(base_size=17) --&gt;
-
-&lt;!-- ``` --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # F-Test and Pufferfish --&gt;
-&lt;!-- ```{r f-puffer} --&gt;
-&lt;!-- knitr::kable(anova(puffer_lm)) --&gt;
-&lt;!-- ``` --&gt;
-
-&lt;!-- &lt;br&gt;&lt;br&gt; --&gt;
-&lt;!-- -- --&gt;
-&lt;!-- We  reject the null hypothesis that resemblance does not explain variability in predator approaches --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Testing the Coefficients --&gt;
-
-&lt;!--  -  F-Tests evaluate whether elements of the model contribute to variability in the data --&gt;
-&lt;!--       - Are modeled predictors just noise? --&gt;
-&lt;!--       - What's the difference between a model with only an intercept and an intercept and slope? --&gt;
-
-&lt;!-- -- --&gt;
-
-&lt;!-- - T-tests evaluate whether coefficients are different from 0 --&gt;
-
-&lt;!-- -- --&gt;
-
-&lt;!-- - Often, F and T agree - but not always --&gt;
-&lt;!--     - T can be more sensitive with multiple predictors --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- background-color: black --&gt;
-&lt;!-- class: center, middle, inverse --&gt;
-
-&lt;!-- ![:scale 90%](images/09/t_distribution.png) --&gt;
-
-&lt;!-- .small[xkcd] --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- background-image: url(images/09/guiness_full.jpg) --&gt;
-&lt;!-- background-position: center --&gt;
-&lt;!-- background-size: contain --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- background-image: url(images/09/gosset.jpg) --&gt;
-&lt;!-- background-position: center --&gt;
-&lt;!-- background-size: contain --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # T-Distributions are What You'd Expect Sampling a Standard Normal Population with a Small Sample Size --&gt;
-
-&lt;!-- - t = mean/SE, DF = n-1 --&gt;
-&lt;!-- - It assumes a normal population with mean of 0 and SD of 1 --&gt;
-
-&lt;!-- ```{r dist_shape_t, fig.height=5} --&gt;
-&lt;!-- x_dists &lt;- data.frame(x=seq(-2.5, 2.5, 0.01)) %&gt;% --&gt;
-&lt;!--   mutate(dn = dnorm(x), --&gt;
-&lt;!--          dt_1 = dt(x, 1), --&gt;
-&lt;!--          dt_2 = dt(x, 2), --&gt;
-&lt;!--          dt_3 = dt(x, 3) --&gt;
-&lt;!--   ) --&gt;
-
-&lt;!-- x_df &lt;- data.frame(x=rnorm(100), x_unif=runif(100)) --&gt;
-
-&lt;!-- ggplot() + --&gt;
-&lt;!--   geom_line(data=x_dists, mapping=aes(x=x, y=dn)) + --&gt;
-&lt;!--   geom_line(data=x_dists, mapping=aes(x=x, y=dt_1), color=""red"") + --&gt;
-&lt;!--   geom_line(data=x_dists, mapping=aes(x=x, y=dt_2), color=""orange"") + --&gt;
-&lt;!--   geom_line(data=x_dists, mapping=aes(x=x, y=dt_3), color=""blue"") + --&gt;
-&lt;!--   theme_classic(base_size=14) + --&gt;
-&lt;!--   annotate(x=c(0.2,0.7,1.1,1.2), y=c(0.4, 0.3, 0.2, 0.1),  --&gt;
-&lt;!--              label=c(""Normal"",""3DF"", ""2DF"", ""1DF""), fill=""white"", --&gt;
-&lt;!--             fontface = ""bold"", geom=""label"") + --&gt;
-&lt;!--   ylab(""density"") --&gt;
-&lt;!-- ``` --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Error in the Slope Estimate --&gt;
-&lt;!-- &lt;br&gt; --&gt;
-
-
-&lt;!-- `\(\Large SE_{b} = \sqrt{\frac{MS_{E}}{SS_{X}}}\)` --&gt;
-
-
-
-&lt;!-- #### 95% CI = `\(b \pm t_{\alpha,df}SE_{b}\)`   --&gt;
-
-&lt;!-- (~ 1.96 when N is large) --&gt;
-
-
-&lt;!-- --- --&gt;
-&lt;!-- # Assessing the Slope with a T-Test --&gt;
-&lt;!-- &lt;br&gt; --&gt;
-&lt;!-- `$$\Large t_{b} = \frac{b - \beta_{0}}{SE_{b}}$$`  --&gt;
-
-&lt;!-- ##### DF=n-2 --&gt;
-
-&lt;!-- `\(H_0: \beta_{0} = 0\)`, but we can test other hypotheses --&gt;
-
-&lt;!-- --- --&gt;
-&lt;!-- # Slope of Puffer Relationship (DF = 1 for Parameter Tests) --&gt;
-&lt;!-- ```{r puffer_t} --&gt;
-&lt;!-- knitr::kable(coef(summary(puffer_lm))) --&gt;
-&lt;!-- ``` --&gt;
-
-&lt;!-- &lt;Br&gt; --&gt;
-&lt;!-- We reject the hypothesis of no slope for resemblance, but fail to reject it for the intercept. --&gt;
-
-&lt;!-- --- --&gt;
-
-&lt;!-- # So, what can we say? --&gt;
-
-&lt;!-- .pull-left[ --&gt;
-&lt;!-- - We reject that there is no relationship between resemblance and predator visits in our experiment.  --&gt;
-&lt;!-- - 0.6 of the variability in predator visits is associated with resemblance.  --&gt;
-&lt;!-- ] --&gt;
-
-&lt;!-- .pull-right[ --&gt;
-&lt;!-- ```{r puffershow} --&gt;
-&lt;!-- ``` --&gt;
-&lt;!-- ] --&gt;
     </textarea>
 <style data-target=""print-only"">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
 <script src=""https://remarkjs.com/downloads/remark-latest.min.js""></script>"
biol607,biol607.github.io,251241d7685156bb9d2e5f436cad6e2e34b14126,jebyrnes,jarrett.byrnes@umb.edu,2023-11-20T15:38:15Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-20T15:38:15Z,typo fix,schedule.Rmd;schedule.html,True,False,True,False,2,2,4,"---FILE: schedule.Rmd---
@@ -206,7 +206,7 @@ __Etherpad:__ https://etherpad.wikimedia.org/p/607-causal-expts-2023
 ```
 `r datestring`   
 **Thanksgiving Week**. 
-__Paper Discussion:__  Discussion of [papers](readings/obs_papers.zip) - see [here](https://docs.google.com/spreadsheets/d/1r0VfnAJ3tuJufnLnrVC6Nx0qiyP4l6QILarKTsduov0/edit?usp=sharing) for assignments    
+__Paper Discussion:__  Discussion of [papers](readings/obs_sampling.zip) - see [here](https://docs.google.com/spreadsheets/d/1r0VfnAJ3tuJufnLnrVC6Nx0qiyP4l6QILarKTsduov0/edit?usp=sharing) for assignments    
 __Optional Reading:__ [Collider Bias and Covid](readings/griffith_collider_bias_covid.pdf)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-obs-2023  
 

---FILE: schedule.html---
@@ -709,7 +709,7 @@ <h3>Week 11.</h3>
 <h3>Week 12.</h3>
 <p>11/20/2023<br />
 <strong>Thanksgiving Week</strong>. <strong>Paper Discussion:</strong>
-Discussion of <a href=""readings/obs_papers.zip"">papers</a> - see <a
+Discussion of <a href=""readings/obs_sampling.zip"">papers</a> - see <a
 href=""https://docs.google.com/spreadsheets/d/1r0VfnAJ3tuJufnLnrVC6Nx0qiyP4l6QILarKTsduov0/edit?usp=sharing"">here</a>
 for assignments<br />
 <strong>Optional Reading:</strong> <a"
biol607,biol607.github.io,40df06bceca085afaaa7dfbbd7f8f562e59f3526,jebyrnes,jarrett.byrnes@umb.edu,2023-11-18T16:01:09Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-18T16:01:09Z,typo fix,lectures/causal_observational.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,01f81885f9779eb3cde20fc9d82636ef7a5e4617,jebyrnes,jarrett.byrnes@umb.edu,2023-11-17T13:36:24Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-17T13:36:24Z,fix interstitials,lectures/causal_observational.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,fed6cb6b7cafa6ae8ceaf546916d8af87177a855,jebyrnes,jarrett.byrnes@umb.edu,2023-11-17T04:34:00Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-17T04:34:00Z,Typo fix,lectures/05_data_viz_principles.pptx;lectures/causality_experiments.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,f5815c2a92c21f9383158dc2c32d27cb294c8afc,jebyrnes,jarrett.byrnes@umb.edu,2023-11-14T19:10:57Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-14T19:10:57Z,fix function error,exams/exam_2023.html,False,False,False,False,1,1,2,"---FILE: exams/exam_2023.html---
@@ -177,7 +177,7 @@ <h3 class=""anchored"" data-anchor-id=""a.-access"">2a. Access</h3>
 </section>
 <section id=""b.-its-big-and-wide"" class=""level3"">
 <h3 class=""anchored"" data-anchor-id=""b.-its-big-and-wide"">2b. It’s big and wide!</h3>
-<p>The data is, well, huge. It’s also wide, with dates as columns. Write a function that, given a state name as its input argument, will output a time series (long data where every row is a day) with columns for date, new daily cases and for cumulative cases in that state.</p>
+<p>The data is, well, huge. It’s also wide, with dates as columns. Write a workflow that will filter to a state (your choice which one!), and then returns a <!-- Write a function that, given a state name as its input argument, will output a--> time series (long data where every row is a day) with columns for date, new daily cases and for cumulative cases in that state.</p>
 <p>Note, let’s make the date column that emerges a true date object. Let’s say you’ve called it <code>date_col</code>. If you mutate it, <code>mutate(date_col = lubridate::mdy(date_col))</code>, it will be turned into a date object that will have a recognized order. {lubridate} is da bomb, and I’m hoping we have some time to cover it in the future.</p>
 <p>Even better - impress yourself (if you want - not required!) and merge it with some other data source to also return cases per 100,000 people.</p>
 </section>"
biol607,biol607.github.io,a38ccc446cac798ac130059f03ca8b20b80786e0,jebyrnes,jarrett.byrnes@umb.edu,2023-11-14T15:47:52Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-14T15:47:52Z,Small fixes,lectures/causal_diagrams.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,56453d3a9eb6d5909cfc3dfb6bf80885f04df096,jebyrnes,jarrett.byrnes@umb.edu,2023-11-08T17:17:20Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-08T17:17:20Z,fix numbering,exams/exam_2023.html,False,False,False,False,21,21,42,"---FILE: exams/exam_2023.html---
@@ -108,13 +108,13 @@ <h2 id=""toc-title"">Table of contents</h2>
   </ul></li>
   <li><a href=""#mix-it-up"" id=""toc-mix-it-up"" class=""nav-link"" data-scroll-target=""#mix-it-up"">5. Mix it up!</a>
   <ul class=""collapse"">
-  <li><a href=""#load-it-up-and-plot."" id=""toc-load-it-up-and-plot."" class=""nav-link"" data-scroll-target=""#load-it-up-and-plot."">5.1 Load it up and plot.</a></li>
-  <li><a href=""#are-you-over-it"" id=""toc-are-you-over-it"" class=""nav-link"" data-scroll-target=""#are-you-over-it"">5.2 Are you <em>over</em> it?</a></li>
-  <li><a href=""#fixed-or-random-intercept"" id=""toc-fixed-or-random-intercept"" class=""nav-link"" data-scroll-target=""#fixed-or-random-intercept"">5.3 Fixed or Random Intercept</a></li>
-  <li><a href=""#what-did-we-do"" id=""toc-what-did-we-do"" class=""nav-link"" data-scroll-target=""#what-did-we-do"">5.4 What did we do</a></li>
-  <li><a href=""#changes-in-parameters"" id=""toc-changes-in-parameters"" class=""nav-link"" data-scroll-target=""#changes-in-parameters"">5.5 Changes in Parameters</a></li>
-  <li><a href=""#dont-soak-up-the-good-stuff"" id=""toc-dont-soak-up-the-good-stuff"" class=""nav-link"" data-scroll-target=""#dont-soak-up-the-good-stuff"">5.5 Don’t Soak up the Good Stuff!</a></li>
-  <li><a href=""#model-evaluation"" id=""toc-model-evaluation"" class=""nav-link"" data-scroll-target=""#model-evaluation"">5.6 Model evaluation</a></li>
+  <li><a href=""#a.-load-it-up-and-plot"" id=""toc-a.-load-it-up-and-plot"" class=""nav-link"" data-scroll-target=""#a.-load-it-up-and-plot"">5a. Load it up and plot</a></li>
+  <li><a href=""#b.-are-you-over-it"" id=""toc-b.-are-you-over-it"" class=""nav-link"" data-scroll-target=""#b.-are-you-over-it"">5b. Are you <em>over</em> it?</a></li>
+  <li><a href=""#c.-fixed-or-random-intercept"" id=""toc-c.-fixed-or-random-intercept"" class=""nav-link"" data-scroll-target=""#c.-fixed-or-random-intercept"">5c. Fixed or Random Intercept</a></li>
+  <li><a href=""#d.-what-did-we-do"" id=""toc-d.-what-did-we-do"" class=""nav-link"" data-scroll-target=""#d.-what-did-we-do"">5d. What did we do</a></li>
+  <li><a href=""#e.-changes-in-parameters"" id=""toc-e.-changes-in-parameters"" class=""nav-link"" data-scroll-target=""#e.-changes-in-parameters"">5e. Changes in Parameters</a></li>
+  <li><a href=""#f.-dont-soak-up-the-good-stuff"" id=""toc-f.-dont-soak-up-the-good-stuff"" class=""nav-link"" data-scroll-target=""#f.-dont-soak-up-the-good-stuff"">5f. Don’t Soak up the Good Stuff!</a></li>
+  <li><a href=""#g.-model-evaluation"" id=""toc-g.-model-evaluation"" class=""nav-link"" data-scroll-target=""#g.-model-evaluation"">5g. Model evaluation</a></li>
   <li><a href=""#impress-yourself-variable-slopes"" id=""toc-impress-yourself-variable-slopes"" class=""nav-link"" data-scroll-target=""#impress-yourself-variable-slopes"">IMPRESS YOURSELF! Variable slopes</a></li>
   </ul></li>
   <li><a href=""#midterm-self-evaluation"" id=""toc-midterm-self-evaluation"" class=""nav-link"" data-scroll-target=""#midterm-self-evaluation"">Midterm Self-Evaluation</a>
@@ -244,8 +244,8 @@ <h3 class=""anchored"" data-anchor-id=""e.-inmpress-yourself-prediction-intervals-f
 <section id=""mix-it-up"" class=""level2"">
 <h2 class=""anchored"" data-anchor-id=""mix-it-up"">5. Mix it up!</h2>
 <p>To explore the consequences of random effects, we are going to look at an example from Vonesh, J. R. and Bolker, B. M. (2005). Compensatory larval responses shift trade-offs associated with predator-induced hatching plasticity. Ecology, 86:1580–1591. In the paper, one thing they explore is the effect of tank size, predator presence, and prey density on larval tadpoles. They go on to look at induced plastic defenses in those tadpoles. It’s a cool paper, and worth a look! But, it also shows something really interesting about logistic regression models - namely, when should we consider them as mixed models.</p>
-<section id=""load-it-up-and-plot."" class=""level3"">
-<h3 class=""anchored"" data-anchor-id=""load-it-up-and-plot."">5.1 Load it up and plot.</h3>
+<section id=""a.-load-it-up-and-plot"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""a.-load-it-up-and-plot"">5a. Load it up and plot</h3>
 <p>Load the data with the following code:</p>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb1""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb1-1""><a href=""#cb1-1"" aria-hidden=""true"" tabindex=""-1""></a>reedfrogs <span class=""ot"">&lt;-</span> <span class=""fu"">read_delim</span>(<span class=""st"">""https://github.com/rmcelreath/rethinking/raw/master/data/reedfrogs.csv""</span>,</span>
@@ -262,28 +262,28 @@ <h3 class=""anchored"" data-anchor-id=""load-it-up-and-plot."">5.1 Load it up and pl
 </div>
 <p>Now with the expanded data, plot it showing who lived, who died (who tells their story…), and how that corresponds to treatment AND tank. Do you see anything to do with within-tank variability?</p>
 </section>
-<section id=""are-you-over-it"" class=""level3"">
-<h3 class=""anchored"" data-anchor-id=""are-you-over-it"">5.2 Are you <em>over</em> it?</h3>
+<section id=""b.-are-you-over-it"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""b.-are-you-over-it"">5b. Are you <em>over</em> it?</h3>
 <p>Often, we treat data like this as a binomial logistic regression. We look at each tank as a set of ‘coin flips’ - some living some dead. Fit a glm here with survivorship determined by pred*size, and then evaluate it for overdispersion. Careful to adjust for number of individuals stocked in a tank! What do you see when you look at overdispersion?</p>
 </section>
-<section id=""fixed-or-random-intercept"" class=""level3"">
-<h3 class=""anchored"" data-anchor-id=""fixed-or-random-intercept"">5.3 Fixed or Random Intercept</h3>
+<section id=""c.-fixed-or-random-intercept"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""c.-fixed-or-random-intercept"">5c. Fixed or Random Intercept</h3>
 <p>One way to fix this problem is a quasi-binomial. But, this is unsatisfying, as it’s a posthoc adjustment and not figured into the likelihood. But, another is to think in this case about what is happening in each tank. Fit a mixed model version of 5.2, and assess its assumptions. How does it compare in overdispersion? Why the difference? What is a random tank effect doing here? And why couldn’t we use a FE of tank with this pred*size model (try it if you don’t believe me - the model will fit, but something will be… off)?</p>
 </section>
-<section id=""what-did-we-do"" class=""level3"">
-<h3 class=""anchored"" data-anchor-id=""what-did-we-do"">5.4 What did we do</h3>
+<section id=""d.-what-did-we-do"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""d.-what-did-we-do"">5d. What did we do</h3>
 <p>Write out the model equations for the glmm you have fit in 5.3. Use LaTeX to write it all out. Explain what your symbols mean in text.</p>
 </section>
-<section id=""changes-in-parameters"" class=""level3"">
-<h3 class=""anchored"" data-anchor-id=""changes-in-parameters"">5.5 Changes in Parameters</h3>
+<section id=""e.-changes-in-parameters"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""e.-changes-in-parameters"">5e. Changes in Parameters</h3>
 <p>Now that you have a model with and without a random intercept, how do the estimates of fixed effects change? Why?</p>
 </section>
-<section id=""dont-soak-up-the-good-stuff"" class=""level3"">
-<h3 class=""anchored"" data-anchor-id=""dont-soak-up-the-good-stuff"">5.5 Don’t Soak up the Good Stuff!</h3>
+<section id=""f.-dont-soak-up-the-good-stuff"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""f.-dont-soak-up-the-good-stuff"">5f. Don’t Soak up the Good Stuff!</h3>
 <p>Sometimes, random effects are used inappropriately. They become sponges for sources of real variation, and then our estimates of fixed effects can be inappropriate if they are not balanced. In this case, we have a balanced experiment. I’d like you to explore this concept a bit, though. Fit every model - from just a random intercept to the full pred*surv + random intercept - and everything in between (that’s 5 models) and visually show how the estimate of the RE standard deviation or variance changes. Explain what is happening here. Note, for the model with only a variable intercept, add the argument <code>nAGQ=20</code> (or play with options otherwise to get it to fit). If you visualize this, make sure you are getting confidence intervals of the RE. It’s illuminating.</p>
 </section>
-<section id=""model-evaluation"" class=""level3"">
-<h3 class=""anchored"" data-anchor-id=""model-evaluation"">5.6 Model evaluation</h3>
+<section id=""g.-model-evaluation"" class=""level3"">
+<h3 class=""anchored"" data-anchor-id=""g.-model-evaluation"">5g. Model evaluation</h3>
 <p>Last, let’s evaluate this model - both with contrasts and a crisp visualization of results. <code>emmeans</code>, <code>modelbased</code>, and other friends might help you here. Or not! You do you, and extract things from the model with other helpers! There’s no one right way to do this, but, make it sing.</p>
 </section>
 <section id=""impress-yourself-variable-slopes"" class=""level3"">"
biol607,biol607.github.io,035946ae195de4313e39bfa01399a574b35779b6,jebyrnes,jarrett.byrnes@umb.edu,2023-11-07T17:15:49Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-07T17:15:49Z,Fix flpped equations,lectures/causal_diagrams.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,5561f09f62e17cab339f3138d6f3ef05e2a00264,jebyrnes,jarrett.byrnes@umb.edu,2023-11-07T17:15:29Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-07T17:15:29Z,Fix spacing,schedule.Rmd;schedule.html,True,False,True,False,13,12,25,"---FILE: schedule.Rmd---
@@ -175,8 +175,8 @@ __MIDTERM__: Due Tues Nov 17th, 5pm. [Get it here](exams/exam_2023.html)
 ```{r next_date, echo=FALSE}
 ```
 `r datestring`   
-__Lecture:__  [Causal Inference](lectures/causal_diagrams.pptx). [Causal Inference and Experiments](./lectures/causality_experiments.pptx)  
-__Lab:__ Daggity and Causal Models. 
+__Lecture:__  [Causal Inference](lectures/causal_diagrams.pptx)     
+__Lab:__ Daggity and Causal Models   
 __Reading:__  [Arif et al. on Structural Causal Models](./readings/Arif_et_al_2022.zip), [Fieberg on Causal Models](https://fw8051statistics4ecologists.netlify.app/causal).   
 __Optional Reading:__ [An Intro to the PO Framework from The Causal Mixtape](https://mixtape.scunning.com/04-potential_outcomes), [Grace and Irvine on designing SCMs](Grace_and_Irvine_2019.pdf), [Does Water Kill?](https://pdf.sciencedirectassets.com/271328/1-s2.0-S1047279716X00105/1-s2.0-S1047279716302800/main.pdf)    
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-causal-2023  
@@ -199,7 +199,7 @@ __Etherpad:__ https://etherpad.wikimedia.org/p/607-causal-expts-2023
 ```{r next_date, echo=FALSE}
 ```
 `r datestring`   
-__Paper Discussion:__  Discussion of [papers](readings/obs_papers.zip) - see [here](https://docs.google.com/spreadsheets/d/1r0VfnAJ3tuJufnLnrVC6Nx0qiyP4l6QILarKTsduov0/edit?usp=sharing) for assignments  
+__Paper Discussion:__  Discussion of [papers](readings/obs_papers.zip) - see [here](https://docs.google.com/spreadsheets/d/1r0VfnAJ3tuJufnLnrVC6Nx0qiyP4l6QILarKTsduov0/edit?usp=sharing) for assignments    
 __Optional Reading:__ [Collider Bias and Covid](readings/griffith_collider_bias_covid.pdf)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-obs-2023  
 
@@ -225,9 +225,9 @@ __Packages for The Week:__ `install.packages(c(""car"", ""MASS"", ""profileModel""))`
 `r datestring`   
 
 
-__Lecture:__ Ways of Knowing: [Cross-Validation and AIC](./lectures/crossvalidation.html), 
-__Lab Topic:__  [Cross-Validation and AIC](lab/crossvalidation.html)
-__Reading:__ [AIC in Behavioral Ecology](https://link.springer.com/article/10.1007/s00265-010-1029-6), [Ellison 1996](http://byrneslab.net/classes/biol607/readings/Ellison_1996_ecol_app.pdf), [Aho et al. 2014](./readings/Aho_2014_ecolog_bic.pdf) 
+__Lecture:__ Ways of Knowing: [Cross-Validation and AIC](./lectures/crossvalidation.html)   
+__Lab Topic:__  [Cross-Validation and AIC](lab/crossvalidation.html)  
+__Reading:__ [AIC in Behavioral Ecology](https://link.springer.com/article/10.1007/s00265-010-1029-6), [Ellison 1996](http://byrneslab.net/classes/biol607/readings/Ellison_1996_ecol_app.pdf), [Aho et al. 2014](./readings/Aho_2014_ecolog_bic.pdf)   
 __Optional Books__ [Model Selection and Multimodel Inference](https://ebookcentral.proquest.com/lib/umboston/reader.action?docID=3035464) 
 __Packages for The Week:__ `install.packages(c(""AICcmodavg"")`   
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-cv-2023  

---FILE: schedule.html---
@@ -655,9 +655,8 @@ <h2>Block 3. Experimental and Observational Study Design and Causal
 <h3>Week 10.</h3>
 <p>11/6/2023<br />
 <strong>Lecture:</strong> <a href=""lectures/causal_diagrams.pptx"">Causal
-Inference</a>. <a href=""./lectures/causality_experiments.pptx"">Causal
-Inference and Experiments</a><br />
-<strong>Lab:</strong> Daggity and Causal Models.
+Inference</a><br />
+<strong>Lab:</strong> Daggity and Causal Models<br />
 <strong>Reading:</strong> <a href=""./readings/Arif_et_al_2022.zip"">Arif
 et al. on Structural Causal Models</a>, <a
 href=""https://fw8051statistics4ecologists.netlify.app/causal"">Fieberg on
@@ -744,15 +743,17 @@ <h3>Week 13.</h3>
 <h3>Week 14</h3>
 <p>12/4/2023</p>
 <p><strong>Lecture:</strong> Ways of Knowing: <a
-href=""./lectures/crossvalidation.html"">Cross-Validation and AIC</a>,
+href=""./lectures/crossvalidation.html"">Cross-Validation and
+AIC</a><br />
 <strong>Lab Topic:</strong> <a
-href=""lab/crossvalidation.html"">Cross-Validation and AIC</a>
+href=""lab/crossvalidation.html"">Cross-Validation and AIC</a><br />
 <strong>Reading:</strong> <a
 href=""https://link.springer.com/article/10.1007/s00265-010-1029-6"">AIC
 in Behavioral Ecology</a>, <a
 href=""http://byrneslab.net/classes/biol607/readings/Ellison_1996_ecol_app.pdf"">Ellison
 1996</a>, <a href=""./readings/Aho_2014_ecolog_bic.pdf"">Aho et
-al. 2014</a> <strong>Optional Books</strong> <a
+al. 2014</a><br />
+<strong>Optional Books</strong> <a
 href=""https://ebookcentral.proquest.com/lib/umboston/reader.action?docID=3035464"">Model
 Selection and Multimodel Inference</a> <strong>Packages for The
 Week:</strong> <code>install.packages(c(""AICcmodavg"")</code><br />"
biol607,biol607.github.io,c034f1ff3c355ff732045972a2c641a76bd31930,jebyrnes,jarrett.byrnes@umb.edu,2023-11-03T12:45:12Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-03T12:45:12Z,Typo fixes,lectures/mixed_models.Rmd;lectures/mixed_models.html;lectures/mixed_models_files/figure-html/mvn-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-28-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-29-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-32-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-6-1.png,True,False,True,False,15,16,31,"---FILE: lectures/mixed_models.Rmd---
@@ -117,7 +117,7 @@ $$\alpha_{j} \sim \mathcal{N}(0, \sigma^2_{\alpha})$$
   
 $$\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$$
 
-- Here, $alpha_j$ is the deviation from the grand mean due to being in cluster j
+- Here, $\alpha_j$ is the deviation from the grand mean due to being in cluster j
 
 ---
 
@@ -251,12 +251,12 @@ $$X_j = 0,1$$
 --
 
 - In truth, that made an assumption:
-$$\alpha_j ~ N(0, \infty)$$
+$$\alpha_j \sim N(0, \infty)$$
 
 --
 
 - With an RE, we assume
-$$\alpha_j ~ N(0, \sigma^2_{cluster})$$
+$$\alpha_j \sim N(0, \sigma^2_{cluster})$$
 
 --
 
@@ -280,7 +280,7 @@ $$\alpha_j ~ N(0, \sigma^2_{cluster})$$
 ---
 # Let's take this to the beach with Tide Height: RIKZ
 
-![:scale 80%](./images/08/denmark-lightsbeach.jpeg)
+![:scale 80%](./images/mixed_models/denmark-lightsbeach.jpeg)
 
 Richness of benthic species in 45 sampling stations along the coastline of The Netherlands, measured by two researches of the the RIKZ (Rijksinstituut voor Kust en Zee), i.e., the Dutch Institute for Coastal and Marine Management.
 
@@ -421,7 +421,7 @@ check_heteroscedasticity(rikz_varint) |> plot()
 ---
 # Are Our REs Normal?
 
-```{r}
+```{r, results = ""hide""}
 check_normality(rikz_varint, effects = ""random"") |> plot()
 ```
 
@@ -759,7 +759,9 @@ $$\begin{pmatrix}
   
   
 $$\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$$
-
+  
+  
+- Note that the hierarchical predictor is multiplied by $X_j$
 ---
 # The Model in R
 

---FILE: lectures/mixed_models.html---
@@ -83,7 +83,7 @@
   
 `$$\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$$`
 
-- Here, `\(alpha_j\)` is the deviation from the grand mean due to being in cluster j
+- Here, `\(\alpha_j\)` is the deviation from the grand mean due to being in cluster j
 
 ---
 
@@ -206,12 +206,12 @@
 --
 
 - In truth, that made an assumption:
-`$$\alpha_j ~ N(0, \infty)$$`
+`$$\alpha_j \sim N(0, \infty)$$`
 
 --
 
 - With an RE, we assume
-`$$\alpha_j ~ N(0, \sigma^2_{cluster})$$`
+`$$\alpha_j \sim N(0, \sigma^2_{cluster})$$`
 
 --
 
@@ -235,7 +235,7 @@
 ---
 # Let's take this to the beach with Tide Height: RIKZ
 
-![:scale 80%](./images/08/denmark-lightsbeach.jpeg)
+![:scale 80%](./images/mixed_models/denmark-lightsbeach.jpeg)
 
 Richness of benthic species in 45 sampling stations along the coastline of The Netherlands, measured by two researches of the the RIKZ (Rijksinstituut voor Kust en Zee), i.e., the Dutch Institute for Coastal and Marine Management.
 
@@ -360,11 +360,6 @@
 ---
 # Are Our REs Normal?
 
-
-```
-[[1]]
-```
-
 &lt;img src=""mixed_models_files/figure-html/unnamed-chunk-9-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
@@ -827,7 +822,9 @@
   
   
 `$$\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$$`
-
+  
+  
+- Note that the hierarchical predictor is multiplied by `\(X_j\)`
 ---
 # The Model in R
 "
biol607,biol607.github.io,02a5a167dbe5cecaad5459034ed0dd238567b382,jebyrnes,jarrett.byrnes@umb.edu,2023-11-03T12:40:24Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-03T12:40:24Z,typo fix,lab/mixed_models_files/figure-html/unnamed-chunk-23-1.png;lab/mixed_models_files/figure-html/unnamed-chunk-34-1.png;lab/mixed_models_files/figure-html/unnamed-chunk-37-1.png;lab/mixed_models_files/figure-html/unnamed-chunk-4-1.png;lab/mixed_models_files/figure-html/unnamed-chunk-41-1.png;lab/mixed_models_files/figure-html/unnamed-chunk-42-1.png,False,False,False,False,0,0,0,
biol607,biol607.github.io,2e059ef6516d2191ed9790c172b794de474bed8a,jebyrnes,jarrett.byrnes@umb.edu,2023-11-03T02:04:28Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-03T02:04:28Z,Typo fixes,lectures/mixed_models.Rmd;lectures/mixed_models.html;lectures/mixed_models_files/figure-html/mvn-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-12-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-15-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-16-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-17-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-18-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-19-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-22-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-23-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-27-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-28-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-29-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-30-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-32-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-33-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-34-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-35-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-36-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-6-1.png,True,False,True,False,41,20,61,"---FILE: lectures/mixed_models.Rmd---
@@ -458,8 +458,13 @@ tidy(rikz_varint) |>
 broom.mixed::tidy(rikz_varint_tmb, effects=""ran_vals"") |>
   knitr::kable(digits = 2) |>
   kableExtra::kable_styling(""striped"")
+```
+
+---
 
-#modelbased::estimate_grouplevel(rikz_varint) |> plot()
+# Visualizing REs?
+```{r}
+modelbased::estimate_grouplevel(rikz_varint) |> plot()
 ```
 
 ---
@@ -493,7 +498,7 @@ r2(rikz_varint_tmb)
      
 --
 
-- The Conditional R<sup>2</sup will always be larger  
+- The Conditional R<sup>2</sup> will always be larger  
      - If Marginal = 0.01, but Conditional = 0.99, what have you explained?
 ---
 class: center, middle
@@ -738,6 +743,7 @@ class: center, middle
 
 ![](images/mixed_models/hyperparameters.jpg)
 
+
 ---
 
 ## Or All One
@@ -757,7 +763,7 @@ $$\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$$
 ---
 # The Model in R
 
-```{r}
+```{r, echo = TRUE}
 rikz_hier <- lmer(log_richness ~ NAP +
                            
                            exposure +

---FILE: lectures/mixed_models.html---
@@ -536,6 +536,11 @@
 
 ---
 
+# Visualizing REs?
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-12-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
 # Combined coefficients
 
 &lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
@@ -610,7 +615,7 @@
      
 --
 
-- The Conditional R&lt;sup&gt;2&lt;/sup will always be larger  
+- The Conditional R&lt;sup&gt;2&lt;/sup&gt; will always be larger  
      - If Marginal = 0.01, but Conditional = 0.99, what have you explained?
 ---
 class: center, middle
@@ -620,29 +625,29 @@
 ---
 # Visualizing Fixed (Marginal) Effects Only
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-14-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-15-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # Visualizing Conditional Effects Only
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-15-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-16-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # Putting it All Together
 
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-16-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-17-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # Putting it All Together
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-17-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-18-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # Putting it All Together
 
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-18-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-19-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 
 ---
@@ -736,13 +741,13 @@
 ---
 # Diagnostics on Multiple REs
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-21-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-22-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # Visualize As Before
 
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-22-1.png"" style=""display: block; margin: auto;"" /&gt;&lt;table&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-23-1.png"" style=""display: block; margin: auto;"" /&gt;&lt;table&gt;
  &lt;thead&gt;
   &lt;tr&gt;
    &lt;th style=""text-align:right;""&gt; R2_conditional &lt;/th&gt;
@@ -806,6 +811,7 @@
 
 ![](images/mixed_models/hyperparameters.jpg)
 
+
 ---
 
 ## Or All One
@@ -826,6 +832,15 @@
 # The Model in R
 
 
+```r
+rikz_hier &lt;- lmer(log_richness ~ NAP +
+                           
+                           exposure +
+                           
+                           (NAP + 1|Beach),
+                    
+                    data = RIKZdat)
+```
 
 
 ---
@@ -968,18 +983,18 @@
 
 # Visualization is an MLR Problem
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-26-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-27-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 
 # Plotting Fixed Effects of Multiple Predictors
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-27-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-28-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 
 ---
 
 # Plotting Everything
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-28-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-29-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 
@@ -1026,7 +1041,7 @@
 ---
 # This is Not Normal
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-29-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-30-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 
@@ -1065,18 +1080,18 @@
 ---
 # Check that prediction
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-31-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-32-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 
 ---
 # Check that Overdispersion
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-32-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-33-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # Check that Overdispersion
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-33-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-34-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # Check the Random Effects
@@ -1086,13 +1101,13 @@
 [[1]]
 ```
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-34-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-35-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 
 # Evaluate Away!
 
-&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-35-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""mixed_models_files/figure-html/unnamed-chunk-36-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ```
 # A tibble: 1 × 2"
biol607,biol607.github.io,c7c087c0d2ce8b6db4a8e8d28f809fad95004a38,jebyrnes,jarrett.byrnes@umb.edu,2023-11-02T14:47:28Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-02T14:47:28Z,Fix code chunk,lectures/mixed_models.Rmd;lectures/mixed_models.html;lectures/mixed_models_files/figure-html/mvn-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-27-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-28-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-31-1.png;lectures/mixed_models_files/figure-html/unnamed-chunk-6-1.png,True,False,True,False,22,9,31,"---FILE: lectures/mixed_models.Rmd---
@@ -912,20 +912,22 @@ Let's try this for a variable-slope variable intercept Poisson model with a Log
 
 
 $$\large \eta_{ij} =  \alpha + \beta_{j}$$
-\
+  
+  
 $$\ \beta_{j} \sim \mathcal{N}(0, \sigma^2_{site})$$
-\
-\
+  
+  
+  
 $$\large log(\widehat{Y_{ij}}) = \eta_{ij}$$
-\
+  
 
 $$\large Y_{ij} \sim \mathcal{P}(\widehat{Y_{ij}})$$
 
 ---
 # We can fit with glmer or glmmTMB
 
 
-```{r}
+```{r, echo = TRUE}
 rikz_pois <- glmer(Richness ~ NAP +
                            
                            (NAP + 1|Beach),

---FILE: lectures/mixed_models.html---
@@ -1031,12 +1031,14 @@
 
 
 `$$\large \eta_{ij} =  \alpha + \beta_{j}$$`
-\
+  
+  
 `$$\ \beta_{j} \sim \mathcal{N}(0, \sigma^2_{site})$$`
-\
-\
+  
+  
+  
 `$$\large log(\widehat{Y_{ij}}) = \eta_{ij}$$`
-\
+  
 
 `$$\large Y_{ij} \sim \mathcal{P}(\widehat{Y_{ij}})$$`
 
@@ -1045,6 +1047,15 @@
 
 
 
+```r
+rikz_pois &lt;- glmer(Richness ~ NAP +
+                           
+                           (NAP + 1|Beach),
+                   
+                    family = poisson(link = ""log""),
+                   
+                    data = RIKZdat)
+```
 
 ---
 # Check that prediction"
biol607,biol607.github.io,d90f2f580426c28812bfbb29f1b23ca97fd94121,jebyrnes,jarrett.byrnes@umb.edu,2023-11-02T14:27:03Z,jebyrnes,jarrett.byrnes@umb.edu,2023-11-02T14:27:03Z,Fix lecture name,lectures/mixed_models.Rmd;lectures/mixed_models.html,True,False,True,False,2,2,4,"---FILE: lectures/mixed_models.Rmd---
@@ -1,5 +1,5 @@
 ---
-title: ""Interaction Effects""
+title: ""Mixed Models""
 output:
   xaringan::moon_reader:
     seal: false

---FILE: lectures/mixed_models.html---
@@ -1,7 +1,7 @@
 <!DOCTYPE html>
 <html lang="""" xml:lang="""">
   <head>
-    <title>Interaction Effects</title>
+    <title>Mixed Models</title>
     <meta charset=""utf-8"" />
     <script src=""libs/header-attrs/header-attrs.js""></script>
     <link href=""libs/remark-css/default.css"" rel=""stylesheet"" />"
biol607,biol607.github.io,870b2cdac1347def72374250f7ffba970f239a2e,jebyrnes,jarrett.byrnes@umb.edu,2023-10-19T15:00:40Z,jebyrnes,jarrett.byrnes@umb.edu,2023-10-19T15:00:40Z,Typo fix,lectures/interactions_lm.Rmd;lectures/interactions_lm.html,True,False,True,False,4,4,8,"---FILE: lectures/interactions_lm.Rmd---
@@ -62,7 +62,7 @@ class: center, middle
 
 # Etherpad
 <br><br>
-<center><h3>https://etherpad.wikimedia.org/p/607-interactions-nonlinearities-2023</h3></center>
+<center><h3>https://etherpad.wikimedia.org/p/607-complex-linear-models-2023</h3></center>
 
 
 ---"
biol607,biol607.github.io,fc8d648138440114b403694184a52949a86cd54e,jebyrnes,jarrett.byrnes@umb.edu,2023-10-19T14:08:30Z,jebyrnes,jarrett.byrnes@umb.edu,2023-10-19T14:08:30Z,Fixing interactions,lectures/images/interactions/main-effects-without.jpg;lectures/interactions_lm.Rmd;lectures/interactions_lm.html,True,False,True,False,40,6,46,"---FILE: lectures/interactions_lm.Rmd---
@@ -62,7 +62,7 @@ class: center, middle
 
 # Etherpad
 <br><br>
-<center><h3>https://etherpad.wikimedia.org/p/607-interactions-nonlinearities-2022</h3></center>
+<center><h3>https://etherpad.wikimedia.org/p/607-interactions-nonlinearities-2023</h3></center>
 
 
 ---
@@ -161,7 +161,7 @@ class: center, middle
 
 2. Evaluating Interaction Effects
 
-3. How to Look at Means and Differences with an Interaction Effect. 
+3. How to Look at Means and Differences with an Interaction Effect  
 
 4. Continuous Variables and Interaction Effects
 
@@ -559,20 +559,23 @@ Or in code:
 keeley_lm_int <- lm(firesev ~ age * elev, data=keeley)
 ```
 
+---
 
 ## Assumption Tests as Usual!
 
 ```{r klm_diag_int}
 check_model(keeley_lm_int)
 ```
 
+---
 
 ## Examine Residuals With Respect to Each Predictor
 
 ```{r klm_diag2_int}
 residualPlots(keeley_lm_int, test=FALSE, fitted = FALSE)
 ```
 
+---
 
 ## Interactions, VIF, and Centering
 ```{r int_vif}
@@ -589,6 +592,7 @@ check_collinearity(keeley_lm_int)
 - If you are worried, **center** your predictors - i.e., $X_i - mean(X)$
       - This can also fix issues with some models not converging
 
+---
 
 ## Interpretation of Centered Coefficients
 $$\huge X_i - \bar{X}$$
@@ -604,6 +608,8 @@ $$\huge X_i - \bar{X}$$
 
 -   Visualization will keep you from getting confused! 
 
+---
+
 ## Interactions, VIF, and Centering
 $$y = \beta_0 + \beta_{1}(x_{1}-\bar{x_{1}}) + \beta_{2}(x_{2}-\bar{x_{2}})+ \beta_{3}(x_{1}-\bar{x_{1}})(x_{2}-\bar{x_{2}})$$
 
@@ -620,6 +626,7 @@ keeley_lm_int_cent <- lm(firesev ~ age_c*elev_c, data=keeley)
 check_collinearity(keeley_lm_int_cent)
 ```
 
+---
 
 ## Coefficients!
 ```{r int_coef}
@@ -632,6 +639,8 @@ R<sup>2</sup> = `r summary(keeley_lm_int)$r.square`.
   
 Note that additive coefficients signify the effect of one predictor in the abscence of all others.
 
+---
+
 ## Centered Coefficients!
 ```{r int_coef_cent}
 tidy(keeley_lm_int_cent) %>% table_out
@@ -643,6 +652,8 @@ R<sup>2</sup> = `r summary(keeley_lm_int_cent)$r.square`
   
 Note that additive coefficients signify the effect of one predictor at the average level of all others.
 
+---
+
 ## Interpretation
 - What the heck does a continuous interaction effect mean?
 
@@ -702,6 +713,8 @@ ggplot() +
   theme_bw(base_size=17)
 ```
 
+---
+
 ## A Heatmap Approach
 ```{r int_heatmap}
 k_pred_grid <- crossing(elev = 100:1200, age = 3:60) %>%
@@ -729,3 +742,7 @@ plot_ly(x=~age, y=~elev, z=~firesev, type=""scatter3d"", mode=""surface"",  data = s
               color = I(""black""))
 ```
 
+---
+class: center, middle
+
+![](images/interactions/main-effects-without.jpg)"
biol607,biol607.github.io,ecd12d880287fb808f5283e739d0d8325aef32ff,jebyrnes,jarrett.byrnes@umb.edu,2023-10-17T14:39:59Z,jebyrnes,jarrett.byrnes@umb.edu,2023-10-17T14:39:59Z,fix small plots,lectures/many_types_of_predictors.Rmd;lectures/many_types_of_predictors.html;lectures/many_types_of_predictors_files/figure-html/unnamed-chunk-17-1.png;lectures/many_types_of_predictors_files/figure-html/unnamed-chunk-2-1.png;lectures/many_types_of_predictors_files/figure-html/unnamed-chunk-6-1.png;lectures/many_types_of_predictors_files/figure-html/unnamed-chunk-7-1.png;lectures/many_types_of_predictors_files/figure-html/unnamed-chunk-8-1.png,True,False,True,False,11,3,14,"---FILE: lectures/many_types_of_predictors.Rmd---
@@ -214,7 +214,9 @@ The effect of category depends on another - e.g. this grazing experiment
 
 ```{r}
 algae <- read.csv(""lectures/data/22/18e3IntertidalAlgae.csv"")
+
 graze_linear <- lm(sqrtarea ~ height + herbivores, data=algae)
+
 ggplot(algae, aes(x = height, y = sqrtarea, color = herbivores)) +
   geom_boxplot()
 ```
@@ -224,7 +226,9 @@ ggplot(algae, aes(x = height, y = sqrtarea, color = herbivores)) +
 
 ```{r}
 algae <- read.csv(""lectures/data/22/18e3IntertidalAlgae.csv"")
+
 graze_linear <- lm(sqrtarea ~ height + herbivores, data=algae)
+
 check_model(graze_linear, check = ""linearity"") |> plot()
 ```
 
@@ -235,20 +239,22 @@ class: center, middle
 
 ---
 # Normality!
+
 ```{r}
-check_model(zoop_lm, check = ""normality"") |> plot()
+check_normality(zoop_lm) |> plot()
 ```
 
 ---
 # HOV!
+
 ```{r}
-check_model(zoop_lm, check = ""homogeneity"") |> plot()
+check_heteroscedasticity(zoop_lm) |> plot()
 ```
 
 ---
 # Collinearity!
 ```{r}
-check_model(zoop_lm, check = ""vif"") |> plot()
+check_collinearity(zoop_lm) |> plot()
 ```
 
 - by definition, not a problem in an experiment

---FILE: lectures/many_types_of_predictors.html---
@@ -196,10 +196,12 @@
 
 ---
 # Normality!
+
 &lt;img src=""many_types_of_predictors_files/figure-html/unnamed-chunk-6-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
 # HOV!
+
 &lt;img src=""many_types_of_predictors_files/figure-html/unnamed-chunk-7-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---"
biol607,biol607.github.io,91156f9fac29b1e03f7b43f3b8a05c978a793d61,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T14:57:35Z,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T14:57:35Z,Fix etherpad link for week 6,schedule.Rmd;schedule.html,True,False,True,False,3,3,6,"---FILE: schedule.Rmd---
@@ -122,7 +122,7 @@ __Lab Data:__  [Multiple Files](lab/data/categorical_data.zip)
 __Reading:__ [Feiberg 3](https://fw8051statistics4ecologists.netlify.app/matrixreg), [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/)   
 __Optional Reading:__ [Analysis of variance with unbalanced data: an update for ecology & evolution](./readings/Hector_et_al-2010-Journal_of_Animal_Ecology.pdf)  
 __Packages for The Week:__ `install.packages(c(""car"", ""emmeans"", ""multcompView"", ""contrast"", ""visreg""))`    
-__Etherpad:__ https://etherpad.wikimedia.org/p/607-categorical_lm-2023  
+__Etherpad:__ https://etherpad.wikimedia.org/p/607-many-predictors-2023  
 __Homework:__ [Models with Categorical Variables](./homework/categorical.html)  
 
 <!-- __In Class Code:__ [comparing two means](./in_class_code/2020/scripts/comparing_means.R)  -->  

---FILE: schedule.html---
@@ -523,8 +523,8 @@ <h3>Week 6</h3>
 <strong>Packages for The Week:</strong>
 <code>install.packages(c(""car"", ""emmeans"", ""multcompView"", ""contrast"", ""visreg""))</code><br />
 <strong>Etherpad:</strong> <a
-href=""https://etherpad.wikimedia.org/p/607-categorical_lm-2023""
-class=""uri"">https://etherpad.wikimedia.org/p/607-categorical_lm-2023</a><br />
+href=""https://etherpad.wikimedia.org/p/607-many-predictors-2023""
+class=""uri"">https://etherpad.wikimedia.org/p/607-many-predictors-2023</a><br />
 <strong>Homework:</strong> <a href=""./homework/categorical.html"">Models
 with Categorical Variables</a></p>
 <!-- __In Class Code:__ [comparing two means](./in_class_code/2020/scripts/comparing_means.R)  -->"
biol607,biol607.github.io,02eb8503b9e3e8bcff414e7ef23809f6defa227c,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T14:45:13Z,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T14:45:13Z,appear typo fix,lectures/mlr.Rmd;lectures/mlr.html,True,False,True,False,2,2,4,"---FILE: lectures/mlr.Rmd---
@@ -761,7 +761,7 @@ library(effectsize)
 effectsize(klm, method = ""basic"")
 ```
 
----
+--
 
 - For linear model, makes intuitive sense to compare strength of association  
 

---FILE: lectures/mlr.html---
@@ -529,7 +529,7 @@
 hetero      |       0.50 | [ 0.33,  0.67]
 ```
 
----
+--
 
 - For linear model, makes intuitive sense to compare strength of association  
 "
biol607,biol607.github.io,b10309eb4d5d523f47349913a12765f354d82c2f,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T01:31:32Z,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T01:31:32Z,css fix,lectures/many_predictors.html,False,False,False,False,6,6,12,"---FILE: lectures/many_predictors.html---
@@ -49,7 +49,7 @@
 code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
 code span.at { color: #7d9029; } /* Attribute */
 code span.bn { color: #40a070; } /* BaseN */
-code span.bu { } /* BuiltIn */
+code span.bu { color: #008000; } /* BuiltIn */
 code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
 code span.ch { color: #4070a0; } /* Char */
 code span.cn { color: #880000; } /* Constant */
@@ -62,7 +62,7 @@
 code span.ex { } /* Extension */
 code span.fl { color: #40a070; } /* Float */
 code span.fu { color: #06287e; } /* Function */
-code span.im { } /* Import */
+code span.im { color: #008000; font-weight: bold; } /* Import */
 code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
 code span.kw { color: #007020; font-weight: bold; } /* Keyword */
 code span.op { color: #666666; } /* Operator */
@@ -508,7 +508,7 @@
 document.getElementsByTagName('head')[0].appendChild(style);
 </script>
 
-    <script src=""libs/header-attrs-2.16/header-attrs.js""></script>
+    <script src=""libs/header-attrs-2.25/header-attrs.js""></script>
     <script src=""libs/kePrint-0.0.1/kePrint.js""></script>
     <link href=""libs/lightable-0.0.1/lightable.css"" rel=""stylesheet"" />
 </head>
@@ -523,10 +523,10 @@ <h2></h2>
 <h2>
 Multiple Predictor Variables in Linear Models
 </h2>
-<!-- Next time, make this JUST MLR and separate others into other lecture 
+<!-- Next time, 
 
-Also, show log transform and predictions at the end.
-Add MLR equation at the beginning before path diagram.
+show log transform and predictions at the end.
+add part on supression of effects/simpsons's paradox lite
 Port to Xaringan
 
 -->"
biol607,biol607.github.io,568f2e1367291ec78dc2a9bae09db3160632040f,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T01:31:15Z,jebyrnes,jarrett.byrnes@umb.edu,2023-10-10T01:31:15Z,typo fixes,lectures/04_simulation_estimation_x.Rmd;lectures/04_simulation_estimation_x.html,True,False,True,False,3,3,6,"---FILE: lectures/04_simulation_estimation_x.Rmd---
@@ -639,7 +639,7 @@ one_sim <- function(){
          upr = m + se)
 }
 
-dat <- map_df(1:100, ~one_sim()) %>%
+dat <- purrr::map_df(1:100, ~one_sim()) %>%
   mutate(sim = 1:100,
          cover = ifelse(lwr < 4 & upr > 4, ""yes"", ""no""))
 
@@ -657,7 +657,7 @@ ggplot(dat,
 ```
 
 ---
-# A Final Note: SE (sample), CI (sample), and SD (population)....
+# To Address some Confusion: SE (sample), CI (sample), and SD (population)....
 
  ![:scale 55%](images/04/cumming_comparison_2007.jpg) 
 

---FILE: lectures/04_simulation_estimation_x.html---
@@ -852,7 +852,7 @@
 &lt;img src=""04_simulation_estimation_x_files/figure-html/CI_sim-1.png"" style=""display: block; margin: auto;"" /&gt;
 
 ---
-# A Final Note: SE (sample), CI (sample), and SD (population)....
+# To Address some Confusion: SE (sample), CI (sample), and SD (population)....
 
  ![:scale 55%](images/04/cumming_comparison_2007.jpg) 
 "
biol607,biol607.github.io,cb393f7f1c4bfa4c0bf40c6b2e075b86c172ad4f,jebyrnes,jarrett.byrnes@umb.edu,2023-09-20T23:46:34Z,jebyrnes,jarrett.byrnes@umb.edu,2023-09-20T23:46:34Z,Typo fix for link,schedule.Rmd;schedule.html,True,False,True,False,4,5,9,"---FILE: schedule.Rmd---
@@ -71,7 +71,7 @@ __Lecture/Lab:__  [Data import, using libraries](https://datacarpentry.org/R-eco
 __Data:__ [Portal Data](https://ndownloader.figshare.com/files/2292169) - and learn more [here](https://portal.weecology.org/)  
 __Reading:__  [Data organization in spreadsheets](./readings/Browman_and_Woo_Spreadsheets.pdf),  R4DS Chapters on [data import](https://r4ds.hadley.nz/spreadsheets),  [pipes](https://r4ds.hadley.nz/workflow-style.html#sec-pipes), [data transformation](https://r4ds.hadley.nz/data-transform.html), [tidy data](https://r4ds.hadley.nz/data-tidy.html)    
 __Optional Reading:__ [10 Commandments for Good Data Managament](https://dynamicecology.wordpress.com/2016/08/22/ten-commandments-for-good-data-management/), [Managing Data Frames with the Dplyr package](https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html), [Strings](http://r4ds.had.co.nz/strings.html), [factors](https://r4ds.hadley.nz/factors.html), and [Dates](http://r4ds.had.co.nz/dates-and-times.html)   
-__Cheat Sheets:__ [Reading data into R]https://rstudio.github.io/cheatsheets/data-import.pdf), [Dplyr cheat sheet](https://rstudio.github.io/cheatsheets/data-transformation.pdf).  
+__Cheat Sheets:__ [Reading data into R](https://rstudio.github.io/cheatsheets/data-import.pdf), [Dplyr cheat sheet](https://rstudio.github.io/cheatsheets/data-transformation.pdf).  
 __Packages:__ `install.packages(c(""dplyr"", ""janitor"", ""skimr"", ""lubridate"", ""tidyr"", ""readr"", ""readxl"", ""tibble""))` - [readr](https://readr.tidyverse.org/),  [readxl](https://readxl.tidyverse.org/),  [tibble](https://tibble.tidyverse.org/),  [skimr](https://docs.ropensci.org/skimr/), 
 [janitor](https://garthtarr.github.io/meatR/janitor.html),
 [visdat](https://docs.ropensci.org/visdat/)  

---FILE: schedule.html---
@@ -427,10 +427,9 @@ <h3>Week 3.</h3>
 href=""http://r4ds.had.co.nz/strings.html"">Strings</a>, <a
 href=""https://r4ds.hadley.nz/factors.html"">factors</a>, and <a
 href=""http://r4ds.had.co.nz/dates-and-times.html"">Dates</a><br />
-<strong>Cheat Sheets:</strong> [Reading data into R]<a
-href=""https://rstudio.github.io/cheatsheets/data-import.pdf""
-class=""uri"">https://rstudio.github.io/cheatsheets/data-import.pdf</a>),
-<a
+<strong>Cheat Sheets:</strong> <a
+href=""https://rstudio.github.io/cheatsheets/data-import.pdf"">Reading
+data into R</a>, <a
 href=""https://rstudio.github.io/cheatsheets/data-transformation.pdf"">Dplyr
 cheat sheet</a>.<br />
 <strong>Packages:</strong>"
biol607,biol607.github.io,1cae0bc62cbe31059f3c028fb9acdd990324f8d3,jebyrnes,jarrett.byrnes@umb.edu,2023-09-06T12:58:53Z,jebyrnes,jarrett.byrnes@umb.edu,2023-09-06T12:58:53Z,typo fix,index.Rmd,True,False,True,False,1,1,2,"---FILE: index.Rmd---
@@ -28,7 +28,7 @@ ggplot(data=penguins,
 
 **Weekly Schedule:** Tuesday & Thursday 11 - 12:30, and Lab Friday 9 - 12:00pm in the Healey Blue Computer Lab and on Zoom for those that cannot make it. The URL will be provided in the course Slack. All will be recorded for those who cannot make it, and URLS posted in the weeks' etherpads.
 
-**Office Hours:** Prof. Byrnes will hold office hours Thursday 2-4 in ISC 3130. Cooper Kimball-Rhimes will hold office hours Tuesday 1-2:30 and Friday 1-2:30.
+**Office Hours:** Prof. Byrnes will hold office hours Thursday 2-4 in ISC 3130. Cooper Kimball-Rhines will hold office hours Tuesday 1-2:30 and Friday 1-2:30.
 
 **Source for this Website:** https://github.com/biol607/biol607.github.io/  
   "
biol607,biol607.github.io,249268cb9ad4ee6996ba6c8a6f794f8e4209d129,jebyrnes,jarrett.byrnes@umb.edu,2023-09-06T12:58:36Z,jebyrnes,jarrett.byrnes@umb.edu,2023-09-06T12:58:36Z,typo fix,index.html;schedule.Rmd,True,False,True,False,3,3,6,"---FILE: index.html---
@@ -297,7 +297,7 @@ <h1 class=""title toc-ignore"">Biol 607/617: Biostatistics and
 Slack. All will be recorded for those who cannot make it, and URLS
 posted in the weeks’ etherpads.</p>
 <p><strong>Office Hours:</strong> Prof. Byrnes will hold office hours
-Thursday 2-4 in ISC 3130. Cooper Kimball-Rhimes will hold office hours
+Thursday 2-4 in ISC 3130. Cooper Kimball-Rhines will hold office hours
 Tuesday 1-2:30 and Friday 1-2:30.</p>
 <p><strong>Source for this Website:</strong> <a
 href=""https://github.com/biol607/biol607.github.io/""

---FILE: schedule.Rmd---
@@ -69,8 +69,8 @@ __Homework:__ [ggplot2 homework](./homework/ggplot.html)
 __Lecture:__  [Data import, using libraries](lectures/06_read_data_libraries.html),  [Data manipulation, summarization, and making it tidy](https://datacarpentry.org/R-ecology-lesson/03-dplyr.html)    
 <!--, [Joins](lectures/18_join.html)  -->
 __Data:__ [Portal Data](https://ndownloader.figshare.com/files/2292169) - and learn more [here](https://portal.weecology.org/)  
-__Reading:__  [Data organization in spreadsheets](./readings/Browman_and_Woo_Spreadsheets.pdf), [10 Commandments for Good Data Managament](https://dynamicecology.wordpress.com/2016/08/22/ten-commandments-for-good-data-management/), R4DS Chapters on [data import](https://r4ds.hadley.nz/data-import.html),  [pipes](https://r4ds.hadley.nz/workflow-pipes.html), [data transformation](https://r4ds.hadley.nz/data-transform.html), [tidy data](https://r4ds.hadley.nz/data-tidy.html)    
-__Optional Reading:__ [Managing Data Frames with the Dplyr package](https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html), [Strings](http://r4ds.had.co.nz/strings.html), [factors](https://r4ds.hadley.nz/factors.html), and [Dates](http://r4ds.had.co.nz/dates-and-times.html)   
+__Reading:__  [Data organization in spreadsheets](./readings/Browman_and_Woo_Spreadsheets.pdf),  R4DS Chapters on [data import](https://r4ds.hadley.nz/spreadsheets),  [pipes](https://r4ds.hadley.nz/workflow-pipes.html), [data transformation](https://r4ds.hadley.nz/data-transform.html), [tidy data](https://r4ds.hadley.nz/data-tidy.html)    
+__Optional Reading:__ [10 Commandments for Good Data Managament](https://dynamicecology.wordpress.com/2016/08/22/ten-commandments-for-good-data-management/), [Managing Data Frames with the Dplyr package](https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html), [Strings](http://r4ds.had.co.nz/strings.html), [factors](https://r4ds.hadley.nz/factors.html), and [Dates](http://r4ds.had.co.nz/dates-and-times.html)   
 __Cheat Sheets:__ [Reading data into R](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf), [Dplyr cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf).  
 __Packages:__ `install.packages(c(""dplyr"", ""janitor"", ""skimr"", ""lubridate"", ""tidyr"", ""readr"", ""readxl"", ""tibble""))` - [readr](https://readr.tidyverse.org/),  [readxl](https://readxl.tidyverse.org/),  [tibble](https://tibble.tidyverse.org/),  [skimr](https://docs.ropensci.org/skimr/), 
 [janitor](https://garthtarr.github.io/meatR/janitor.html),"
biol607,biol607.github.io,2252e0a5b590345ed40f4a80469176ca1ad21bb3,jebyrnes,jarrett.byrnes@umb.edu,2022-12-06T15:37:38Z,jebyrnes,jarrett.byrnes@umb.edu,2022-12-06T15:37:38Z,typo fix.,lectures/crossvalidation.Rmd,True,False,True,False,1,1,2,"---FILE: lectures/crossvalidation.Rmd---
@@ -664,7 +664,7 @@ aictab(mods, modnames = paste(""Order"", 1:10),
 
 ```
 
-.center[So... how much better are the top models? What about Parsimody? What would you say?]
+.center[So... how much better are the top models? What about Parsimony? What would you say?]
 
 ---
 # Inferential Frameworks of Today"
biol607,biol607.github.io,20768e62b8b26b794d342af40de422dc0d3d0c10,jebyrnes,jarrett.byrnes@umb.edu,2022-11-18T15:36:50Z,jebyrnes,jarrett.byrnes@umb.edu,2022-11-18T15:36:50Z,fix exam typo,exams/exam_2022.html;exams/exam_2022_files/libs/bootstrap/bootstrap.min.css;exams/exam_2022_files/libs/quarto-html/quarto.js,False,False,False,False,111,12,123,"---FILE: exams/exam_2022.html---
@@ -2,7 +2,7 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.0.36"">
+<meta name=""generator"" content=""quarto-1.1.189"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
@@ -12,10 +12,15 @@
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
-span.underline{text-decoration: underline;}
-div.column{display: inline-block; vertical-align: top; width: 50%;}
+div.columns{display: flex; gap: min(4vw, 1.5em);}
+div.column{flex: auto; overflow-x: auto;}
 div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
 ul.task-list{list-style: none;}
+ul.task-list li input[type=""checkbox""] {
+  width: 0.8em;
+  margin: 0 0.8em 0.2em -1.6em;
+  vertical-align: middle;
+}
 </style>
 
 
@@ -38,7 +43,7 @@
 
 <div id=""quarto-content"" class=""page-columns page-rows-contents page-layout-article"">
 <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
-  <nav id=""TOC"" role=""doc-toc"">
+  <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
     <h2 id=""toc-title"">Table of contents</h2>
    
   <ul>
@@ -236,14 +241,14 @@ <h2 class=""anchored"" data-anchor-id=""something-generalized"">5. Something General
 <p>In their <a href=""https://doi.org/10.1007/s00442-011-1958-4"">2011 paper</a>, Stanton-Geddes and Anderson assessed the role of a facultative mutualism between a legume and soil rhizobia in limiting the plant’s range. After publishing, they deposited their data at Dryad http://datadryad.org/resource/doi:10.5061/dryad.8566. As part of their lab experiment, they looked at a variety of plant properties after growing plants with rhizobia from different regions. We want to look at what happened in that experiment during the March 12th sampling event.</p>
 <section id=""a.-fit-a-glm"" class=""level3"">
 <h3 class=""anchored"" data-anchor-id=""a.-fit-a-glm"">5a. Fit a glm</h3>
-<p>Load the data. Vizualize. Then, fit and evaluate a generalized linear model with your choice of error distribution looking at the effect of rhizobial region and plant height as measured on march 12 as predictors of # of leaves on march 12. Does your model meet assumptions? If not, refit with a different. Why did you chose this (or these) error distribution(s?</p>
+<p>Load the data. Vizualize. Then, fit and evaluate a generalized linear model with your choice of error distribution looking at the effect of rhizobial region and plant height as measured on march 12 as predictors of # of leaves on march 12. Does your model meet assumptions? If not, refit with a different. Why did you chose this (or these) error distribution(s)?</p>
 <div class=""cell"">
 
 </div>
 </section>
 <section id=""b.-evaluate-your-treatments"" class=""level3"">
 <h3 class=""anchored"" data-anchor-id=""b.-evaluate-your-treatments"">5b. Evaluate your treatments</h3>
-<p>Which rhizobia enable more leaves relative to the control? And in what direction?</p>
+<p>Which rhizobial regions enable more leaves relative to the control? And in what direction?</p>
 <div class=""cell"">
 
 </div>
@@ -374,7 +379,8 @@ <h3 class=""anchored"" data-anchor-id=""k.-what-goals-do-you-have-for-yourself-for-
   for (var i=0; i<noterefs.length; i++) {
     const ref = noterefs[i];
     tippyHover(ref, function() {
-      let href = ref.getAttribute('href');
+      // use id or data attribute instead here
+      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);

---FILE: exams/exam_2022_files/libs/quarto-html/quarto.js---
@@ -6,7 +6,7 @@ const sectionChanged = new CustomEvent(""quarto-sectionChanged"", {
 });
 
 window.document.addEventListener(""DOMContentLoaded"", function (_event) {
-  const tocEl = window.document.querySelector('nav[role=""doc-toc""]');
+  const tocEl = window.document.querySelector('nav.toc-active[role=""doc-toc""]');
   const sidebarEl = window.document.getElementById(""quarto-sidebar"");
   const leftTocEl = window.document.getElementById(""quarto-sidebar-toc-left"");
   const marginSidebarEl = window.document.getElementById(
@@ -275,7 +275,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
         const convertToMenu = () => {
           for (const child of el.children) {
             child.style.opacity = 0;
-            child.style.display = ""none"";
+            child.style.overflow = ""hidden"";
           }
 
           const toggleContainer = window.document.createElement(""div"");
@@ -378,7 +378,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
         const convertToSidebar = () => {
           for (const child of el.children) {
             child.style.opacity = 1;
-            clone.style.display = null;
+            child.style.overflow = null;
           }
 
           const placeholderEl = window.document.getElementById(
@@ -653,6 +653,99 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
   highlightReaderToggle(isReaderMode());
 });
 
+// grouped tabsets
+window.addEventListener(""pageshow"", (_event) => {
+  function getTabSettings() {
+    const data = localStorage.getItem(""quarto-persistent-tabsets-data"");
+    if (!data) {
+      localStorage.setItem(""quarto-persistent-tabsets-data"", ""{}"");
+      return {};
+    }
+    if (data) {
+      return JSON.parse(data);
+    }
+  }
+
+  function setTabSettings(data) {
+    localStorage.setItem(
+      ""quarto-persistent-tabsets-data"",
+      JSON.stringify(data)
+    );
+  }
+
+  function setTabState(groupName, groupValue) {
+    const data = getTabSettings();
+    data[groupName] = groupValue;
+    setTabSettings(data);
+  }
+
+  function toggleTab(tab, active) {
+    const tabPanelId = tab.getAttribute(""aria-controls"");
+    const tabPanel = document.getElementById(tabPanelId);
+    if (active) {
+      tab.classList.add(""active"");
+      tabPanel.classList.add(""active"");
+    } else {
+      tab.classList.remove(""active"");
+      tabPanel.classList.remove(""active"");
+    }
+  }
+
+  function toggleAll(selectedGroup, selectorsToSync) {
+    for (const [thisGroup, tabs] of Object.entries(selectorsToSync)) {
+      const active = selectedGroup === thisGroup;
+      for (const tab of tabs) {
+        toggleTab(tab, active);
+      }
+    }
+  }
+
+  function findSelectorsToSyncByLanguage() {
+    const result = {};
+    const tabs = Array.from(
+      document.querySelectorAll(`div[data-group] a[id^='tabset-']`)
+    );
+    for (const item of tabs) {
+      const div = item.parentElement.parentElement.parentElement;
+      const group = div.getAttribute(""data-group"");
+      if (!result[group]) {
+        result[group] = {};
+      }
+      const selectorsToSync = result[group];
+      const value = item.innerHTML;
+      if (!selectorsToSync[value]) {
+        selectorsToSync[value] = [];
+      }
+      selectorsToSync[value].push(item);
+    }
+    return result;
+  }
+
+  function setupSelectorSync() {
+    const selectorsToSync = findSelectorsToSyncByLanguage();
+    Object.entries(selectorsToSync).forEach(([group, tabSetsByValue]) => {
+      Object.entries(tabSetsByValue).forEach(([value, items]) => {
+        items.forEach((item) => {
+          item.addEventListener(""click"", (_event) => {
+            setTabState(group, value);
+            toggleAll(value, selectorsToSync[group]);
+          });
+        });
+      });
+    });
+    return selectorsToSync;
+  }
+
+  const selectorsToSync = setupSelectorSync();
+  for (const [group, selectedName] of Object.entries(getTabSettings())) {
+    const selectors = selectorsToSync[group];
+    // it's possible that stale state gives us empty selections, so we explicitly check here.
+    if (selectors) {
+      toggleAll(selectedName, selectors);
+    }
+  }
+});
+
 function throttle(func, wait) {
   let waiting = false;
   return function () {"
biol607,biol607.github.io,a80828687d404720d9b6d29fd873ec51d5dbeec9,jebyrnes,jarrett.byrnes@umb.edu,2022-11-04T13:01:43Z,jebyrnes,jarrett.byrnes@umb.edu,2022-11-04T13:01:43Z,Typo fix,lab/12_gzlm.Rmd;lab/12_gzlm.html,True,False,True,False,11,11,22,"---FILE: lab/12_gzlm.Rmd---
@@ -483,7 +483,7 @@ emmeans(sodium_beta, specs = ~Supplement) |>
 
 Or if we had a continuous covariate, we could get fitted values and error and put them into the model.
 
-### 5.1 Exercises
+### 4.1 Exercises
 
 1) Try it out with some of my old data on docks looking at how predators shape the percent cover of bare space - `dock_pred_div_byrnes.xls`. You might have to add 0.01 to the 0 values to make it work. OR - if using glmmTMB, explore the `zi` argument!
 "
biol607,biol607.github.io,1bf377f67150c02386172aa10c5d790b4f5dc007,jebyrnes,jarrett.byrnes@umb.edu,2022-11-03T14:35:08Z,jebyrnes,jarrett.byrnes@umb.edu,2022-11-03T14:35:08Z,typo fix,lectures/modeling_variance.Rmd,True,False,True,False,7,7,14,"---FILE: lectures/modeling_variance.Rmd---
@@ -1,5 +1,5 @@
 ---
-title: ""Linear Regression and Frequentist Hypothesis Testing""
+title: ""Modeling Variance""
 output:
   xaringan::moon_reader:
     seal: false
@@ -109,7 +109,7 @@ $$\Large \boldsymbol{\sigma_i} = ...$$
 $$\Large \boldsymbol{\hat{Y}} = \boldsymbol{\beta X} $$  
 <br><bR>
 
-$$\Large \boldsymbol{Y} \sim \mathcal{D}(\boldsymbol{\hat{Y}}, \boldsymbol{\tau})$$
+$$\Large \boldsymbol{Y} \sim \mathcal{D}(\boldsymbol{\hat{Y}}, \boldsymbol{\theta})$$
 
 ---
 # Out of the Normal
@@ -168,7 +168,7 @@ This implies each point contains as much information as every other
 
 **Weighted Least Squares:**  
 
-$SS_{residuals} = W_i\sum(Y_{i} - \widehat{Y})^2$
+$SS_{residuals} = \sum W_i(Y_{i} - \widehat{Y})^2$
 
 Implies each data point might have less information - porportional to variance
 
@@ -194,9 +194,9 @@ If we also model the *off diagonal* of $\sigma^2$ we get into **generalized leas
 # Iteratively Reweighted Least Squares Algorithm
 $$\hat{y_i} = \beta_0 + \beta_1 x_i$$
 $$\sigma^2_i =  \sigma^2 f(x_i)$$
-$SS_{residuals} = W_i\sum(Y_{i} - \widehat{Y})^2$
+$SS_{residuals} = \sum W_i(Y_{i} - \widehat{Y})^2$
 
-1. Start with wi = 1  
+1. Start with all wi = 1  
 
 2. Use least squares to estimate $\beta$  
 
@@ -654,10 +654,10 @@ $$\Large f(\hat{Y_i}) = \eta_{i}$$
 .red[f is a link function]
 <br><br>
 
-$$\Large Y_i \sim \mathcal{D}(\hat{Y_i}, \tau)$$
+$$\Large Y_i \sim \mathcal{D}(\hat{Y_i}, \theta)$$
 <br><br>
 
-Here $\mathcal{D}$ is some distribution from the exponential family, and $\tau$ are additional parameters specifying the shape of the residual distribution
+Here $\mathcal{D}$ is some distribution from the exponential family, and $\theta$ are additional parameters specifying the shape of the residual distribution
 
 ---
 # So, the Linear Model is Just a Special Case"
biol607,biol607.github.io,c49bd72a23281c0f825f7ad8a3f51bbd4a5a384c,jebyrnes,jarrett.byrnes@umb.edu,2022-11-01T14:47:08Z,jebyrnes,jarrett.byrnes@umb.edu,2022-11-01T14:47:08Z,Typo fix,lectures/gls.Rmd,True,False,True,False,1,1,2,"---FILE: lectures/gls.Rmd---
@@ -363,7 +363,7 @@ squid_gls_month <- gls(Testisweight ~ DML, data=squid,
 
                        weights = varComb(varFixed(~DML),
                                          
-                                         varFixed(~MONTH)))
+                                         varIdent( form = ~ 1|MONTH)))
 ```
 
 ## Weighted v. Naieve Fit"
biol607,biol607.github.io,6aa9f73b2a2e4123f366e16f02325ab093fe6293,jebyrnes,jarrett.byrnes@umb.edu,2022-11-01T03:51:35Z,jebyrnes,jarrett.byrnes@umb.edu,2022-11-01T03:51:35Z,Fix typo,homework/interactions.html;homework/interactions.qmd,True,False,True,False,2,2,4,"---FILE: homework/interactions.html---
@@ -87,7 +87,7 @@ <h2 class=""anchored"" data-anchor-id=""replicated-regression-and-interaction-effec
 </section>
 <section id=""interactions-with-continuous-variables"" class=""level2"">
 <h2 class=""anchored"" data-anchor-id=""interactions-with-continuous-variables"">2. Interactions with Continuous Variables</h2>
-<p>Scientists wanted to simulate how different biological interactions might influence the carbon burial potential of sinking algae in the deep ocean. L’et use <a href=""data/carbon_burial_sims.csv"">this simulated data</a> which features sinking rate, microbial abundance, and detritovore abundance as predictors of net carbon sequestration.</p>
+<p>Scientists wanted to simulate how different biological interactions might influence the carbon burial potential of sinking algae in the deep ocean. Let’s use <a href=""data/c_burial_sims.csv"">this simulated data</a> which features sinking rate, microbial abundance, and detritovore abundance as predictors of net carbon sequestration.</p>
 <p><strong>2.1</strong> Load the data, inspect it, and fit a model with a 3-way interaction, Do you meet assumptions?</p>
 <p><strong>2.2</strong> Now the fun part - inference. What do the coefficients tell you?</p>
 <p><strong>2.3</strong> OK - that’s a lot. Use your skills of visualization do tease out what the data is telling us. You can use <code>visreg()</code> or <code>augment()</code> with <code>data_grid()</code> or whatever you would like. Make this model make sense so that you can tell your audience how these three parameters work together to influence carbon burial!</p>

---FILE: homework/interactions.qmd---
@@ -31,7 +31,7 @@ In lab, we looked at an experiment from my time as a graduate student manipulati
 
 ## 2. Interactions with Continuous Variables
 
-Scientists wanted to simulate how different biological interactions might influence the carbon burial potential of sinking algae in the deep ocean. L'et use [this simulated data](data/carbon_burial_sims.csv) which features sinking rate, microbial abundance, and detritovore abundance as predictors of net carbon sequestration.
+Scientists wanted to simulate how different biological interactions might influence the carbon burial potential of sinking algae in the deep ocean. Let's use [this simulated data](data/c_burial_sims.csv) which features sinking rate, microbial abundance, and detritovore abundance as predictors of net carbon sequestration.
 
 **2.1** Load the data, inspect it, and fit a model with a 3-way interaction, Do you meet assumptions?
 "
biol607,biol607.github.io,e53ac6e1797c90c4bb74e19a9899bfd87b9016f0,jebyrnes,jarrett.byrnes@umb.edu,2022-10-28T20:49:54Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-28T20:49:54Z,Fix no newline problem,index.Rmd;index_files/figure-html/demoplot-1.png,True,False,True,False,1,1,2,"---FILE: index.Rmd---
@@ -35,4 +35,4 @@ ggplot(data=penguins,
 **Source for this Data:** [Palmer Penguins](https://github.com/allisonhorst/palmerpenguins) from Allison Horst
 &nbsp;  
 &nbsp;  
-&nbsp;
\ No newline at end of file
+&nbsp;"
biol607,biol607.github.io,c57b4644acaff6279f4dea65cf86365fa6c195c8,jebyrnes,jarrett.byrnes@umb.edu,2022-10-27T14:19:37Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-27T14:19:37Z,add notes and fix equation error,lectures/categorical_interactions.Rmd,True,False,True,False,2,2,4,"---FILE: lectures/categorical_interactions.Rmd---
@@ -12,7 +12,7 @@ output:
       countIncrementalSlides: false
 ---
 class: center, middle
-
+<!-- make the interaction plots make more sense and relate to a question -->
 
 # When The Effect of One Category Depends on Another
 ![](images/anova/yoda_factorial.jpg)
@@ -132,7 +132,7 @@ check_model(mod, check = ""linearity"", panel = FALSE) |> perform_plot()
 
 # The Linear Model Can Accomodate Many Flavors of Nonlinearity
 
-$$\hat{y_i} = \beta_0 + \beta_1 x_{1i} = \beta_2 x_{2i}$$
+$$\hat{y_i} = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}$$
 $$y_i \sim N(\hat{y_i}, \sigma)$$
 --
 Could become..."
biol607,biol607.github.io,70fbb58f163286c8accb48ec2a14b09d3c770eb5,jebyrnes,jarrett.byrnes@umb.edu,2022-10-21T17:24:10Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-21T17:24:10Z,Fix typos,lab/many_predictor_types.html;lab/many_predictor_types.qmd;lab/many_predictor_types_files/figure-html/k_assume-1.png;lab/many_predictor_types_files/figure-html/neand_tests-1.png;lab/many_predictor_types_files/figure-html/zoop_assume-1.png,True,False,True,False,7,4,11,"---FILE: lab/many_predictor_types.html---
@@ -153,6 +153,7 @@ <h1 class=""title"">Many Types of Predictors in Linear Models</h1>
 
 </header>
 
+<!-- Make MLR either faded examples or give more on datasets -->
 <section id=""multiple-linear-regression"" class=""level2"">
 <h2 class=""anchored"" data-anchor-id=""multiple-linear-regression"">1. Multiple Linear Regression</h2>
 <p>Multiple linear regression is conceptually very similar to ANCOVA. Let’s use the keeley fire severity plant richness data to see it in action.</p>
@@ -510,7 +511,7 @@ <h3 class=""anchored"" data-anchor-id=""faded-examples"">2.4 Faded Examples</h3>
 <span id=""cb38-13""><a href=""#cb38-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb38-14""><a href=""#cb38-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co"">#pairwise comparison</span></span>
 <span id=""cb38-15""><a href=""#cb38-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">emmeans</span>(__________, <span class=""at"">spec =</span> <span class=""sc"">~</span> ________) <span class=""sc"">|&gt;</span></span>
-<span id=""cb38-16""><a href=""#cb38-16"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">contrast</span>(<span class=""at"">type =</span> <span class=""st"">""pairwise""</span>) <span class=""sc"">|&gt;</span></span>
+<span id=""cb38-16""><a href=""#cb38-16"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">contrast</span>(<span class=""at"">method =</span> <span class=""st"">""pairwise""</span>) <span class=""sc"">|&gt;</span></span>
 <span id=""cb38-17""><a href=""#cb38-17"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">confint</span>()</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Wow, not that different, save adding one more term and the residualPlots.</p>
@@ -534,7 +535,7 @@ <h3 class=""anchored"" data-anchor-id=""faded-examples"">2.4 Faded Examples</h3>
 <span id=""cb39-13""><a href=""#cb39-13"" aria-hidden=""true"" tabindex=""-1""></a></span>
 <span id=""cb39-14""><a href=""#cb39-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co"">#pairwise comparison</span></span>
 <span id=""cb39-15""><a href=""#cb39-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">emmeans</span>(__________, <span class=""at"">spec =</span> <span class=""sc"">~</span> ________) <span class=""sc"">|&gt;</span></span>
-<span id=""cb39-16""><a href=""#cb39-16"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">contrast</span>(<span class=""at"">type =</span> <span class=""st"">""___________""</span>) <span class=""sc"">|&gt;</span></span>
+<span id=""cb39-16""><a href=""#cb39-16"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">contrast</span>(<span class=""at"">method =</span> <span class=""st"">""___________""</span>) <span class=""sc"">|&gt;</span></span>
 <span id=""cb39-17""><a href=""#cb39-17"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">________</span>()</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Did that last one pass the test of non-additivity?</p>

---FILE: lab/many_predictor_types.qmd---
@@ -6,6 +6,8 @@ format:
     toc: true
 ---
 
+<!-- Make MLR either faded examples or give more on datasets -->
+
 ## 1. Multiple Linear Regression
 
 Multiple linear regression is conceptually very similar to ANCOVA. Let's use the keeley fire severity plant richness data to see it in action.
@@ -308,7 +310,7 @@ ________(______)
 
 #pairwise comparison
 emmeans(__________, spec = ~ ________) |>
-  contrast(type = ""pairwise"") |>
+  contrast(method = ""pairwise"") |>
   confint()
 ```
 
@@ -335,7 +337,7 @@ ________(______)
 
 #pairwise comparison
 emmeans(__________, spec = ~ ________) |>
-  contrast(type = ""___________"") |>
+  contrast(method = ""___________"") |>
   ________()
 ```
 "
biol607,biol607.github.io,68caa936c0decbaf0ba5a8c865926678be9f9c98,jebyrnes,jarrett.byrnes@umb.edu,2022-10-20T14:17:39Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-20T14:17:39Z,typo fix,lab/11_glm_aic.Rmd,True,False,True,False,1,1,2,"---FILE: lab/11_glm_aic.Rmd---
@@ -178,7 +178,7 @@ Using the following workflow, analyze these data sets.
 
 
 ## 2. Multiple Linear Regression
-Multiple linear regression is conceptially very similar to ANCOVA. Let's use the keeley fire severity plant richness data to see it in action.
+Multiple linear regression is conceptually very similar to ANCOVA. Let's use the keeley fire severity plant richness data to see it in action.
 
 ```{r keeley}
 keeley <- read.csv(""data/Keeley_rawdata_select4.csv"")"
biol607,biol607.github.io,f65aba2c82e870a6789e46503013fbfbb6f1e756,jebyrnes,jarrett.byrnes@umb.edu,2022-10-18T19:15:02Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-18T19:15:02Z,Fix lecture typos,lectures/many_predictors.Rmd;lectures/many_predictors.html;lectures/many_predictors_files/figure-revealjs/unnamed-chunk-1-1.jpeg;lectures/many_predictors_files/figure-revealjs/unnamed-chunk-10-1.jpeg;lectures/many_predictors_files/figure-revealjs/unnamed-chunk-25-1.jpeg;lectures/many_predictors_files/figure-revealjs/unnamed-chunk-6-1.jpeg,True,False,True,False,854,4,858,"---FILE: lectures/many_predictors.Rmd---
@@ -0,0 +1,843 @@
+---
+title:
+output:
+  revealjs::revealjs_presentation:
+    reveal_options:
+      slideNumber: true
+      previewLinks: true
+    theme: white
+    center: false
+    transition: fade
+    self_contained: false
+    lib_dir: libs
+    css: style.css
+---
+
+## 
+```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
+library(knitr)
+opts_chunk$set(fig.height=4.5, comment=NA, 
+               warning=FALSE, message=FALSE, 
+               dev=""jpeg"", echo=FALSE)
+library(dplyr)
+library(tidyr)
+library(broom)
+library(ggplot2)
+library(car)
+library(visreg)
+library(emmeans)
+library(performance)
+
+theme_set(theme_bw(base_size = 16))
+
+```
+![](images/23/wonka_mult_regression.jpg)
+<h2> Multiple Predictor Variables in Linear Models</h2>
+
+
+<!-- Next time, make this JUST MLR and separate others into other lecture 
+
+Also, show log transform and predictions at the end.
+Add MLR equation at the beginning before path diagram.
+Port to Xaringan
+
+-->
+
+##
+\ 
+\
+\
+<h3>https://etherpad.wikimedia.org/p/607-many-predictors-2022</h3>
+
+## Data Generating Processes Until Now
+
+-   One predictor with one response\
+    \
+
+-   Or multiple possible treatment levels, each 0/1
+
+## We Have Seen Many Predictors Before
+
+$$y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}, \qquad x_{i} = 0,1$$  
+
+```{r load_brains}
+brainGene <- read.csv(""lectures/data/19/15q06DisordersAndGeneExpression.csv"") %>%
+  mutate(group = forcats::fct_relevel(group, c(""control"", ""schizo"", ""bipolar"")))
+
+ggplot(brainGene, aes(x=group, y=PLP1.expression, fill = group)) +
+  geom_boxplot() +
+  scale_fill_discrete(guide=FALSE)+
+  theme_bw(base_size=17)
+```
+
+## The Expansiveness of the Linear Model
+
+$$\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} $$  
+
+-   This equation is huge. X can be anything - categorical,
+    continuous, squared, sine, etc.
+
+-   There can be straight additivity, or interactions
+
+
+
+## Many Additive Predictors
+
+1.   Multiple Linear Regression
+
+2.   Many Categories with Many Levels  
+
+3.   Combining Categorical and Continuous Predictors
+
+
+## Multiple Linear Regression: A Graphical View
+
+![image](./images/23/regression2.png){width=""60.00000%""}
+
+
+<div style=""text-align:left"">Curved double-headed arrow indicates COVARIANCE between predictors that we account for.  
+\
+We estimate the effect of each predictor **controlling** for all others.  
+\
+</div>
+
+
+## Calculating Multiple Regression Coefficients with OLS
+
+$$\boldsymbol{Y} = \boldsymbol{b X} + \boldsymbol{\epsilon}$$
+
+<div style=""text-align:left"">
+Remember in Simple Linear Regression $b = \frac{cov_{xy}}{var_{x}}$?\
+\
+In Multiple Linear Regression
+$\boldsymbol{b} = \boldsymbol{cov_{xy}}\boldsymbol{S_{x}^{-1}}$\
+\
+where $\boldsymbol{cov_{xy}}$ is the covariances of $\boldsymbol{x_i}$
+with $\boldsymbol{y}$ and $\boldsymbol{S_{x}^{-1}}$ is the
+variance/covariance matrix of all *Independent variables*\
+</div>
+
+
+##  {data-background=""images/23/fires.jpg""}
+<div style=""bottom:100%; text-align:left; background:goldenrod"">Five year study of wildfires & recovery in Southern California shurblands in 1993. 90 plots (20 x 50m)  
+\
+(data from Jon Keeley et al.)</div>
+
+
+
+## What causes species richness?
+
+- Distance from fire patch 
+- Elevation
+- Abiotic index
+- Patch age
+- Patch heterogeneity
+- Severity of last fire
+- Plant cover
+
+## Many Things may Influence Species Richness
+
+```{r keeley_pairs}
+keeley <- read.csv(""lectures/data/23/Keeley_rawdata_select4.csv"")
+pairs(keeley)
+```
+
+## Our Model
+\
+\
+$$Richness_i =\beta_{0}+ \beta_{1} cover_i +\beta_{2} firesev_i + \beta_{3}hetero_i +\epsilon_i$$
+\
+\
+<div class=""fragment"">
+
+```{r mlr, echo=TRUE}
+
+klm <- lm(rich ~ cover + firesev + hetero, data=keeley)
+```
+</div>
+
+## Testing Assumptions
+> - Data Generating Process: Linearity \
+\
+> - Error Generating Process: Normality & homoscedasticity of residuals  
+\
+> - Data: Outliers not influencing residuals  
+\
+> - Predictors: **Minimal multicollinearity**
+
+## Did We Match our Data?
+
+```{r}
+check_predictions(klm)
+```
+
+## How About That Linearity?
+
+```{r}
+check_model(klm, check = ""linearity"") |> plot()
+```
+
+## OK, Normality of Residuals?
+```{r}
+check_normality(klm) |> plot()
+```
+
+## OK, Normality of qResiduals?
+```{r}
+check_normality(klm) |> plot(""qq"")
+```
+
+## No Heteroskedasticity?
+```{r}
+check_heteroscedasticity(klm) |> plot()
+```
+
+## Outliers?
+```{r}
+check_outliers(klm) |> plot()
+```
+
+## 
+![](./images/23/gosling_multicollinearity.jpg)
+
+## Why Worry about Multicollinearity?
+
+> - Adding more predictors decreases precision of estimates  
+\
+> - If predictors are too collinear, can lead to difficulty fitting model  
+\
+> - If predictors are too collinear, can inflate SE of estimates further  
+\
+> - If predictors are too collinear, are we *really* getting **unique information**
+
+## Checking for Multicollinearity: Correlation Matrices
+
+```{r klm_cor, size=""normalsize""}
+with(keeley, cor(cbind(cover, firesev, hetero)))
+``` 
+
+<div style=""text-align:left"">Correlations over 0.4 can
+be problematic, but, meh, they may be OK even as high as 0.8. \
+</div>
+
+
+## Checking for Multicollinearity: Variance Inflation Factor
+
+> - Consider $y = \beta_{0} + \beta_{1}x_{1}  + \beta_{2}x_{2} + \epsilon$ \
+\
+> - And $X_{1} = \alpha_{0} + \alpha_{2}x_{2} + \epsilon_j$ \
+\
+> - $var(\beta_{1}) = \frac{\sigma^2}{(n-1)\sigma^2_{X_1}}\frac{1}{1-R^{2}_1}$
+\
+\
+<span class=""fragment"">$$VIF = \frac{1}{1-R^2_{1}}$$ </span>
+
+## Checking for Multicollinearity: Variance Inflation Factor
+$$VIF_1 = \frac{1}{1-R^2_{1}}$$ 
+
+
+```{r klm_vif, fig.height = 4}
+check_collinearity(klm) |> plot()
+``` 
+
+<div style=""text-align:left"">VIF $>$ 5 or 10 can be problematic and indicate an unstable solution.</div>
+
+## What Do We Do with High Collinearity?
+
+> - Cry.  
+\
+> - Evaluate **why**  
+\ 
+> - Can drop a predictor if information is redundant  
+\
+> - Can combine predictors into an index
+>     - Add them? Or other combination.  
+>     - PCA for orthogonal axes  
+>     - Factor analysis to compress into one variable
+
+## What does it all mean: the coefficients
+$$Richness_i =\beta_{0}+ \beta_{1} cover_i +\beta_{2} firesev_i + \beta_{3}hetero_i +\epsilon_i$$
+```{r keeley_coe}
+tidy(klm) |>
+  dplyr::select(1:3) |>
+  knitr::kable(digits = 2) |>
+  kableExtra::kable_styling()
+``` 
+
+> - $\beta_0$ - the intercept -  is the # of species when **all other predictors are 0**  
+>     - Note the very large SE  
+  
+>  - All other $\beta$s are the effect of a 1 unit increase on # of species  
+>     - They are **not** on the same scale
+>     - They are each in the scale of species per unit of individual x
+
+## How Much Variation is Associated with the Predictors
+```{r}
+glance(klm) |>
+  dplyr::select(1:2)  |>
+  knitr::kable(digits = 2) |>
+  kableExtra::kable_styling()
+  
+```
+
+- 41% of the varition in # of species is associated with the predictors  
+\
+- Note that this is **all model**, not individual predictors 
+
+## Comparing Coefficients on the Same Scale
+
+$$r_{xy} = b_{xy}\frac{sd_{x}}{sd_{y}}$$ 
+
+```{r keeley_std}
+library(effectsize) 
+effectsize(klm, method = ""basic"")
+```
+
+- For linear model, makes intuitive sense to compare strength  
+\
+- Note, this is Pearson's correlation, so, it's in units of $sd_y/sd_x$
+
+## So, Uh, How Do We Visualize This?
+
+```{r klm_see_effects}
+
+qplot(cover, rich, data=keeley, colour=firesev, size=firesev) +
+  theme_bw(base_size=14) + 
+  scale_color_gradient(low=""yellow"", high=""purple"") +
+  scale_size_continuous(range=c(1,10))
+```
+
+## Visualization Strategies for Multivariate Models
+
+- Plot the effect of each variable holding the other variables constant  
+     - Mean, Median, 0
+     - Or your choice!  
+\
+- Plot **counterfactual scenarios** from model
+     - Can match data (and be shown as such)
+     - Can be exploring the response surface
+
+
+
+## Added Variable Plot to Show Unique Contributions when Holding Others at 0
+```{r klm_avplot}
+avPlots(klm)
+```
+
+## Plots at Median of Other Variables
+
+```{r klm_visreg, fig.height=6, fig.width = 10}
+library(patchwork)
+klm_vr <- visreg::visreg(klm, cex.lab=1.3,  gg = TRUE)
+
+klm_vr[[1]] + 
+klm_vr[[2]] +
+klm_vr[[3]]
+``` 
+
+## Counterfactual Predictions Overlaid on Data
+
+```{r crazy}
+pred_info <- crossing(cover = seq(0,1.5, length.out=100),
+                      firesev=c(2,5,8)) %>%
+  crossing(hetero=c(0.5, 0.8)) %>%
+  modelr::add_predictions(klm, var=""rich"") %>%
+  dplyr::mutate(hetero_split = hetero)
+
+het_labeller <- as_labeller(c(`0.5` = ""hetero: 0.5"", `0.8` = ""hetero: 0.8""))
+
+
+ggplot(pred_info, mapping=aes(x=cover, y=rich)) +
+  geom_line(lwd=1.5, mapping=aes(color=factor(firesev), group=paste(firesev, hetero))) +
+    facet_wrap(vars(hetero_split), labeller = het_labeller) +
+  geom_point(data=keeley %>% 
+               dplyr::mutate(hetero_split = ifelse(hetero<mean(hetero), 0.5, 0.8))) +
+  theme_bw(base_size=14) +
+  labs(color = ""firesev"")
+```
+
+## Counterfactual Surfaces at Means of Other Variables
+```{r}
+visreg::visreg2d(klm, ""cover"", ""firesev"")
+```
+
+
+## 
+\
+\
+![](images/23/matrix_regression.jpg)
+
+This is an incredibly powerful technique at teasing apart different correlated influences!
+
+
+## Many Additive Predictors
+
+1.   Multiple Linear Regression
+
+2.   <font color = ""red"">Many Categories with Many Levels</font>  
+
+3.   Combining Categorical and Continuous Predictors
+
+
+## We've Now Done Multiple Continuous Predictors
+
+
+$$y_{i} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{i}$$
+\
+$$\epsilon_{i} \sim \mathcal{N}(0, \sigma)$$
+
+## We've Previously Done One Categorical Variable with Many Levels 
+
+$$y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}$$
+\
+$$\epsilon_{ij} \sim \mathcal{N}(0, \sigma), \qquad x_{i} = 0,1$$  
+
+> (hey, wait, isn't that kinda the same model.... but where you can only belong to one level of one category?)
+
+## Now... Two Categories, Each with Many Levels, as Predictors
+\
+$$y_{ijk} = \beta_{0} + \sum \beta_{i}x_{ik} + \sum \beta_{j}x_{jk} + \epsilon_{ijk}$$  
+\
+$$\epsilon_{ijk} \sim N(0, \sigma^{2} ), \qquad x_{\_k} = 0,1$$ 
+\
+
+- This model is similar to MLR, but, now we multiple categories instead of multiple continuous predictors  
+\
+- This can be extended to as many categories as we want with linear algebra  
+\
+$$Y = \beta X + \epsilon$$
+
+## Multiple Predictors: A Graphical View
+
+![image](./images/23/regression2.png){width=""60.00000%""}
+
+
+<div style=""text-align:left"">Curved double-headed arrow indicates COVARIANCE between predictors that we account for.  
+\
+We estimate the effect of each predictor **controlling** for all others.  
+\
+</div>
+
+## We Commonly Encounter Multiple Predictors in Randomized Controlled Blocked Designs
+![image](./images/21/blocked_designs/Slide4.jpg){width=""70.00000%""} 
+
+
+## An Experiment with Orthogonal Treatments: A Graphical View
+
+![image](./images/23/anova.png){width=""60.00000%""}
+
+- This is convenient for estimation  
+\
+- Observational data is not always so nice, which is OK!
+
+## Effects of Stickleback Density on Zooplankton
+<br><br>
+![image](./images/21/daphnia.jpeg){width=""40.00000%""}
+![image](./images/21/threespine_stickleback.jpeg){width=""50.00000%""}
+
+Units placed across a lake so that 1 set of each treatment was ’blocked’ together
+
+
+## Treatment and Block Effects
+
+```{r zooplankton_boxplot}
+zoop <- read.csv(""lectures/data/21/18e2ZooplanktonDepredation.csv"") %>%
+  mutate(block = factor(block))
+
+a <- ggplot(zoop,
+       aes(x = treatment, y = zooplankton)) +
+  geom_boxplot()
+
+b <- ggplot(zoop,
+       aes(x = block, y = zooplankton)) +
+  geom_boxplot()
+
+a + b
+```
+
+## Multiway Categorical Model
+>- Many different treatment types  
+>     - 2-Way is for Treatment and block
+>     - 3-Way for, e.g., Sticklebacks, Nutrients, and block
+>     - 4-way, etc., all possible  
+\
+>- For experiments, we assume treatments are fully orthogonal  
+>        - Each type of treatment type A has all levels of treatment type B
+>        - E.g., Each stickleback treatment is present in each block  
+\
+> - Experiment is **balanced** for **simple effects**  
+>      - Simple effect is the unique combination of two or more treatments  
+>      - Balance implies the sample size for each treatment combination is the same 
+>      - But, hey, this is more for inference, rather than **estimation**
+
+
+## Fitting a Model with Mutiple Categorical Predictors
+```{r, echo = TRUE}
+zoop_lm <- lm(zooplankton ~ treatment + 
+                block, data=zoop)
+
+zoop_lm
+```
+
+> Note the treatment contrasts!
+
+## Assumptions of Categorical Models with Many Categories
+-   Independence of data points  
+  
+-   Normality within groups (of residuals)  
+  
+-   No relationship between fitted and residual values  
+  
+-   Homoscedasticity (homogeneity of variance) of groups  
+  
+- No *collinearity* between treatments
+  
+-   <font color = ""red"">Additivity of Treatments</font>  
+
+## The Usual on Predictions
+```{r}
+check_predictions(zoop_lm) |> plot()
+```
+
+## Linearity (and additivity!)
+```{r}
+check_model(zoop_lm, check = ""linearity"") |> plot()
+```
+
+## What is Non-Additivity?
+The effect of category depends on another
+
+```{r}
+algae <- read.csv(""lectures/data/22/18e3IntertidalAlgae.csv"")
+graze_linear <- lm(sqrtarea ~ height + herbivores, data=algae)
+ggplot(algae, aes(x = height, y = sqrtarea, color = herbivores)) +
+  geom_boxplot()
+```
+
+## Non-Additivity is Parabolic
+
+```{r}
+algae <- read.csv(""lectures/data/22/18e3IntertidalAlgae.csv"")
+graze_linear <- lm(sqrtarea ~ height + herbivores, data=algae)
+check_model(graze_linear, check = ""linearity"") |> plot()
+```
+
+## Normality!
+```{r}
+check_model(zoop_lm, check = ""normality"") |> plot()
+```
+
+## HOV!
+```{r}
+check_model(zoop_lm, check = ""homogeneity"") |> plot()
+```
+
+## Collinearity!
+```{r}
+check_model(zoop_lm, check = ""vif"") |> plot()
+```
+
+- by definition, not a problem in an experiment
+
+## How do We Understand the Modeled Results?
+
+- Coefficients (but treatment contrasts)  
+  
+- Expected means of levels of each category
+     - Average over other categories
+
+- Differences between levels of each category
+
+## Coefficients and Treatment Contrasts
+
+```{r trt_cont}
+tidy(zoop_lm)|>
+  dplyr::select(1:3) |>
+  knitr::kable(digits = 2) |>
+  kableExtra::kable_styling()
+```
+
+- Intercept is block 1, treatment control  
+\
+- Other coefs are all deviation from control in block 1
+
+## Means Averaging Over Other Category
+
+```{r}
+emmeans(zoop_lm, ~treatment)
+
+
+emmeans(zoop_lm, ~block)
+```
+
+## Can then visualize the Expected Means
+
+```{r}
+
+trt_m <- emmeans(zoop_lm, ~treatment) |>
+  confint() |>
+  tidy()
+
+ggplot(zoop,
+       aes(x = treatment, y = zooplankton)) +
+  geom_point(color = ""grey"") +
+  geom_pointrange(data = trt_m,
+                  mapping = aes(y = estimate,
+                      ymin = estimate - std.error,
+                      ymax = estimate + std.error),
+                  color = ""red"")
+```
+
+## And Look at Differences
+
+```{r}
+emmeans(zoop_lm, ~treatment) |>
+  contrast(""pairwise"") |>
+  confint(adjust = ""none"") 
+```
+
+## It's All One
+
+**The Linear Model**
+$$\boldsymbol{Y} = \boldsymbol{b X} + \boldsymbol{\epsilon}$$
+
+**Multiple Continuous Predictors**
+$$y_{i} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{i}$$
+
+
+**Many Categorical Predictors**
+$$y_{ijk} = \beta_{0} + \sum \beta_{i}x_{ik} + \sum \beta_{j}x_{jk} + \epsilon_{ijk}$$  
+
+
+## Many Additive Predictors
+
+1.   Multiple Linear Regression
+
+2.   Many Categories with Many Levels  
+
+3.   <font color = ""red"">Combining Categorical and Continuous Predictors</font>
+
+
+
+## Mixing Continuous and Categorical Predictors: Analysis of Covariance
+
+$$y_{ij} = \beta_0   + \sum\beta_j x_{ij} + \beta_{j+1}x_{i} + \epsilon_{ij}$$  
+
+$$ x_{ij} = 0,1 \qquad \epsilon \sim \mathcal{N}(0,\sigma)$$
+
+
+-   Categorical Variable + a continuous predictor\
+\
+-   Often used to correct for a gradient or some continuous variable affecting outcome\
+\
+-   OR used to correct a regression due to additional groups that may throw off slope estimates\
+      - e.g. Simpson's Paradox: A positive relationship between test scores and academic performance can be masked by gender differences
+
+## What is Simpson's Paradox: Penguin Example
+
+```{r}
+library(palmerpenguins)
+
+ggplot(penguins,
+       aes(x = bill_length_mm, y = bill_depth_mm)) +
+  geom_point() +
+  stat_smooth(method = ""lm"")
+```
+
+
+## What is Simpson's Paradox: Penguin Example
+
+```{r}
+library(palmerpenguins)
+
+ggplot(penguins,
+       aes(x = bill_length_mm, y = bill_depth_mm,
+           color = species)) +
+  geom_point() +
+  stat_smooth(method = ""lm"")
+```
+
+> Note: This can happen with just continuous variables as well
+
+## Neanderthals and Categorical/Continuous Variables
+
+![image](./images/23/neanlooking.jpeg){width=""60.00000%""}
+
+Who had a bigger brain: Neanderthals or us?
+
+
+
+## The Means Look the Same...
+
+```{r neand_boxplot}
+neand <- read.csv(""lectures/data/23/18q09NeanderthalBrainSize.csv"")
+neand_plot_box <- qplot(species, lnbrain, data=neand, fill=species, geom=""boxplot"")  + theme_bw()
+neand_plot_box
+```
+
+
+
+## But there appears to be a Relationship Between Body and Brain Mass
+
+```{r neand_plot}
+neand_plot <- qplot(lnmass, lnbrain, data=neand, color=species, size=I(3))  + theme_bw()
+neand_plot
+```
+
+
+
+## And Mean Body Mass is Different
+
+```{r neand_boxplot2}
+neand_plot_box2 <- qplot(species, lnmass, data=neand, fill=species, geom=""boxplot"")  + theme_bw()
+neand_plot_box2
+```
+
+## 
+\
+\
+![](images/23/redo_analysis.jpg)
+
+## Categorical Model with a Continuous Covariate for Control
+
+```{r neand_model, echo=FALSE}
+neand_lm <- lm(lnbrain ~ species + lnmass, data=neand)
+
+```
+
+
+```{r neand_plot_fit, fig.height=5, fig.width=7}
+neand <- cbind(neand, predict(neand_lm, interval=""confidence""))
+
+neand_fit <- ggplot(data = neand,
+       aes(x = lnmass, y = lnbrain, color = species, group = species)) +
+  geom_point(size = 3) +
+  geom_line(aes(y=fit)) + 
+  geom_ribbon(aes(ymin=lwr, ymax=upr), 
+              fill=""lightgrey"", 
+              color = NA,
+              alpha=0.5) 
+
+neand_fit
+```
+
+Evaluate a categorical effect(s), controlling for a *covariate* (parallel lines)\
+ 
+Groups modify the *intercept*.
+
+
+
+## Assumptions are the Same!
+-   Independence of data points
+
+-   Normality and homoscedacticity within groups (of residuals)
+
+-   No relationship between fitted and residual values
+
+-   Additivity of Treatment and Covariate (Parallel Slopes)
+
+
+## Linearity Assumption KEY
+```{r zoop_assumptions, fig.height=7}
+check_model(neand_lm, check = ""linearity"") |> plot()
+```
+
+## Test for Parallel Slopes
+We test a model where
+$$y_{ijk} = \beta_0 + \beta_{1}x_1  + \sum_{j}^{i=1}\beta_j x_{ij} + \sum_{j}^{i=1}\beta_{k}x_1 x_{ij} + \epsilon_ijk$$
+<div class=""fragment"">
+
+```{r parallel_slopes}
+neand_lm_int <- lm(lnbrain ~ species * lnmass, data=neand)
+
+tidy(neand_lm_int) |>
+  dplyr::select(1:3) |>
+knitr::kable() |>
+  kableExtra::kable_styling()
+```
+
+</div>
+\
+<div class=""fragment"">If you have an interaction, welp, that's a different model - slopes vary by group!</div>
+
+## VIF Also *Very* Important
+```{r}
+check_collinearity(neand_lm) |> plot()
+```
+
+## Usual Normality Assumption
+```{r}
+check_normality(neand_lm) |>plot(""qq"")
+```
+
+## Usual HOV Assumption
+```{r}
+check_heteroscedasticity(neand_lm) |>plot()
+```
+
+
+## Usual Outlier Assumption
+```{r}
+check_outliers(neand_lm) |>plot()
+```
+
+## The Results
+
+- We can look at coefficients  
+\
+- We can look at means adjusted for covariate  
+\
+- Visualize! Visualize! Visualize!
+
+## Those Coefs
+
+```{r}
+tidy(neand_lm) |>
+  dplyr::select(1:3) |>
+  knitr::kable(digits = 2) |>
+  kableExtra::kable_styling()
+```
+
+- Intercept is species = neanderthal, but lnmass = 0?  
+\
+- Categorical coefficient is defiation from intercept for recent  
+\
+- lnmass coefficient is change in ln brain mass per change in 1 unit of ln mass
+
+
+## Groups Means at Mean of Covariate
+```{r cr_lsmanes}
+emmeans(neand_lm, ~species) %>%
+  confint() |>
+ knitr::kable(digits=3) |>
+  kableExtra::kable_styling()
+```
+
+Can also evaluate for other levels of the covariate as is interesting
+
+## Difference Between Groups at Mean of Covariate
+```{r cr_lsmanes_diff}
+contrast(emmeans(neand_lm, ~species,
+                 at = list(lnmass = mean(neand$lnmass))), method = ""tukey"") %>%
+  confint() |>
+ knitr::kable(digits=3) |>
+  kableExtra::kable_styling()
+```
+
+## Vsualizing Result Says it All!
+```{r, fig.height=6}
+neand_fit
+```
+
+## Or Plot at the Mean Level of the Covariate
+```{r}
+visreg::visreg(neand_lm, ""species"", gg = TRUE) 
+```
+
+## Extensions of the Linear Additive Model
+
+- Wow, we sure can fit a lot in there!
+\
+- Categorical is just continuous as 0/1  
+\
+- So, we can build a LOT of models, limited only by our imagination!
\ No newline at end of file

---FILE: lectures/many_predictors.html---
@@ -523,15 +523,22 @@ <h2></h2>
 <h2>
 Multiple Predictor Variables in Linear Models
 </h2>
+<!-- Next time, make this JUST MLR and separate others into other lecture 
+
+Also, show log transform and predictions at the end.
+Add MLR equation at the beginning before path diagram.
+Port to Xaringan
+
+-->
 </section>
 <section id=""section-1"" class=""slide level2"">
 <h2></h2>
  <br />
 <br />
 
 <h3>
-<a href=""https://etherpad.wikimedia.org/p/607-mlr-2022""
-class=""uri"">https://etherpad.wikimedia.org/p/607-mlr-2022</a>
+<a href=""https://etherpad.wikimedia.org/p/607-many-predictors-2022""
+class=""uri"">https://etherpad.wikimedia.org/p/607-many-predictors-2022</a>
 </h3>
 </section>
 <section id=""data-generating-processes-until-now"" class=""slide level2"">
@@ -1061,8 +1068,8 @@ <h2>Effects of Stickleback Density on Zooplankton</h2>
 <h2>Treatment and Block Effects</h2>
 <p><img src=""many_predictors_files/figure-revealjs/zooplankton_boxplot-1.jpeg"" width=""768"" /></p>
 </section>
-<section id=""multiway-cateogircal-model"" class=""slide level2"">
-<h2>Multiway Cateogircal Model</h2>
+<section id=""multiway-categorical-model"" class=""slide level2"">
+<h2>Multiway Categorical Model</h2>
 <ul>
 <li class=""fragment"">Many different treatment types
 <ul>"
biol607,biol607.github.io,ff44a7dfe230718759210d843d391e694e97ca10,jebyrnes,jarrett.byrnes@umb.edu,2022-10-14T01:25:01Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-14T01:25:01Z,Fix lab errors.,lab/categorical_models.html;lab/categorical_models.qmd;lab/categorical_models_files/figure-html/assumptions-1.png;lab/categorical_models_files/figure-html/pred_check-1.png;lab/categorical_models_files/figure-html/unnamed-chunk-2-1.png,True,False,True,False,16,10,26,"---FILE: lab/categorical_models.html---
@@ -286,7 +286,7 @@ <h2 class=""anchored"" data-anchor-id=""adjusting-for-hov-violation"">1.2 Adjusting
 </div>
 </div>
 <p>This works! But, remember, answers will have a different meaning in coefficients.</p>
-<p>Last, we can model the variance with the nlme library using the <code>glmmTMB</code> library which has a nice interface for specifying error structure with the <code>dispformula</code> argument that works just like specifying y ~ x relationships (save that we don’t need to specify y - it’s the variance in this case).</p>
+<p>Last, we can model the variance with <code>glmmTMB</code> library which has a nice interface for specifying error structure with the <code>dispformula</code> argument that works just like specifying y ~ x relationships (save that we don’t need to specify y - it’s the variance in this case).</p>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb13""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb13-1""><a href=""#cb13-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(glmmTMB)</span>
 <span id=""cb13-2""><a href=""#cb13-2"" aria-hidden=""true"" tabindex=""-1""></a></span>
@@ -351,9 +351,11 @@ <h2 class=""anchored"" data-anchor-id=""model-results"">1.3 Model Results</h2>
   adj. R2: 0.185</code></pre>
 </div>
 </div>
-<p>The difference between the two is interesting - perhaps the addition of a parameter doesn’t increase fit very much?</p>
+<p>The difference between the two is interesting - perhaps the addition of a parameter doesn’t increase fit very much? Remember</p>
+<p><span class=""math display"">\[R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}\]</span></p>
+<p>where k is the number of parameters. If a parameter doesn’t contribute very much, then the adjusted R<sup>2</sup> won’t increase as much as the classical R<sup>2</sup>.</p>
 <p>Regardless, if we want to look at the actual estimated means and their difference, we need the <code>emmeans</code> package. It is a FABULOUS package for querying fit models, and we’ll use it hand in hand with <code>modelr</code> and others in the future as well as using it to explore models with many predictors - next week!</p>
-<p>Let’s start by looking at those estimated means</p>
+<p>Let’s start by looking at those estimated means. Se use the <code>specs</code> argument to specify which predictor or predictors we are interested in. We’ll get fancier with it next week.</p>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb23""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb23-1""><a href=""#cb23-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(emmeans)</span>
 <span id=""cb23-2""><a href=""#cb23-2"" aria-hidden=""true"" tabindex=""-1""></a></span>
@@ -459,7 +461,7 @@ <h2 class=""anchored"" data-anchor-id=""lm-and-many-categories"">2.1 LM and Many Cat
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb36""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb36-1""><a href=""#cb36-1"" aria-hidden=""true"" tabindex=""-1""></a>knees_lm <span class=""ot"">&lt;-</span> <span class=""fu"">lm</span>(shift <span class=""sc"">~</span> treatment, <span class=""at"">data=</span>knees)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
-<p>Now, there are two things to notice here. One, note that treatment is a either a character or factor. If it is not, because we are using <code>lm()</code>, it will be fit like a linear regression. So, beware!</p>
+<p>Note that treatment is a either a character or factor. If it is not, because we are using <code>lm()</code>, it will be fit like a linear regression. So, beware!</p>
 <p>We can see how this was turned into dummy coding with <code>model.matrix()</code></p>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb37""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb37-1""><a href=""#cb37-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">model.matrix</span>(knees_lm)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
@@ -669,7 +671,7 @@ <h3 class=""anchored"" data-anchor-id=""unplanned-pairwise-comparisons"">2.3.3 Unpla
 <p><img src=""categorical_models_files/figure-html/plot_tukey-1.png"" class=""img-fluid"" width=""672""></p>
 </div>
 </div>
-<p>We can also, using our tukey method of adjustment, get “groups” - i.e., see which groups are statistically the same versus different.</p>
+<p>We can also, using our tukey method of adjustment, get “groups” - i.e., see which groups are likely the same versus different.</p>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb55""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb55-1""><a href=""#cb55-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(multcomp)</span>
 <span id=""cb55-2""><a href=""#cb55-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">cld</span>(knees_em, <span class=""at"">adjust=</span><span class=""st"">""tukey""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>

---FILE: lab/categorical_models.qmd---
@@ -149,7 +149,7 @@ plot(surv_mod_rank, which = 1)
 
 This works! But, remember, answers will have a different meaning in coefficients.
 
-Last, we can model the variance with the nlme library using the `glmmTMB` library which has a nice interface for specifying error structure with the `dispformula` argument that works just like specifying y ~ x relationships (save that we don't need to specify y - it's the variance in this case).
+Last, we can model the variance with `glmmTMB` library which has a nice interface for specifying error structure with the `dispformula` argument that works just like specifying y ~ x relationships (save that we don't need to specify y - it's the variance in this case).
 
 ```{r}
 library(glmmTMB)
@@ -196,11 +196,15 @@ Useful! We can see the results in context here. We can also look at how much var
 r2(surv_mod)
 ```
 
-The difference between the two is interesting - perhaps the addition of a parameter doesn't increase fit very much?
+The difference between the two is interesting - perhaps the addition of a parameter doesn't increase fit very much? Remember
+
+$$R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}$$
+
+where k is the number of parameters. If a parameter doesn't contribute very much, then the adjusted R<sup>2</sup> won't increase as much as the classical R<sup>2</sup>.
 
 Regardless, if we want to look at the actual estimated means and their difference, we need the `emmeans` package. It is a FABULOUS package for querying fit models, and we'll use it hand in hand with `modelr` and others in the future as well as using it to explore models with many predictors - next week!
 
-Let's start by looking at those estimated means
+Let's start by looking at those estimated means. Se use the `specs` argument to specify which predictor or predictors we are interested in. We'll get fancier with it next week.
 
 ```{r}
 library(emmeans)
@@ -275,7 +279,7 @@ As the underlying model of ANOVA is a linear one, we fit ANOVAs using `lm()` jus
 knees_lm <- lm(shift ~ treatment, data=knees)
 ```
 
-Now, there are two things to notice here. One, note that treatment is a either a character or factor. If it is not, because we are using `lm()`, it will be fit like a linear regression. So, beware!
+Note that treatment is a either a character or factor. If it is not, because we are using `lm()`, it will be fit like a linear regression. So, beware!
 
 We can see how this was turned into dummy coding with `model.matrix()`
 
@@ -399,7 +403,7 @@ contrast(knees_em,
   geom_vline(xintercept = 0, color = ""red"", lty=2)
 ```
 
-We can also, using our tukey method of adjustment, get ""groups"" - i.e., see which groups are statistically the same versus different.
+We can also, using our tukey method of adjustment, get ""groups"" - i.e., see which groups are likely the same versus different.
 
 ```{r groups}
 library(multcomp)"
biol607,biol607.github.io,a99e20000cf556e26ebbe229980cf6a7edf78c66,jebyrnes,jarrett.byrnes@umb.edu,2022-10-12T22:52:39Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-12T22:52:39Z,Fixing notation,lectures/anova_1.Rmd;lectures/anova_1.html;lectures/anova_1_files/figure-html/unnamed-chunk-1-1.png;lectures/anova_1_files/figure-html/unnamed-chunk-6-1.png,True,False,True,False,8,8,16,"---FILE: lectures/anova_1.Rmd---
@@ -204,13 +204,13 @@ Underlying linear model with control = intercept, dummy variable for schizo
 
 ---
 # Linear Dummy Variable (Fixed Effect) Model
-$$\large y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}, \qquad x_{i} = 0,1$$  
+$$\large y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}, \qquad x_{i} = 0,1$$  
 $$\epsilon_{ij} \sim N(0, \sigma^{2})$$
 
 - i = replicate, j = group  
 
 
-- $x_{i}$ inidicates presence/abscence (1/0) of a category  
+- $x_{ij}$ inidicates presence/abscence (1/0) of level j for individual i  
      - This coding is called a **Dummy variable**  
 
 - Note similarities to a linear regression  
@@ -239,7 +239,7 @@ $$\epsilon_{ij} \sim N(0, \sigma^{2} )$$
 ---
 # Partioning Model to See What Varies
 
-$$\large y_{ij} = \bar{y} + (\bar{y}_{i} - \bar{y}) + ({y}_{ij} - \bar{y}_{i})$$
+$$\large y_{ij} = \bar{y} + (\bar{y}_{j} - \bar{y}) + ({y}_{ij} - \bar{y}_{j})$$
 
 - i = replicate, j = group  
 
@@ -352,7 +352,7 @@ check_normality(brain_lm) |> plot(""qq"")
 
 ---
 # R Fits with Treatment Contrasts
-$$y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}$$
+$$y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}$$
 
 ```{r trt_means}
 tidy(brain_lm) |>

---FILE: lectures/anova_1.html---
@@ -134,13 +134,13 @@
 
 ---
 # Linear Dummy Variable (Fixed Effect) Model
-`$$\large y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}, \qquad x_{i} = 0,1$$`  
+`$$\large y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}, \qquad x_{i} = 0,1$$`  
 `$$\epsilon_{ij} \sim N(0, \sigma^{2})$$`
 
 - i = replicate, j = group  
 
 
-- `\(x_{i}\)` inidicates presence/abscence (1/0) of a category  
+- `\(x_{ij}\)` inidicates presence/abscence (1/0) of level j for individual i  
      - This coding is called a **Dummy variable**  
 
 - Note similarities to a linear regression  
@@ -169,7 +169,7 @@
 ---
 # Partioning Model to See What Varies
 
-`$$\large y_{ij} = \bar{y} + (\bar{y}_{i} - \bar{y}) + ({y}_{ij} - \bar{y}_{i})$$`
+`$$\large y_{ij} = \bar{y} + (\bar{y}_{j} - \bar{y}) + ({y}_{ij} - \bar{y}_{j})$$`
 
 - i = replicate, j = group  
 
@@ -301,7 +301,7 @@
 
 ---
 # R Fits with Treatment Contrasts
-`$$y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}$$`
+`$$y_{ij} = \beta_{0} + \sum \beta_{j}x_{ij} + \epsilon_{ij}$$`
 
 &lt;table class=""table"" style=""margin-left: auto; margin-right: auto;""&gt;
  &lt;thead&gt;"
biol607,biol607.github.io,6ad5ddc074a082075dcf433b93f9400fecebfd00,jebyrnes,jarrett.byrnes@umb.edu,2022-10-11T16:13:10Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-11T16:13:10Z,Fix errors in t as lm,lectures/t_as_lm.Rmd;lectures/t_as_lm.html,True,False,True,False,23,2,25,"---FILE: lectures/t_as_lm.Rmd---
@@ -121,7 +121,7 @@ $$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$
 $$\epsilon_i \sim N(0, \sigma)$$
 ]
 
-But, what if $\beta_1$ was just 0,1?
+But, what if $x_i$ was just 0,1?
 
 
 ---
@@ -606,6 +606,17 @@ emmeans(horn_mod, ~Status) |>
 --
 A One Sample T-Test.
 
+
+---
+
+# How Much Variation was Explained?
+
+R<sup>2</sup> = `r glance(horn_mod) |> pull(1) |> round(2)`
+
+
+```{r lizard_mean, warning=FALSE}
+```
+
 ---
 
 # Dummy Codding for Dummy Models

---FILE: lectures/t_as_lm.html---
@@ -84,7 +84,7 @@
 `$$\epsilon_i \sim N(0, \sigma)$$`
 ]
 
-But, what if `\(\beta_1\)` was just 0,1?
+But, what if `\(x_i\)` was just 0,1?
 
 
 ---
@@ -700,6 +700,16 @@
 --
 A One Sample T-Test.
 
+
+---
+
+# How Much Variation was Explained?
+
+R&lt;sup&gt;2&lt;/sup&gt; = 0.09
+
+
+&lt;img src=""t_as_lm_files/figure-html/lizard_mean-1.png"" style=""display: block; margin: auto;"" /&gt;
+
 ---
 
 # Dummy Codding for Dummy Models"
biol607,biol607.github.io,53a9fdd4e2c5c21abf0d0366167c99d0ad8a785e,jebyrnes,jarrett.byrnes@umb.edu,2022-10-11T12:59:40Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-11T12:59:40Z,minor fixes,lectures/t_as_lm.Rmd;lectures/t_as_lm.html;lectures/t_as_lm_files/figure-html/lizard_mean-1.png;lectures/t_as_lm_files/figure-html/posterior-1.png;lectures/t_as_lm_files/figure-html/unnamed-chunk-1-1.png;lectures/t_as_lm_files/figure-html/unnamed-chunk-16-1.png;lectures/t_as_lm_files/figure-html/unnamed-chunk-2-1.png;lectures/t_as_lm_files/figure-html/unnamed-chunk-3-1.png;lectures/t_as_lm_old.Rmd,True,False,True,False,771,18,789,"---FILE: lectures/t_as_lm.Rmd---
@@ -136,24 +136,21 @@ Horns prevent these lizards from being eaten by birds. Are horn lengths differen
 
 ---
 background-image: url(""./images/09/guiness_full.jpg"")
+background-position: center
+background-size: contain
 
 ---
 class: center, middle
 
 ![](./images/09/gosset.jpg)
 ---
-class: center, middle
 
 # The T-Test
 
-![](./images/09/t_unpaired.png)
---
-
-- Briefly, data is used to calculate a value  
-
+.center[![](./images/09/t_unpaired.png)]
 --
 
-- This value is compared to a reference distribution
+- Briefly, data is used to calculate a value to compare to a reference distribution  
 
 --
 
@@ -361,14 +358,19 @@ check_heteroscedasticity(horn_mod) |> plot() +
 - You can transform the response variable  
      - log() or asinh()
      
+--
+     
 - You can model the RANKS of the outcome variable
      - This is classically known as a Mann-Whitney U or Wilcoxon Rank Sum test  
      - Your question is does the variable effect the *ranks* of the outcome  
      - Need large sample size, not very precise
-     
+
+--
+
+
 - You can model the variance  
      - Classically, this is known as Welch's T-Test  
-     - relax assumption of HOV
+     - Relax assumption of HOV
 
 
 ---
@@ -604,6 +606,8 @@ emmeans(horn_mod, ~Status) |>
 --
 A One Sample T-Test.
 
+---
+
 # Dummy Codding for Dummy Models
 
 1. The Categorical as Continuous

---FILE: lectures/t_as_lm.html---
@@ -99,24 +99,21 @@
 
 ---
 background-image: url(""./images/09/guiness_full.jpg"")
+background-position: center
+background-size: contain
 
 ---
 class: center, middle
 
 ![](./images/09/gosset.jpg)
 ---
-class: center, middle
 
 # The T-Test
 
-![](./images/09/t_unpaired.png)
---
-
-- Briefly, data is used to calculate a value  
-
+.center[![](./images/09/t_unpaired.png)]
 --
 
-- This value is compared to a reference distribution
+- Briefly, data is used to calculate a value to compare to a reference distribution  
 
 --
 
@@ -416,14 +413,19 @@
 - You can transform the response variable  
      - log() or asinh()
      
+--
+     
 - You can model the RANKS of the outcome variable
      - This is classically known as a Mann-Whitney U or Wilcoxon Rank Sum test  
      - Your question is does the variable effect the *ranks* of the outcome  
      - Need large sample size, not very precise
-     
+
+--
+
+
 - You can model the variance  
      - Classically, this is known as Welch's T-Test  
-     - relax assumption of HOV
+     - Relax assumption of HOV
 
 
 ---
@@ -698,6 +700,8 @@
 --
 A One Sample T-Test.
 
+---
+
 # Dummy Codding for Dummy Models
 
 1. The Categorical as Continuous

---FILE: lectures/t_as_lm_old.Rmd---
@@ -0,0 +1,745 @@
+---
+title: ""It's All Linear Models""
+output:
+  xaringan::moon_reader:
+    seal: false
+    lib_dir: libs
+    css: [default, shinobi, default-fonts, style.css]
+    nature:
+      beforeInit: ""my_macros.js""
+      highlightStyle: github
+      highlightLines: true
+      countIncrementalSlides: false
+---
+class: center, middle
+
+# It's All Linear Models: T-Test Edition
+
+![image](./images/all_linear_models/linear_regression_everywhere.jpg)
+
+```{r setup, include=FALSE}
+library(knitr)
+library(ggplot2)
+library(dplyr)
+library(tidyr)
+library(broom)
+library(readr)
+
+opts_chunk$set(fig.height=6, 
+               fig.width = 8,
+               fig.align = ""center"",
+               comment=NA, 
+               warning=FALSE, 
+               echo = FALSE,
+               message = FALSE)
+
+options(htmltools.dir.version = FALSE)
+theme_set(theme_bw(base_size=24))
+```
+
+---
+class: center, middle
+
+# Etherpad
+<br><br>
+<center><h3>https://etherpad.wikimedia.org/p/607-linear_everywhere-2020</h3></center>
+
+---
+# YES, YOU KNOW EVERYTHING
+
+1. Beastiary of Everything As a Linear Model
+
+2. T-Tests as a Linear Model
+
+---
+# Up until now, we've done this
+$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$
+$$\epsilon_i \sim N(0, \sigma)$$
+
+```{r pp}
+library(palmerpenguins)
+penguins <- penguins %>% drop_na(sex)
+ggplot(penguins,
+       aes(x = body_mass_g, y = flipper_length_mm)) +
+  geom_point() +
+  stat_smooth(method = ""lm"", color = ""red"")
+```
+
+---
+
+# But we have wider classes of problems...
+### Comparing Two Means
+
+```{r}
+ggplot(penguins,
+       aes(y = body_mass_g, x = sex)) +
+  geom_point(alpha = 0.3, position = position_jitter(width = 0.2))  +
+  stat_summary(fun.data = mean_cl_boot, color = ""red"") +
+  labs(subtitle = ""Is Body Mass Different Between\nDifferent Penguin Sexes?"")
+```
+
+--
+$$mass_i = \beta_0 + \beta_1 sex_i + \epsilon_i$$
+---
+
+# But we have wider classes of problems...
+### Comparing Many Means
+
+```{r}
+ggplot(penguins,
+       aes(y = body_mass_g, x = species)) +
+  geom_point(alpha = 0.3, position = position_jitter(width = 0.2))  +
+  stat_summary(fun.data = mean_cl_boot, color = ""red"") +
+  labs(subtitle = ""Is Body Mass Different Between\nDifferent Penguin Species?"")
+```
+
+--
+$$mass_i =  \beta_1 adelie_i + \beta_2 chinstrap_i + \beta_3 gentoo_i + \epsilon_i$$
+---
+
+# But we have wider classes of problems...
+### Many Predictors
+
+```{r}
+library(modelr)
+
+bill_mod <- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm, data = penguins)
+
+pred <- data_grid(penguins,
+                  flipper_length_mm=seq_range(flipper_length_mm,2),
+                  bill_depth_mm = seq_range(bill_depth_mm,4)
+                  )
+
+pred <- pred %>%
+  add_predictions(bill_mod, var = ""bill_length_mm"")
+
+ggplot(penguins,
+       aes(x = flipper_length_mm, y = bill_length_mm, color = bill_depth_mm, group = cut_interval(bill_depth_mm, 6))) +
+  geom_point(alpha = 0.3, position = position_jitter(width = 0.2))  +
+  geom_line(data = pred, aes(group = bill_depth_mm),
+            size = 1.3) +
+  labs(subtitle = ""How do flipper length and bill depth\ninfluence bill length?"",
+        color = ""Bill Depth\n(mm)"",
+       x = ""Flipper Length (mm)"", y = ""Bill Length (mm)"") +
+  scale_color_binned(type = ""viridis"")
+```
+--
+$$length_i = \beta_0 + \beta_1 flipper_i + \beta_2 depth_i+ \epsilon_i  $$
+---
+
+# But we have wider classes of problems...
+### One Predictor Modifies Another
+```{r, fig.height = 5}
+ggplot(penguins,
+       aes(x = body_mass_g, y = flipper_length_mm, color = species)) +
+  geom_point(alpha = 0.3, position = position_jitter(width = 0.2))  +
+  stat_smooth(method = ""lm"") +
+  labs(subtitle = ""How do body mass and species influence\nflipper length?"")
+```
+
+--
+$$\small flipper_i = \beta_0 mass_i + \beta_1 adelie_i + \beta_2 chinstrap_i + \beta_3 gentoo_i +$$
+$$\small\beta_4 mass_iadelie_i + \beta_5 mass_ichinstrap_i + \beta_6 mass_igentoo_i + \epsilon_i  $$
+---
+class:center, middle
+![:scale 65%](images/all_linear_models/glm_meme.png)
+---
+
+# But we have wider classes of problems...
+### Nonlinear Models and Transformation
+
+```{r metabolism, fig.height = 5}
+metabolism <- read.csv(""./data/25/17q16PrimateMassMetabolicRate.csv"")
+mod <- lm(log(bmr.watts) ~ I(log(mass.g)), data = metabolism)
+
+pred_m <- data.frame(mass.g = seq(0.1,80000, length.out = 200)) 
+
+pred_m <- cbind(pred_m, 
+                    predict(mod, newdata = pred_m,
+                  interval = ""confidence"") %>%
+  as_tibble() %>%
+  mutate(bmr.watts = exp(fit),
+         lwr = exp(lwr),
+         upr = exp(upr)))
+
+  
+meta_plot <- ggplot(metabolism, mapping=aes(x=mass.g, y=bmr.watts)) +
+  geom_point(size=2) +
+  theme_bw(base_size=17) +
+  geom_line(data = pred_m, color = ""red"", size = 1.5) +
+  geom_ribbon(data = pred_m, aes(ymin = lwr, ymax = upr), 
+              color = ""lightgrey"", alpha = 0.3)+
+  labs(subtitle = ""How does body mass influence\nmetabolic rate?"")
+
+meta_plot
+```
+
+--
+
+$$log(bmr_i) = log(\beta_0) + \beta_1 log(mass_i) + \epsilon_i$$
+$$bmr_i = \beta_0 mass_i^{\beta_1}  \epsilon_i$$
+---
+
+# But we have wider classes of problems...
+### Non-Normal Nonlinear Models
+```{r crypto_data, fig.height = 5}
+crypto <- read.csv(""data/25/cryptoDATA.csv"") %>%
+  mutate(success=Y/N)
+
+qplot(Dose, success, data=crypto) +
+  theme_bw(base_size=16) +
+  ylab(""Fraction of Mice Infected"") +
+  stat_smooth(aes(weight = N),
+              method = ""glm"",
+              method.args = list(family = binomial),
+              color = ""red"") +
+  labs(subtitle = ""Dose-Infection Curve for Mouse\nDrug Trial"")
+```
+
+--
+
+$$infected_i \sim B(prob.\ infected_i, size = trials)$$
+$$logit(prob.\ infected_i) = \beta_0 + \beta_1 Dose_i$$
+---
+
+# But we have wider classes of problems...
+### Goodness of Fit
+.pull-left[
+```{r}
+library(tibble)
+days <- c(""Sunday"", ""Monday"", ""Tuesday"", ""Wednesday"", ""Thursday"", ""Friday"", ""Saturday"")
+birth_vec <- c(33,	41,	63,	63,	47,	56,	47)
+births <- tibble(`Day of the Week` = days, Births = birth_vec)
+
+kable(births, ""html"") %>% 
+  kableExtra::kable_styling(bootstrap_options = ""striped"")
+```
+]
+
+.pull-right[
+```{r, fig.height = 8}
+glm_chisq <- glm(Births ~ `Day of the Week`,
+                 family = poisson(),
+                 data = births)
+
+ggplot(births,
+       aes(x = Births, y = `Day of the Week`)) +
+  geom_point(size = 2) +
+  labs(subtitle = ""Are births spread evenly\nacross the week?"")+
+  theme_bw(base_size = 24)
+```
+]
+--
+
+$$births_i \sim P(\widehat{births_i})$$
+$$log(\widehat{births_i}) = \beta_1Monday_i + \beta_2Tuesday_i+...$$
+
+---
+
+# But we have wider classes of problems...
+### Contingency Tables
+
+.pull-left[
+![](./images/10/eizaguirre_lab_fish_parasite.jpg)
+```{r ctab, echo=FALSE}
+my_counts <- tribble(
+  ~Is_Eaten, ~Infection, ~Count,
+  ""Eaten by Birds"", ""Uninfected"", 1,
+  ""Eaten by Birds"", ""Lightly Infected"", 10, 
+  ""Eaten by Birds"", ""Heavily Infected"", 37,
+  ""Not Eaten by Birds"", ""Uninfected"", 49,
+  ""Not Eaten by Birds"", ""Lightly Infected"", 35,
+  ""Not Eaten by Birds"", ""Heavily Infected"", 9
+  
+)
+
+ctab <- xtabs(Count ~ Is_Eaten + Infection, data = my_counts)
+
+#kable(ctab, ""html"") %>% kableExtra::kable_styling(""striped"")
+
+```
+
+]
+
+.pull-right[
+```{r, fig.height = 8}
+ggplot(my_counts,
+       aes(y = Count, fill = Infection, x = Is_Eaten)) +
+  geom_col(position = ""dodge"") +
+  xlab("""") +
+  theme_bw(base_size = 25)
+```
+]
+
+--
+
+$$count_i \sim P(\widehat{count_i})$$
+$$log(\widehat{count_i}) = \beta_1eaten_i + \beta_2infection_i+ \beta_3 eaten_i *infection_i$$
+
+---
+# IT'S ALL VARIANTS OF A (GENERALIZED) LINEAR MODEL
+
+$$\large f(\widehat{y_i}) = \beta_0 + \beta_1 x1_i + \beta_2 x2_i + \ldots$$
+
+<br>
+  
+$$\large y_i \sim Dist(mean = \widehat{y_i}, dispersion = s(\widehat{y_i}))$$
+  
+
+
+  
+
+- X can be continuous OR discrete (turned into 1 and 0)   
+- Y can be continous, ranks, etc.  
+- f(Y) can be an identity, log, logit, or other relationship  
+- Dist can be normal, or really anything  
+- s(Y) can be just a single number, a linear, or nonlinear function  
+
+---
+class:center, middle
+
+![](images/all_linear_models/Gump-Linear-Regression-company.jpg)
+
+---
+# YES, YOU KNOW EVERYTHING
+
+1. Beastiary of Everything As a Linear Model
+
+2. .red[T-Tests as a Linear Model]
+
+---
+# Classical Approach: T-test
+![](images/09/t_distribution.png)
+
+---
+# Classical Approach: T-test
+![](images/09/t.png)
+
+
+---
+
+# The One-Sample T-Test
+
+- We take some data
+
+- We estimate it's mean and SE
+
+- We calculate whether it is different from some hypothesis (usually 0)
+
+--
+
+- But.... isn't this just an intercept only model?
+
+$$\Large y_i = \beta_0 + \epsilon_i$$
+
+---
+
+# Consider Climate-Change Driven Range Shifts
+
+- Is the distribution of range-shifts 0?
+
+```{r}
+shift <- read_csv(""https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter11/chap11q01RangeShiftsWithClimateChange.csv"")
+
+library(patchwork)
+(ggplot(shift,
+       aes(x = elevationalRangeShift)) +
+  geom_density()) +
+(ggplot(shift,
+       aes(x = elevationalRangeShift)) +
+  geom_histogram(bins = 10)) 
+
+```
+
+.small[Chen, I-C., J. K. Hill, R. Ohlemüller, D. B. Roy, and C,. D. Thomas. 2011. Science 333:1024-1026.]
+
+---
+
+# Testing a Mean with an Intercept Only Model
+
+```{r in_only, echo = TRUE}
+shift_mod <- lm(elevationalRangeShift ~ 1, data = shift)
+
+shift_t <- t.test(shift$elevationalRangeShift)
+```
+
+LM Coefficients versus T-Test
+```{r}
+display_tab <- . %>%
+  kable(""html"") %>%
+  kableExtra::kable_styling()
+
+tidy(shift_mod) %>% display_tab
+
+tidy(shift_t)[,1:3] %>% display_tab
+```
+
+---
+
+# What if we are looking at Change Between Pairs
+
+- Classically, we have the **Paired T-Test**
+
+- We look at differences between pairs
+     - Could be One Individual over time
+     - Could be two plots next to each other
+     - Deliciously simple
+     
+- This is just an intercept only model where the Difference is our response variable
+
+$$\Large (y1 - y2)_i = \beta_0 + \epsilon_i$$
+---
+# Does bird immunococompetence decrease after a testosterone implant?
+.center[
+![](images/09/blackbird.jpg)
+]
+
+```{r load_blackbird}
+blackbird <- read.csv(""./data/09/12e2BlackbirdTestosterone.csv"") %>%
+  mutate(Bird = 1:n())
+
+b_tidy <- gather(blackbird, When, Antibody, -c(Bird)) %>%
+  filter((When %in% c(""Before"", ""After""))) %>%
+  mutate(When = forcats::fct_rev(When))
+```
+
+---
+# Differences in Antibody Performance
+```{r blackbird_plot}
+ggplot(data=b_tidy, aes(x=When, y=Antibody, group=Bird)) +
+  geom_point(color=""red"") +
+  geom_line() +
+  theme_bw(base_size=18)
+```
+
+---
+# Again, Intercept Only Model v. T-Test
+```{r diff_mod}
+diff_mod <- lm((After - Before) ~ 1, data = blackbird)
+
+diff_t <- t.test(blackbird$After, blackbird$Before,
+                 paired = TRUE)
+```
+
+LM Coefficients versus T-Test
+```{r}
+tidy(diff_mod) %>% display_tab
+
+tidy(diff_t)[,1:3] %>% display_tab
+```
+
+---
+# Consider Comparing Two Means
+#### Consider the Horned Lizard 
+.center[
+![:scale 50%](images/09/horned_lizard.jpg)
+]
+
+
+Horns prevent these lizards from being eaten by birds. Are horn lengths different between living and dead lizards, indicating selection pressure?
+
+---
+# The Data
+```{r lizard_load, warning=FALSE}
+library(readr)
+lizards <- read_csv(""./data/09/12e3HornedLizards.csv"",
+                    col_types = ""di"") %>%
+  mutate(Status = ifelse(Survive==1, ""Living"", ""Dead"")) %>%
+  filter(!is.na(`Squamosal horn length`))
+
+ggplot(lizards) +
+  aes(x=Status, y=`Squamosal horn length`, fill=Status) +
+  geom_boxplot() +
+  theme_bw(base_size=17)
+```
+
+---
+# Looking at Means and SE
+```{r lizard_mean, warning=FALSE}
+liz_mean <- ggplot(lizards) +
+  aes(x=Status, y=`Squamosal horn length`) +
+  geom_jitter(alpha = 0.2) +
+  stat_summary(color = ""red"", fun.data = mean_cl_boot)
+
+liz_mean
+```
+
+
+---
+# What is Really Going On?
+```{r lizard_mean, warning=FALSE}
+```
+--
+What if we think of Dead = 0, Living = 1
+
+---
+# Let's look at it a different way
+
+.center[
+![](images/all_linear_models/graph-me.png)
+]
+---
+# This is Just a Linear Regression
+```{r}
+lizards <- lizards %>%
+  mutate(Status_numeric = as.numeric(as.factor(Status))-1)
+
+ggplot(lizards) +
+  aes(x=Status_numeric, y=`Squamosal horn length`) +
+  geom_point(alpha = 0.2) +
+  stat_summary(color = ""red"", fun.data = mean_cl_boot) +
+stat_smooth(method = ""lm"", color = ""red"")
+```
+
+$$Length_i = \beta_0 + \beta_1 Status_i + \epsilon_i$$
+---
+# You're Not a Dummy, Even if Your Code Is
+
+
+$$Length_i = \beta_0 + \beta_1 Status_i + \epsilon_i$$
+
+- Setting $Status_i$ to 0 or 1 (Dead or Living) is called Dummy Coding
+
+- We can always turn groups into ""Dummy"" 0 or 1
+
+- We could even fit a model wit no $\beta_0$ and code Dead = 0 or 1 and Living = 0 or 1
+
+- This approach works for any **nominal variable**
+
+---
+# How Do We Analyze This?
+
+- T-Test of Coefficients tells you difference between dead and live
+
+--
+
+- Wait, did you say **T-Test**?
+     - Yes, we're doing *the same thing* in coefficient evaluation
+     
+--
+
+- F-tests still useful for looking at variation explained
+    - Also $R^2$
+    
+--
+
+- You can even use likelihood, Bayes, CV, and other tools
+
+--
+
+- You get more for your money with linear models!
+
+---
+# Fit and Compare!
+
+```{r t_mod, echo = TRUE}
+t_mod <- lm(`Squamosal horn length` ~ Status, 
+            data = lizards)
+```
+
+Coefficients from Linear Model:
+```{r t_comp}
+tidy(t_mod)[2,] %>% 
+  kable() %>%
+  kableExtra::kable_styling()
+```
+<!--
+F-Test
+```{r}
+tidy(anova(t_mod)) %>% 
+  kable() %>%
+  kableExtra::kable_styling()
+```
+-->
+
+T-Test
+```{r}
+tidy(t.test(`Squamosal horn length` ~ Status, 
+            data = lizards,
+            var.equal = TRUE))[,c(1:5)]%>% 
+  kable() %>%
+  kableExtra::kable_styling()
+
+```
+
+---
+# Variatons on a theme: Non-Parametric Mann-Whitney U
+
+How about those residuals?
+
+```{r}
+tidy(shapiro.test(residuals(t_mod))) %>% 
+  kable() %>%
+  kableExtra::kable_styling()
+
+hist(residuals(t_mod))
+```
+
+--
+
+Classical approach: Non-Parametric Mann-Whitney-U Test
+
+- But, are you really going to remember that?
+
+- Simpler to remember, we just transform Y values to ranks, and voila, we have a general technique
+
+---
+# Signed Rank versus Original Values
+
+```{r ranks}
+lizards <- lizards %>%
+  mutate(horn_rank = rank(`Squamosal horn length`))
+
+ggplot(lizards,
+       aes(x = horn_rank, y = `Squamosal horn length`, 
+           color = Status)) +
+  geom_point(size = 4, alpha = 0.5) +
+  scale_color_manual(values = c(""red"", ""blue""))
+```
+
+---
+# Signed Rank as Data
+
+```{r ranks_data}
+lizards <- lizards %>%
+  mutate(horn_rank = rank(`Squamosal horn length`))
+
+ggplot(lizards,
+       aes(y = horn_rank, x = Status, 
+           color = Status)) +
+  geom_point(size = 2, alpha = 0.5) +
+  scale_color_manual(values = c(""red"", ""blue""))
+```
+
+---
+# Signed Rank as Boxplots
+
+```{r ranks_boxplot}
+lizards <- lizards %>%
+  mutate(horn_rank = rank(`Squamosal horn length`))
+
+ggplot(lizards,
+       aes(y = horn_rank, x = Status, 
+           fill = Status)) +
+  geom_boxplot() +
+  scale_fill_manual(values = c(""red"", ""blue""))
+```
+
+---
+# Variations on a theme: Welch's T-Test
+
+```{r mwrank, echo = TRUE}
+mwu_mod <- lm(rank(`Squamosal horn length`) ~ Status, 
+            data = lizards)
+
+mwu <- wilcox.test(`Squamosal horn length` ~ Status, 
+            data = lizards,)
+```
+
+Linear Model on Ranks
+```{r}
+tidy(mwu_mod)[2,] %>% kable(""html"")
+```
+
+Mann-Whitney-U/Wilcoxon Rank Test
+```{r}
+tibble(MWU_p_value = mwu$p.value) %>% kable(""html"")
+```
+
+---
+# Variations on a theme: Welch's T-Test
+
+Or.... let variance be different between groups.
+i = data point, j = group
+
+$$y_{ij} = \beta_0 + \beta_1x_j + \epsilon_{ij}$$
+$$\epsilon_{ij} \sim N(0, \sigma_j)$$
+
+--
+
+Fit using **weighted least squares** - we weight LS by inverse of variance  
+
+
+--
+
+Classically, this is known as Welch's T-Test, and is the default for R
+   - Computes a pooled variance based on unequal variance/sample size
+
+---
+# Weighting by Variance
+
+```{r}
+
+lizards <- lizards %>%
+  mutate(squamosal_horn_length = `Squamosal horn length`)
+```
+
+```{r wls, echo = TRUE}
+library(nlme)
+weighted_mod <- gls(squamosal_horn_length ~ Status,
+                  weights = varIdent(form = ~1|Status),
+                  data = lizards,
+                  method = ""ML"")
+```
+
+Weighted Least Squares Results
+```{r}
+broom.mixed::tidy(weighted_mod)[2,] %>%
+  kable( ""html"") %>%
+  kableExtra::kable_styling()
+```
+
+Welch's T-Test Results
+```{r welch}
+tidy(t.test(squamosal_horn_length ~ Status,
+            data = lizards,
+            var.equal=FALSE)
+     )[,1:5] %>%
+    kable(""html"") %>%
+  kableExtra::kable_styling()
+
+```
+---
+# Don't forget...
+
+.center[
+
+![:scale 55%](images/all_linear_models/i-dont-always-5acc6f.jpg)
+
+]
+---
+# Excess Baggage
+
+- In this framework, you have to do ALL assumption tests
+
+--
+
+- BUT, t-tests have THE SAME ASSUMPTIONS
+
+--
+
+- BECASUE THEY ARE JUST LINEAR MODELS RE-ARRANGED
+
+--
+
+- AND, you gain flexibility in execution
+
+--
+
+- AND, you don't have to remember an arcane taxonomy of tests
+
+--
+
+$$\large Y = \beta X + \epsilon$$
+
+---
+# Care for Your Golem
+.center[
+![:scale 50%](images/09/golem.png)
+
+Just remember, if you are not careful, even with the simplest data, you can still burn down Prague!]
+
+---
+class: center, middle
+
+![:scale 55%](images/all_linear_models/curve-fitting-methods-and-the-messages-they-send-quadratic-logarithmic-linear-63265166.png)
\ No newline at end of file"
biol607,biol607.github.io,5288569219f57ea1ffd3824d1221a1ce45b12bb0,jebyrnes,jarrett.byrnes@umb.edu,2022-10-07T03:29:58Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-07T03:29:58Z,Fix menu issue.,lab/06_lm.Rmd;lab/06_lm.html;lab/06_lm_files/figure-html/performance-1.png;lab/06_lm_files/figure-html/predcheck-1.png;lab/06_lm_files/figure-html/unnamed-chunk-1-1.png,True,False,True,False,10,12,22,"---FILE: lab/06_lm.Rmd---
@@ -33,7 +33,8 @@ Let's go through each step with an example of seals. Are older seals larger?
 
 ### 0. Load and visualize the data
 
-## Are Older Seals Bigger?
+OK, this should be second nature by now. Load it. Make a plot of just the data.
+
 ```{r}
 library(dplyr)
 library(ggplot2)
@@ -42,8 +43,7 @@ theme_set(theme_bw(base_size = 16))
 seals <- read.csv(""lab/data/17e8ShrinkingSeals Trites 1996.csv"")
 
 seal_base <- ggplot(seals, aes(x=age.days, y=length.cm)) +
-  geom_point() +
-  theme_grey(base_size=14)
+  geom_point() 
 
 seal_base
 ```"
biol607,biol607.github.io,4be39ab1d998088304580623627c1a5e0662ece5,jebyrnes,jarrett.byrnes@umb.edu,2022-10-07T02:05:31Z,jebyrnes,jarrett.byrnes@umb.edu,2022-10-07T02:05:31Z,Fix numbering,homework/function_iteration.html;homework/function_iteration.qmd,True,False,True,False,4,4,8,"---FILE: homework/function_iteration.html---
@@ -195,11 +195,11 @@ <h2 class=""anchored"" data-anchor-id=""precision-and-sampling-the-exponential"">3.
 <div class=""cell"">
 
 </div>
-<p>&nbsp; &nbsp; 3e. With this result, group by rate and sample size again and calculate the sd of each measure. Then plot the resulting curves showing the influnce of sample size on the precision of our estimate for mean and median. What does this tell you? Note, again, you might want to think about <code>pivot_longer()</code> to get everything into a nice format for use with <code>ggplot()</code></p>
+<p>&nbsp; &nbsp; 3f. With this result, group by rate and sample size again and calculate the sd of each measure. Then plot the resulting curves showing the influnce of sample size on the precision of our estimate for mean and median. What does this tell you? Note, again, you might want to think about <code>pivot_longer()</code> to get everything into a nice format for use with <code>ggplot()</code></p>
 <div class=""cell"">
 
 </div>
-<p>&nbsp; &nbsp; 3f. Well. What should your sample size be and why under different rates?</p>
+<p>&nbsp; &nbsp; 3g. Well. What should your sample size be and why under different rates?</p>
 <hr>
 </section>
 <section id=""impress-yourself"" class=""level2"">

---FILE: homework/function_iteration.qmd---
@@ -152,7 +152,7 @@ ggplot(sims,
   facet_grid(cols = vars(samp_size), rows = vars(rate))
 ```
 
-&nbsp; &nbsp; 3e. With this result, group by rate and sample size again and calculate the sd of each measure. Then plot the resulting curves showing the influnce of sample size on the precision of our estimate for mean and median. What does this tell you? Note, again, you might want to think about `pivot_longer()` to get everything into a nice format for use with `ggplot()`
+&nbsp; &nbsp; 3f. With this result, group by rate and sample size again and calculate the sd of each measure. Then plot the resulting curves showing the influnce of sample size on the precision of our estimate for mean and median. What does this tell you? Note, again, you might want to think about `pivot_longer()` to get everything into a nice format for use with `ggplot()`
 
 ```{r}
 sims_long <- sims |>
@@ -172,7 +172,7 @@ ggplot(sims_long_se,
   facet_wrap(vars(stat))
 ```
 
-&nbsp; &nbsp; 3f. Well. What should your sample size be and why under different rates?
+&nbsp; &nbsp; 3g. Well. What should your sample size be and why under different rates?
 
 ----------
 "
biol607,biol607.github.io,3d2eaaa8c6f35eece19c7779a9a7b782a3b4cdcc,jebyrnes,jarrett.byrnes@umb.edu,2022-09-29T20:12:34Z,jebyrnes,jarrett.byrnes@umb.edu,2022-09-29T20:12:34Z,typo fix,lab/02_sim_samp.Rmd;lab/02_sim_samp.html,True,False,True,False,4,0,4,"---FILE: lab/02_sim_samp.Rmd---
@@ -18,6 +18,8 @@ knitr::opts_chunk$set(echo = TRUE,
 # libraries
 library(dplyr)
 library(ggplot2)
+
+#set that theme
 theme_set(theme_bw(base_size = 12))
 ```
 ## 1. A purrr-fect simulation

---FILE: lab/02_sim_samp.html---
@@ -1519,6 +1519,8 @@ <h2>0. Some setup</h2>
 <pre class=""r""><code># libraries
 library(dplyr)
 library(ggplot2)
+
+#set that theme
 theme_set(theme_bw(base_size = 12))</code></pre>
 </div>
 <div id=""a-purrr-fect-simulation"" class=""section level2"">"
biol607,biol607.github.io,e12bbc6f92edd765380bcd242405cc67f19c16d5,jebyrnes,jarrett.byrnes@umb.edu,2022-09-28T18:20:12Z,jebyrnes,jarrett.byrnes@umb.edu,2022-09-28T18:20:12Z,Fixing cheapsheet links.,schedule.Rmd;schedule.html,True,False,True,False,3,3,6,"---FILE: schedule.Rmd---
@@ -75,7 +75,7 @@ __Lecture:__  [Data import, using libraries](lectures/06_read_data_libraries.htm
 __Data:__ [Portal Data](https://ndownloader.figshare.com/files/2292169) - and learn more [here](https://portal.weecology.org/)  
 __Reading:__  [Data organization in spreadsheets](./readings/Browman_and_Woo_Spreadsheets.pdf), [10 Commandments for Good Data Managament](https://dynamicecology.wordpress.com/2016/08/22/ten-commandments-for-good-data-management/), W&G Chapters on [data import](https://r4ds.hadley.nz/data-import.html),  [pipes](https://r4ds.hadley.nz/workflow-pipes.html), [data transformation](https://r4ds.hadley.nz/data-transform.html), [tidy data](https://r4ds.hadley.nz/data-tidy.html)    
 __Optional Reading:__ [Managing Data Frames with the Dplyr package](https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html), [Strings](http://r4ds.had.co.nz/strings.html), [factors](https://r4ds.hadley.nz/factors.html), and [Dates](http://r4ds.had.co.nz/dates-and-times.html)   
-__Cheat Sheets:__ [Reading data into R](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf), [Dplyr cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfPaperpile).  
+__Cheat Sheets:__ [Reading data into R](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf), [Dplyr cheat sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf).  
 __Packages:__ `install.packages(c(""dplyr"", ""janitor"", ""skimr"", ""lubridate"", ""tidyr"", ""readr"", ""readxl"", ""tibble""))` - [readr](https://readr.tidyverse.org/),  [readxl](https://readxl.tidyverse.org/),  [tibble](https://tibble.tidyverse.org/),  [skimr](https://docs.ropensci.org/skimr/), 
 [janitor](https://garthtarr.github.io/meatR/janitor.html),
 [visdat](https://docs.ropensci.org/visdat/)  

---FILE: schedule.html---
@@ -438,9 +438,9 @@ <h3>Week 3.</h3>
 href=""https://r4ds.hadley.nz/factors.html"">factors</a>, and <a
 href=""http://r4ds.had.co.nz/dates-and-times.html"">Dates</a><br />
 <strong>Cheat Sheets:</strong> <a
-href=""https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf"">Reading
+href=""https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf"">Reading
 data into R</a>, <a
-href=""https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfPaperpile"">Dplyr
+href=""https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf"">Dplyr
 cheat sheet</a>.<br />
 <strong>Packages:</strong>
 <code>install.packages(c(""dplyr"", ""janitor"", ""skimr"", ""lubridate"", ""tidyr"", ""readr"", ""readxl"", ""tibble""))</code>"
biol607,biol607.github.io,3a79437a0440f250d8e04e2bee6fc132cc02db3c,jebyrnes,jarrett.byrnes@umb.edu,2022-09-27T14:46:27Z,jebyrnes,jarrett.byrnes@umb.edu,2022-09-27T14:46:27Z,smol fix to spacing,lectures/functions.Rmd,True,False,True,False,1,1,2,"---FILE: lectures/functions.Rmd---
@@ -1,5 +1,5 @@
 ---
-title:
+title: 
 css: style.css
 output:
   revealjs::revealjs_presentation:"
biol607,biol607.github.io,13fd94b93686b23a873e33a182502e27c1ed2198,jebyrnes,jarrett.byrnes@umb.edu,2022-09-23T03:13:55Z,jebyrnes,jarrett.byrnes@umb.edu,2022-09-23T03:13:55Z,Fix a linebreak,schedule.Rmd;schedule.html,True,False,True,False,2,2,4,"---FILE: schedule.Rmd---
@@ -78,7 +78,7 @@ __Optional Reading:__ [Strings](http://r4ds.had.co.nz/strings.html), [factors](h
 __Cheat Sheets:__ [Reading data into R](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf).  
 __Packages:__ `install.packages(c(""dplyr"", ""janitor"", ""skimr"", ""lubridate"", ""tidyr"", ""readr"", ""readxl"", ""tibble""))` - [readr](https://readr.tidyverse.org/),  [readxl](https://readxl.tidyverse.org/),  [tibble](https://tibble.tidyverse.org/),  [skimr](https://docs.ropensci.org/skimr/), 
 [janitor](https://garthtarr.github.io/meatR/janitor.html),
-[visdat](https://docs.ropensci.org/visdat/)
+[visdat](https://docs.ropensci.org/visdat/)  
 __Etherpad:__ http://etherpad.wikimedia.org/p/607-tidy-2022  
 <!-- __In Class Code:__ [functions](in_class_code/2020/scripts/functions.R), [loading data](in_class_code/2020/scripts/loading_data.R), [data reshaping](in_class_code/2020/scripts/tidyr.R), [created data](in_class_code/2020/data/test_data.xlsx) -->
 __Homework:__ [Functions and Pivot homework](./homework/04_fun_pivot.html)  

---FILE: schedule.html---
@@ -447,7 +447,7 @@ <h3>Week 3.</h3>
 href=""https://tibble.tidyverse.org/"">tibble</a>, <a
 href=""https://docs.ropensci.org/skimr/"">skimr</a>, <a
 href=""https://garthtarr.github.io/meatR/janitor.html"">janitor</a>, <a
-href=""https://docs.ropensci.org/visdat/"">visdat</a>
+href=""https://docs.ropensci.org/visdat/"">visdat</a><br />
 <strong>Etherpad:</strong> <a
 href=""http://etherpad.wikimedia.org/p/607-tidy-2022""
 class=""uri"">http://etherpad.wikimedia.org/p/607-tidy-2022</a><br />"
biol607,biol607.github.io,a80f20fbbe6898f36dedcae08d399f3ff5711094,jebyrnes,jarrett.byrnes@umb.edu,2022-09-13T16:34:11Z,jebyrnes,jarrett.byrnes@umb.edu,2022-09-13T16:34:11Z,Animation fixes,lectures/05_data_viz_principles.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,cf8123d20403c27c888adf71a426576dd644133f,jebyrnes,jarrett.byrnes@umb.edu,2022-09-04T19:51:29Z,jebyrnes,jarrett.byrnes@umb.edu,2022-09-04T19:51:29Z,typo fix,schedule.Rmd,True,False,True,False,1,1,2,"---FILE: schedule.Rmd---
@@ -149,7 +149,7 @@ __Lecture:__ Tidy Tuesday for Election Day, [It
 s All Linear Models; t-test edition](lectures/t_as_lm.html)      
 __Lab Topic:__ [Comparing Means](lab/comparing_two_means.html), Final Proposal Meetings	  
 __Reading:__ [Common statistical tests are linear models](https://lindeloev.github.io/tests-as-linear/), W&S Chapters 11-12    
-__Packages for The Week:__ ` install.packages(c(""car"", ""emmeans"", ""multcompView"", ""contrast""))`  
+__Packages for The Week:__ `install.packages(c(""car"", ""emmeans"", ""multcompView"", ""contrast""))`  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-linear_everywhere-2022
  __In Class Code:__ [comparing two means](./in_class_code/2020/scripts/comparing_means.R)  
    "
biol607,biol607.github.io,92a050fc619fe677950a032c849d169a4d5351a6,jebyrnes,jarrett.byrnes@umb.edu,2020-12-10T17:44:30Z,jebyrnes,jarrett.byrnes@umb.edu,2020-12-10T17:44:30Z,Typo fix.,lectures/causal_models_regression.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,9fc727e003f303825b9e023a9a45d9d4ee58f618,jebyrnes,jarrett.byrnes@umb.edu,2020-12-10T15:51:34Z,jebyrnes,jarrett.byrnes@umb.edu,2020-12-10T15:51:34Z,Typo fix,lectures/expt_designs.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,ad588c56e3c2ad8180dcc9de8c0c183a572d8304,jebyrnes,jarrett.byrnes@umb.edu,2020-11-24T14:36:28Z,jebyrnes,jarrett.byrnes@umb.edu,2020-11-24T14:36:28Z,Fixed bad alpha in HW,homework/glm.Rmd;homework/glm.html,True,False,True,False,2,2,4,"---FILE: homework/glm.Rmd---
@@ -141,7 +141,7 @@ leveneTest(foul_mod_porp)
 ```
 
 
-**2.4** Great! So, take us home! Using NHST with an alpha of 0.8 (why not), what does this fit model tell you about whether predation matters given how I have described the system? Feel free to replot the data or fit model results if helpful
+**2.4** Great! So, take us home! Using NHST with an alpha of 0.08 (why not), what does this fit model tell you about whether predation matters given how I have described the system? Feel free to replot the data or fit model results if helpful
 
 ```{r}
 #no int?

---FILE: homework/glm.html---
@@ -390,7 +390,7 @@ <h2>2. Comparing Means from Multiple Categories</h2>
 <li><p>Calculate difference in logit cover (so, logist(initial cover) - logit(final cover)). Logit transformations linearize percent cover data, and are often all that is needed to work percent cover into a linear model. You can use <code>car::logit()</code> for this.</p></li>
 </ol>
 <p>Try all three methods. Which one works so that you can produce valid inference?</p>
-<p><strong>2.4</strong> Great! So, take us home! Using NHST with an alpha of 0.8 (why not), what does this fit model tell you about whether predation matters given how I have described the system? Feel free to replot the data or fit model results if helpful</p>
+<p><strong>2.4</strong> Great! So, take us home! Using NHST with an alpha of 0.08 (why not), what does this fit model tell you about whether predation matters given how I have described the system? Feel free to replot the data or fit model results if helpful</p>
 </div>
 <div id=""comparing-means-with-covariates"" class=""section level2"">
 <h2>3. Comparing Means with Covariates</h2>"
biol607,biol607.github.io,ef842444339cf1db97ef4a03ace715859f6194bf,jebyrnes,jarrett.byrnes@umb.edu,2020-11-10T15:34:30Z,jebyrnes,jarrett.byrnes@umb.edu,2020-11-10T15:34:30Z,Fix finite population SD error,lectures/anova_1.Rmd;lectures/anova_1_files/figure-html/banova-1.png,True,False,True,False,9,10,19,"---FILE: lectures/anova_1.Rmd---
@@ -500,20 +500,19 @@ car::Anova(brain_glm) %>% tidy %>%
 # BANOVA: Compare SD due to Groups versus SD due to Residual
 
 ```{r banova}
-chains <- brainGene %>%
-  add_fitted_draws(brain_brm) %>%
-  mutate(.residual = PLP1.expression - .value) %>%
-  group_by(.draw) %>%
-  summarize(residual_sd = .residual[1],
-            group_sd = sd(.value))
 
 
-chains %>%
+sd_chains <- modelr::data_grid(brainGene,
+                             group = unique(group)) %>%
+  add_fitted_draws(brain_brm) %>%
+  group_by(.draw) %>%
+  summarize(group_sd = sd(.value)) %>%
+  mutate(residual_sd = as.data.frame(brain_brm)$sigma) %>%
   pivot_longer(cols = c(group_sd, residual_sd),
                names_to = ""type"",
-               values_to = ""value"") %>%
-  
-  ggplot(aes(x = value, y = type)) +
+               values_to = ""value"") 
+
+ggplot(sd_chains, aes(x = value, y = type)) +
   stat_halfeye() + 
   labs(y = """", x = """")
 ```"
biol607,biol607.github.io,73463fedb0d3cd0a32d83848736dcb8e90c4ee44,jebyrnes,jarrett.byrnes@umb.edu,2020-11-10T15:25:17Z,jebyrnes,jarrett.byrnes@umb.edu,2020-11-10T15:25:17Z,Fixed link to bayesian lm lab,schedule.Rmd;schedule.html,True,False,True,False,2,2,4,"---FILE: schedule.Rmd---
@@ -134,7 +134,7 @@ __Reading:__ [Ellison 1996](http://byrneslab.net/classes/biol607/readings/Elliso
 __Additional Reading__: [how to choose a prior](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations), [bayesian t-tests](https://vuorre.netlify.com/post/2017/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/), [regression models with brms](https://magesblog.com/post/2015-09-01-bayesian-regression-models-using-stan/), [rethinking with brms](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/) (many very cool examples), [brms tutorials](https://www.rensvandeschoot.com/tutorials/brms/), [How to use rstanarm](https://cran.r-project.org/web/packages/rstanarm/vignettes/rstanarm.html), [Linear Models in rstanarm](https://cran.r-project.org/web/packages/rstanarm/vignettes/lm.html), [Bayesian basics with R](https://m-clark.github.io/bayesian-basics/)  
 __Packages for The Week:__ `install.packages(c(""brms"", ""rstanarm"", ""arm"", ""rethinking""))` and `devtools::install_github(""mjskay/tidybayes"")`  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-cv-bayes-2020, https://etherpad.wikimedia.org/p/607-bayes-lm-2020    
-__In Class Code:__ [Cross-Validation](in_class_code/2020/scripts/crossvalidation.R), [Bayesian Demo](in_class_code/2020/excel_exercises/bayes_xls.xlsx), [Bayesian Linear Model](in_class_code/2020/bayes_lm.R)   
+__In Class Code:__ [Cross-Validation](in_class_code/2020/scripts/crossvalidation.R), [Bayesian Demo](in_class_code/2020/excel_exercises/bayes_xls.xlsx), [Bayesian Linear Model](in_class_code/2020/scripts/bayes_lm.R)   
 __Homework:__ [CV and some Bayesian Grid sampling](homework/cv_homework.html)  
 
 **MIDTERM**: [here](exams/exam2020.html)  

---FILE: schedule.html---
@@ -419,7 +419,7 @@ <h3>Week 7 &amp; 8.</h3>
 <strong>Additional Reading</strong>: <a href=""https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations"">how to choose a prior</a>, <a href=""https://vuorre.netlify.com/post/2017/how-to-compare-two-groups-with-robust-bayesian-estimation-using-r-stan-and-brms/"">bayesian t-tests</a>, <a href=""https://magesblog.com/post/2015-09-01-bayesian-regression-models-using-stan/"">regression models with brms</a>, <a href=""https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/"">rethinking with brms</a> (many very cool examples), <a href=""https://www.rensvandeschoot.com/tutorials/brms/"">brms tutorials</a>, <a href=""https://cran.r-project.org/web/packages/rstanarm/vignettes/rstanarm.html"">How to use rstanarm</a>, <a href=""https://cran.r-project.org/web/packages/rstanarm/vignettes/lm.html"">Linear Models in rstanarm</a>, <a href=""https://m-clark.github.io/bayesian-basics/"">Bayesian basics with R</a><br />
 <strong>Packages for The Week:</strong> <code>install.packages(c(""brms"", ""rstanarm"", ""arm"", ""rethinking""))</code> and <code>devtools::install_github(""mjskay/tidybayes"")</code><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-cv-bayes-2020"" class=""uri"">https://etherpad.wikimedia.org/p/607-cv-bayes-2020</a>, <a href=""https://etherpad.wikimedia.org/p/607-bayes-lm-2020"" class=""uri"">https://etherpad.wikimedia.org/p/607-bayes-lm-2020</a><br />
-<strong>In Class Code:</strong> <a href=""in_class_code/2020/scripts/crossvalidation.R"">Cross-Validation</a>, <a href=""in_class_code/2020/excel_exercises/bayes_xls.xlsx"">Bayesian Demo</a>, <a href=""in_class_code/2020/bayes_lm.R"">Bayesian Linear Model</a><br />
+<strong>In Class Code:</strong> <a href=""in_class_code/2020/scripts/crossvalidation.R"">Cross-Validation</a>, <a href=""in_class_code/2020/excel_exercises/bayes_xls.xlsx"">Bayesian Demo</a>, <a href=""in_class_code/2020/scripts/bayes_lm.R"">Bayesian Linear Model</a><br />
 <strong>Homework:</strong> <a href=""homework/cv_homework.html"">CV and some Bayesian Grid sampling</a></p>
 <p><strong>MIDTERM</strong>: <a href=""exams/exam2020.html"">here</a></p>
 </div>"
biol607,biol607.github.io,d906d5eb3a28bbf7e2ab17bd915292ee85e848f5,jebyrnes,jarrett.byrnes@umb.edu,2020-11-02T15:02:14Z,jebyrnes,jarrett.byrnes@umb.edu,2020-11-02T15:02:14Z,fix errors,exams/exam2020.html,False,False,False,False,10,11,21,"---FILE: exams/exam2020.html---
@@ -396,7 +396,7 @@ <h3>2a) Access (5 points)</h3>
 </div>
 <div id=""b-its-big-and-wide-10-points"" class=""section level3"">
 <h3>2b) It’s big and wide! (10 Points)</h3>
-<p>The data is, well, huge. It’s also wide, with dates as columns. Write a function that, given a state, will output a timeseries (long data) of cummulative cases in that state as well as new daily cases.</p>
+<p>The data is, well, huge. It’s also wide, with dates as columns. Write a function that, given a state, will output a time series (long data where every row is a day) of cummulative cases in that state as well as new daily cases.</p>
 <p>Note, let’s make the date column that emerges a true date object. Let’s say you’ve called it <code>date_col</code>. If you mutate it, <code>mutate(date_col = lubridate::mdy(date_col))</code>, it will be turned into a date object that will have a recognized order. {lubridate} is da bomb, and I’m hoping we have some time to cover it in the future.</p>
 <p>+5 extra credit for merging it with some other data source to also return cases per 100,000 people.</p>
 </div>
@@ -406,7 +406,7 @@ <h3>2c) Let’s get visual! (10 Points)</h3>
 </div>
 <div id=""d-at-our-fingertips-10-points"" class=""section level3"">
 <h3>2d) At our fingertips (10 Points)</h3>
-<p>Cool. Now, write a function that will take what you did above, and do it dynamically for any state. +3 EC if you highlight points of interest - but dynamically using the data. Note, you might need to do some funky stuff to make things fit well in the plot for this one. Or, meh.</p>
+<p>Cool. Now, write a function that will take what you did above, and create a plot for any state - so, I enter Alaska and I get the plot for Alaska! +2 if it can do daily or cumulative cases - or cases per 100,000 if you did that above. +3 EC if you highlight points of interest - but dynamically using the data. Note, you might need to do some funky stuff to make things fit well in the plot for this one. Or, meh.</p>
 </div>
 <div id=""d-extra-credit-go-wild-on-data-viz-5-points-each"" class=""section level3"">
 <h3>2d Extra Credit) Go wild on data viz (5 Points each)</h3>
@@ -416,12 +416,11 @@ <h3>2d Extra Credit) Go wild on data viz (5 Points each)</h3>
 <div id=""lets-get-philosophical.-10-points"" class=""section level2"">
 <h2>3) Let’s get philosophical. (10 points)</h2>
 <p>We have discussed multiple inferential frameworks this semester. Frequentist NHST, Likelihood and model comparison, Baysian probabilistic thinking, Assessment of Predictive Ability (which spans frameworks!), and more. We’ve talked about Popper and Lakatos. Put these pieces of the puzzle together and look deep within yourself.</p>
-<p>What do you feel if the inferential framework that you adopt as a scientist? Why? Include in your answer why you prefer the inferential tools (e.g. confidence intervals, test statistics, out-of-sample prediction, posterior probabilities, etc.) of your chosen worldview and why you do not like the ones of the other one. This includes defining just what those different tools mean, as well as relating them to the things you study. <strong>extra credit for citing and discussing outside sources - one point per source/point</strong></p>
+<p>What do you feel is the inferential framework that you adopt as a scientist? Why? Include in your answer why you prefer the inferential tools (e.g. confidence intervals, test statistics, out-of-sample prediction, posterior probabilities, etc.) of your chosen worldview and why you do not like the ones of the other one. This includes defining just what those different tools mean, as well as relating them to the things you study. <strong>extra credit for citing and discussing outside sources - one point per source/point</strong></p>
 </div>
 <div id=""bayes-theorem-10-points"" class=""section level2"">
 <h2>4) Bayes Theorem (10 points)</h2>
-<p>I’ve referenced the following figure a few times. I’d like you to demonstrate your understanding of Bayes Theorem <strong>by hand</strong> showing what <strong>the probability of the sun exploding is given the yes</strong>. Assume that your prior probability that the sun explodes is p(Sun Explodes) = 0.0001 (I’ll leave it to you to get p(Sun Doesn’t Explode). The rest of the information you need - and some you don’t - is in the cartoon - p(Yes | Explodes), p(Yes | Doesn’t Explode), p(No | Explodes), p(No | Doesn’t Explode). And remember -</p>
-<p><span class=""math display"">\[\large p(H|D) = \frac{p(D|H)p(H)}{p(D)}\]</span></p>
+<p>I’ve referenced the following figure a few times. I’d like you to demonstrate your understanding of Bayes Theorem <strong>by hand</strong> (e.g. calculate it out and show your work - you can do this all in R, I’m not a monster) showing what is <strong>the probability of the sun exploding is given that the device said yes</strong>. Assume that your prior probability that the sun explodes is p(Sun Explodes) = 0.0001 (I’ll leave it to you to get p(Sun Doesn’t Explode). The rest of the information you need - and some you don’t - is in the cartoon - p(Yes | Explodes), p(Yes | Doesn’t Explode), p(No | Explodes), p(No | Doesn’t Explode).</p>
 <p><img src=""data:image/png;base64,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"" /></p>
 <div id=""a-extra-credit-10-points"" class=""section level3"">
 <h3>4a Extra Credit (10 Points)</h3>
@@ -435,16 +434,16 @@ <h2>5) Quailing at the Prospect of Linear Models</h2>
 <h3>5a) Three fits (10 points)</h3>
 <p>To begin with, I’d like you to fit the relationship that describes how Tarsus (leg) length predicts upper beak (Culmen) length. Fit this relationship using least squares, likelihood, and Bayesian techniques. For each fit, demonstrate that the necessary assumptions have been met. Note, functions used to fit with likelihood and Bayes may or may not behave well when fed NAs. So look out for those errors.</p>
 </div>
-<div id=""three-interpretations-10-points"" class=""section level3"">
-<h3>5.2 Three interpretations (10 points)</h3>
+<div id=""b-three-interpretations-10-points"" class=""section level3"">
+<h3>5b) Three interpretations (10 points)</h3>
 <p>OK, now that we have fits, take a look! Do the coefficients and their associated measures of error in their estimation match? How would we interpret the results from these different analyses differently? Or would we? Note, confint works on <code>lm</code> objects as well.</p>
 </div>
-<div id=""everyday-im-profilin-10-points"" class=""section level3"">
-<h3>5.3 Everyday I’m Profilin’ (10 points)</h3>
+<div id=""c-everyday-im-profilin-10-points"" class=""section level3"">
+<h3>5c) Everyday I’m Profilin’ (10 points)</h3>
 <p>For your likelihood fit, are your profiles well behaved? For just the slope, use grid sampling to create a profile. You’ll need to write functions for this, sampling the whole grid of slope and intercept, and then take out the relevant slices as we have done before. Use the results from the fit above to provide the reasonable bounds of what you should be profiling over (3SE should do). Is it well behaved? Plot the profile and give the 80% and 95% CI (remember how we use the chisq here!). Verify your results with <code>profileModel</code>.</p>
 </div>
-<div id=""the-power-of-the-prior-10-points"" class=""section level3"">
-<h3>5.4 The Power of the Prior (10 points)</h3>
+<div id=""d-the-power-of-the-prior-10-points"" class=""section level3"">
+<h3>5d) The Power of the Prior (10 points)</h3>
 <p>This data set is pretty big. After excluding NAs in the variables we’re interested in, it’s over 766 lines of data! Now, a lot of data can overwhelm a strong prior. But only to a point. Show first that there is enough data here that a prior for the slope with an estimate of 0.7 and a sd of 0.01 is overwhelmed and produces similar results to the default prior. How different are the results from the original?</p>
 <p>Second, randomly sample 10, 100, 300, and 500 data points. At which level is our prior overwhelmed (e.g., the prior slope becomes highly unlikely)? Communicate that visually in the best way you feel gets the point across, and explain your reasoning.</p>
 <p>+4 for a function that means you don’t have to copy and paste the model over and over. + 4 more if you use <code>map()</code> in combination with a tibble to make this as code-efficient as possible. This will also make visualization easier.</p>"
biol607,biol607.github.io,59d5193189fa6922c8080b0979722b78a8e1962a,jebyrnes,jarrett.byrnes@umb.edu,2020-10-29T16:29:39Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-29T16:29:39Z,Fixing typos,lectures/bayesian_lm_inference.Rmd;lectures/bayesian_lm_inference.html;lectures/bayesian_lm_inference_files/figure-html/converge-1.png;lectures/bayesian_lm_inference_files/figure-html/dist_fit-1.png;lectures/bayesian_lm_inference_files/figure-html/fit_fig-1.png;lectures/bayesian_lm_inference_files/figure-html/mcmc_params-1.png;lectures/bayesian_lm_inference_files/figure-html/pareto-1.png;lectures/bayesian_lm_inference_files/figure-html/pp_check-1.png;lectures/bayesian_lm_inference_files/figure-html/pred-1.png;lectures/bayesian_lm_inference_files/figure-html/pred-2.png;lectures/bayesian_lm_inference_files/figure-html/puffer_pred_brms-1.png;lectures/bayesian_lm_inference_files/figure-html/puffershow-1.png;lectures/bayesian_lm_inference_files/figure-html/r2-1.png;lectures/bayesian_lm_inference_files/figure-html/slopeprob-1.png;lectures/bayesian_lm_inference_files/figure-html/unnamed-chunk-1-1.png;lectures/bayesian_lm_inference_files/figure-html/unnamed-chunk-2-1.png;lectures/bayesian_lm_inference_files/figure-html/waicplot-1.png;lectures/bayesian_lm_inference_files/figure-html/waicplot_2-1.png,True,False,True,False,10,10,20,"---FILE: lectures/bayesian_lm_inference.Rmd---
@@ -406,7 +406,7 @@ $$\large lppd = \sum_i log \sum_s {p(y_i | \theta_s)}$$
 - Kinda nice, no?
 
 - It will have the same problem as MSE, etc., for overfitting
-     - More parameters = better fit = lower llpd!
+     - More parameters = better fit = higher lppd!
      
 ---
 
@@ -421,17 +421,17 @@ $$\large lppd = \sum_i log \sum_s {p(y_i | \theta_s)}$$
 ---
 # Penalty for Too Many Parameters
 
-- As # of parameters goes up, llpd goes down
+- As # of parameters goes up, lppd goes up
 
-- BUT - as # of parameters goes up, variance in llpd for each point goes up
-     - More parameters, higher SE per parameter, more variance in llpd
+- BUT - as # of parameters goes up, variance in lppd for each point goes up
+     - More parameters, higher SE per parameter, more variance in lppd
 
 --
 
 $$\large penalty = \sum_ivar(lppd_i)$$
 --
 
-- akin to # of parameters pentalty
+- akin to # of parameters penalty
 
 ---
 # The Widely Applicable Information Criterion

---FILE: lectures/bayesian_lm_inference.html---
@@ -340,7 +340,7 @@
 - Kinda nice, no?
 
 - It will have the same problem as MSE, etc., for overfitting
-     - More parameters = better fit = lower llpd!
+     - More parameters = better fit = higher lppd!
      
 ---
 
@@ -355,17 +355,17 @@
 ---
 # Penalty for Too Many Parameters
 
-- As # of parameters goes up, llpd goes down
+- As # of parameters goes up, lppd goes up
 
-- BUT - as # of parameters goes up, variance in llpd for each point goes up
-     - More parameters, higher SE per parameter, more variance in llpd
+- BUT - as # of parameters goes up, variance in lppd for each point goes up
+     - More parameters, higher SE per parameter, more variance in lppd
 
 --
 
 `$$\large penalty = \sum_ivar(lppd_i)$$`
 --
 
-- akin to # of parameters pentalty
+- akin to # of parameters penalty
 
 ---
 # The Widely Applicable Information Criterion"
biol607,biol607.github.io,71409d590ff35ff9b12120a405590fedca5a22a3,jebyrnes,jarrett.byrnes@umb.edu,2020-10-22T15:11:37Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-22T15:11:37Z,Weird error,lectures/bayes.Rmd;lectures/bayes.html,True,False,True,False,0,10,10,"---FILE: lectures/bayes.Rmd---
@@ -483,11 +483,6 @@ You can discuss the probability that your parameter is opposite in sign to its p
 
 4. .red[Priors]
 
----
-# Priors
-.center[
-![:width 55%](images/15/frequentists_vs_bayesians.png)
-]
 ---
 # The Influence of Priors
 ![](images/15/priors_and_lik.jpg)

---FILE: lectures/bayes.html---
@@ -374,11 +374,6 @@
 
 4. .red[Priors]
 
----
-# Priors
-.center[
-![:width 55%](images/15/frequentists_vs_bayesians.png)
-]
 ---
 # The Influence of Priors
 ![](images/15/priors_and_lik.jpg)"
biol607,biol607.github.io,c5a02191ec1f4679bd0bd851ac5458c65383dece,jebyrnes,jarrett.byrnes@umb.edu,2020-10-21T19:26:58Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-21T19:26:58Z,Numbering fix,homework/07_likelihood.Rmd;homework/07_likelihood.html,True,False,True,False,10,10,20,"---FILE: homework/07_likelihood.Rmd---
@@ -64,7 +64,7 @@ grid_samp %>% filter(logLik == max(logLik))
 puffer_lm
 ```
 
-**2. Surfaces!** Filter the dataset to the MLE of the SD. Plot the surface for the slope and intercept in whatever way you find most compelling. You might want to play around with zooming in to different regions, etc. Have fun!
+**4. Surfaces!** Filter the dataset to the MLE of the SD. Plot the surface for the slope and intercept in whatever way you find most compelling. You might want to play around with zooming in to different regions, etc. Have fun!
 
 ```{r ggplot}
 library(ggplot2)
@@ -78,7 +78,7 @@ ggplot(grid_samp %>% filter(resid_sd == 2.9)
 
 ```
 
-**4. GLM!** Now, compare those results to results from glm. Show the profiles and confidence intervals from `glm()` for the slope and intercept. Also show how you validate assumptions.
+**5. GLM!** Now, compare those results to results from glm. Show the profiles and confidence intervals from `glm()` for the slope and intercept. Also show how you validate assumptions.
 
 ```{r puffer_glm}
 puffer_glm <- glm(predators ~ resemblance, data = puffer,
@@ -97,7 +97,7 @@ confint(puffer_glm)
 ```
 
 
-**EC 5. Get Outside of GLM!** So, often, we have more complex models than the above. There are a variety of optimizers out there, and packages for accessing them. One of the best is `bbmle` by Ecologist Ben Bolker (whose dad is emeritus at UMB in computer science! Go visit him! He's fantastic!)  
+**EC 6. Get Outside of GLM!** So, often, we have more complex models than the above. There are a variety of optimizers out there, and packages for accessing them. One of the best is `bbmle` by Ecologist Ben Bolker (whose dad is emeritus at UMB in computer science! Go visit him! He's fantastic!)  
   
 Load up `'bbmle` and try out `mle2`. It's a bit different, in that the first argument is a function that *minimizes* the log likelihood (not maximizes). The second argument is a list of start values - e.g. `list(slope = 2, intercept = 5, resid_sd = 2)`. Try and fit your model with `mle2` using start values close to the actual estimates. Look at the summary and plot the profile. Note, you might get a lot of errors because it will try impossible values of your residual SD. Also, note thatyou'll have to rewrite your likelihood function to return the negative log likelihood (or write a wrapper that does so). A small thing
 
@@ -114,7 +114,7 @@ plot(profile(puffer_mle2))
 
 ```
 
-**EC 5a. Start values!** What happens if you start with start values *very* far away from the initial values. Failing here is fine. But what do you think is happening, and what does this say about the value of start values?
+**EC 6a. Start values!** What happens if you start with start values *very* far away from the initial values. Failing here is fine. But what do you think is happening, and what does this say about the value of start values?
 
 ```{r mle2_fail}
  mle2(min_lik_fun, 
@@ -126,7 +126,7 @@ plot(profile(puffer_mle2))
 #Start values are very important and can alter the outcome of your models. Try either ""good"" start values, or try multiple ones and make sure your results converge.
 ```
 
-**EC 5b Algorithms!** By default, `mle2` uses the Nelder-Mead algorithm via the `optim` function. What happens if you add an `method` argument to ""SANN"" or ""L-BFGS-B"" (and for the later, which is bounded sampling, give it a `lower` argument for your residual value, so it's always positive). See `?optim` for some more guidance. Do these both converge to the same value? Based on their profiles, do you trust them? (Note, Simulated annealing takes a looooong time. Go have a cuppa while the profile for that one runs).
+**EC 6b Algorithms!** By default, `mle2` uses the Nelder-Mead algorithm via the `optim` function. What happens if you add an `method` argument to ""SANN"" or ""L-BFGS-B"" (and for the later, which is bounded sampling, give it a `lower` argument for your residual value, so it's always positive). See `?optim` for some more guidance. Do these both converge to the same value? Based on their profiles, do you trust them? (Note, Simulated annealing takes a looooong time. Go have a cuppa while the profile for that one runs).
  
 ```{r}
 

---FILE: homework/07_likelihood.html---
@@ -382,12 +382,12 @@ <h4 class=""author"">Biol 607</h4>
 <h2>Puffers!</h2>
 <p>Let’s look at the <a href=""http://biol607.github.io/homework/data/16q11PufferfishMimicry%20Caley%20&amp;%20Schluter%202003.csv"">pufferfish data</a> with likelihood!</p>
 <p><strong>3. Grid Sampling!</strong> Based on Friday’s lab, load up the pufferfish data and use grid sampling to find the MLE of the slope, intercept and residual SD of this model. Feel free to eyeball results from an <code>lm()</code> fit to get reasonable values. Try not to do this for a grid of more than ~100K points (more if you want!). It’s ok to be coarse. Compare to lm.</p>
-<p><strong>2. Surfaces!</strong> Filter the dataset to the MLE of the SD. Plot the surface for the slope and intercept in whatever way you find most compelling. You might want to play around with zooming in to different regions, etc. Have fun!</p>
-<p><strong>4. GLM!</strong> Now, compare those results to results from glm. Show the profiles and confidence intervals from <code>glm()</code> for the slope and intercept. Also show how you validate assumptions.</p>
-<p><strong>EC 5. Get Outside of GLM!</strong> So, often, we have more complex models than the above. There are a variety of optimizers out there, and packages for accessing them. One of the best is <code>bbmle</code> by Ecologist Ben Bolker (whose dad is emeritus at UMB in computer science! Go visit him! He’s fantastic!)</p>
+<p><strong>4. Surfaces!</strong> Filter the dataset to the MLE of the SD. Plot the surface for the slope and intercept in whatever way you find most compelling. You might want to play around with zooming in to different regions, etc. Have fun!</p>
+<p><strong>5. GLM!</strong> Now, compare those results to results from glm. Show the profiles and confidence intervals from <code>glm()</code> for the slope and intercept. Also show how you validate assumptions.</p>
+<p><strong>EC 6. Get Outside of GLM!</strong> So, often, we have more complex models than the above. There are a variety of optimizers out there, and packages for accessing them. One of the best is <code>bbmle</code> by Ecologist Ben Bolker (whose dad is emeritus at UMB in computer science! Go visit him! He’s fantastic!)</p>
 <p>Load up <code>&#39;bbmle</code> and try out <code>mle2</code>. It’s a bit different, in that the first argument is a function that <em>minimizes</em> the log likelihood (not maximizes). The second argument is a list of start values - e.g. <code>list(slope = 2, intercept = 5, resid_sd = 2)</code>. Try and fit your model with <code>mle2</code> using start values close to the actual estimates. Look at the summary and plot the profile. Note, you might get a lot of errors because it will try impossible values of your residual SD. Also, note thatyou’ll have to rewrite your likelihood function to return the negative log likelihood (or write a wrapper that does so). A small thing</p>
-<p><strong>EC 5a. Start values!</strong> What happens if you start with start values <em>very</em> far away from the initial values. Failing here is fine. But what do you think is happening, and what does this say about the value of start values?</p>
-<p><strong>EC 5b Algorithms!</strong> By default, <code>mle2</code> uses the Nelder-Mead algorithm via the <code>optim</code> function. What happens if you add an <code>method</code> argument to “SANN” or “L-BFGS-B” (and for the later, which is bounded sampling, give it a <code>lower</code> argument for your residual value, so it’s always positive). See <code>?optim</code> for some more guidance. Do these both converge to the same value? Based on their profiles, do you trust them? (Note, Simulated annealing takes a looooong time. Go have a cuppa while the profile for that one runs).</p>
+<p><strong>EC 6a. Start values!</strong> What happens if you start with start values <em>very</em> far away from the initial values. Failing here is fine. But what do you think is happening, and what does this say about the value of start values?</p>
+<p><strong>EC 6b Algorithms!</strong> By default, <code>mle2</code> uses the Nelder-Mead algorithm via the <code>optim</code> function. What happens if you add an <code>method</code> argument to “SANN” or “L-BFGS-B” (and for the later, which is bounded sampling, give it a <code>lower</code> argument for your residual value, so it’s always positive). See <code>?optim</code> for some more guidance. Do these both converge to the same value? Based on their profiles, do you trust them? (Note, Simulated annealing takes a looooong time. Go have a cuppa while the profile for that one runs).</p>
 </div>
 
 "
biol607,biol607.github.io,861b6715035b9c7bfd07f4b32989116df3a13d2a,jebyrnes,jarrett.byrnes@umb.edu,2020-10-20T18:08:31Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-20T18:08:31Z,Small fixes,lectures/crossvalidation.Rmd;lectures/crossvalidation.html;lectures/crossvalidation_files/figure-html/unnamed-chunk-1-1.png;lectures/crossvalidation_files/figure-html/unnamed-chunk-2-1.png;lectures/crossvalidation_files/figure-html/unnamed-chunk-3-1.png,True,False,True,False,13,10,23,"---FILE: lectures/crossvalidation.Rmd---
@@ -13,7 +13,10 @@ output:
 ---
 class: center, middle
 
-# Maximum Likelihood Estimation and Likelihood Ratio Testing<br>
+# Cross-Validation and AIC<br>
+
+<!-- next year, more on overfitting and generality
+versus specificity -->
 
 ![:scale 55%](images/cv/princess_bride_cv.jpg)
 ```{r setup, include=FALSE}

---FILE: lectures/crossvalidation.html---
@@ -13,7 +13,7 @@
 
 class: center, middle
 
-# Maximum Likelihood Estimation and Likelihood Ratio Testing&lt;br&gt;
+# Cross-Validation and AIC&lt;br&gt;
 
 ![:scale 55%](images/cv/princess_bride_cv.jpg)
 
@@ -185,13 +185,13 @@
 RMSE Temperature Model:
 
 ```
-[1] 27.14688
+[1] 24.25059
 ```
 
 RMSE Intercept Only Model:
 
 ```
-[1] 30.39648
+[1] 28.88506
 ```
 
 This is the estimate of the SD of the training set - which is acceptable for your predictions?
@@ -239,13 +239,13 @@
 
 |id    |      mse|     rmse|
 |:-----|--------:|--------:|
-|Fold1 | 556.4524| 23.58924|
-|Fold2 | 546.9447| 23.38685|
-|Fold3 | 465.6191| 21.57821|
-|Fold4 | 520.2648| 22.80931|
-|Fold5 | 578.0592| 24.04286|
+|Fold1 | 572.3166| 23.92314|
+|Fold2 | 500.1096| 22.36313|
+|Fold3 | 536.9860| 23.17296|
+|Fold4 | 505.1393| 22.47530|
+|Fold5 | 557.2699| 23.60657|
 
-Temperature Model Score: 533.4680436  
+Temperature Model Score: 534.3642903  
 
 Intercept Only Score: 788.2974 &lt;!-- eh, I was lazy, and just reran the above with a different model --&gt;
 "
biol607,biol607.github.io,7c2d233d7e6752ba84d8391fbe9438737c4e30d6,jebyrnes,jarrett.byrnes@umb.edu,2020-10-15T16:46:10Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-15T16:46:10Z,Title fix,lectures/mle_cv.Rmd;lectures/mle_cv.html,True,False,True,False,4,4,8,"---FILE: lectures/mle_cv.Rmd---
@@ -13,9 +13,9 @@ output:
 ---
 class: center, middle
 
-# Likelihood and Cross-Validation<br>
+# Maximum Likelihood Estimation and Likelihood Ratio Testing<br>
 
-![:scale 64%](images/13/hey_gurl_liklihood.jpeg)
+![:scale 55%](images/13/hey_gurl_liklihood.jpeg)
 ```{r setup, include=FALSE}
 library(knitr)
 library(ggplot2)

---FILE: lectures/mle_cv.html---
@@ -14,9 +14,9 @@
 
 class: center, middle
 
-# Likelihood and Cross-Validation&lt;br&gt;
+# Maximum Likelihood Estimation and Likelihood Ratio Testing&lt;br&gt;
 
-![:scale 64%](images/13/hey_gurl_liklihood.jpeg)
+![:scale 55%](images/13/hey_gurl_liklihood.jpeg)
 
 
 "
biol607,biol607.github.io,7f7b84fcab9f28eadf670ec6644fedd25c2133be,jebyrnes,jarrett.byrnes@umb.edu,2020-10-15T05:20:07Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-15T05:20:07Z,Typo fix,lectures/linear_regression_nht.Rmd,True,False,True,False,1,1,2,"---FILE: lectures/linear_regression_nht.Rmd---
@@ -75,7 +75,7 @@ puffer <- read.csv(""./data/11/16q11PufferfishMimicry Caley & Schluter 2003.csv"")
 ![:scale 80%](./images/11/puffer_mimics.jpg)
 ]
 ---
-## Question of the day: Does Resembling a Pufferfish Reduce Predator Visits?
+# Question of the day: Does Resembling a Pufferfish Reduce Predator Visits?
 ```{r puffershow}
 pufferplot <- ggplot(puffer, mapping=aes(x=resemblance, y=predators)) +
   ylab(""Predator Approaches per Trial"") + "
biol607,biol607.github.io,c41ecf08021469fbc178f210b244a91e01eabbef,jebyrnes,jarrett.byrnes@umb.edu,2020-10-14T17:58:43Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-14T17:58:43Z,Fixing homework typos,homework/06_correlation_regression.Rmd;homework/06_correlation_regression.html,True,False,True,False,11,189,200,"---FILE: homework/06_correlation_regression.Rmd---
@@ -76,13 +76,15 @@ https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter17/chap17q25Beetl
 ![](images/17-25a.png)
 
 ![](images/17-25b.png)
+
 e. Do any other diagnostics misbehave?
 
 
 
 ## 6. W&S Chapter 17-30
 https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter17/chap17q30NuclearTeeth.csv
 ![](images/17-30.png)
+
 d. Using `predict()` and `geom_ribbon()` in ggplot2, reproduce the above plot showing data, fit, fit interval, and prediction interval.
 
 "
biol607,biol607.github.io,354190dedf3973a8ac8889dbe0c0292b0f03fc71,jebyrnes,jarrett.byrnes@umb.edu,2020-10-14T14:02:04Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-14T14:02:04Z,Fix typos,lectures/linear_regression_nht.Rmd;lectures/linear_regression_nht.html,True,False,True,False,2,2,4,"---FILE: lectures/linear_regression_nht.Rmd---
@@ -116,7 +116,7 @@ wolfplot + stat_smooth(method = ""lm"")
 ---
 class: center, middle
 
-# So.... how do you know if you draw conclusions from an experiment or observation?
+# So.... how do you draw conclusions from an experiment or observation?
 
 
 ---

---FILE: lectures/linear_regression_nht.html---
@@ -72,7 +72,7 @@
 ---
 class: center, middle
 
-# So.... how do you know if you draw conclusions from an experiment or observation?
+# So.... how do you draw conclusions from an experiment or observation?
 
 
 ---"
biol607,biol607.github.io,15900943a022a774f142db3e2e3278ce3cad99e6,jebyrnes,jarrett.byrnes@umb.edu,2020-10-10T18:50:17Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-10T18:50:17Z,Regression hw and code,homework/06_correlation_regression.Rmd;homework/06_correlation_regression.html;homework/images/16_15.png;homework/images/16_19.png;homework/images/17_19.png;in_class_code/2020/data/17e8ShrinkingSeals Trites 1996.csv;in_class_code/2020/data/17q02ZooMortality Clubb and Mason 2003 replica.csv;in_class_code/2020/data/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv;in_class_code/2020/data/17q24DEETMosquiteBites.csv;in_class_code/2020/scripts/linear_regression.R;in_class_code/2020/scripts/reprex.R;schedule.Rmd;schedule.html,True,False,True,False,10168,47,10215,"---FILE: homework/06_correlation_regression.Rmd---
@@ -5,18 +5,26 @@ output: html_document
 ---
 
 ```{r setup, include=FALSE}
-knitr::opts_chunk$set(echo = FALSE)
+knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
 library(tidyverse)
 ```
 
 Note: Datasets are available at http://whitlockschluter.zoology.ubc.ca/data so you don't have to type anything in (and have to load it!)  
 
+Many of the questions below can be addressed using functions in some places. Feel free to do so, but they can also be done without. Your call!
+
 
 ## 1. Correlation - W&S Chapter 16
+Data at https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter16/chap16q15LanguageGreyMatter.csv
+![](images/16_15.png)
+
+## 2. Correlation - W&S Chapter 16
 
-Questions 15, 19
+Data at https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter16/chap16q19LiverPreparation.csv
 
-## 2. Correlation SE
+![](./images/16_19.png)
+
+## 3. Correlation SE
 
 Consider the following dataset:
 
@@ -33,20 +41,20 @@ knitr::kable(mat, ""html"") %>% kableExtra::kable_styling(""striped"")
 
 ```
 
-### 2a.
+### 3a.
 Are these two variables correlated? What is the output of `cor()` here. What does a test show you?
 
 ```{r, eval=FALSE}
 cor.test(mat$cats, mat$happiness_score)
 ```
 
-### 2b.
+### 3b.
 What is the SE of the correlation based on the info from `cor.test()`
 ```{r, eval=FALSE}
-(0.7877084 - 0.3136313 )/2
+(0.91578829 - 0.6758738 )/2
 ```
 
-### 2c.
+### 3c.
 Now, what is the SE via simulation? To do this, you'll need to use `cor()` and get the relevant parameter from the output (remember - you get a matrix back, so, what's the right index!), `replicate()`, and `sample()` or `dplyr::sample_n()` with `replace=TRUE` to get, let's say, 1000 correlations. How does this compare to your value above?
 
 ```{r}
@@ -59,11 +67,13 @@ sd(ses)
 ```
 
 
-## 3. W&S Chapter 17
+## 4. W&S Chapter 17
+Data at https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter17/chap17q19GrasslandNutrientsPlantSpecies.csv
+
+![](./images/17_19.png)
 
-Questions 19, 30, 31
 
-## 4. Intervals and simulation
+## 5. Intervals and simulation
 
 Fit the deet and bites model from lab.
 
@@ -96,7 +106,7 @@ rmnorm(4, mean = coef(deet_mod), varcov = vcov(deet_mod))
 
 produces a number of draws of the variance and the covariance!
 
-### 4a. Fit simulations!
+### 5a. Fit simulations!
 Using `geom_abline()` make a plot that has the following layers and shows that these simulated lines match up well with the fit CI. 1) the data, 2) the lm fit with a CI, and 3) simulated lines. You might have to much around to make it look as good as possible.
 
 
@@ -110,7 +120,7 @@ ggplot(deet, aes(dose, bites)) +
   stat_smooth(data = deet, method=lm, fill = ""red"")
 ```
 
-### 4b. Prediction simulations!
+### 5b. Prediction simulations!
 
 That's all well and good, but what about the prediction intervals? To each line, we can add some error drawn from the residual standard deviation. That residual can either be extracted from `summary()` or you can get the `sd` of `residuals`.
 

---FILE: in_class_code/2020/data/17q02ZooMortality Clubb and Mason 2003 replica.csv---
@@ -0,0 +1 @@
+homerange,mortality0.272531793,40.60653066,220.740818221,01.221402758,01.105170918,111.648721271,132.718281828,171.349858808,251.491824698,241.648721271,271.105170918,291.221402758,331.491824698,333.669296668,423.320116923,334.055199967,204.953032424,194.953032424,256.049647464,2522.19795128,65
\ No newline at end of file

---FILE: in_class_code/2020/data/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv---
@@ -0,0 +1,13 @@
+leanness,lossrate
+7,0.103
+7,0.097
+6.2,0.09
+5,0.091
+4.4,0.071
+3.3,0.024
+3.6,0.014
+2.8,0.041
+2.4,0.031
+2.1,0.01
+2.1,0.006
+1.7,0.002

---FILE: in_class_code/2020/data/17q24DEETMosquiteBites.csv---
@@ -0,0 +1,53 @@
+""dose"",""bites""
+1.5,4.06201920231798
+1.4,4.183300132670378
+2,3.5355339059327378
+2.3,3.5355339059327378
+2.3,3.24037034920393
+2.5,3.24037034920393
+3.4,3.5355339059327378
+3.5,3.5355339059327378
+3.4,3.391164991562634
+3.5,3.24037034920393
+4.5,3.082207001484488
+4.4,2.9154759474226504
+4.5,2.9154759474226504
+4.9,2.5495097567963922
+5,2.5495097567963922
+5.1,2.345207879911715
+5.8,1.224744871391589
+6,.7071067811865476
+4.7,1.5811388300841898
+4.8,1.8708286933869707
+4.6,1.8708286933869707
+4.8,2.345207879911715
+4.5,2.345207879911715
+4.1,2.345207879911715
+3.5,2.7386127875258306
+3.3,2.9154759474226504
+3,2.9154759474226504
+3,3.24037034920393
+2.6,3.082207001484488
+2.5,2.7386127875258306
+2.6,2.7386127875258306
+2.2,2.7386127875258306
+2,2.9154759474226504
+2.2,2.5495097567963922
+2.2,2.5495097567963922
+2.5,2.5495097567963922
+2.6,2.5495097567963922
+3.1,2.345207879911715
+3.1,1.224744871391589
+3.2,1.5811388300841898
+3.2,2.1213203435596424
+3.5,1.8708286933869707
+3.6,1.8708286933869707
+3.6,1.224744871391589
+3.5,1.224744871391589
+3.6,1.5811388300841898
+3.9,1.8708286933869707
+5.2,1.8708286933869707
+3.7,1.8708286933869707
+3.1,1.5811388300841898
+3.9,1.5811388300841898
+3.8,1.5811388300841898

---FILE: in_class_code/2020/scripts/linear_regression.R---
@@ -0,0 +1,264 @@
+#'--------------------------------
+#' @title Linear Regression!
+#' 
+#' @date 2020-10-09
+#'--------------------------------
+
+
+# The Fundamental Steps of executing a regression ####
+# 1. Load the data
+# 2. Visualize the data - just to detect problems and perform a cursory 
+#    test of assumptions!
+# 3. Fit the model.
+# 4. Use the fit model to test assumptions
+# 5. Evaluate the model
+# 6. Visualize the fit model
+
+# Seal Model ####
+library(dplyr)
+library(ggplot2)
+library(tidyr)
+
+# 1. Load the data ####
+seals <- read.csv(""data/17e8ShrinkingSeals Trites 1996.csv"")
+
+# what's here?!
+
+str(seals)
+summary(seals)
+#visdat
+skimr::skim(seals)
+
+# 2. plot it! ####
+seal_plot <- ggplot(data = seals,
+       mapping = aes(x = age.days, y = length.cm)) +
+  geom_point(alpha = 0.5)
+
+seal_plot
+
+
+# 3. Fit the model
+
+# formulae are in the y ~ x format
+seal_lm <- lm(length.cm ~ age.days, data = seals)
+
+seal_lm
+coef(seal_lm)
+
+# 4. Use the fit model to test assumptions ####
+
+# Does the distribution of our predictions match our data?
+seal_sims <- simulate(seal_lm, nsim = 20) %>%
+  pivot_longer(
+    cols = everything(),
+    names_to = ""sim"",
+    values_to = ""length.cm""
+  )
+
+ggplot() +
+  geom_density(data = seal_sims,
+               mapping = aes(x = length.cm, group = sim), 
+               size = 0.2)  +
+  geom_density(data = seals,
+               mapping = aes(x = length.cm),
+               size = 2, color = ""blue"")
+
+# Is there a relationship between fitted and residual values?
+plot(seal_lm, which = 1)
+
+# with ggplot
+library(ggfortify)
+autoplot(seal_lm, which = 1, ncol = 1)
+
+# Did we satisfy normality and homoskedacticity using a qq plot and 
+# levene test
+residuals(seal_lm) %>% hist()
+
+plot(seal_lm, which = 2)
+
+# residuals(seal_lm) %>% shapiro.test() # doesn't work due to large sample size
+
+# Look for outliers with leverage
+plot(seal_lm, which = 4) #over 1 is a problem maybe?
+plot(seal_lm, which = 5)
+
+
+
+#leverage
+dat <- tibble(x = c(1:10, 100),
+              y = c(rnorm(10, x), 50))
+
+ggplot(data = dat[,-11],
+       aes(x = x, y = y)) +
+  geom_point() +
+  stat_smooth(method = ""lm"")
+
+fit <- lm(y ~ x, data = dat)
+fit_no_outlier <- lm(y~x, data = dat[-11,])
+plot(fit, which = 4)
+plot(fit, which = 5)
+
+
+coef(fit)
+coef(fit_no_outlier)
+
+
+# 5. Evaluate the model ####
+library(broom)
+
+# F-test
+# Did we explain any variation in the data other than noise?
+# Null hypothesis - our model should have just as much explanatory 
+# power as the noise we observe - var(model)/var(error) = F ratio
+# If we get a small probability value, we reject the null
+
+anova(seal_lm)
+
+anova(seal_lm) %>%
+  tidy()
+
+# T-test of parameters
+# If we divide a parameter by it's SE = t
+# We can use that to see if we can reject the hypothesis that our
+# paramter = 0
+
+summary(seal_lm)
+
+#tidy output from broom
+tidy(seal_lm)
+glance(seal_lm)
+
+# just the r2
+summary(seal_lm)$r.squared
+summary(seal_lm)$coef
+
+
+# 5. Visualize your model ####
+
+#Fit interval
+seal_plot +
+  stat_smooth(method = ""lm"") #shows error around our FIT
+
+fit_seals <- predict(seal_lm, 
+                     interval = ""confidence"") %>%
+  as_tibble() %>%
+  rename(lwr_ci = lwr,
+         upr_ci = upr)
+
+seals <- cbind(seals, fit_seals)
+
+ggplot(seals,
+       aes(x = age.days, 
+           ymin = lwr_ci, 
+           ymax = upr_ci,
+           y = fit)) +
+  geom_ribbon() +
+  geom_line(color = ""blue"")
+
+
+# Predicition Interval
+predict_seals <- predict(seal_lm,
+                         interval = ""prediction"") %>%
+  as_tibble() %>%
+  rename(lwr_pi = lwr,
+         upr_pi = upr)
+
+seals <- cbind(seals, predict_seals) 
+
+# fix the names - but, note, . now is _ 
+# sorry! This is an argument for using readr::read_csv
+seals <- janitor::clean_names(seals)
+
+ggplot(seals,
+       aes(x = age_days,
+           y = fit,
+           ymin = lwr_pi,
+           ymax = upr_pi)) +
+  geom_ribbon(alpha = 0.3) +
+  geom_line(color = ""blue"", size = 2) +
+  geom_point(mapping = aes(y = length_cm))
+
+
+# let's visually compare the fit interval and the prediction intervals
+ggplot(data = seals,
+       mapping = aes(x = age_days,
+                     y = length_cm)) +
+  #prediction interval
+  geom_ribbon(mapping = aes(ymin = lwr_pi,
+                            ymax = upr_pi),
+              alpha = 0.5) +
+  # fit interval - just coefficient error (precision)
+  geom_ribbon(mapping = aes(ymin = lwr_ci,
+                            ymax = upr_ci),
+              color = ""red"",
+              alpha = 0.5)
+#-----------
+# Faded examples
+
+#load the data
+fat <- read.csv(""./data/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv"")
+
+#initial visualization to determine if lm is appropriate
+fat_plot <- ggplot(data=fat, aes(x=leanness, y=lossrate)) + 
+  geom_point()
+fat_plot
+
+fat_mod <- lm(lossrate ~ leanness, data=fat)
+
+#assumptions
+simulate(fat_mod, nsim = 100) %>%
+  pivot_longer(cols = everything(), 
+               names_to = ""sim"", values_to = ""lossrate"") %>%
+  ggplot(aes(x = lossrate)) +
+  geom_density(aes(group = sim), size = 0.2) +
+  geom_density(data = fat, color = ""blue"", size = 2)
+
+plot(fat_mod, which=1)
+plot(fat_mod, which=2)
+
+#f-tests of model
+anova(fat_mod)
+
+#t-tests of parameters
+summary(fat_mod)
+
+#plot with line
+fat_plot + 
+  stat_smooth(method=lm, formula=y~x)
+
+#--- add some fade
+
+deet <- read.csv(""./data/17q24DEETMosquiteBites.csv"")
+
+deet_plot <- ggplot(data=deet, aes(x=dose, y=bites)) + 
+  geom_point()
+
+deet_plot
+
+deet_mod <- lm(bites ~ dose, data=deet)
+
+#assumptions
+simulate(deet_mod, nsim = 100) %>%
+  pivot_longer(cols = everything(), 
+               names_to = ""sim"", values_to = ""bites"") %>%
+  ggplot(aes(x = bites)) +
+  geom_density(aes(group = sim), lwd = 0.2) +
+  geom_density(data = deet, color = ""blue"", lwd = 2)
+
+
+plot(___, which=1)
+plot(___, which=2)
+
+#f-tests of model
+anova(___)
+
+#t-tests of parameters
+summary(___)
+
+#plot with line
+deet_plot + 
+  stat_smooth(method=lm, formula=y~x)
+
+
+
+

---FILE: in_class_code/2020/scripts/reprex.R---
@@ -0,0 +1,30 @@
+library(palmerpenguins)
+
+penguins
+
+
+mean(penguins$bill_length_mm)
+
+str(penguins$bill_length_mm)
+
+# How do I get mean to work is NAs are in a vector?
+
+# Here is a reprex that reproduces the error identified in the question
+
+x <- c(1,2,3,NA,4)
+mean(x)
+
+
+#-----#
+
+``` r
+# How do I get mean to work is NAs are in a vector?
+
+# Here is a reprex that reproduces the error identified in the question
+
+x <- c(1,2,3,NA,4)
+mean(x)
+#> [1] NA
+```
+
+<sup>Created on 2020-10-09 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>
\ No newline at end of file

---FILE: schedule.Rmd---
@@ -104,8 +104,8 @@ __Lecture:__ Introduction to Regression: [Correlation and Regression](lectures/l
 __Lab Topic:__ [Linear regression, diagnostics, visualization](lab/06_lm.html), and [data](lab/data_06.zip) \
 __Reading:__ W&S 16-17, W&G on [model basics](http://r4ds.had.co.nz/model-basics.html), [model building](http://r4ds.had.co.nz/model-building.html)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-lm-2020  
-__In Class Code:__ [lm](in_class_code_2020/scripts/lm_code.R)  
-__Homework:__ [Correlation and Linear Models](homework_2020/06_correlation_regression.html)  
+__In Class Code:__ [reproducible examples](in_class_code_2020/scripts/reprex.R), [lm](in_class_code_2020/scripts/linear_regression.R)  
+__Homework:__ [Correlation and Linear Models](homework/06_correlation_regression.html)  
   
 ### Week 6.   
 ```{r next_date, echo=FALSE}

---FILE: schedule.html---
@@ -395,8 +395,8 @@ <h3>Week 5.</h3>
 <strong>Lab Topic:</strong> <a href=""lab/06_lm.html"">Linear regression, diagnostics, visualization</a>, and <a href=""lab/data_06.zip"">data</a><br />
 <strong>Reading:</strong> W&amp;S 16-17, W&amp;G on <a href=""http://r4ds.had.co.nz/model-basics.html"">model basics</a>, <a href=""http://r4ds.had.co.nz/model-building.html"">model building</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-lm-2020"" class=""uri"">https://etherpad.wikimedia.org/p/607-lm-2020</a><br />
-<strong>In Class Code:</strong> <a href=""in_class_code_2020/scripts/lm_code.R"">lm</a><br />
-<strong>Homework:</strong> <a href=""homework_2020/06_correlation_regression.html"">Correlation and Linear Models</a></p>
+<strong>In Class Code:</strong> <a href=""in_class_code_2020/scripts/reprex.R"">reproducible examples</a>, <a href=""in_class_code_2020/scripts/linear_regression.R"">lm</a><br />
+<strong>Homework:</strong> <a href=""homework/06_correlation_regression.html"">Correlation and Linear Models</a></p>
 </div>
 <div id=""week-6."" class=""section level3"">
 <h3>Week 6.</h3>"
biol607,biol607.github.io,1de9d4574825ab8d8e4037cd54b3aeb6fe71d66f,jebyrnes,jarrett.byrnes@umb.edu,2020-10-09T01:42:08Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-09T01:42:08Z,small line fix,lab/06_lm.Rmd,True,False,True,False,0,1,1,"---FILE: lab/06_lm.Rmd---
@@ -18,7 +18,6 @@ opts_chunk$set(fig.height=6,
                warning=FALSE, 
                echo = FALSE,
                message = FALSE)
-
 ```
 
 Believe it or not, despite all of the complexity under the hood, fitting a linear model in R with least squares is quite simple with a straightfoward workflow."
biol607,biol607.github.io,3bf1d3364bf89a8ecb3f45cb3af01a85156566b8,jebyrnes,jarrett.byrnes@umb.edu,2020-10-09T01:37:39Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-09T01:37:39Z,Update regression lab,lab/06_lm.Rmd;lab/06_lm.html;lab/06_lm_files/figure-html/assumptions-1.png;lab/06_lm_files/figure-html/assumptions_fit_resid-1.png;lab/06_lm_files/figure-html/cooks-1.png;lab/06_lm_files/figure-html/ggfortify-1.png;lab/06_lm_files/figure-html/leverage-1.png;lab/06_lm_files/figure-html/modelr_fit-1.png;lab/06_lm_files/figure-html/obs_resid-1.png;lab/06_lm_files/figure-html/predict_plot-1.png;lab/06_lm_files/figure-html/qq-1.png;lab/06_lm_files/figure-html/resid_hist-1.png;lab/06_lm_files/figure-html/show_data-1.png;lab/06_lm_files/figure-html/unnamed-chunk-1-1.png;lab/06_lm_files/figure-html/unnamed-chunk-5-1.png,True,False,True,False,302,290,592,"---FILE: lab/06_lm.Rmd---
@@ -8,6 +8,19 @@ output:
     toc_float: true
 ---
 
+```{r setup, include=FALSE}
+library(knitr)
+
+opts_chunk$set(fig.height=6, 
+               fig.width = 8,
+               fig.align = ""center"",
+               comment=NA, 
+               warning=FALSE, 
+               echo = FALSE,
+               message = FALSE)
+
+```
+
 Believe it or not, despite all of the complexity under the hood, fitting a linear model in R with least squares is quite simple with a straightfoward workflow.
 
 1. Load the data
@@ -25,6 +38,7 @@ Let's go through each step with an example of seals. Are older seals larger?
 ```{r}
 library(dplyr)
 library(ggplot2)
+theme_set(theme_bw(base_size = 16))
 
 seals <- read.csv(""./data/17e8ShrinkingSeals Trites 1996.csv"")
 
@@ -60,8 +74,46 @@ coef(seal_lm)
 But that's getting ahead of ourselves...
 
 ### 2. Evaluating Assumptions
+First, simulating! R provides a method for pulling the fit from the model and simulating different residuals. Now, granted, we *really* want to also incorporate error in the coefficients - which you can do - but.... gimme time to write a package that does that (haven't found one yet - see [here](http://imachordata.com/2020/09/24/simulating-posterior-predictions-from-non-bayesian-fits/)). But, to simulate from our model with residual error
+
+```{r}
+seal_sims <- simulate(seal_lm, nsim = 100)
+```
+
+So, what's in this object?
+
+```{r}
+dim(seal_sims)
+```
+
+Ah, so, 100 sims as columns. But to plot, we want them as rows. Oh wait - a job for pivot!
+
+```{r}
+library(tidyr)
+
+seal_sims <- seal_sims %>%
+  pivot_longer(
+    cols = everything(),
+    names_to = ""sim"",
+    values_to = ""length.cm""
+  )
+```
+
+We can now use this to plot the distribution of length in our data versus our model.
 
-R also provides a 1-stop shop for evaluating functions. Fit model objects can typically be plotted. Now, it uses base plot, so, we'll use the `par` function to setup a 2x2 plotting area.
+```{r}
+ggplot() +
+  geom_density(data = seal_sims,
+               aes(x = length.cm, group = sim),
+               lwd = 0.02) +
+  geom_density(data = seals, 
+               aes(x = length.cm),
+               color = ""blue"", lwd = 2)
+```
+
+NOICE!
+
+R  provides a 1-stop shop for evaluating all other model assumptions Fit model objects can typically be plotted. Now, it uses base plot, so, we'll use the `par` function to setup a 2x2 plotting area.
 
 ```{r assumptions}
 par(mfrow = c(2,2)) #2 rows, 2 columns
@@ -75,7 +127,7 @@ Whoah - that's a lot! And, there's no figure with Cook's D or a histogram of res
 
 OK, breathe.
 
-`plot.lm()` actualyl generates even more plots than shown here. You can specify what plot you want with the `which` argument, but will need to look at `?plot.lm` to know just what to look at.
+`plot.lm()` actually generates even more plots than shown here. You can specify what plot you want with the `which` argument, but will need to look at `?plot.lm` to know just what to look at.
 
 I have five plots I really like to look at - four of which `plot.lm()` will generate. Those four are the fitted versus residuals:
 
@@ -136,6 +188,13 @@ qplot(pred, resid, data=seals) +
   stat_smooth(method=""lm"")
 ```
 
+Finally, if you didn't like this without ggplot2, the package {ggfortify} provides an autoplot method to make these plots. Using the same which argument. 
+
+```{r ggfortify}
+library(ggfortify)
+autoplot(seal_lm, ncol = 1, which = 1)
+```
+
 ### 3. Putting it to the test
 OK, ok, everything looks fine. Now, how do we test our model.
 
@@ -240,6 +299,13 @@ fat_plot
 fat_mod <- lm(lossrate ~ leanness, data=fat)
 
 #assumptions
+simulate(fat_mod, nsim = 100) %>%
+  pivot_longer(cols = everything(), 
+               names_to = ""sim"", values_to = ""lossrate"") %>%
+  ggplot(aes(x = lossrate)) +
+  geom_density(aes(group = sim), lwd = 0.2) +
+  geom_density(data = fat, color = ""blue"", lwd = 2)
+
 plot(fat_mod, which=1)
 plot(fat_mod, which=2)
 
@@ -268,6 +334,14 @@ deet_plot
 deet_mod <- lm(bites ~ dose, data=deet)
 
 #assumptions
+simulate(___, nsim = 100) %>%
+  pivot_longer(cols = everything(), 
+               names_to = ""sim"", values_to = ""___"") %>%
+  ggplot(aes(x = ___)) +
+  geom_density(aes(group = sim), lwd = 0.2) +
+  geom_density(data = ___, color = ""blue"", lwd = 2)
+
+
 plot(___, which=1)
 plot(___, which=2)
 
@@ -295,6 +369,14 @@ We might suspect that the relationship was nonlinear. Let's see how a simple log
 deet_mod_log <- lm(log(bites) ~ dose, data=deet)
 
 #assumptions
+simulate(___, nsim = 100) %>%
+  pivot_longer(cols = everything(), 
+               names_to = ""sim"", values_to = ""log_bites"") %>%
+  mutate(bites = exp(log_bites)) %>%
+  ggplot(aes(x = ___)) +
+  geom_density(aes(group = sim), lwd = 0.2) +
+  geom_density(data = ___, color = ""blue"", lwd = 2)
+
 plot(___, which=1)
 plot(___, which=2)
 
@@ -324,6 +406,13 @@ ___
 zoo_mod <- lm(___, data=___)
 
 #assumptions
+simulate(___, nsim = 100) %>%
+  ___(cols = ___(), 
+               names_to = ""___"", values_to = ""___"") %>%
+  ggplot(aes(x = ___)) +
+  ___(aes(group = ___), lwd = 0.2) +
+  ___(data = ___, color = ""blue"", lwd = 2)
+
 plot(___, which=1)
 plot(___, which=2)
 
@@ -348,6 +437,14 @@ That definitely wasn't linear. Look at that outlier! Let's log our y and see how
 zoo_mod_log <- lm(log(___) ~ ___, ___=___)
 
 #assumptions
+____(___, nsim = 100) %>%
+  ___(cols = ___(), 
+               names_to = ""___"", values_to = ""___"") %>%
+  mutate(____ = ___(____))
+  ggplot(aes(x = ___)) +
+  ___(aes(____ = ___), lwd = 0.2) +
+  ___(data = ___, color = ""blue"", lwd = 2)
+  
 ___(___)
 ___(___)
 "
biol607,biol607.github.io,e561ed05d15eb583c0e13d861dde2dc9b7f0aa39,jebyrnes,jarrett.byrnes@umb.edu,2020-10-08T17:33:52Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-08T17:33:52Z,fixes,lectures/linear_regression_details.Rmd;lectures/linear_regression_details.html,True,False,True,False,3,1,4,"---FILE: lectures/linear_regression_details.Rmd---
@@ -439,6 +439,7 @@ autoplot(wolf_mod, ncol = 1, which = 4)
 
 - If from a nonlinearity, consider transformation
 
+---
 
 # Assumptions (in rough descending order of importance)
 
@@ -450,7 +451,7 @@ autoplot(wolf_mod, ncol = 1, which = 4)
 
 4. Additivity and Linearity: compare model v. data!
 
-5. Independence of Errors: conisder sampling design
+5. Independence of Errors: consider sampling design
 
 6. Equal Variance of Errors: evaluate res-fit 
 

---FILE: lectures/linear_regression_details.html---
@@ -324,6 +324,7 @@
 
 - If from a nonlinearity, consider transformation
 
+---
 
 # Assumptions (in rough descending order of importance)
 "
biol607,biol607.github.io,5d8424ff7d9be6373cc0553993eca95c3a646160,jebyrnes,jarrett.byrnes@umb.edu,2020-10-08T15:00:52Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-08T15:00:52Z,regression 2 add,"lectures/images/regression/Professor_Imre_Lakatos,_c1960s.jpg;lectures/images/regression/lakatos_structure.png;lectures/linear_regression_details.Rmd;lectures/linear_regression_details.html;lectures/linear_regression_details_files/figure-html/PredictionRange3-1.png;lectures/linear_regression_details_files/figure-html/add_obs-1.png;lectures/linear_regression_details_files/figure-html/add_p-1.png;lectures/linear_regression_details_files/figure-html/dist_shape_t-1.png;lectures/linear_regression_details_files/figure-html/f-1.png;lectures/linear_regression_details_files/figure-html/grandmean-1.png;lectures/linear_regression_details_files/figure-html/linefit-1.png;lectures/linear_regression_details_files/figure-html/lsq-1.png;lectures/linear_regression_details_files/figure-html/our_data-1.png;lectures/linear_regression_details_files/figure-html/predRange2-1.png;lectures/linear_regression_details_files/figure-html/predRange4-1.png;lectures/linear_regression_details_files/figure-html/predictionRange1-1.png;lectures/linear_regression_details_files/figure-html/puffer_only_scatter-1.png;lectures/linear_regression_details_files/figure-html/pufferadd-1.png;lectures/linear_regression_details_files/figure-html/pufferout-1.png;lectures/linear_regression_details_files/figure-html/pufferout_cook-1.png;lectures/linear_regression_details_files/figure-html/pufferqq-1.png;lectures/linear_regression_details_files/figure-html/puffershow-1.png;lectures/linear_regression_details_files/figure-html/resfit-1.png;lectures/linear_regression_details_files/figure-html/resfit_puffer-1.png;lectures/linear_regression_details_files/figure-html/sims-1.png;lectures/linear_regression_details_files/figure-html/slopedist-1.png;lectures/linear_regression_details_files/figure-html/ssr-1.png;lectures/linear_regression_details_files/figure-html/wolf_scatterplot-1.png;lectures/linear_regression_details_files/figure-html/wolfadd-1.png;lectures/linear_regression_details_files/figure-html/wolfqq-1.png;lectures/linear_regression_details_files/figure-html/wolfsims-1.png;schedule.Rmd;schedule.html",True,False,True,False,1834,2,1836,"---FILE: lectures/linear_regression_details.Rmd---
@@ -0,0 +1,954 @@
+---
+title: ""Sampling and Simulation""
+output:
+  xaringan::moon_reader:
+    seal: false
+    lib_dir: libs
+    css: [default, shinobi, default-fonts, style.css]
+    nature:
+      beforeInit: ""my_macros.js""
+      highlightStyle: github
+      highlightLines: true
+      countIncrementalSlides: false
+---
+class: center, middle
+
+# Evaluating Fit Linear Models
+<br>
+![:scale 55%](images/12/linear_regression_love.gif)
+
+```{r setup, include=FALSE}
+library(knitr)
+library(ggplot2)
+library(dplyr)
+library(tidyr)
+library(mvtnorm)
+library(broom)
+
+opts_chunk$set(fig.height=6, 
+               fig.width = 8,
+               fig.align = ""center"",
+               comment=NA, 
+               warning=FALSE, 
+               echo = FALSE,
+               message = FALSE)
+
+options(htmltools.dir.version = FALSE)
+theme_set(theme_bw(base_size=18))
+```
+
+
+```{r puffer, include=FALSE}
+puffer <- read.csv(""./data/11/16q11PufferfishMimicry Caley & Schluter 2003.csv"")
+puffer_lm <- lm(predators ~ resemblance, data=puffer)
+```
+
+
+---
+class: center, middle
+
+# Etherpad
+<br><br>
+<center><h3>https://etherpad.wikimedia.org/p/607-lm-2020</h3></center>
+
+
+---
+
+
+# Putting Linear Regression Into Practice with Pufferfish
+
+```{r pufferload}
+puffer <- read.csv(""./data/11/16q11PufferfishMimicry Caley & Schluter 2003.csv"")
+```
+
+.pull-left[
+- Pufferfish are toxic/harmful to predators  
+<br>
+- Batesian mimics gain protection from predation - why?
+<br><br>
+- Evolved response to appearance?
+<br><br>
+- Researchers tested with mimics varying in toxic pufferfish resemblance
+]
+
+.pull-right[
+![:scale 80%](./images/11/puffer_mimics.jpg)
+]
+---
+## Question of the day: Does Resembling a Pufferfish Reduce Predator Visits?
+```{r puffershow}
+pufferplot <- ggplot(puffer, mapping=aes(x=resemblance, y=predators)) +
+  ylab(""Predator Approaches per Trial"") + 
+  xlab(""Dissimilarity to Toxic Pufferfish"")  +
+  geom_point(size = 3) +
+  theme_bw(base_size=24) 
+
+pufferplot + stat_smooth(method = ""lm"")
+```
+
+```{r wolf_scatterplot, include = FALSE}
+wolves <- read.csv(""./data/11/16e2InbreedingWolves.csv"") %>%
+  mutate(inbreeding_coefficient = inbreeding.coefficient)
+
+wolfplot <- ggplot(data=wolves, mapping=aes(x=inbreeding.coefficient, y=pups)) +
+xlab(""Inbreeding Coefficient"") + ylab(""# of Pups"") +
+geom_point(size=3) +
+theme_bw(base_size=24) 
+
+wolf_mod <- lm(pups ~ inbreeding_coefficient,
+               data = wolves)
+
+wolfplot + stat_smooth(method = ""lm"")
+```
+
+
+---
+
+# Digging Deeper into Regression
+
+1. Assumptions: Is our fit valid? 
+
+2. How did we fit this model?
+
+3. How do we draw inference from this model?
+
+
+---
+
+# You are now a Statistical Wizard. Be Careful. Your Model is a Golem.
+(sensu Richard McElreath)
+
+
+.center[.middle[![:scale 45%](images/09/golem.png)]]
+
+---
+
+# A Case of ""Great"" versus ""Not as Great"" Fits...
+
+.pull-left[
+
+![:scale 80%](./images/11/puffer_mimics.jpg)
+
+]
+
+.pull-right[
+
+![](./images/11/CUTE_WOLF_PUPS_by_horsesrock44.jpeg)
+
+]
+
+---
+# The Two Fits
+
+.pull-left[
+<br><br>
+```{r puffershow, fig.height=5, fig.width=5}
+```
+
+]
+
+.pull-right[
+
+<br><br>
+```{r wolf_scatterplot, fig.height=5, fig.width=5}
+```
+]
+
+---
+# Assumptions (in rough descending order of importance)
+
+1. Validity
+
+2. Representativeness
+
+3. Model captures features in the data
+
+4. Additivity and Linearity
+
+5. Independence of Errors
+
+6. Equal Variance of Errors 
+
+7. Normality of Errors
+
+8. Minimal Outlier Influence
+
+---
+# Validity: Do X and Y Reflect Concepts I'm interested In
+
+
+```{r puffershow, fig.height=7, fig.width=7}
+```
+
+
+What if predator approaches is not a good measure of recognition?  Or mimics just don't look like fish?
+
+---
+class: middle
+
+# Solution to lack of validity:  
+
+
+## Reframe your question! Change your framing! Question your life choices!
+
+---
+
+# Representativeness: Does Your Data Represent the Population?
+
+#### For example, say this is your result...
+
+
+```{r predictionRange1}
+set.seed(10201)
+par(mfrow=c(1,2))
+x <- sort(runif(100,0,100))
+y1 <- rnorm(10, 0.5*x[51:60], 2)
+y <- rnorm(100, 0.5*x, 2)
+
+poorlm <- lm(y1 ~ x[51:60])
+plot(y1 ~ x[51:60], pch=19, xlab=""X"", ylab=""Y"")
+aplot <- qplot(x[51:60], y1)  + theme_bw(base_size=16) + xlab(""X"") + ylab(""Y"")+ stat_smooth(method=""lm"", lwd=1.5, col=""red"", lty=2)
+aplot
+```
+
+---
+class: center, middle
+
+# But is that all there is to X in nature?
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+#### What if you are looking at only a piece of the variation in X in your population?
+
+```{r predRange2}
+aplot+xlim(c(0,100))
+```
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+#### How should you have sampled this population for a representative result?
+
+```{r PredictionRange3}
+bplot <- aplot+geom_point(mapping=aes(x=x, y=y))  + 
+  theme_bw(base_size=16) + xlab(""X"") + ylab(""Y"")
+bplot
+```
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+#### It's better to have more variation in X than just a bigger N
+
+
+```{r predRange4}
+y2 <- y[seq(0,100,10)]
+x2 <- x[seq(0,100,10)]
+cplot <- qplot(x2, y2) + geom_point(size=3)  + theme_bw(base_size=16) + xlab(""X"") + ylab(""Y"")+ stat_smooth(method=""lm"", lwd=1.5, col=""blue"", lty=1)
+cplot
+```
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+- Always question if you did a good job sampling
+
+- Use natural history and the literature to get the bounds of values
+
+- If experimenting, make sure your treatment levels are representative
+
+- If you realize post-hoc they are not, **qualify your conclusions**
+
+---
+# Model captures features in the data
+```{r puffershow}
+```
+
+Does the model seem to fit the data? Are there any deviations? Can be hard to see...
+
+
+---
+# Simulating implications from the model to see if we match features in the data
+
+```{r sims}
+sims_simulate <- simulate(puffer_lm, nsim = 100)%>%
+  pivot_longer(cols = everything(),
+               names_to = ""sim"",
+               values_to = ""predators"")
+
+
+ggplot() +
+  geom_density(data = sims_simulate, 
+               aes(x = predators, group = sim), lwd = 0.1) +
+  geom_density(data = puffer, aes(x = predators),
+               lwd = 2, color = ""blue"") +
+  labs(title = ""Distribution of Predator Visits in Our Data\nand as Predicted By Model"")
+```
+
+Is anything off?
+
+---
+# But what to wolves say to you?
+
+```{r wolfsims}
+sims_simulate <- simulate(wolf_mod, nsim = 100)%>%
+  pivot_longer(cols = everything(),
+               names_to = ""sim"",
+               values_to = ""pups"")
+
+
+ggplot() +
+  geom_density(data = sims_simulate, 
+               aes(x = pups, group = sim), lwd = 0.1) +
+  geom_density(data = wolves, aes(x = pups),
+               lwd = 2, color = ""blue"") +
+  labs(title = ""Distribution of Pups in Our Data\nand as Predicted By Model"")
+```
+
+---
+
+# Additivity and Linearity: Should account for all of the variation between residual and fitted values - what you want
+
+```{r pufferadd}
+
+ggplot(data = puffer,
+       aes(x = predators, y = fitted(puffer_lm))) +
+  geom_point() +
+  stat_smooth(method = ""lm"", fill = NA)
+```
+
+---
+
+# Additivity and Linearity: Wolf Problems?
+
+```{r wolfadd}
+ggplot(data = wolves,
+       aes(x = pups, y = fitted(wolf_mod))) +
+  geom_point() +
+  stat_smooth(method = ""lm"", fill = NA)
+```
+
+--
+**Solutions:** Nonlinear transformations or a better model!
+
+---
+
+# Independence of Errors
+
+- Are all replicates TRULY independent
+
+- Did they come from the same space, time, etc.
+
+- Non-independence can introduce **BIAS**
+     - SEs too small (at the least)
+     - Causal inference invalid
+     
+- Incoporate Non-independence into models (many methods)
+
+---
+
+# Equal Variance of Errors: No Pattern to Residuals and Fitted Values
+
+```{r resfit_puffer}
+library(ggfortify)
+autoplot(puffer_lm, which = 1, ncol = 1)
+```
+
+---
+
+# Equal Variance of Errors: What is up with intermediate Wolf Values
+```{r resfit}
+autoplot(wolf_mod, which = 1, ncol = 1)
+```
+---
+
+# Equal Variance of Errors: Problems and Solutions
+
+- Shapes (cones, footballs, etc.) with no bias in fitted v. residual relationship
+
+- A linear relationship indicates an additivity problem
+
+- Can solve with a better model (more predictors)
+
+- Can solve with weighting by X values, if source of heteroskedasticity known
+     - This actually means we model the variance as a function of X
+     - $\epsilon_i \sim(N, f(x_i))$
+ 
+- Minor problem for coefficient estimates
+
+- Major problem for doing inference and prediction as it changes error
+
+---
+# Normality of errors: Did we fit the error generating process that we observed?
+
+- We assumed $\epsilon_i \sim N(0,\sigma)$ - but is that right?
+
+- Can assess with a QQ-plot
+     - Do quantiles of the residuals match quantiles of a normal distribution?
+     
+- Again, minor problem for coefficient estimates  
+
+- Major problem for doing inference and prediction, as it changes error
+
+---
+# Equal Variance of Errors: Puffers
+
+```{r pufferqq}
+autoplot(puffer_lm, which = 2, ncol = 1)
+```
+
+---
+# Equal Variance of Errors: Wolves underpredict at High Levels
+
+```{r wolfqq, fig.height = 5, fig.width = 6}
+autoplot(wolf_mod, which = 2, ncol = 1)
+```
+
+--
+
+```{r}
+shapiro.test(residuals(wolf_mod))
+```
+
+---
+# Outliers: Cook's D
+
+```{r pufferout}
+autoplot(puffer_lm, ncol = 1, which = 4)
+```
+
+Want no values > 1
+
+---
+# Outliers: Cook's D - wolves OK
+
+```{r pufferout_cook}
+autoplot(wolf_mod, ncol = 1, which = 4)
+```
+
+---
+
+# Everyone worries about outliers, but...
+
+- Are they real?
+
+- Do they indicate a problem or a nonlinearity?
+
+- Remove only as a dead last resort
+
+- If from a nonlinearity, consider transformation
+
+
+# Assumptions (in rough descending order of importance)
+
+1. Validity: only you know!
+
+2. Representativeness: look at nature
+
+3. Model captures features in the data: compare model v. data!
+
+4. Additivity and Linearity: compare model v. data!
+
+5. Independence of Errors: conisder sampling design
+
+6. Equal Variance of Errors: evaluate res-fit 
+
+7. Normality of Errors: evaluate qq and levene test
+
+8. Minimal Outlier Influence: evaluate Cook's D
+
+---
+
+# Digging Deeper into Regression
+
+1. Assumptions: Is our fit valid? 
+
+2. .red[How did we fit this model?]
+
+3. How do we draw inference from this model?
+
+---
+# So, uh.... How would you fit a line here?
+
+```{r puffer_only_scatter}
+pufferplot
+```
+
+---
+
+# Lots of Possible Lines - How would you decide?
+
+```{r lsq}
+library(mnormt)
+set.seed(697)
+x<-1:10
+y<-rnorm(10, mean=x,sd=2)
+a<-lm(y~x)
+ab <- rmnorm(3, coef(a), vcov(a))
+
+par(mfrow=c(1,3))
+for(i in 1:3){
+  plot(x,y,pch=19, cex=1.5)
+  abline(a=ab[i,1], b=ab[i,2], lwd=2)
+  segments(x,ab[i,1] + x*ab[i,2],x,y, col=""red"", lwd=2)
+}
+
+```
+
+---
+
+# Method of Model Fitting
+
+1. Least Squares
+  - Conceptually Simple
+  - Minimizes distance between fit and residuals
+  - Approximations of quantities based on frequentist logic
+  
+2. Likelihood
+  - Flexible to many error distributions and other problems
+  - Produces likelihood surface of different parameter values
+  - Equivalent to least square for Gaussian likelihood
+  - Approximations of quantities based on frequentist logic
+
+3. Bayesian
+  - Incorporates prior knowledge
+  - Probability for any parameter is likelihood * prior
+  - Superior for quantifying uncertainty
+  - With ""flat"" priors, equivalent to least squares/likelihood
+  - Analytic or simulated calculation of quantities
+
+---
+
+# Basic Principles of Least Squares Regression
+
+$\widehat{Y} = \beta_0 + \beta_1 X + \epsilon$ where $\beta_0$ = intercept, $\beta_1$ = slope
+
+```{r linefit}
+set.seed(697)
+x<-1:10
+y<-rnorm(10, mean=x,sd=2)
+a<-lm(y~x)
+plot(x,y,pch=19, cex=1.5)
+abline(a, lwd=2)
+segments(x,fitted(a),x,y, col=""red"", lwd=2)
+``` 
+
+Minimize Residuals defined as $SS_{residuals} = \sum(Y_{i} - \widehat{Y})^2$
+
+---
+class: center, middle
+
+# Let's try it out!
+
+---
+
+# Analytic Solution: Solving for Slope
+<br><br>
+
+$\LARGE b=\frac{s_{xy}}{s_{x}^2}$ $= \frac{cov(x,y)}{var(x)}$
+
+--
+
+$\LARGE = r_{xy}\frac{s_{y}}{s_{x}}$
+
+
+
+---
+
+# Analytic Solution: Solving for Intercept
+<br><br>
+Least squares regression line always goes through the mean of X and Y  
+
+
+$\Large \bar{Y} = \beta_0 + \beta_1 \bar{X}$
+
+<br><br>
+
+--
+$\Large \beta_0 = \bar{Y} - \beta_1  \bar{X}$
+
+---
+
+# Digging Deeper into Regression
+
+1. Assumptions: Is our fit valid? 
+
+2. How did we fit this model?
+
+3. .red[How do we draw inference from this model?]
+
+---
+# Inductive v. Deductive Reasoning
+
+<br><br>
+**Deductive Inference:** A larger theory is used to devise
+many small tests.
+
+
+**Inductive Inference:** Small pieces of evidence are used
+to shape a larger theory and degree of belief.
+---
+
+# Applying Different Styles of Inference
+
+- **Null Hypothesis Testing**: What's the probability that things are not influencing our data?
+      - Deductive
+
+- **Cross-Validation**: How good are you at predicting new data?
+      - Deductive
+
+- **Model Comparison**: Comparison of alternate hypotheses
+      - Deductive or Inductive
+
+- **Probabilistic Inference**: What's our degree of belief in a data?
+      - Inductive
+
+---
+# Null Hypothesis Testing is a Form of Deductive Inference
+
+.pull-left[
+![:scale 55%](./images/07/Karl_Popper_wikipedia.jpeg)
+
+Falsification of hypotheses is key! <br><br>
+
+A theory should be considered scientific if, and only if, it is falsifiable.
+
+]
+
+--
+.pull-right[
+![:scale 55%](./images/regression/Professor_Imre_Lakatos,_c1960s.jpg)
+
+Look at a whole research program and falsify auxilliary hypotheses
+]
+
+
+---
+# A Bigger View of Dedictive Inference
+
+![](./images/regression/lakatos_structure.png)
+
+.small[https://plato.stanford.edu/entries/lakatos/#ImprPoppScie]
+
+---
+
+# Reifying Refutation - What is the probability something is false?
+
+What if our hypothesis was that the resemblance-predator relationship was 2:1. We know our SE of our estimate is 0.57, so, we have a distribution of what we **could** observe.
+
+```{r slopedist, fig.height = 5}
+dat_slope <- tibble(x = seq(-1,5,length.out = 200),
+                    y = dnorm(x, 2, 0.57))
+
+slopedist <- ggplot(dat_slope,
+                    aes(x = x, y = y)) +
+  geom_line() +
+  labs(x = ""Hypothesized Slope"", y = ""Probability Density"")
+
+slopedist
+```
+
+---
+# Reifying Refutation - What is the probability something is false?
+
+BUT - our estimated slope is 3.
+
+```{r add_obs, fig.height = 5}
+slopedist +
+  geom_vline(xintercept = 3, color = ""red"", lty = 2) 
+```
+
+---
+# To falsify the 2:1 hypothesis, we need to know the probability of observing 3, or something GREATER than 3.
+
+We want to know if we did this experiment again and again, what's the probability of observing what we saw or worse (frequentist!)
+
+```{r add_p, fig.height = 5}
+dat_obs <- tibble(x = seq(3,5,length.out = 200),
+                    y = dnorm(x, 2, 0.57),
+                  ymin = 0)
+
+slopedist +
+  geom_vline(xintercept = 3, color = ""red"", lty = 2) +
+  geom_ribbon(aes(ymin = ymin, ymax = y), 
+              data = dat_obs, fill = ""red"", alpha = 0.5)
+```
+--
+
+Probability = `r round(1-pnorm(3, 2, 0.57),3)`
+
+--
+Note: We typically would multiply this by 2 to look at extremes in both tails.
+
+---
+class: center, middle
+
+# Null hypothesis testing is asking what is the probability of our observation or more extreme observation given that some null expectation is true.
+
+### (it is .red[**NOT**] the probability of any particular alternate hypothesis being true)
+
+---
+# R.A. Fisher and The P-Value For Null Hypotheses
+
+.pull-left[
+![](./images/07/fisher2.jpeg)
+]
+
+.pull-right[
+P-value: The Probability of making an observation or more extreme
+observation given that the null hypothesis is true.
+]
+
+---
+# Applying Fisher: Evaluation of a Test Statistic
+
+We  use our data to calculate a **test statistic** that maps to a value
+of the null distribution. 
+
+We can then calculate the probability of observing our data, or of observing data even more extreme, given that the null hypothesis is true.
+
+
+$$\large P(X \leq Data | H_{0})$$
+
+
+---
+# Problems with P
+
+- Most people don't understand it.
+     - See American Statistical Society' recent statements
+     
+--
+- Like SE, it gets smaller with sample size!
+--
+- Neyman-Pearson Null Hypothesis Significance Testing
+     - For Industrial Quality Control, NHST was introduced to establish cutoffs of reasonable p, called an $\alpha$
+     - This corresponds to Confidence intervals - 1-$\alpha$ = CI of interest
+     - This has become weaponized so that $\alpha = 0.05$ has become a norm.... and often determines if something is worthy of being published?
+     - Chilling effect on science
+     
+--
+- We don't know how to talk about it
+
+---
+# How do you talk about results from a p-value?
+
+- Based on your experimental design, what is a reasonable range of p-values to expect if the null is false
+
+- Smaller p values indicate stronger support for rejection, larger ones weaker. Use that language.
+
+- Accumulate multiple lines of evidence so that the entire edifice of your research does not rest on a single p-value!!!!
+
+---
+# For example, what does p = 0.061 mean?
+
+- There is a 6.1% chance of obtaining the observed data or more extreme data given that the null hypothesis is true.
+
+- If you choose to reject the null, you have a ~ 1 in 16 chance of being wrong
+
+- Are you comfortable with that? 
+
+- OR - What other evidence would you need to make you more or less comfortable?
+
+---
+
+# Common Regression Test Statistics
+
+- Does my model explain variability in the data?
+     - **Null Hypothesis**: The ratio of variability from your predictors versus noise is 1
+     - **Test Statistic**: F distribution (describes ratio of two variances)
+     
+- Are my coefficients not 0?
+    - **Null Hypothesis**: Coefficients are 0  
+    - **Test Statistic**: T distribution (normal distribution modified for low sample size)
+    
+---
+# Does my model explain variability in the data?
+
+Ho = The model predicts no variation in the data.  
+
+Ha = The model predicts variation in the data.
+
+--
+
+To evaluate these hypotheses, we need to have a measure of variation explained by data versus error - the sums of squares!
+
+--
+$$SS_{Total} = SS_{Regression} + SS_{Error}$$
+---
+
+# Sums of Squares of Error, Visually
+```{r linefit}
+``` 
+
+---
+# Sums of Squares of Regression, Visually
+```{r grandmean}
+set.seed(697)
+
+plot(x,y,pch=19, cex=0, cex.lab=1.5, cex.axis=1.1)
+abline(a, lwd=2)
+#segments(x,fitted(a),x,y, col=""red"", lwd=2)
+points(mean(x), mean(y), col=""blue"", pch=15)
+``` 
+
+---
+# Sums of Squares of Regression, Visually
+```{r ssr}
+set.seed(697)
+
+plot(x,y,pch=19, cex=0, cex.lab=1.5, cex.axis=1.1)
+abline(a, lwd=2)
+points(mean(x), mean(y), col=""blue"", pch=15)
+points(x, fitted(a), col=""blue"", pch=1)
+``` 
+
+Distance from $\hat{y}$ to $\bar{y}$
+
+---
+# Components of the Total Sums of Squares
+
+$SS_{R} = \sum(\hat{Y_{i}} - \bar{Y})^{2}$, df=1
+
+$SS_{E} = \sum(Y_{i} - \hat{Y}_{i})^2$, df=n-2
+
+
+--
+To compare them, we need to correct for different DF. This is the Mean
+Square.
+
+MS=SS/DF
+
+e.g, $MS_{E} = \frac{SS_{E}}{n-2}$
+
+---
+
+# The F Distribution and Ratios of Variances
+
+$F = \frac{MS_{R}}{MS_{E}}$ with DF=1,n-2 
+
+```{r f}
+
+x<-seq(0,6,.01)
+qplot(x,df(x,1,25), geom=""line"",  xlab=""Y"", ylab=""df(Y)"") + 
+  theme_bw(base_size=17)
+
+```
+
+---
+# F-Test and Pufferfish
+```{r f-puffer}
+knitr::kable(anova(puffer_lm))
+```
+
+<br><br>
+--
+We  reject the null hypothesis that resemblance does not explain variability in predator approaches
+
+---
+# Testing the Coefficients
+
+ -  F-Tests evaluate whether elements of the model contribute to variability in the data
+      - Are modeled predictors just noise?
+      - What's the difference between a model with only an intercept and an intercept and slope?
+
+--
+
+- T-tests evaluate whether coefficients are different from 0
+
+--
+
+- Often, F and T agree - but not always
+    - T can be more sensitive with multiple predictors
+
+---
+background-color: black
+class: center, middle, inverse
+
+![:scale 90%](images/09/t_distribution.png)
+
+.small[xkcd]
+
+---
+background-image: url(images/09/guiness_full.jpg)
+background-position: center
+background-size: contain
+
+---
+background-image: url(images/09/gosset.jpg)
+background-position: center
+background-size: contain
+
+---
+# T-Distributions are What You'd Expect Sampling a Standard Normal Population with a Small Sample Size
+
+- t = mean/SE, DF = n-1
+- It assumes a normal population with mean of 0 and SD of 1
+
+```{r dist_shape_t, fig.height=5}
+x_dists <- data.frame(x=seq(-2.5, 2.5, 0.01)) %>%
+  mutate(dn = dnorm(x),
+         dt_1 = dt(x, 1),
+         dt_2 = dt(x, 2),
+         dt_3 = dt(x, 3)
+  )
+
+x_df <- data.frame(x=rnorm(100), x_unif=runif(100))
+
+ggplot() +
+  geom_line(data=x_dists, mapping=aes(x=x, y=dn)) +
+  geom_line(data=x_dists, mapping=aes(x=x, y=dt_1), color=""red"") +
+  geom_line(data=x_dists, mapping=aes(x=x, y=dt_2), color=""orange"") +
+  geom_line(data=x_dists, mapping=aes(x=x, y=dt_3), color=""blue"") +
+  theme_classic(base_size=14) +
+  annotate(x=c(0.2,0.7,1.1,1.2), y=c(0.4, 0.3, 0.2, 0.1), 
+             label=c(""Normal"",""3DF"", ""2DF"", ""1DF""), fill=""white"",
+            fontface = ""bold"", geom=""label"") +
+  ylab(""density"")
+```
+
+---
+# Error in the Slope Estimate
+<br>
+
+
+$\Large SE_{b} = \sqrt{\frac{MS_{E}}{SS_{X}}}$
+
+
+
+#### 95% CI = $b \pm t_{\alpha,df}SE_{b}$  
+
+(~ 1.96 when N is large)
+
+
+---
+# Assessing the Slope with a T-Test
+<br>
+$$\Large t_{b} = \frac{b - \beta_{0}}{SE_{b}}$$ 
+
+##### DF=n-2
+
+$H_0: \beta_{0} = 0$, but we can test other hypotheses
+
+---
+# Slope of Puffer Relationship (DF = 1 for Parameter Tests)
+```{r puffer_t}
+knitr::kable(coef(summary(puffer_lm)))
+```
+
+<Br>
+We reject the hypothesis of no slope for resemblance, but fail to reject it for the intercept.
+
+---
+
+# So, what can we say?
+
+.pull-left[
+- We reject that there is no relationship between resemblance and predator visits in our experiment. 
+- `r round(summary(puffer_lm)$r.squared, 2)` of the variability in predator visits is associated with resemblance. 
+]
+
+.pull-right[
+```{r puffershow}
+```
+]
\ No newline at end of file

---FILE: lectures/linear_regression_details.html---
@@ -0,0 +1,878 @@
+<!DOCTYPE html>
+<html lang="""" xml:lang="""">
+  <head>
+    <title>Sampling and Simulation</title>
+    <meta charset=""utf-8"" />
+    <link href=""libs/remark-css/default.css"" rel=""stylesheet"" />
+    <link href=""libs/remark-css/shinobi.css"" rel=""stylesheet"" />
+    <link href=""libs/remark-css/default-fonts.css"" rel=""stylesheet"" />
+    <link rel=""stylesheet"" href=""style.css"" type=""text/css"" />
+  </head>
+  <body>
+    <textarea id=""source"">
+
+class: center, middle
+
+# Evaluating Fit Linear Models
+&lt;br&gt;
+![:scale 55%](images/12/linear_regression_love.gif)
+
+
+
+
+
+
+
+---
+class: center, middle
+
+# Etherpad
+&lt;br&gt;&lt;br&gt;
+&lt;center&gt;&lt;h3&gt;https://etherpad.wikimedia.org/p/607-lm-2020&lt;/h3&gt;&lt;/center&gt;
+
+
+---
+
+
+# Putting Linear Regression Into Practice with Pufferfish
+
+
+
+.pull-left[
+- Pufferfish are toxic/harmful to predators  
+&lt;br&gt;
+- Batesian mimics gain protection from predation - why?
+&lt;br&gt;&lt;br&gt;
+- Evolved response to appearance?
+&lt;br&gt;&lt;br&gt;
+- Researchers tested with mimics varying in toxic pufferfish resemblance
+]
+
+.pull-right[
+![:scale 80%](./images/11/puffer_mimics.jpg)
+]
+---
+## Question of the day: Does Resembling a Pufferfish Reduce Predator Visits?
+&lt;img src=""linear_regression_details_files/figure-html/puffershow-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+
+
+
+---
+
+# Digging Deeper into Regression
+
+1. Assumptions: Is our fit valid? 
+
+2. How did we fit this model?
+
+3. How do we draw inference from this model?
+
+
+---
+
+# You are now a Statistical Wizard. Be Careful. Your Model is a Golem.
+(sensu Richard McElreath)
+
+
+.center[.middle[![:scale 45%](images/09/golem.png)]]
+
+---
+
+# A Case of ""Great"" versus ""Not as Great"" Fits...
+
+.pull-left[
+
+![:scale 80%](./images/11/puffer_mimics.jpg)
+
+]
+
+.pull-right[
+
+![](./images/11/CUTE_WOLF_PUPS_by_horsesrock44.jpeg)
+
+]
+
+---
+# The Two Fits
+
+.pull-left[
+&lt;br&gt;&lt;br&gt;
+&lt;img src=""linear_regression_details_files/figure-html/puffershow-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+]
+
+.pull-right[
+
+&lt;br&gt;&lt;br&gt;
+&lt;img src=""linear_regression_details_files/figure-html/wolf_scatterplot-1.png"" style=""display: block; margin: auto;"" /&gt;
+]
+
+---
+# Assumptions (in rough descending order of importance)
+
+1. Validity
+
+2. Representativeness
+
+3. Model captures features in the data
+
+4. Additivity and Linearity
+
+5. Independence of Errors
+
+6. Equal Variance of Errors 
+
+7. Normality of Errors
+
+8. Minimal Outlier Influence
+
+---
+# Validity: Do X and Y Reflect Concepts I'm interested In
+
+
+&lt;img src=""linear_regression_details_files/figure-html/puffershow-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+
+What if predator approaches is not a good measure of recognition?  Or mimics just don't look like fish?
+
+---
+class: middle
+
+# Solution to lack of validity:  
+
+
+## Reframe your question! Change your framing! Question your life choices!
+
+---
+
+# Representativeness: Does Your Data Represent the Population?
+
+#### For example, say this is your result...
+
+
+&lt;img src=""linear_regression_details_files/figure-html/predictionRange1-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+class: center, middle
+
+# But is that all there is to X in nature?
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+#### What if you are looking at only a piece of the variation in X in your population?
+
+&lt;img src=""linear_regression_details_files/figure-html/predRange2-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+#### How should you have sampled this population for a representative result?
+
+&lt;img src=""linear_regression_details_files/figure-html/PredictionRange3-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+#### It's better to have more variation in X than just a bigger N
+
+
+&lt;img src=""linear_regression_details_files/figure-html/predRange4-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Representativeness: Does Your Data Represent the Population?
+
+- Always question if you did a good job sampling
+
+- Use natural history and the literature to get the bounds of values
+
+- If experimenting, make sure your treatment levels are representative
+
+- If you realize post-hoc they are not, **qualify your conclusions**
+
+---
+# Model captures features in the data
+&lt;img src=""linear_regression_details_files/figure-html/puffershow-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+Does the model seem to fit the data? Are there any deviations? Can be hard to see...
+
+
+---
+# Simulating implications from the model to see if we match features in the data
+
+&lt;img src=""linear_regression_details_files/figure-html/sims-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+Is anything off?
+
+---
+# But what to wolves say to you?
+
+&lt;img src=""linear_regression_details_files/figure-html/wolfsims-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Additivity and Linearity: Should account for all of the variation between residual and fitted values - what you want
+
+&lt;img src=""linear_regression_details_files/figure-html/pufferadd-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Additivity and Linearity: Wolf Problems?
+
+&lt;img src=""linear_regression_details_files/figure-html/wolfadd-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+--
+**Solutions:** Nonlinear transformations or a better model!
+
+---
+
+# Independence of Errors
+
+- Are all replicates TRULY independent
+
+- Did they come from the same space, time, etc.
+
+- Non-independence can introduce **BIAS**
+     - SEs too small (at the least)
+     - Causal inference invalid
+     
+- Incoporate Non-independence into models (many methods)
+
+---
+
+# Equal Variance of Errors: No Pattern to Residuals and Fitted Values
+
+&lt;img src=""linear_regression_details_files/figure-html/resfit_puffer-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Equal Variance of Errors: What is up with intermediate Wolf Values
+&lt;img src=""linear_regression_details_files/figure-html/resfit-1.png"" style=""display: block; margin: auto;"" /&gt;
+---
+
+# Equal Variance of Errors: Problems and Solutions
+
+- Shapes (cones, footballs, etc.) with no bias in fitted v. residual relationship
+
+- A linear relationship indicates an additivity problem
+
+- Can solve with a better model (more predictors)
+
+- Can solve with weighting by X values, if source of heteroskedasticity known
+     - This actually means we model the variance as a function of X
+     - `\(\epsilon_i \sim(N, f(x_i))\)`
+ 
+- Minor problem for coefficient estimates
+
+- Major problem for doing inference and prediction as it changes error
+
+---
+# Normality of errors: Did we fit the error generating process that we observed?
+
+- We assumed `\(\epsilon_i \sim N(0,\sigma)\)` - but is that right?
+
+- Can assess with a QQ-plot
+     - Do quantiles of the residuals match quantiles of a normal distribution?
+     
+- Again, minor problem for coefficient estimates  
+
+- Major problem for doing inference and prediction, as it changes error
+
+---
+# Equal Variance of Errors: Puffers
+
+&lt;img src=""linear_regression_details_files/figure-html/pufferqq-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Equal Variance of Errors: Wolves underpredict at High Levels
+
+&lt;img src=""linear_regression_details_files/figure-html/wolfqq-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+--
+
+
+```
+
+	Shapiro-Wilk normality test
+
+data:  residuals(wolf_mod)
+W = 0.9067, p-value = 0.02992
+```
+
+---
+# Outliers: Cook's D
+
+&lt;img src=""linear_regression_details_files/figure-html/pufferout-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+Want no values &gt; 1
+
+---
+# Outliers: Cook's D - wolves OK
+
+&lt;img src=""linear_regression_details_files/figure-html/pufferout_cook-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Everyone worries about outliers, but...
+
+- Are they real?
+
+- Do they indicate a problem or a nonlinearity?
+
+- Remove only as a dead last resort
+
+- If from a nonlinearity, consider transformation
+
+
+# Assumptions (in rough descending order of importance)
+
+1. Validity: only you know!
+
+2. Representativeness: look at nature
+
+3. Model captures features in the data: compare model v. data!
+
+4. Additivity and Linearity: compare model v. data!
+
+5. Independence of Errors: conisder sampling design
+
+6. Equal Variance of Errors: evaluate res-fit 
+
+7. Normality of Errors: evaluate qq and levene test
+
+8. Minimal Outlier Influence: evaluate Cook's D
+
+---
+
+# Digging Deeper into Regression
+
+1. Assumptions: Is our fit valid? 
+
+2. .red[How did we fit this model?]
+
+3. How do we draw inference from this model?
+
+---
+# So, uh.... How would you fit a line here?
+
+&lt;img src=""linear_regression_details_files/figure-html/puffer_only_scatter-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Lots of Possible Lines - How would you decide?
+
+&lt;img src=""linear_regression_details_files/figure-html/lsq-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Method of Model Fitting
+
+1. Least Squares
+  - Conceptually Simple
+  - Minimizes distance between fit and residuals
+  - Approximations of quantities based on frequentist logic
+  
+2. Likelihood
+  - Flexible to many error distributions and other problems
+  - Produces likelihood surface of different parameter values
+  - Equivalent to least square for Gaussian likelihood
+  - Approximations of quantities based on frequentist logic
+
+3. Bayesian
+  - Incorporates prior knowledge
+  - Probability for any parameter is likelihood * prior
+  - Superior for quantifying uncertainty
+  - With ""flat"" priors, equivalent to least squares/likelihood
+  - Analytic or simulated calculation of quantities
+
+---
+
+# Basic Principles of Least Squares Regression
+
+`\(\widehat{Y} = \beta_0 + \beta_1 X + \epsilon\)` where `\(\beta_0\)` = intercept, `\(\beta_1\)` = slope
+
+&lt;img src=""linear_regression_details_files/figure-html/linefit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+Minimize Residuals defined as `\(SS_{residuals} = \sum(Y_{i} - \widehat{Y})^2\)`
+
+---
+class: center, middle
+
+# Let's try it out!
+
+---
+
+# Analytic Solution: Solving for Slope
+&lt;br&gt;&lt;br&gt;
+
+`\(\LARGE b=\frac{s_{xy}}{s_{x}^2}\)` `\(= \frac{cov(x,y)}{var(x)}\)`
+
+--
+
+`\(\LARGE = r_{xy}\frac{s_{y}}{s_{x}}\)`
+
+
+
+---
+
+# Analytic Solution: Solving for Intercept
+&lt;br&gt;&lt;br&gt;
+Least squares regression line always goes through the mean of X and Y  
+
+
+`\(\Large \bar{Y} = \beta_0 + \beta_1 \bar{X}\)`
+
+&lt;br&gt;&lt;br&gt;
+
+--
+`\(\Large \beta_0 = \bar{Y} - \beta_1  \bar{X}\)`
+
+---
+
+# Digging Deeper into Regression
+
+1. Assumptions: Is our fit valid? 
+
+2. How did we fit this model?
+
+3. .red[How do we draw inference from this model?]
+
+---
+# Inductive v. Deductive Reasoning
+
+&lt;br&gt;&lt;br&gt;
+**Deductive Inference:** A larger theory is used to devise
+many small tests.
+
+
+**Inductive Inference:** Small pieces of evidence are used
+to shape a larger theory and degree of belief.
+---
+
+# Applying Different Styles of Inference
+
+- **Null Hypothesis Testing**: What's the probability that things are not influencing our data?
+      - Deductive
+
+- **Cross-Validation**: How good are you at predicting new data?
+      - Deductive
+
+- **Model Comparison**: Comparison of alternate hypotheses
+      - Deductive or Inductive
+
+- **Probabilistic Inference**: What's our degree of belief in a data?
+      - Inductive
+
+---
+# Null Hypothesis Testing is a Form of Deductive Inference
+
+.pull-left[
+![:scale 55%](./images/07/Karl_Popper_wikipedia.jpeg)
+
+Falsification of hypotheses is key! &lt;br&gt;&lt;br&gt;
+
+A theory should be considered scientific if, and only if, it is falsifiable.
+
+]
+
+--
+.pull-right[
+![:scale 55%](./images/regression/Professor_Imre_Lakatos,_c1960s.jpg)
+
+Look at a whole research program and falsify auxilliary hypotheses
+]
+
+
+---
+# A Bigger View of Dedictive Inference
+
+![](./images/regression/lakatos_structure.png)
+
+.small[https://plato.stanford.edu/entries/lakatos/#ImprPoppScie]
+
+---
+
+# Reifying Refutation - What is the probability something is false?
+
+What if our hypothesis was that the resemblance-predator relationship was 2:1. We know our SE of our estimate is 0.57, so, we have a distribution of what we **could** observe.
+
+&lt;img src=""linear_regression_details_files/figure-html/slopedist-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Reifying Refutation - What is the probability something is false?
+
+BUT - our estimated slope is 3.
+
+&lt;img src=""linear_regression_details_files/figure-html/add_obs-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# To falsify the 2:1 hypothesis, we need to know the probability of observing 3, or something GREATER than 3.
+
+We want to know if we did this experiment again and again, what's the probability of observing what we saw or worse (frequentist!)
+
+&lt;img src=""linear_regression_details_files/figure-html/add_p-1.png"" style=""display: block; margin: auto;"" /&gt;
+--
+
+Probability = 0.04
+
+--
+Note: We typically would multiply this by 2 to look at extremes in both tails.
+
+---
+class: center, middle
+
+# Null hypothesis testing is asking what is the probability of our observation or more extreme observation given that some null expectation is true.
+
+### (it is .red[**NOT**] the probability of any particular alternate hypothesis being true)
+
+---
+# R.A. Fisher and The P-Value For Null Hypotheses
+
+.pull-left[
+![](./images/07/fisher2.jpeg)
+]
+
+.pull-right[
+P-value: The Probability of making an observation or more extreme
+observation given that the null hypothesis is true.
+]
+
+---
+# Applying Fisher: Evaluation of a Test Statistic
+
+We  use our data to calculate a **test statistic** that maps to a value
+of the null distribution. 
+
+We can then calculate the probability of observing our data, or of observing data even more extreme, given that the null hypothesis is true.
+
+
+`$$\large P(X \leq Data | H_{0})$$`
+
+
+---
+# Problems with P
+
+- Most people don't understand it.
+     - See American Statistical Society' recent statements
+     
+--
+- Like SE, it gets smaller with sample size!
+--
+- Neyman-Pearson Null Hypothesis Significance Testing
+     - For Industrial Quality Control, NHST was introduced to establish cutoffs of reasonable p, called an `\(\alpha\)`
+     - This corresponds to Confidence intervals - 1-$\alpha$ = CI of interest
+     - This has become weaponized so that `\(\alpha = 0.05\)` has become a norm.... and often determines if something is worthy of being published?
+     - Chilling effect on science
+     
+--
+- We don't know how to talk about it
+
+---
+# How do you talk about results from a p-value?
+
+- Based on your experimental design, what is a reasonable range of p-values to expect if the null is false
+
+- Smaller p values indicate stronger support for rejection, larger ones weaker. Use that language.
+
+- Accumulate multiple lines of evidence so that the entire edifice of your research does not rest on a single p-value!!!!
+
+---
+# For example, what does p = 0.061 mean?
+
+- There is a 6.1% chance of obtaining the observed data or more extreme data given that the null hypothesis is true.
+
+- If you choose to reject the null, you have a ~ 1 in 16 chance of being wrong
+
+- Are you comfortable with that? 
+
+- OR - What other evidence would you need to make you more or less comfortable?
+
+---
+
+# Common Regression Test Statistics
+
+- Does my model explain variability in the data?
+     - **Null Hypothesis**: The ratio of variability from your predictors versus noise is 1
+     - **Test Statistic**: F distribution (describes ratio of two variances)
+     
+- Are my coefficients not 0?
+    - **Null Hypothesis**: Coefficients are 0  
+    - **Test Statistic**: T distribution (normal distribution modified for low sample size)
+    
+---
+# Does my model explain variability in the data?
+
+Ho = The model predicts no variation in the data.  
+
+Ha = The model predicts variation in the data.
+
+--
+
+To evaluate these hypotheses, we need to have a measure of variation explained by data versus error - the sums of squares!
+
+--
+`$$SS_{Total} = SS_{Regression} + SS_{Error}$$`
+---
+
+# Sums of Squares of Error, Visually
+&lt;img src=""linear_regression_details_files/figure-html/linefit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Sums of Squares of Regression, Visually
+&lt;img src=""linear_regression_details_files/figure-html/grandmean-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Sums of Squares of Regression, Visually
+&lt;img src=""linear_regression_details_files/figure-html/ssr-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+Distance from `\(\hat{y}\)` to `\(\bar{y}\)`
+
+---
+# Components of the Total Sums of Squares
+
+`\(SS_{R} = \sum(\hat{Y_{i}} - \bar{Y})^{2}\)`, df=1
+
+`\(SS_{E} = \sum(Y_{i} - \hat{Y}_{i})^2\)`, df=n-2
+
+
+--
+To compare them, we need to correct for different DF. This is the Mean
+Square.
+
+MS=SS/DF
+
+e.g, `\(MS_{E} = \frac{SS_{E}}{n-2}\)`
+
+---
+
+# The F Distribution and Ratios of Variances
+
+`\(F = \frac{MS_{R}}{MS_{E}}\)` with DF=1,n-2 
+
+&lt;img src=""linear_regression_details_files/figure-html/f-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# F-Test and Pufferfish
+
+|            | Df|   Sum Sq|    Mean Sq|  F value|   Pr(&gt;F)|
+|:-----------|--:|--------:|----------:|--------:|--------:|
+|resemblance |  1| 255.1532| 255.153152| 27.37094| 5.64e-05|
+|Residuals   | 18| 167.7968|   9.322047|       NA|       NA|
+
+&lt;br&gt;&lt;br&gt;
+--
+We  reject the null hypothesis that resemblance does not explain variability in predator approaches
+
+---
+# Testing the Coefficients
+
+ -  F-Tests evaluate whether elements of the model contribute to variability in the data
+      - Are modeled predictors just noise?
+      - What's the difference between a model with only an intercept and an intercept and slope?
+
+--
+
+- T-tests evaluate whether coefficients are different from 0
+
+--
+
+- Often, F and T agree - but not always
+    - T can be more sensitive with multiple predictors
+
+---
+background-color: black
+class: center, middle, inverse
+
+![:scale 90%](images/09/t_distribution.png)
+
+.small[xkcd]
+
+---
+background-image: url(images/09/guiness_full.jpg)
+background-position: center
+background-size: contain
+
+---
+background-image: url(images/09/gosset.jpg)
+background-position: center
+background-size: contain
+
+---
+# T-Distributions are What You'd Expect Sampling a Standard Normal Population with a Small Sample Size
+
+- t = mean/SE, DF = n-1
+- It assumes a normal population with mean of 0 and SD of 1
+
+&lt;img src=""linear_regression_details_files/figure-html/dist_shape_t-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Error in the Slope Estimate
+&lt;br&gt;
+
+
+`\(\Large SE_{b} = \sqrt{\frac{MS_{E}}{SS_{X}}}\)`
+
+
+
+#### 95% CI = `\(b \pm t_{\alpha,df}SE_{b}\)`  
+
+(~ 1.96 when N is large)
+
+
+---
+# Assessing the Slope with a T-Test
+&lt;br&gt;
+`$$\Large t_{b} = \frac{b - \beta_{0}}{SE_{b}}$$` 
+
+##### DF=n-2
+
+`\(H_0: \beta_{0} = 0\)`, but we can test other hypotheses
+
+---
+# Slope of Puffer Relationship (DF = 1 for Parameter Tests)
+
+|            | Estimate| Std. Error|  t value| Pr(&gt;&amp;#124;t&amp;#124;)|
+|:-----------|--------:|----------:|--------:|------------------:|
+|(Intercept) | 1.924694|  1.5064163| 1.277664|          0.2176012|
+|resemblance | 2.989492|  0.5714163| 5.231724|          0.0000564|
+
+&lt;Br&gt;
+We reject the hypothesis of no slope for resemblance, but fail to reject it for the intercept.
+
+---
+
+# So, what can we say?
+
+.pull-left[
+- We reject that there is no relationship between resemblance and predator visits in our experiment. 
+- 0.6 of the variability in predator visits is associated with resemblance. 
+]
+
+.pull-right[
+&lt;img src=""linear_regression_details_files/figure-html/puffershow-1.png"" style=""display: block; margin: auto;"" /&gt;
+]
+    </textarea>
+<style data-target=""print-only"">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
+<script src=""https://remarkjs.com/downloads/remark-latest.min.js""></script>
+<script src=""my_macros.js""></script>
+<script>var slideshow = remark.create({
+""highlightStyle"": ""github"",
+""highlightLines"": true,
+""countIncrementalSlides"": false
+});
+if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
+  window.dispatchEvent(new Event('resize'));
+});
+(function(d) {
+  var s = d.createElement(""style""), r = d.querySelector("".remark-slide-scaler"");
+  if (!r) return;
+  s.type = ""text/css""; s.innerHTML = ""@page {size: "" + r.style.width + "" "" + r.style.height +""; }"";
+  d.head.appendChild(s);
+})(document);
+
+(function(d) {
+  var el = d.getElementsByClassName(""remark-slides-area"");
+  if (!el) return;
+  var slide, slides = slideshow.getSlides(), els = el[0].children;
+  for (var i = 1; i < slides.length; i++) {
+    slide = slides[i];
+    if (slide.properties.continued === ""true"" || slide.properties.count === ""false"") {
+      els[i - 1].className += ' has-continuation';
+    }
+  }
+  var s = d.createElement(""style"");
+  s.type = ""text/css""; s.innerHTML = ""@media print { .has-continuation { display: none; } }"";
+  d.head.appendChild(s);
+})(document);
+// delete the temporary CSS (for displaying all slides initially) when the user
+// starts to view slides
+(function() {
+  var deleted = false;
+  slideshow.on('beforeShowSlide', function(slide) {
+    if (deleted) return;
+    var sheets = document.styleSheets, node;
+    for (var i = 0; i < sheets.length; i++) {
+      node = sheets[i].ownerNode;
+      if (node.dataset[""target""] !== ""print-only"") continue;
+      node.parentNode.removeChild(node);
+    }
+    deleted = true;
+  });
+})();
+(function() {
+  ""use strict""
+  // Replace <script> tags in slides area to make them executable
+  var scripts = document.querySelectorAll(
+    '.remark-slides-area .remark-slide-container script'
+  );
+  if (!scripts.length) return;
+  for (var i = 0; i < scripts.length; i++) {
+    var s = document.createElement('script');
+    var code = document.createTextNode(scripts[i].textContent);
+    s.appendChild(code);
+    var scriptAttrs = scripts[i].attributes;
+    for (var j = 0; j < scriptAttrs.length; j++) {
+      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
+    }
+    scripts[i].parentElement.replaceChild(s, scripts[i]);
+  }
+})();
+(function() {
+  var links = document.getElementsByTagName('a');
+  for (var i = 0; i < links.length; i++) {
+    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
+      links[i].target = '_blank';
+    }
+  }
+})();
+// adds .remark-code-has-line-highlighted class to <pre> parent elements
+// of code chunks containing highlighted lines with class .remark-code-line-highlighted
+(function(d) {
+  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
+  const preParents = [];
+  const findPreParent = function(line, p = 0) {
+    if (p > 1) return null; // traverse up no further than grandparent
+    const el = line.parentElement;
+    return el.tagName === ""PRE"" ? el : findPreParent(el, ++p);
+  };
+
+  for (let line of hlines) {
+    let pre = findPreParent(line);
+    if (pre && !preParents.includes(pre)) preParents.push(pre);
+  }
+  preParents.forEach(p => p.classList.add(""remark-code-has-line-highlighted""));
+})(document);</script>
+
+<script>
+slideshow._releaseMath = function(el) {
+  var i, text, code, codes = el.getElementsByTagName('code');
+  for (i = 0; i < codes.length;) {
+    code = codes[i];
+    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
+      text = code.textContent;
+      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
+          /^\$\$(.|\s)+\$\$$/.test(text) ||
+          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
+        code.outerHTML = code.innerHTML;  // remove <code></code>
+        continue;
+      }
+    }
+    i++;
+  }
+};
+slideshow._releaseMath(document);
+</script>
+<!-- dynamically load mathjax for compatibility with self-contained -->
+<script>
+(function () {
+  var script = document.createElement('script');
+  script.type = 'text/javascript';
+  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
+  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
+    script.src  = script.src.replace(/^https?:/, '');
+  document.getElementsByTagName('head')[0].appendChild(script);
+})();
+</script>
+  </body>
+</html>

---FILE: schedule.Rmd---
@@ -100,7 +100,7 @@ __Homework:__ [Functions and Pivot homework](./homework/04_fun_pivot.html)
 ```{r next_date, echo=FALSE}
 ```
 `r datestring`   
-__Lecture:__ Introduction to Regression: [Correlation and Regression](lectures/linear_regression.html), [Fit and Precision](lectures/12_linear_model_fit.html)  
+__Lecture:__ Introduction to Regression: [Correlation and Regression](lectures/linear_regression.html), [Fit and Precision](lectures/linear_regression_details.html)  
 __Lab Topic:__ [Linear regression, diagnostics, visualization](lab/06_lm.html), and [data](lab/data_06.zip) \
 __Reading:__ W&S 16-17, W&G on [model basics](http://r4ds.had.co.nz/model-basics.html), [model building](http://r4ds.had.co.nz/model-building.html)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-lm-2020  

---FILE: schedule.html---
@@ -391,7 +391,7 @@ <h2>Block 2: Regression and Inference</h2>
 <div id=""week-5."" class=""section level3"">
 <h3>Week 5.</h3>
 <p>10/5/2020<br />
-<strong>Lecture:</strong> Introduction to Regression: <a href=""lectures/linear_regression.html"">Correlation and Regression</a>, <a href=""lectures/12_linear_model_fit.html"">Fit and Precision</a><br />
+<strong>Lecture:</strong> Introduction to Regression: <a href=""lectures/linear_regression.html"">Correlation and Regression</a>, <a href=""lectures/linear_regression_details.html"">Fit and Precision</a><br />
 <strong>Lab Topic:</strong> <a href=""lab/06_lm.html"">Linear regression, diagnostics, visualization</a>, and <a href=""lab/data_06.zip"">data</a><br />
 <strong>Reading:</strong> W&amp;S 16-17, W&amp;G on <a href=""http://r4ds.had.co.nz/model-basics.html"">model basics</a>, <a href=""http://r4ds.had.co.nz/model-building.html"">model building</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-lm-2020"" class=""uri"">https://etherpad.wikimedia.org/p/607-lm-2020</a><br />"
biol607,biol607.github.io,951fdb81b7cc50dab1e4d4508213bb3fea7dad05,jebyrnes,jarrett.byrnes@umb.edu,2020-10-06T14:27:07Z,jebyrnes,jarrett.byrnes@umb.edu,2020-10-06T14:27:07Z,New regression lecture,lectures/linear_regression.Rmd;lectures/linear_regression.html;lectures/linear_regression_files/figure-html/centplot-1.png;lectures/linear_regression_files/figure-html/corLevels-1.png;lectures/linear_regression_files/figure-html/cor_and_reg-1.png;lectures/linear_regression_files/figure-html/fit-1.png;lectures/linear_regression_files/figure-html/logplot-1.png;lectures/linear_regression_files/figure-html/mvnorm_persp-1.png;lectures/linear_regression_files/figure-html/rnormPlot_cor-1.png;lectures/linear_regression_files/figure-html/rnormPlot_cov-1.png;lectures/linear_regression_files/figure-html/unnamed-chunk-1-1.png;lectures/linear_regression_files/figure-html/unnamed-chunk-2-1.png;lectures/linear_regression_files/figure-html/wolf_scatterplot-1.png;schedule.Rmd;schedule.html,True,False,True,False,1663,2,1665,"---FILE: lectures/linear_regression.Rmd---
@@ -0,0 +1,780 @@
+---
+title: ""Sampling and Simulation""
+output:
+  xaringan::moon_reader:
+    seal: false
+    lib_dir: libs
+    css: [default, shinobi, default-fonts, style.css]
+    nature:
+      beforeInit: ""my_macros.js""
+      highlightStyle: github
+      highlightLines: true
+      countIncrementalSlides: false
+---
+class: center, middle
+
+# Linear Regression
+
+
+![image](./images/11/correlation_xkcd.jpg)
+
+```{r setup, include=FALSE}
+library(knitr)
+library(ggplot2)
+library(dplyr)
+library(modelr)
+library(tidyr)
+library(mvtnorm)
+library(broom)
+library(arm)
+
+opts_chunk$set(fig.height=7, 
+               fig.width = 10,
+               fig.align = ""center"",
+               comment=NA, 
+               warning=FALSE, 
+               echo = FALSE,
+               message = FALSE)
+
+options(htmltools.dir.version = FALSE)
+theme_set(theme_bw(base_size=16))
+```
+
+---
+class: center, middle
+
+# Etherpad
+<br><br>
+<center><h3>https://etherpad.wikimedia.org/p/607-lm-2020</h3></center>
+
+---
+
+# The Steps of Statistical Modeling
+
+1. What is your question?
+2. What model of the world matches your question?
+3. Is your model valid?
+4. Query your model to answer your question.
+
+---
+
+# The Steps of Statistical Modeling
+
+1. What is your question?.red[
+2. What model of the world matches your question?
+3. Is your model valid?
+4. Query your model to answer your question.]
+
+---
+
+# Our question of the day: What is the relationship between inbreeding coefficient and litter size in wolves?
+
+.pull-left[
+
+```{r wolf_scatterplot, fig.height=5, fig.width=5}
+wolves <- read.csv(""./data/11/16e2InbreedingWolves.csv"")
+
+wolfplot <- ggplot(data=wolves, mapping=aes(x=inbreeding.coefficient, y=pups)) +
+xlab(""Inbreeding Coefficient"") + ylab(""# of Pups"") +
+geom_point(size=3) +
+theme_bw(base_size=24)
+
+wolfplot
+```
+
+]
+
+.pull-right[
+
+<br><br>
+![](./images/11/CUTE_WOLF_PUPS_by_horsesrock44.jpeg)
+]
+
+---
+
+# Roll that beautiful linear regression with 95% CI footage
+
+```{r fit}
+wolfplot +
+  stat_smooth(method = ""lm"")
+```
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. What do the error coefficients of a regression mean?
+
+4. Correlation and Regression
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+
+# What is a regression?
+
+.center[.Large[y = a + bx + error]]
+
+--
+
+This is 90% of the modeling you will ever do because...
+
+--
+
+Everything is a linear model!
+
+- multiple parameters (x1, x2, etc...)
+
+- nonlinear transformations of y or x
+
+- multiplicative terms (b * x1 * x2) are still additive
+
+- generalized linear models with non-normal error
+
+- and so much more....
+
+---
+class:center, middle
+
+# EVERYTHING IS A LINEAR MODEL
+
+---
+
+# Linear Regression
+<br>
+$\Large y_i = \beta_0 + \beta_1 x_i + \epsilon_i$  
+$\Large \epsilon_i \sim^{i.i.d.} N(0, \sigma)$  
+<Br><br><br>
+.large[
+Then it’s code in the data, give the keyboard a punch  
+Then cross-correlate and break for some lunch  
+Correlate, tabulate, process and screen  
+Program, printout, regress to the mean  
+  
+-White Coller Holler by Nigel Russell
+]
+
+---
+
+# Regressions You Have Seen
+.large[
+Classic style:
+
+$$y_i = \beta_0 + \beta_1  x_i + \epsilon_i$$
+$$\epsilon_i \sim N(0, \sigma)$$
+]
+
+
+--
+-----
+
+.large[
+
+Prediction as Part of Error: 
+
+$$\hat{y_i} = \beta_0 + \beta_1  x_i$$
+$$y_i \sim N(\hat{y_i}, \sigma)$$
+]
+
+
+--
+
+-----
+
+.large[
+Matrix Style: 
+$$Y = X \beta + \epsilon$$
+]
+
+---
+
+# These All Are Equation-Forms of This Relationship
+
+```{r fit}
+```
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. .red[What do regression coefficients mean?]
+
+3. What do the error coefficients of a regression mean?
+
+4. Correlation and Regression
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+
+# What are we doing with regression?
+
+### Goals:  
+
+--
+1. Association
+
+  - What is the strength of a relationship between two quantities
+  - Not causal
+  
+--
+
+2. Prediction
+  - If we have two groups that differ in their X value by 1 unit, what is the average difference in their Y unit?
+  - Not causal
+
+--
+
+3. Counterfactual
+  - What would happen to an individual if their value of X increased by one unit?
+  - Causal reasoning!
+  
+---
+
+# What Can We Say About This?
+
+```{r fit}
+```
+
+---
+
+# Model Coefficients: Slope
+
+```{r coefs}
+wolf_mod <- lm(pups ~ inbreeding.coefficient,
+               data = wolves)
+
+tidy(wolf_mod)[,1:3] %>%
+  kable(digits = 3, ""html"") %>%
+  kableExtra::kable_styling(""striped"")
+```
+
+--
+1. **Association:** A one unit increase in inbreeding coefficient is associated with ~11 fewer pups, on average.
+
+2. **Prediction:** A new wolf with an inbreeding coefficient 1 unit greater than a second new wolf will have ~11 fewer pups, on average.
+
+3. **Counterfactual:** If an individual wolf had had its inbreeding coefficient 1 unit higher, it would have ~11 fewer pups.
+
+---
+# Which of these is the correct thing to say? When?
+
+1. **Association:** A one unit increase in inbreeding coefficient is associated with ~11 fewer pups, on average.
+
+2. **Prediction:** A new wolf with an inbreeding coefficient 1 unit greater than a second new wolf will have ~11 fewer pups, on average.
+
+3. **Counterfactual:** If an individual wolf had had its inbreeding coefficient 1 unit higher, it would have ~11 fewer pups.
+
+---
+
+# 11 Fewer Pups? What would be, then, a Better Way to Talk About this Slope?
+
+```{r fit}
+```
+
+---
+# Model Coefficients: Intercept
+
+```{r coefs}
+```
+
+<br><br>
+
+--
+
+When the inbreeding coefficient is 0, a wolves will have ~6.6 pups, on average.
+
+
+---
+
+# Intercept Has Direct Interpretation on the Visualization
+
+```{r fit}
+```
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. .red[What do the error coefficients of a regression mean?]
+
+4. Correlation and Regression
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+
+# Two kinds of error
+
+1. Fit error - error due to lack of precision in estimates  
+      - Coefficient SE
+      - Precision of estimates
+  
+2. Residual error - error due to variability not explained by X.
+      - Residual SD (from $\epsilon_i$)
+      
+---
+
+# Precision: coefficient SEs
+
+```{r coefs}
+```
+
+--
+<br><br>
+
+- Shows precision of ability to estimate coefficients  
+
+- Gets smaller with bigger sample size!  
+
+- Remember, ~ 2 SE covered 95% CI  
+
+- Comes from likelihood surface...but we'll get there
+
+---
+# Visualizing Precision: 95% CI (~2 SE)
+```{r fit}
+```
+
+---
+# Visualizing Precision with Simulation from your Model
+
+```{r}
+coef_sim <- sim(wolf_mod)
+
+wolfplot +
+  geom_abline(slope = coef(coef_sim)[,2],
+              intercept = coef(coef_sim)[,1],
+              alpha = 0.2) +
+  stat_smooth(method = ""lm"", fill = NA, color = 'red', lwd = 2)
+```
+
+---
+
+# Residual Error
+
+```{r error}
+glance(wolf_mod)[c(1,3)] %>%
+  knitr::kable(digits = 3, ""html"") %>%
+  kableExtra::kable_styling(""striped"")
+```
+
+- Sigma  is the SD of the residual 
+
+$$\Large \epsilon_i \sim N(0,\sigma)$$
+
+- How much does does # of pups vary beyond the relationship with inbreeding coefficient?
+
+- For any number of pups estimated on average, ~68% of the # of pups observed will fall within ~1.5 of that number 
+
+---
+
+# Visualizing Residual Error's Implications
+
+
+```{r}
+
+coef_sim_1k <- sim(wolf_mod, n.sims = 1e3)
+
+res <- rnorm(length(sigma.hat(coef_sim_1k)), 0, sigma.hat(coef_sim_1k))
+
+wolfplot +
+  geom_abline(slope = mean(coef(coef_sim_1k)[,2]),
+              intercept = mean(coef(coef_sim_1k)[,1])+res,
+              alpha = 0.07) +
+  stat_smooth(method = ""lm"", fill = NA, color = 'red', lwd = 2)
+```
+---
+
+# Residual Error -> Variance Explained
+
+```{r error}
+```
+
+
+- $\large{R^2 = 1 - \frac{\sigma^2_{residual}}{\sigma^2_y}}$
+  - Fraction of the variation in Y related to X.
+  - Here, 36.9% of the variation in pups is related to variation in Inbreeding Coefficient
+  - Relates to r, the Pearson correlation coefficient
+
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. What do the error coefficients of a regression mean?
+
+4. .red[Correlation and Regression]
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+# What is Correlation?
+
+* The change in standard deviations of variable x per change in 1 SD of variable y  
+     * Clear, right?  
+
+  
+  
+ * Assesses the degree of association between two variables
+  
+  
+ * But, unitless (sort of)
+     * Between -1 and 1
+
+---
+# Calculating Correlation: Start with Covariance
+
+Describes the relationship between two variables. Not scaled.
+
+
+--
+
+$\sigma_{xy}$ = population level covariance  
+$s_{xy}$ = covariance in your sample
+--
+
+.pull-left[
+<br><br><br>
+$$\sigma_{XY} = \frac{\sum (X-\bar{X})(y-\bar{Y})}{n-1}$$
+]
+
+--
+
+.pull-right[
+```{r rnormPlot_cov, echo=FALSE, fig.height=4, fig.width=5}
+#create a data frame to show a multivariate normal distribution
+sigma <- matrix(c(3,2,2,4), ncol=2)
+vals <- rmvnorm(n=500, mean=c(1,2), sigma=sigma)
+
+nums<-seq(-5,5,.2)
+data_mvnorm<-expand.grid(x1=nums, x2=nums)
+data_mvnorm$freq<-dmvnorm(data_mvnorm, sigma=sigma)
+
+#make up some fake data
+set.seed(697)
+data_rmnorm<-as.data.frame(rmvnorm(400, sigma=sigma))
+names(data_rmnorm)=c(""x"", ""y"")
+data_rmnorm$y<-data_rmnorm$y + 3
+
+plot(y~x, pch=19, data=data_rmnorm, cex.lab=1.4)
+text(-4,8, paste(""cov(x,y) = "",round(cov(data_rmnorm)[1,2],3), sep=""""))
+
+```
+]
+
+---
+
+# Pearson Correlation
+
+Describes the relationship between two variables.  
+Scaled between -1 and 1.  
+<br>  
+$\large \rho_{xy}$ = population level correlation, $\large r_{xy}$ = correlation in
+your sample
+<div id=""left"" class=""fragment"">
+<br><br><br>
+$$\Large\rho_{xy} = \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}$$
+</div>
+
+<div id=""right"" class=""fragment"">
+```{r rnormPlot_cor, echo=FALSE, fig.height=4, fig.width=5}
+
+plot((y-mean(y))/sd(y)~I(x/sd(x)), pch=19, data=data_rmnorm, 
+     xlab=""\nZ transformed x"", ylab = ""Z transformed y"", cex.lab=1.4)
+text(-2,2, paste(""cor(x,y) = "",round(cor(data_rmnorm)[1,2],3), sep=""""))
+
+```
+</div>
+
+---
+# Assumptions of Pearson Correlation
+
+.pull-left[
+-   Observations are from a **random sample**  
+  
+  
+-   Each observation is **independent**  
+  
+  
+-   X and Y are from a **Normal Distribution**
+     - Weaker assumption
+]
+
+.pull-right[
+```{r mvnorm_persp, echo=FALSE, fig.height=7, fig.width=7}
+
+facetCols<-heat.colors(length(unique(data_mvnorm$freq)))
+data_mvnorm$fCols<-as.numeric(as.factor(data_mvnorm$freq))
+with(data_mvnorm, 
+     persp(nums, nums, matrix(freq, nrow=length(nums)),
+           xlab=""x"", ylab=""y"", zlab=""Frequency"",
+           theta = -90, phi = 25,  ltheta=145, border=gray(0.2),
+           col=""lightblue"", cex.axis=2
+           ))
+```
+
+]
+
+---
+# The meaning of r
+
+Y is perfectly predicted by X if r = -1 or 1.  
+<br><br>
+$R^2$ = the porportion of variation in y explained by x
+
+```{r corLevels, echo=FALSE, fig.height=6, fig.width=8}
+
+set.seed(1001)
+par(mfrow=c(2,2))
+for(i in c(0.2, 0.4, 0.6, 0.8)){
+  xy<-as.data.frame(rmvnorm(200, sigma=matrix(c(1, i, i, 1), byrow=T, nrow=2)))
+  names(xy)<-c(""x"", ""y"")
+  plot(y~x, data=xy, mar=c(3,1,1,2), main=paste(""r = "", round(cor(xy)[1,2],2), sep=""""))
+  
+}
+par(mfrow=c(1,1))
+```
+
+---
+# Get r in your bones...
+<br><br><br>
+<center>.large[.middle[http://guessthecorrelation.com/]]</center>
+
+---
+# Example: Wolf Breeding and Litter Size
+
+.pull-left[
+
+```{r wolf_scatterplot, fig.height=5, fig.width=5}
+```
+
+]
+
+.pull-right[
+
+<br><br>
+![](./images/11/CUTE_WOLF_PUPS_by_horsesrock44.jpeg)
+]
+
+---
+# Example: Wolf Inbreeding and Litter Size
+
+Covariance Matrix:
+```{r wolf_cov}
+round(cov(wolves),2)
+```
+
+--
+
+Correlation Matrix:
+```{r wolf_cor}
+round(cor(wolves),2)
+```
+
+--
+
+Yes, you can estimate a SE (`cor.test()` or bootstrapping)
+
+---
+
+# Wait, so, how does Correlation relate to Regression? Slope versus r...
+
+
+$\LARGE b=\frac{s_{xy}}{s_{x}^2}$ $= \frac{cov(x,y)}{var(x)}$
+  
+--
+<br><br>
+$\LARGE = r_{xy}\frac{s_{y}}{s_{x}}$
+
+---
+# Correlation v. Regression Coefficients
+
+```{r cor_and_reg}
+
+set.seed(1001)
+sampdf <- data.frame(x=1:50)
+sampdf <- within(sampdf, {
+       y1 <- rnorm(50, 3*x, 10)
+       y2 <- rnorm(50, 3*x, 40)
+       y3 <- y2/3
+})
+
+#cor(sampdf)
+#lines and slopes
+par(mfrow=c(1,3))
+par(cex.lab=3, cex.axis=2.1, cex.main=3)
+plot(y1 ~ x, data=sampdf, main=""Slope = 3, r = 0.98"", ylim=c(-60, 180))
+abline(lm(y1~x, data=sampdf), lwd=2, col=""red"")
+plot(y2 ~ x, data=sampdf, main=""Slope = 3, r = 0.72"", ylim=c(-60, 180))
+abline(lm(y2~x, data=sampdf), lwd=2, col=""red"")
+plot(y3 ~ x, data=sampdf, main=""Slope = 1, r = 0.72"", ylim=c(-60, 180))
+abline(lm(y3~x, data=sampdf), lwd=2, col=""red"")
+par(mfrow=c(1,1))
+
+```
+
+---
+# Or really, r is just the coefficient of a fit lm with a z-transform of our predictors
+
+$$\Large z_i = \frac{x_i - \bar{x}}{\sigma_x}$$
+.large[
+- When we z-transform variables, we put them on *the same scale*
+
+- The covariance between two z-transformed variables is their correlation!
+]
+
+---
+# Correlation versus Standardized Regression: It's the Same Picture
+
+$$z(y_i) = \beta_0 + \beta_1 z(x_i) + \epsilon_i$$
+
+```{r coefs_cor}
+wolves <- wolves %>%
+  mutate(pups_std = (pups - mean(pups))/sd(pups),
+         inbreeding_std = (inbreeding.coefficient - mean(inbreeding.coefficient))/sd(inbreeding.coefficient),
+         )
+
+cor_mod <- lm(pups_std ~inbreeding_std, data = wolves )
+
+tidy(cor_mod)[,1:3] %>%
+  kable(digits = 3, ""html"") %>%
+  kableExtra::kable_styling(""striped"")
+```
+
+versus correlation: `r with(wolves, cor(cbind(pups, inbreeding.coefficient)))[1,2] %>% round(3)`
+
+---
+class:center, middle
+
+# EVERYTHING IS A LINEAR MODEL
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. What do the error coefficients of a regression mean?
+
+4. Correlation and Regression
+
+5. .red[Transformation and Model Structure for More Sensible Coefficients]
+
+
+---
+# Two Common Modifications (transformations) to Regression
+1. Centering your X variable  
+
+     - Many times X = 0 is silly
+     
+     - E.g., if you use year, are you going to regress back to 0?
+     
+     - Centering X allows you to evaluate a meaningful intercept 
+           - what is Y at the mean of X  
+            
+--
+
+2. Log Transforming Y
+
+     - Often, Y cannot be negative  
+     
+     - And/or the process generating Y is *multiplicative*  
+     
+     - Log(Y) can fix this and other sins. 
+     
+     - **VERY** common, but, what do the coefficients mean? 
+---
+
+# Centering X to generate a meaningful intercept
+
+$$x_{i  \space centered} = x_i - mean(x)$$
+
+```{r coefs_cent}
+wolves <- wolves %>%
+  mutate(inbreeding.centered = inbreeding.coefficient -
+           mean(inbreeding.coefficient))
+
+cent_mod <- lm(pups ~inbreeding.centered, data = wolves )
+
+tidy(cent_mod)[,1:3] %>%
+  kable(digits = 3, ""html"") %>%
+  kableExtra::kable_styling(""striped"")
+```
+
+Intercept implies wolves with the average level of inbreeding in this study have ~4 pups. Wolves with higher inbreeding have fewer pups, wolves with lower inbreeding have more.
+
+---
+
+# Centering X to generate a meaningful intercept
+
+```{r centplot}
+ggplot(data=wolves, mapping=aes(x=inbreeding.centered, y=pups)) +
+  xlab(""Inbreeding Coefficient"") + 
+  ylab(""# of Pups"") +
+  geom_point(size=3) +
+  theme_bw(base_size=24) +
+  stat_smooth(method = ""lm"") +
+  geom_vline(xintercept = 0, color = ""red"", lty = 2)
+```
+
+---
+
+# Other Ways of Looking at This Relationship: Log Transformation of Y
+
+$$log(y_i) = \beta_0 + \beta_1 x_i + \epsilon_i$$
+  - relationship is now curved
+  - cannot have negative pups (yay!)
+
+```{r logplot, fig.height=5, fig.width = 8}
+wolfplot +
+  stat_smooth(method = ""glm"", 
+              method.args = list(family=gaussian(link = ""log"")))
+```
+
+---
+
+# Model Coefficients: Log Slope
+
+```{r logcoefs}
+wolf_mod_log <- lm(log(pups) ~ inbreeding.coefficient,
+               data = wolves)
+
+tidy(wolf_mod_log)[,1:3] %>%
+  kable(digits = 3, ""html"") %>%
+  kableExtra::kable_styling(""striped"")
+```
+
+--
+To understand the coefficient, remember
+$$y_i = e^{\beta_0 + \beta_1 x_i + \epsilon_i}$$
+
+exp(-2.994) = 0.05, so, a 1 unit increase in x causes y to change to 5% of its original value (a 95% loss) so...
+
+--
+
+**Association:** A one unit increase in inbreeding coefficient is associated with having 95% fewer pups, on average.
+
+---
+
+# You are now a Statistical Wizard. Be Careful. Your Model is a Golem.
+(sensu Richard McElreath)
+
+
+.center[.middle[![:scale 45%](images/09/golem.png)]]
\ No newline at end of file

---FILE: lectures/linear_regression.html---
@@ -0,0 +1,879 @@
+<!DOCTYPE html>
+<html lang="""" xml:lang="""">
+  <head>
+    <title>Sampling and Simulation</title>
+    <meta charset=""utf-8"" />
+    <link href=""libs/remark-css/default.css"" rel=""stylesheet"" />
+    <link href=""libs/remark-css/shinobi.css"" rel=""stylesheet"" />
+    <link href=""libs/remark-css/default-fonts.css"" rel=""stylesheet"" />
+    <script src=""libs/kePrint/kePrint.js""></script>
+    <link rel=""stylesheet"" href=""style.css"" type=""text/css"" />
+  </head>
+  <body>
+    <textarea id=""source"">
+
+class: center, middle
+
+# Linear Regression
+
+
+![image](./images/11/correlation_xkcd.jpg)
+
+
+
+---
+class: center, middle
+
+# Etherpad
+&lt;br&gt;&lt;br&gt;
+&lt;center&gt;&lt;h3&gt;https://etherpad.wikimedia.org/p/607-lm-2020&lt;/h3&gt;&lt;/center&gt;
+
+---
+
+# The Steps of Statistical Modeling
+
+1. What is your question?
+2. What model of the world matches your question?
+3. Is your model valid?
+4. Query your model to answer your question.
+
+---
+
+# The Steps of Statistical Modeling
+
+1. What is your question?.red[
+2. What model of the world matches your question?
+3. Is your model valid?
+4. Query your model to answer your question.]
+
+---
+
+# Our question of the day: What is the relationship between inbreeding coefficient and litter size in wolves?
+
+.pull-left[
+
+&lt;img src=""linear_regression_files/figure-html/wolf_scatterplot-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+]
+
+.pull-right[
+
+&lt;br&gt;&lt;br&gt;
+![](./images/11/CUTE_WOLF_PUPS_by_horsesrock44.jpeg)
+]
+
+---
+
+# Roll that beautiful linear regression with 95% CI footage
+
+&lt;img src=""linear_regression_files/figure-html/fit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. What do the error coefficients of a regression mean?
+
+4. Correlation and Regression
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+
+# What is a regression?
+
+.center[.Large[y = a + bx + error]]
+
+--
+
+This is 90% of the modeling you will ever do because...
+
+--
+
+Everything is a linear model!
+
+- multiple parameters (x1, x2, etc...)
+
+- nonlinear transformations of y or x
+
+- multiplicative terms (b * x1 * x2) are still additive
+
+- generalized linear models with non-normal error
+
+- and so much more....
+
+---
+class:center, middle
+
+# EVERYTHING IS A LINEAR MODEL
+
+---
+
+# Linear Regression
+&lt;br&gt;
+`\(\Large y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)`  
+`\(\Large \epsilon_i \sim^{i.i.d.} N(0, \sigma)\)`  
+&lt;Br&gt;&lt;br&gt;&lt;br&gt;
+.large[
+Then it’s code in the data, give the keyboard a punch  
+Then cross-correlate and break for some lunch  
+Correlate, tabulate, process and screen  
+Program, printout, regress to the mean  
+  
+-White Coller Holler by Nigel Russell
+]
+
+---
+
+# Regressions You Have Seen
+.large[
+Classic style:
+
+`$$y_i = \beta_0 + \beta_1  x_i + \epsilon_i$$`
+`$$\epsilon_i \sim N(0, \sigma)$$`
+]
+
+
+--
+-----
+
+.large[
+
+Prediction as Part of Error: 
+
+`$$\hat{y_i} = \beta_0 + \beta_1  x_i$$`
+`$$y_i \sim N(\hat{y_i}, \sigma)$$`
+]
+
+
+--
+
+-----
+
+.large[
+Matrix Style: 
+`$$Y = X \beta + \epsilon$$`
+]
+
+---
+
+# These All Are Equation-Forms of This Relationship
+
+&lt;img src=""linear_regression_files/figure-html/fit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. .red[What do regression coefficients mean?]
+
+3. What do the error coefficients of a regression mean?
+
+4. Correlation and Regression
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+
+# What are we doing with regression?
+
+### Goals:  
+
+--
+1. Association
+
+  - What is the strength of a relationship between two quantities
+  - Not causal
+  
+--
+
+2. Prediction
+  - If we have two groups that differ in their X value by 1 unit, what is the average difference in their Y unit?
+  - Not causal
+
+--
+
+3. Counterfactual
+  - What would happen to an individual if their value of X increased by one unit?
+  - Causal reasoning!
+  
+---
+
+# What Can We Say About This?
+
+&lt;img src=""linear_regression_files/figure-html/fit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Model Coefficients: Slope
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; estimate &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; std.error &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; (Intercept) &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 6.567 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.791 &lt;/td&gt;
+  &lt;/tr&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; inbreeding.coefficient &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -11.447 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 3.189 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+--
+1. **Association:** A one unit increase in inbreeding coefficient is associated with ~11 fewer pups, on average.
+
+2. **Prediction:** A new wolf with an inbreeding coefficient 1 unit greater than a second new wolf will have ~11 fewer pups, on average.
+
+3. **Counterfactual:** If an individual wolf had had its inbreeding coefficient 1 unit higher, it would have ~11 fewer pups.
+
+---
+# Which of these is the correct thing to say? When?
+
+1. **Association:** A one unit increase in inbreeding coefficient is associated with ~11 fewer pups, on average.
+
+2. **Prediction:** A new wolf with an inbreeding coefficient 1 unit greater than a second new wolf will have ~11 fewer pups, on average.
+
+3. **Counterfactual:** If an individual wolf had had its inbreeding coefficient 1 unit higher, it would have ~11 fewer pups.
+
+---
+
+# 11 Fewer Pups? What would be, then, a Better Way to Talk About this Slope?
+
+&lt;img src=""linear_regression_files/figure-html/fit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Model Coefficients: Intercept
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; estimate &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; std.error &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; (Intercept) &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 6.567 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.791 &lt;/td&gt;
+  &lt;/tr&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; inbreeding.coefficient &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -11.447 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 3.189 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+&lt;br&gt;&lt;br&gt;
+
+--
+
+When the inbreeding coefficient is 0, a wolves will have ~6.6 pups, on average.
+
+
+---
+
+# Intercept Has Direct Interpretation on the Visualization
+
+&lt;img src=""linear_regression_files/figure-html/fit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. .red[What do the error coefficients of a regression mean?]
+
+4. Correlation and Regression
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+
+# Two kinds of error
+
+1. Fit error - error due to lack of precision in estimates  
+      - Coefficient SE
+      - Precision of estimates
+  
+2. Residual error - error due to variability not explained by X.
+      - Residual SD (from `\(\epsilon_i\)`)
+      
+---
+
+# Precision: coefficient SEs
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; estimate &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; std.error &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; (Intercept) &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 6.567 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.791 &lt;/td&gt;
+  &lt;/tr&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; inbreeding.coefficient &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -11.447 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 3.189 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+--
+&lt;br&gt;&lt;br&gt;
+
+- Shows precision of ability to estimate coefficients  
+
+- Gets smaller with bigger sample size!  
+
+- Remember, ~ 2 SE covered 95% CI  
+
+- Comes from likelihood surface...but we'll get there
+
+---
+# Visualizing Precision: 95% CI (~2 SE)
+&lt;img src=""linear_regression_files/figure-html/fit-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Visualizing Precision with Simulation from your Model
+
+&lt;img src=""linear_regression_files/figure-html/unnamed-chunk-1-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Residual Error
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; r.squared &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; sigma &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0.369 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 1.523 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+- Sigma  is the SD of the residual 
+
+`$$\Large \epsilon_i \sim N(0,\sigma)$$`
+
+- How much does does # of pups vary beyond the relationship with inbreeding coefficient?
+
+- For any number of pups estimated on average, ~68% of the # of pups observed will fall within ~1.5 of that number 
+
+---
+
+# Visualizing Residual Error's Implications
+
+
+&lt;img src=""linear_regression_files/figure-html/unnamed-chunk-2-1.png"" style=""display: block; margin: auto;"" /&gt;
+---
+
+# Residual Error -&gt; Variance Explained
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; r.squared &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; sigma &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0.369 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 1.523 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+
+- `\(\large{R^2 = 1 - \frac{\sigma^2_{residual}}{\sigma^2_y}}\)`
+  - Fraction of the variation in Y related to X.
+  - Here, 36.9% of the variation in pups is related to variation in Inbreeding Coefficient
+  - Relates to r, the Pearson correlation coefficient
+
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. What do the error coefficients of a regression mean?
+
+4. .red[Correlation and Regression]
+
+5. Transformation and Model Structure for More Sensible Coefficients
+
+---
+# What is Correlation?
+
+* The change in standard deviations of variable x per change in 1 SD of variable y  
+     * Clear, right?  
+
+  
+  
+ * Assesses the degree of association between two variables
+  
+  
+ * But, unitless (sort of)
+     * Between -1 and 1
+
+---
+# Calculating Correlation: Start with Covariance
+
+Describes the relationship between two variables. Not scaled.
+
+
+--
+
+`\(\sigma_{xy}\)` = population level covariance  
+`\(s_{xy}\)` = covariance in your sample
+--
+
+.pull-left[
+&lt;br&gt;&lt;br&gt;&lt;br&gt;
+`$$\sigma_{XY} = \frac{\sum (X-\bar{X})(y-\bar{Y})}{n-1}$$`
+]
+
+--
+
+.pull-right[
+&lt;img src=""linear_regression_files/figure-html/rnormPlot_cov-1.png"" style=""display: block; margin: auto;"" /&gt;
+]
+
+---
+
+# Pearson Correlation
+
+Describes the relationship between two variables.  
+Scaled between -1 and 1.  
+&lt;br&gt;  
+`\(\large \rho_{xy}\)` = population level correlation, `\(\large r_{xy}\)` = correlation in
+your sample
+&lt;div id=""left"" class=""fragment""&gt;
+&lt;br&gt;&lt;br&gt;&lt;br&gt;
+`$$\Large\rho_{xy} = \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}$$`
+&lt;/div&gt;
+
+&lt;div id=""right"" class=""fragment""&gt;
+&lt;img src=""linear_regression_files/figure-html/rnormPlot_cor-1.png"" style=""display: block; margin: auto;"" /&gt;
+&lt;/div&gt;
+
+---
+# Assumptions of Pearson Correlation
+
+.pull-left[
+-   Observations are from a **random sample**  
+  
+  
+-   Each observation is **independent**  
+  
+  
+-   X and Y are from a **Normal Distribution**
+     - Weaker assumption
+]
+
+.pull-right[
+&lt;img src=""linear_regression_files/figure-html/mvnorm_persp-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+]
+
+---
+# The meaning of r
+
+Y is perfectly predicted by X if r = -1 or 1.  
+&lt;br&gt;&lt;br&gt;
+`\(R^2\)` = the porportion of variation in y explained by x
+
+&lt;img src=""linear_regression_files/figure-html/corLevels-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Get r in your bones...
+&lt;br&gt;&lt;br&gt;&lt;br&gt;
+&lt;center&gt;.large[.middle[http://guessthecorrelation.com/]]&lt;/center&gt;
+
+---
+# Example: Wolf Breeding and Litter Size
+
+.pull-left[
+
+&lt;img src=""linear_regression_files/figure-html/wolf_scatterplot-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+]
+
+.pull-right[
+
+&lt;br&gt;&lt;br&gt;
+![](./images/11/CUTE_WOLF_PUPS_by_horsesrock44.jpeg)
+]
+
+---
+# Example: Wolf Inbreeding and Litter Size
+
+Covariance Matrix:
+
+```
+                       inbreeding.coefficient  pups
+inbreeding.coefficient                   0.01 -0.11
+pups                                    -0.11  3.52
+```
+
+--
+
+Correlation Matrix:
+
+```
+                       inbreeding.coefficient  pups
+inbreeding.coefficient                   1.00 -0.61
+pups                                    -0.61  1.00
+```
+
+--
+
+Yes, you can estimate a SE (`cor.test()` or bootstrapping)
+
+---
+
+# Wait, so, how does Correlation relate to Regression? Slope versus r...
+
+
+`\(\LARGE b=\frac{s_{xy}}{s_{x}^2}\)` `\(= \frac{cov(x,y)}{var(x)}\)`
+  
+--
+&lt;br&gt;&lt;br&gt;
+`\(\LARGE = r_{xy}\frac{s_{y}}{s_{x}}\)`
+
+---
+# Correlation v. Regression Coefficients
+
+&lt;img src=""linear_regression_files/figure-html/cor_and_reg-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+# Or really, r is just the coefficient of a fit lm with a z-transform of our predictors
+
+`$$\Large z_i = \frac{x_i - \bar{x}}{\sigma_x}$$`
+.large[
+- When we z-transform variables, we put them on *the same scale*
+
+- The covariance between two z-transformed variables is their correlation!
+]
+
+---
+# Correlation versus Standardized Regression: It's the Same Picture
+
+`$$z(y_i) = \beta_0 + \beta_1 z(x_i) + \epsilon_i$$`
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; estimate &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; std.error &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; (Intercept) &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.000 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.166 &lt;/td&gt;
+  &lt;/tr&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; inbreeding_std &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.608 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.169 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+versus correlation: -0.608
+
+---
+class:center, middle
+
+# EVERYTHING IS A LINEAR MODEL
+
+---
+
+# Regressiong to Be Mean
+
+1. What is regression?  
+
+2. What do regression coefficients mean?
+
+3. What do the error coefficients of a regression mean?
+
+4. Correlation and Regression
+
+5. .red[Transformation and Model Structure for More Sensible Coefficients]
+
+
+---
+# Two Common Modifications (transformations) to Regression
+1. Centering your X variable  
+
+     - Many times X = 0 is silly
+     
+     - E.g., if you use year, are you going to regress back to 0?
+     
+     - Centering X allows you to evaluate a meaningful intercept 
+           - what is Y at the mean of X  
+            
+--
+
+2. Log Transforming Y
+
+     - Often, Y cannot be negative  
+     
+     - And/or the process generating Y is *multiplicative*  
+     
+     - Log(Y) can fix this and other sins. 
+     
+     - **VERY** common, but, what do the coefficients mean? 
+---
+
+# Centering X to generate a meaningful intercept
+
+`$$x_{i  \space centered} = x_i - mean(x)$$`
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; estimate &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; std.error &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; (Intercept) &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 3.958 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.311 &lt;/td&gt;
+  &lt;/tr&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; inbreeding.centered &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -11.447 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 3.189 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+Intercept implies wolves with the average level of inbreeding in this study have ~4 pups. Wolves with higher inbreeding have fewer pups, wolves with lower inbreeding have more.
+
+---
+
+# Centering X to generate a meaningful intercept
+
+&lt;img src=""linear_regression_files/figure-html/centplot-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Other Ways of Looking at This Relationship: Log Transformation of Y
+
+`$$log(y_i) = \beta_0 + \beta_1 x_i + \epsilon_i$$`
+  - relationship is now curved
+  - cannot have negative pups (yay!)
+
+&lt;img src=""linear_regression_files/figure-html/logplot-1.png"" style=""display: block; margin: auto;"" /&gt;
+
+---
+
+# Model Coefficients: Log Slope
+
+&lt;table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;""&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:left;""&gt; term &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; estimate &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; std.error &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; (Intercept) &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 1.944 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.215 &lt;/td&gt;
+  &lt;/tr&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:left;""&gt; inbreeding.coefficient &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -2.994 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.869 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
+
+--
+To understand the coefficient, remember
+`$$y_i = e^{\beta_0 + \beta_1 x_i + \epsilon_i}$$`
+
+exp(-2.994) = 0.05, so, a 1 unit increase in x causes y to change to 5% of its original value (a 95% loss) so...
+
+--
+
+**Association:** A one unit increase in inbreeding coefficient is associated with having 95% fewer pups, on average.
+
+---
+
+# You are now a Statistical Wizard. Be Careful. Your Model is a Golem.
+(sensu Richard McElreath)
+
+
+.center[.middle[![:scale 45%](images/09/golem.png)]]
+    </textarea>
+<style data-target=""print-only"">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
+<script src=""https://remarkjs.com/downloads/remark-latest.min.js""></script>
+<script src=""my_macros.js""></script>
+<script>var slideshow = remark.create({
+""highlightStyle"": ""github"",
+""highlightLines"": true,
+""countIncrementalSlides"": false
+});
+if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
+  window.dispatchEvent(new Event('resize'));
+});
+(function(d) {
+  var s = d.createElement(""style""), r = d.querySelector("".remark-slide-scaler"");
+  if (!r) return;
+  s.type = ""text/css""; s.innerHTML = ""@page {size: "" + r.style.width + "" "" + r.style.height +""; }"";
+  d.head.appendChild(s);
+})(document);
+
+(function(d) {
+  var el = d.getElementsByClassName(""remark-slides-area"");
+  if (!el) return;
+  var slide, slides = slideshow.getSlides(), els = el[0].children;
+  for (var i = 1; i < slides.length; i++) {
+    slide = slides[i];
+    if (slide.properties.continued === ""true"" || slide.properties.count === ""false"") {
+      els[i - 1].className += ' has-continuation';
+    }
+  }
+  var s = d.createElement(""style"");
+  s.type = ""text/css""; s.innerHTML = ""@media print { .has-continuation { display: none; } }"";
+  d.head.appendChild(s);
+})(document);
+// delete the temporary CSS (for displaying all slides initially) when the user
+// starts to view slides
+(function() {
+  var deleted = false;
+  slideshow.on('beforeShowSlide', function(slide) {
+    if (deleted) return;
+    var sheets = document.styleSheets, node;
+    for (var i = 0; i < sheets.length; i++) {
+      node = sheets[i].ownerNode;
+      if (node.dataset[""target""] !== ""print-only"") continue;
+      node.parentNode.removeChild(node);
+    }
+    deleted = true;
+  });
+})();
+(function() {
+  ""use strict""
+  // Replace <script> tags in slides area to make them executable
+  var scripts = document.querySelectorAll(
+    '.remark-slides-area .remark-slide-container script'
+  );
+  if (!scripts.length) return;
+  for (var i = 0; i < scripts.length; i++) {
+    var s = document.createElement('script');
+    var code = document.createTextNode(scripts[i].textContent);
+    s.appendChild(code);
+    var scriptAttrs = scripts[i].attributes;
+    for (var j = 0; j < scriptAttrs.length; j++) {
+      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
+    }
+    scripts[i].parentElement.replaceChild(s, scripts[i]);
+  }
+})();
+(function() {
+  var links = document.getElementsByTagName('a');
+  for (var i = 0; i < links.length; i++) {
+    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
+      links[i].target = '_blank';
+    }
+  }
+})();
+// adds .remark-code-has-line-highlighted class to <pre> parent elements
+// of code chunks containing highlighted lines with class .remark-code-line-highlighted
+(function(d) {
+  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
+  const preParents = [];
+  const findPreParent = function(line, p = 0) {
+    if (p > 1) return null; // traverse up no further than grandparent
+    const el = line.parentElement;
+    return el.tagName === ""PRE"" ? el : findPreParent(el, ++p);
+  };
+
+  for (let line of hlines) {
+    let pre = findPreParent(line);
+    if (pre && !preParents.includes(pre)) preParents.push(pre);
+  }
+  preParents.forEach(p => p.classList.add(""remark-code-has-line-highlighted""));
+})(document);</script>
+
+<script>
+slideshow._releaseMath = function(el) {
+  var i, text, code, codes = el.getElementsByTagName('code');
+  for (i = 0; i < codes.length;) {
+    code = codes[i];
+    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
+      text = code.textContent;
+      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
+          /^\$\$(.|\s)+\$\$$/.test(text) ||
+          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
+        code.outerHTML = code.innerHTML;  // remove <code></code>
+        continue;
+      }
+    }
+    i++;
+  }
+};
+slideshow._releaseMath(document);
+</script>
+<!-- dynamically load mathjax for compatibility with self-contained -->
+<script>
+(function () {
+  var script = document.createElement('script');
+  script.type = 'text/javascript';
+  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
+  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
+    script.src  = script.src.replace(/^https?:/, '');
+  document.getElementsByTagName('head')[0].appendChild(script);
+})();
+</script>
+  </body>
+</html>

---FILE: schedule.Rmd---
@@ -9,6 +9,7 @@ NEXT TIME I TEACH 607
 2. Plotting
 3. Functions, tidy data, dplyr
 4. Sampling and Simulation
+5. Cut modular programming. Introduce it via questions.
 
 -->
   While the topics covered are broad, each week will feature different examples from genetics, ecology, molecular, and evolutionary biology highlighting uses of each individual set of techniques.    
@@ -99,7 +100,7 @@ __Homework:__ [Functions and Pivot homework](./homework/04_fun_pivot.html)
 ```{r next_date, echo=FALSE}
 ```
 `r datestring`   
-__Lecture:__ Introduction to Regression: [Correlation and Regression](lectures/11_cor_linear_model.html), [Fit and Precision](lectures/12_linear_model_fit.html)  
+__Lecture:__ Introduction to Regression: [Correlation and Regression](lectures/linear_regression.html), [Fit and Precision](lectures/12_linear_model_fit.html)  
 __Lab Topic:__ [Linear regression, diagnostics, visualization](lab/06_lm.html), and [data](lab/data_06.zip) \
 __Reading:__ W&S 16-17, W&G on [model basics](http://r4ds.had.co.nz/model-basics.html), [model building](http://r4ds.had.co.nz/model-building.html)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-lm-2020  

---FILE: schedule.html---
@@ -323,6 +323,7 @@ <h1 class=""title toc-ignore"">Course Schedule and Readings</h1>
 2. Plotting
 3. Functions, tidy data, dplyr
 4. Sampling and Simulation
+5. Cut modular programming. Introduce it via questions.
 
 -->
 <p>While the topics covered are broad, each week will feature different examples from genetics, ecology, molecular, and evolutionary biology highlighting uses of each individual set of techniques.</p>
@@ -390,7 +391,7 @@ <h2>Block 2: Regression and Inference</h2>
 <div id=""week-5."" class=""section level3"">
 <h3>Week 5.</h3>
 <p>10/5/2020<br />
-<strong>Lecture:</strong> Introduction to Regression: <a href=""lectures/11_cor_linear_model.html"">Correlation and Regression</a>, <a href=""lectures/12_linear_model_fit.html"">Fit and Precision</a><br />
+<strong>Lecture:</strong> Introduction to Regression: <a href=""lectures/linear_regression.html"">Correlation and Regression</a>, <a href=""lectures/12_linear_model_fit.html"">Fit and Precision</a><br />
 <strong>Lab Topic:</strong> <a href=""lab/06_lm.html"">Linear regression, diagnostics, visualization</a>, and <a href=""lab/data_06.zip"">data</a><br />
 <strong>Reading:</strong> W&amp;S 16-17, W&amp;G on <a href=""http://r4ds.had.co.nz/model-basics.html"">model basics</a>, <a href=""http://r4ds.had.co.nz/model-building.html"">model building</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-lm-2020"" class=""uri"">https://etherpad.wikimedia.org/p/607-lm-2020</a><br />"
biol607,biol607.github.io,771edc719ee1cea52f4a0536b562c0f7d0a2c0fb,jebyrnes,jarrett.byrnes@umb.edu,2020-09-17T13:51:01Z,jebyrnes,jarrett.byrnes@umb.edu,2020-09-17T13:51:01Z,Fix security vulnerability,lectures/01_intro_files/reveal.js-3.3.0/package.json,False,False,False,False,2,1,3,"---FILE: lectures/01_intro_files/reveal.js-3.3.0/package.json---
@@ -25,7 +25,8 @@
     ""express"": ""~4.13.3"",
     ""grunt-cli"": ""~0.1.13"",
     ""mustache"": ""~2.2.1"",
-    ""socket.io"": ""~1.3.7""
+    ""socket.io"": ""~1.3.7"",
+    ""node-sass"": "">=4.13.1""
   },
   ""devDependencies"": {
     ""grunt"": ""~0.4.5"","
biol607,biol607.github.io,78cd6c7a3f6e10d91a9a3828de879c1e70726fc8,jebyrnes,jarrett.byrnes@umb.edu,2020-09-10T14:21:58Z,jebyrnes,jarrett.byrnes@umb.edu,2020-09-10T14:21:58Z,Fix etherpad,lectures/02_r_intro.Rmd;lectures/02_r_intro.html,True,False,True,False,5,5,10,"---FILE: lectures/02_r_intro.Rmd---
@@ -10,7 +10,7 @@ output:
 
 Hello!  So, today we're going to begin to code in R. We're going to cover the basics of using R with a focus on data frame objects.
 
-But let's begin. To talk about what's going on and take notes offline, use this week's etherpad at https://etherpad.wikimedia.org/p/607-r-intro-2020
+But let's begin. To talk about what's going on and take notes offline, use this week's etherpad at https://etherpad.wikimedia.org/p/607-intro-2020
 
 ## Let's start at the very beginning - directory structure!
 

---FILE: lectures/02_r_intro.html---
@@ -1516,7 +1516,7 @@ <h4 class=""author"">Intro to Computational Data Analysis for Biology</h4>
 
 
 <p>Hello! So, today we’re going to begin to code in R. We’re going to cover the basics of using R with a focus on data frame objects.</p>
-<p>But let’s begin. To talk about what’s going on and take notes offline, use this week’s etherpad at <a href=""https://etherpad.wikimedia.org/p/607-r-intro-2020"" class=""uri"">https://etherpad.wikimedia.org/p/607-r-intro-2020</a></p>
+<p>But let’s begin. To talk about what’s going on and take notes offline, use this week’s etherpad at <a href=""https://etherpad.wikimedia.org/p/607-intro-2020"" class=""uri"">https://etherpad.wikimedia.org/p/607-intro-2020</a></p>
 <div id=""lets-start-at-the-very-beginning---directory-structure"" class=""section level2"">
 <h2>Let’s start at the very beginning - directory structure!</h2>
 <p>Everything we do this semester is going to be scripted. There will be scripts. There will be data. There will be figures. If you’re working on a project, you’ll also have a manuscript, tables, raw data, derived, data, and more. Ai ya! That’s a lot of stuff. Putting it all in one directory is a <strong>terrible</strong> idea, as you will inevitably get lost.</p>
@@ -1704,15 +1704,15 @@ <h2>Functions to generate vectors for fun and profit</h2>
 <pre><code>##  [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0</code></pre>
 <p>Second, 10 random numbers between 0 and 100</p>
 <pre class=""r""><code>runif(10, min = 0, max = 100)</code></pre>
-<pre><code>##  [1] 80.5215166 72.3655995 22.1670252 64.3577743 99.0682981 87.8096879
-##  [7]  0.6709387 83.7260379 11.1156256 49.7273510</code></pre>
+<pre><code>##  [1] 89.18258 23.42808 97.21304 73.41101 95.43178 54.03394 62.91280 84.20774
+##  [9] 24.04156 40.77182</code></pre>
 <p>Now, vectors are neat, as they allow us to introduce two more concepts. First, some functions take vectors as input, and return other types of objects. For example, let’s say we wanted to sum everything in <code>my_vector</code> above. And then get the average of a bunch of random numbers between 0 and 100</p>
 <pre class=""r""><code>sum(my_vector)</code></pre>
 <pre><code>## [1] 5050</code></pre>
 <pre class=""r""><code>#a function in a function!
 #oh my!
 mean(runif(10, min = 0, max = 100))</code></pre>
-<pre><code>## [1] 66.83689</code></pre>
+<pre><code>## [1] 55.20371</code></pre>
 <p>OH! Notice I nested a function inside of a function. YES! You can do that. But only when you <strong>really</strong> need to. To keep track of things, it’s often better practice to create an object with a variable name that has meaning to you, and <em>then</em> feed that as an argument to another function.</p>
 </div>
 <div id=""digging-into-your-variables"" class=""section level2"">"
biol607,biol607.github.io,5e358107e01e5da8398340ad08c69d5917d3637a,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-12-11T19:35:33Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-12-11T19:35:33Z,fixing typo,lectures/iteration.Rmd;lectures/iteration.html,True,False,True,False,27,27,54,"---FILE: lectures/iteration.Rmd---
@@ -12,7 +12,7 @@ output:
     self_contained: false
     lib_dir: libs
 ---
-## {data-background=""./Images/iteration/maxresdefault.jpg""}
+## {data-background=""./images/iteration/maxresdefault.jpg""}
 
 <br><br><br>
 <h1 style=""background-color:white; font-size: 300%;""><center>Iteration</center></h2>
@@ -67,14 +67,14 @@ opts_chunk$set(fig.height=5, fig.width=7, comment=NA,
 
 ## Isn't this what computers/robots are all about?
 <Br><br>
-![](./Images/iteration/assembly_line.gif)
+![](./images/iteration/assembly_line.gif)
 
 
 ## The Map Paradigm
-![](./Images/iteration/map.png)
+![](./images/iteration/map.png)
 
 ## 
-![](./Images/iteration/purrr.png){width=60%}
+![](./images/iteration/purrr.png){width=60%}
 
 ## Map functions
 - Take a list or vector as input  
@@ -106,10 +106,10 @@ map(df, median)
 ```
 
 ## The Map Paradigm
-![](./Images/iteration/map.png)
+![](./images/iteration/map.png)
 
 ## What if I don't want a list
-![](./Images/iteration/meowtini.jpg)
+![](./images/iteration/meowtini.jpg)
 
 ## The world of maps
 - `map()` makes a list.
@@ -183,7 +183,7 @@ gapminder_list <- map(files, read_csv)
 The nice thing about a list is that we can just use `map()` on it in the future!
 
 ##
-![](./Images/iteration/to_purr_or_not.jpg)
+![](./images/iteration/to_purr_or_not.jpg)
 
 ## Is there More?
 <br><br>

---FILE: lectures/iteration.html---
@@ -512,7 +512,7 @@
     <div class=""slides"">
 
 
-<section id=""section"" class=""slide level2"" data-background=""./Images/iteration/maxresdefault.jpg"">
+<section id=""section"" class=""slide level2"" data-background=""./images/iteration/maxresdefault.jpg"">
 <h2></h2>
 <br><br><br>
 <h1 style=""background-color:white; font-size: 300%;"">
@@ -578,15 +578,15 @@ <h2>What do we know how to do?</h2>
 </section>
 <section id=""isnt-this-what-computersrobots-are-all-about"" class=""slide level2"">
 <h2>Isn’t this what computers/robots are all about?</h2>
-<p><Br><br> <img data-src=""./Images/iteration/assembly_line.gif"" /></p>
+<p><Br><br> <img data-src=""./images/iteration/assembly_line.gif"" /></p>
 </section>
 <section id=""the-map-paradigm"" class=""slide level2"">
 <h2>The Map Paradigm</h2>
-<p><img data-src=""./Images/iteration/map.png"" /></p>
+<p><img data-src=""./images/iteration/map.png"" /></p>
 </section>
 <section id=""section-1"" class=""slide level2"">
 <h2></h2>
-<p><img data-src=""./Images/iteration/purrr.png"" style=""width:60.0%"" /></p>
+<p><img data-src=""./images/iteration/purrr.png"" style=""width:60.0%"" /></p>
 </section>
 <section id=""map-functions"" class=""slide level2"">
 <h2>Map functions</h2>
@@ -619,24 +619,24 @@ <h2>Median Example</h2>
 <a class=""sourceLine"" id=""cb2-3"" data-line-number=""3""></a>
 <a class=""sourceLine"" id=""cb2-4"" data-line-number=""4""><span class=""kw"">map</span>(df, median)</a></code></pre></div>
 <pre><code>$a
-[1] -0.009649784
+[1] -0.008132079
 
 $b
-[1] 0.1999248
+[1] -0.4114584
 
 $c
-[1] 0.676125
+[1] 0.1897872
 
 $d
-[1] -0.5562476</code></pre>
+[1] -0.4339227</code></pre>
 </section>
 <section id=""the-map-paradigm-1"" class=""slide level2"">
 <h2>The Map Paradigm</h2>
-<p><img data-src=""./Images/iteration/map.png"" /></p>
+<p><img data-src=""./images/iteration/map.png"" /></p>
 </section>
 <section id=""what-if-i-dont-want-a-list"" class=""slide level2"">
 <h2>What if I don’t want a list</h2>
-<p><img data-src=""./Images/iteration/meowtini.jpg"" /></p>
+<p><img data-src=""./images/iteration/meowtini.jpg"" /></p>
 </section>
 <section id=""the-world-of-maps"" class=""slide level2"">
 <h2>The world of maps</h2>
@@ -653,22 +653,22 @@ <h2>The world of maps</h2>
 <h2>More medians</h2>
 <div class=""sourceCode"" id=""cb4""><pre class=""sourceCode r""><code class=""sourceCode r""><a class=""sourceLine"" id=""cb4-1"" data-line-number=""1""><span class=""kw"">map_dbl</span>(df, median)</a></code></pre></div>
 <pre><code>           a            b            c            d 
--0.009649784  0.199924771  0.676124958 -0.556247552 </code></pre>
+-0.008132079 -0.411458445  0.189787164 -0.433922735 </code></pre>
 <div class=""sourceCode"" id=""cb6""><pre class=""sourceCode r""><code class=""sourceCode r""><a class=""sourceLine"" id=""cb6-1"" data-line-number=""1""><span class=""kw"">map_chr</span>(df, median)</a></code></pre></div>
 <pre><code>          a           b           c           d 
-&quot;-0.009650&quot;  &quot;0.199925&quot;  &quot;0.676125&quot; &quot;-0.556248&quot; </code></pre>
+&quot;-0.008132&quot; &quot;-0.411458&quot;  &quot;0.189787&quot; &quot;-0.433923&quot; </code></pre>
 <div class=""sourceCode"" id=""cb8""><pre class=""sourceCode r""><code class=""sourceCode r""><a class=""sourceLine"" id=""cb8-1"" data-line-number=""1""><span class=""kw"">map_df</span>(df, median)</a></code></pre></div>
 <pre><code># A tibble: 1 x 4
-         a     b     c      d
-     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
-1 -0.00965 0.200 0.676 -0.556</code></pre>
+         a      b     c      d
+     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
+1 -0.00813 -0.411 0.190 -0.434</code></pre>
 </section>
 <section id=""what-if-i-have-more-than-one-argument"" class=""slide level2"">
 <h2>What if I have more than one argument?</h2>
 <div class=""sourceCode"" id=""cb10""><pre class=""sourceCode r""><code class=""sourceCode r""><a class=""sourceLine"" id=""cb10-1"" data-line-number=""1""><span class=""co"">#add extra arguments at the end</span></a>
 <a class=""sourceLine"" id=""cb10-2"" data-line-number=""2""><span class=""kw"">map_dbl</span>(df, median, <span class=""dt"">na.rm=</span>T)</a></code></pre></div>
 <pre><code>           a            b            c            d 
--0.009649784  0.199924771  0.676124958 -0.556247552 </code></pre>
+-0.008132079 -0.411458445  0.189787164 -0.433922735 </code></pre>
 </section>
 <section id=""what-if-i-have-more-than-one-argument-1"" class=""slide level2"">
 <h2>What if I have more than one argument?</h2>
@@ -679,7 +679,7 @@ <h2>What if I want a more flexible syntax?</h2>
 <div class=""sourceCode"" id=""cb12""><pre class=""sourceCode r""><code class=""sourceCode r""><a class=""sourceLine"" id=""cb12-1"" data-line-number=""1""><span class=""co"">#using ~ and . (or .x)</span></a>
 <a class=""sourceLine"" id=""cb12-2"" data-line-number=""2""><span class=""kw"">map_dbl</span>(df, <span class=""op"">~</span><span class=""kw"">median</span>(.x, <span class=""dt"">na.rm=</span>T))</a></code></pre></div>
 <pre><code>           a            b            c            d 
--0.009649784  0.199924771  0.676124958 -0.556247552 </code></pre>
+-0.008132079 -0.411458445  0.189787164 -0.433922735 </code></pre>
 </section>
 <section id=""you-try"" class=""slide level2"">
 <h2>You try!</h2>
@@ -734,7 +734,7 @@ <h2>Or - keep ’em in a list</h2>
 </section>
 <section id=""section-2"" class=""slide level2"">
 <h2></h2>
-<p><img data-src=""./Images/iteration/to_purr_or_not.jpg"" /></p>
+<p><img data-src=""./images/iteration/to_purr_or_not.jpg"" /></p>
 </section>
 <section id=""is-there-more"" class=""slide level2"">
 <h2>Is there More?</h2>
@@ -758,8 +758,8 @@ <h2>Use list values AND indices</h2>
 <div class=""sourceCode"" id=""cb17""><pre class=""sourceCode r""><code class=""sourceCode r""><a class=""sourceLine"" id=""cb17-1"" data-line-number=""1"">x &lt;-<span class=""st""> </span><span class=""kw"">runif</span>(<span class=""dv"">10</span>)</a>
 <a class=""sourceLine"" id=""cb17-2"" data-line-number=""2""></a>
 <a class=""sourceLine"" id=""cb17-3"" data-line-number=""3""><span class=""kw"">imap_dbl</span>(x, <span class=""op"">~</span><span class=""st""> </span>.x <span class=""op"">+</span><span class=""st""> </span>.y)</a></code></pre></div>
-<pre><code> [1]  1.476911  2.430700  3.047691  4.859135  5.913922  6.308767  7.248104
- [8]  8.411745  9.703914 10.020662</code></pre>
+<pre><code> [1]  1.047554  2.113172  3.127363  4.622543  5.212873  6.944407  7.563184
+ [8]  8.402763  9.872780 10.497505</code></pre>
 </section>
 <section id=""and-more"" class=""slide level2"">
 <h2>And more…</h2>"
biol607,biol607.github.io,fa6a0c260931db47c460a7a97bacdfbe25ded32b,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-11-09T15:16:33Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-11-09T15:16:33Z,fixing typo,schedule.Rmd;schedule.html,True,False,True,False,4,5,9,"---FILE: schedule.Rmd---
@@ -144,7 +144,7 @@ __In Class Code:__ [Tidy](in_class_code_2016/09_tidy.R), [markdown options](in_c
 __Lecture:__ Experimental design and ANOVA [part 1](lectures/19_expts_anova.html), [part 2](lectures/20_anova_2.html)    
 __Lab Topic:__ [One-Way ANOVA](lab/10_anova.html) and [data](lab/data/15e1KneesWhoSayNight.csv), Midterm work session	  
 __Reading:__ W&S Chapter 14-15  
-__Packages for The Week:__ `install.packages(c(""car"", ""emmeans"", ""multcompView"", ""contrast""))  
+__Packages for The Week:__ ` install.packages(c(""car"", ""emmeans"", ""multcompView"", ""contrast""))`
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-anova-2018
   
   
@@ -153,7 +153,7 @@ __Etherpad:__ https://etherpad.wikimedia.org/p/607-anova-2018
 ```
 `r datestring`   
 __Lectures:__ Experimental Design in a Multicausal World - [Multiway ANOVA](lectures/21_multiway_anova.html), [Factorial ANOVA](lectures/22_factorial_anova.html)    
-__Lab Topic:__  Discussion of Hurlbert, [Factorial ANOVA](lab/10_anova.html)  
+__Lab Topic:__  Discussion of Hurlbert, Cottingham, [Factorial ANOVA](lab/10a_anova.html)  
 __Lab Data:__  [Multiple Files](lab/data_10.zip)  
 __Reading:__ W&S 18, [Hurlbert 1984](http://byrneslab.net/classes/biol607/readings/Hurlbert_1984_eco_mono.pdf), [Cottingham et al. 2005](http://byrneslab.net/classes/biol607/readings/cottingham_et_al_2005_frontiers_all.pdf)  
 __In Class Code:__ [lots of anova](./in_class_code_2016/10_anova.R)  

---FILE: schedule.html---
@@ -341,14 +341,13 @@ <h3>Week 10.</h3>
 <strong>Lecture:</strong> Experimental design and ANOVA <a href=""lectures/19_expts_anova.html"">part 1</a>, <a href=""lectures/20_anova_2.html"">part 2</a><br />
 <strong>Lab Topic:</strong> <a href=""lab/10_anova.html"">One-Way ANOVA</a> and <a href=""lab/data/15e1KneesWhoSayNight.csv"">data</a>, Midterm work session<br />
 <strong>Reading:</strong> W&amp;S Chapter 14-15<br />
-<strong>Packages for The Week:</strong> `install.packages(c(“car”, “emmeans”, “multcompView”, “contrast”))<br />
-<strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-anova-2018"" class=""uri"">https://etherpad.wikimedia.org/p/607-anova-2018</a></p>
+<strong>Packages for The Week:</strong> <code>install.packages(c(&quot;car&quot;, &quot;emmeans&quot;, &quot;multcompView&quot;, &quot;contrast&quot;))</code> <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-anova-2018"" class=""uri"">https://etherpad.wikimedia.org/p/607-anova-2018</a></p>
 </div>
 <div id=""week-11."" class=""section level3"">
 <h3>Week 11.</h3>
 <p>11/12/2018<br />
 <strong>Lectures:</strong> Experimental Design in a Multicausal World - <a href=""lectures/21_multiway_anova.html"">Multiway ANOVA</a>, <a href=""lectures/22_factorial_anova.html"">Factorial ANOVA</a><br />
-<strong>Lab Topic:</strong> Discussion of Hurlbert, <a href=""lab/10_anova.html"">Factorial ANOVA</a><br />
+<strong>Lab Topic:</strong> Discussion of Hurlbert, Cottingham, <a href=""lab/10a_anova.html"">Factorial ANOVA</a><br />
 <strong>Lab Data:</strong> <a href=""lab/data_10.zip"">Multiple Files</a><br />
 <strong>Reading:</strong> W&amp;S 18, <a href=""http://byrneslab.net/classes/biol607/readings/Hurlbert_1984_eco_mono.pdf"">Hurlbert 1984</a>, <a href=""http://byrneslab.net/classes/biol607/readings/cottingham_et_al_2005_frontiers_all.pdf"">Cottingham et al. 2005</a><br />
 <strong>In Class Code:</strong> <a href=""./in_class_code_2016/10_anova.R"">lots of anova</a></p>"
biol607,biol607.github.io,bf028d2e6abd78c5b8a1f7422649422985dbe44d,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-11-02T14:44:22Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-11-02T14:44:22Z,fixing zoo brms,lab/08_bayes.Rmd;lab/08_bayes.html,True,False,True,False,8,6,14,"---FILE: lab/08_bayes.Rmd---
@@ -667,7 +667,7 @@ deet_plot +
 Do longer lived species also have larger home ranges? Let's test this!
 ```{r eval=FALSE}
 
-zoo <- read.csv(""./data/06/17q02ZooMortality Clubb and Mason 2003 replica.csv"")
+zoo <- read.csv(""../data/17q02ZooMortality Clubb and Mason 2003 replica.csv"")
 
 zoo_plot <- ggplot(data=___, aes(x=mortality, y=homerange)) + 
   ___()
@@ -676,9 +676,10 @@ ___
 
 
 #fit the model!
-zoo_mod <- stan_glm(___ ~ ___,
+zoo_mod <- brm(___ ~ ___,
                 data = ____, 
-                family=___)
+                family=___,
+                file = ""zoo_mod.Rds"")
 
 #model assumptions
 deet_fit <- predict(____) %>% as_tibble"
biol607,biol607.github.io,55ba59efb35a5d733ca5ec168353ab5c6e17d529,Jarrett Byrnes,jarrett.byrnes@csmxum00014844.isc3.umb.edu,2018-10-11T20:56:46Z,Jarrett Byrnes,jarrett.byrnes@csmxum00014844.isc3.umb.edu,2018-10-11T20:56:46Z,updating linear regression lab,lab/06_lm.Rmd;lab/06_lm.html;lab/06_lm_cache/html/__packages;lab/06_lm_cache/html/simFit_86c865b5f446948c68e4ea8b156970a8.RData;lab/06_lm_cache/html/simFit_86c865b5f446948c68e4ea8b156970a8.rdx;lab/06_lm_files/figure-html/assumptions-1.png;lab/06_lm_files/figure-html/assumptions_fit_resid-1.png;lab/06_lm_files/figure-html/cooks-1.png;lab/06_lm_files/figure-html/leverage-1.png;lab/06_lm_files/figure-html/modelr_fit-1.png;lab/06_lm_files/figure-html/obs_resid-1.png;lab/06_lm_files/figure-html/predict_plot-1.png;lab/06_lm_files/figure-html/qq-1.png;lab/06_lm_files/figure-html/resid_hist-1.png;lab/06_lm_files/figure-html/show_data-1.png;lab/06_lm_files/figure-html/unnamed-chunk-1-1.png;lab/06a_power_analysis_cache/html/__packages;lab/06a_power_analysis_cache/html/simFit_9857951c073bc37f4e915e5b329149f1.RData;lab/06a_power_analysis_cache/html/simFit_9857951c073bc37f4e915e5b329149f1.rdb;lab/06a_power_analysis_cache/html/simFit_9857951c073bc37f4e915e5b329149f1.rdx,True,False,True,False,1431,357,1788,"---FILE: lab/06_lm.Rmd---
@@ -1,8 +1,11 @@
 ---
 title: ""Linear Regression""
 author: ""Bill 607""
-date: ""October 13, 2016""
-output: html_document
+output:
+  html_document:
+    toc: true
+    toc_depth: 5
+    toc_float: true
 ---
 
 Believe it or not, despite all of the complexity under the hood, fitting a linear model in R with least squares is quite simple with a straightfoward workflow.
@@ -23,7 +26,7 @@ Let's go through each step with an example of seals. Are older seals larger?
 library(dplyr)
 library(ggplot2)
 
-seals <- read.csv(""./data/06/17e8ShrinkingSeals Trites 1996.csv"")
+seals <- read.csv(""./data/17e8ShrinkingSeals Trites 1996.csv"")
 
 seal_base <- ggplot(seals, aes(x=age.days, y=length.cm)) +
   geom_point() +
@@ -227,7 +230,7 @@ Fist, the relationship between how lean you are and how quickly you lose fat. Im
 
 ```{r, eval=FALSE}
 library(ggplot2)
-fat <- read.csv(""./data/06/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv"")
+fat <- read.csv(""./data/17q04BodyFatHeatLoss Sloan and Keatinge 1973 replica.csv"")
 
 #initial visualization to determine if lm is appropriate
 fat_plot <- ggplot(data=fat, aes(x=leanness, y=lossrate)) + 
@@ -255,7 +258,7 @@ fat_plot +
 For your first faded example, let's look at the relationship between DEET and mosquito bites.
 
 ```{r eval=FALSE}
-deet <- read.csv(""./data/06/17q24DEETMosquiteBites.csv"")
+deet <- read.csv(""./data/17q24DEETMosquiteBites.csv"")
 
 deet_plot <- ggplot(data=___, aes(x=dose, y=bites)) + 
   geom_point()
@@ -311,7 +314,7 @@ deet_plot +
 Do longer lived species also have larger home ranges? Let's test this!
 ```{r eval=FALSE}
 
-zoo <- read.csv(""./data/06/17q02ZooMortality Clubb and Mason 2003 replica.csv"")
+zoo <- read.csv(""./data/17q02ZooMortality Clubb and Mason 2003 replica.csv"")
 
 zoo_plot <- ggplot(data=___, aes(x=mortality, y=homerange)) + 
   ___()
@@ -362,126 +365,3 @@ zoo_plot +
 ```
 
 
-##### 6. Power
-
-OK, this is great, but what about power?  Let's look at our seal fit again - maybe we want to know what would have happened with a lower sample size?
-
-Note, I'm going to use a knitr function to neaten up the table for markdown. Try it in your own homeworks!
-
-```{r seal_coef}
-knitr::kable(tidy(seal_lm))
-```
-
-What about the residual SD?
-
-```{r seal_SD}
-knitr::kable(glance(seal_lm)[,1:3])
-```
-
-All right - there are our target values.  Now, we can change a lof ot things. The effect size (slope), range of values, sigma, and more. But let's try sample size.
-
-
-###### 5.1 Make a Table of Parameters, some of which vary
-
-The first step of a power analysis is to setup a data frame with all of the different possibilities that you might want to assess for power. For linear regression, we have the following parameters, all of which might vary:
-
-1. Slope
-2. Intercept
-3. Residual variance
-4. Sample Size
-5. Range of X values
-
-To do a power analysis via simulation for a linear regression, we begin by building our simulation data frame with the parameters and information that varies. In this case, sample size.
-
-```{r simPop}
-set.seed(607)
-
-simPopN <- data.frame(slope = 0.00237, 
-                      intercept=115.767,
-                      sigma = 5.6805,
-                      n=10:100) 
-```
-
-###### 5.2 Expand to have a number of rows for each simulated sample with each parameter combination
-
-OK, now we need to expand this out, and have some number of samples for each n.For that, we can use the function in `tidyr` (same library as crossings), `expand()`.
-
-```{r add_samp_size}
-library(tidyr)
-
-simPopN <- simPopN %>%
-  group_by(slope, intercept, sigma, n) %>%
-  expand(reps = 1:n) %>%
-  ungroup()
-```
-
-###### 5.3 Expand to create repeated simulated data sets for each combination of parameters
-
-Now, if we want to simulate each of these, say, 500 times, we need to assign unique sim numbers, so for each n and sim number we have a unique data set.
-
-```{r increase_sims}
-simPopN <- simPopN  %>%
-  crossing(sim = 1:500)
-```
-
-Great - almost ready to go! Now we just need to add in fitted values. Fortunately, as `rnorm()` works with vectors, we can just use a mutate here. We'll also need to simulate random draws of ages, but that's just another random number.
-
-###### 5.3 Simulate the data
-
-```{r add_length}
-simPopN <- simPopN %>%
-  mutate(age.days = runif(n(), 1000, 8500)) %>%
-  mutate(length.cm = rnorm(n(), intercept + slope*age.days, sigma))
-```
-
-Yatzee! Ready to run!
-
-###### 5.4 Fit models and extract coefficients
-
-
-First, we now need to generate a lot of fit models. Dplyr doesn't take too kindly to including fit things, so, we can use two powerful functions here - first, `nest` and `unnest()` allow us to collapse grouped data down into little pieces and re-expand it.
-```{r simNest}
-fits <- simPopN %>%
-    group_by(slope, intercept, sigma, n, sim) %>%
-    nest()
-
-fits
-```
-Second, the `map` function in the purrr library allows us to iterate over different levels or grouped data frames, and perform some function. In this case, we'll fit a model, get it's coefficients using `broom`.
-
-```{r simFit, cache=TRUE}
-fits <- fits %>%
-    mutate(mod = purrr:::map(data, ~lm(length.cm ~ age.days, data=.))) %>%
-    mutate(coefs = purrr::map(mod, ~tidy(.)))
-
-fits  
-```
-
-Last, we cleanup - we `unnest`, which takes one set of coefficients out. We'll also filter for just the slope.
-
-```{r simUnnest}
-fits <- fits %>%
-  unnest(coefs) %>%
-  ungroup() %>%
-  filter(term == ""age.days"")
-
-fits
-```
-
-###### 5.5 Calculate your Power
-
-Notice that we do indeed have p-values, so we can use these fits to get power for each sample size. We can now do our normal process - in this case grouping by sample size - to get power. And then we can plot the result!
-
-```{r pow}
-pow <- fits %>%
-    group_by(n) %>%
-    summarise(power = 1-sum(p.value>0.05)/n()) %>%
-    ungroup() 
-
-
-qplot(n, power,  data=pow, geom=c(""point"", ""line"")) +
-  theme_bw(base_size=17) +
-  geom_hline(yintercept=0.8, lty=2)
-```
-
-

---FILE: lab/06_lm_cache/html/__packages---
@@ -1,10 +1,4 @@
 base
-methods
-datasets
-utils
-grDevices
-graphics
-stats
 dplyr
 ggplot2
 modelr

---FILE: lab/06a_power_analysis_cache/html/__packages---
@@ -0,0 +1,6 @@
+base
+dplyr
+tidyr
+broom
+ggplot2
+purrr"
biol607,biol607.github.io,4a4563b218758361e820a6dee66e55c0619478b7,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-10-10T15:53:49Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-10-10T15:53:49Z,fixing homework,homework_2018/05_t_chisq_power.Rmd;homework_2018/05_t_chisq_power.html,True,False,True,False,208,14,222,"---FILE: homework_2018/05_t_chisq_power.Rmd---
@@ -7,10 +7,15 @@ output: html_document
 ```{r setup, include=FALSE}
 knitr::opts_chunk$set(echo = FALSE)
 ```
-<!-- next year, purge /data/05 from all -->
-## 1. W&S $\chi^2$ questions...  
+
+Note: Datasets are available at http://whitlockschluter.zoology.ubc.ca/data so you don't have to type anything in (and have to load it!)  
+\
+
+## 1. W&S $\chi^2$ questions
+Please answer W&S Chapter 8 Questions 12 and 24, and Chapter 9 Questions 16 and 27. User R where possible.
   
-## 2. W&S t-test questions...  
+## 2. W&S t-test questions  
+Please Answer W&S Chapter 11 Question 21 and Chapter 12 Questions 20,  26, and 30
   
 ## 3. Power and T
 "
biol607,biol607.github.io,3436c8426b327cf78a087d18a1ee76ff4a235a7b,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-10-09T01:53:08Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2018-10-09T01:53:08Z,fixing schedule,schedule.Rmd;schedule.html,True,False,True,False,6,5,11,"---FILE: schedule.Rmd---
@@ -72,7 +72,7 @@ __Lab Topic:__ [Distributions in R, Frequentist Hypothesis testing via simulatio
 __Reading:__ W&S 5-7, W&G Chapter 7, 16, [Abraham Lincoln and Confidence Intervals](http://andrewgelman.com/2016/11/23/abraham-lincoln-confidence-intervals/) and links therein    
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-hypotheses-2018  
 __Quiz:__ http://tinyurl.com/hyp-pre-quiz  
-__In Class Code:__  [Distributions and Power](in_class_code_2016/05_distributions_power.R)\
+<!-- __In Class Code:__  [Distributions and Power](in_class_code_2018/05_distributions_power.R)\ -->
 __Homework:__ [hypothesis testing and power](./homework_2018/04_hypothesis_power.html)  
 
   
@@ -88,7 +88,8 @@ __Discussion Reading:__ [ASA Statement on P-Values](http://byrneslab.net/classes
 (sign up in [here](https://etherpad.wikimedia.org/p/607-t_tests-2018))  (also feel free to read them all)  
 __Additional Readings on P-Values__: [Peaceful negotiation in the face of so-called ‘methodological terrorism’](https://errorstatistics.com/2016/10/01/a-new-front-in-the-statistics-wars-peaceful-negotiation-in-the-face-of-so-called-methodological-terrorism/), [P-value madness: A puzzle about the latest test ban (or ‘don’t ask, don’t tell’)](https://errorstatistics.com/2015/10/10/p-value-madness-a-puzzle-about-the-latest-test-ban-or-dont-ask-dont-tell/), [The Paradox of Replication, and the vindication of the P-value (but she can go deeper) ](https://errorstatistics.com/2015/08/31/the-paradox-of-replication-and-the-vindication-of-the-p-value-but-she-can-go-deeper-i/)  
 __Etherpad__: https://etherpad.wikimedia.org/p/607-t_tests-2018  
-<!-- __In Class Code:__ [t and chi square](in_class_code_2016/06_t_chisq.R) -->
+__In Class Code:__ [t and chi square](in_class_code_2018/scripts/chisq_t.R)  
+__Homework:__ [Chi Square and T Tests](./homework_2018/05_t_chisq_power.html)  
   
 ### Week 6.   
 ```{r next_date, echo=FALSE}

---FILE: schedule.html---
@@ -278,8 +278,7 @@ <h3>Week 4.</h3>
 <strong>Reading:</strong> W&amp;S 5-7, W&amp;G Chapter 7, 16, <a href=""http://andrewgelman.com/2016/11/23/abraham-lincoln-confidence-intervals/"">Abraham Lincoln and Confidence Intervals</a> and links therein<br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-hypotheses-2018"" class=""uri"">https://etherpad.wikimedia.org/p/607-hypotheses-2018</a><br />
 <strong>Quiz:</strong> <a href=""http://tinyurl.com/hyp-pre-quiz"" class=""uri"">http://tinyurl.com/hyp-pre-quiz</a><br />
-<strong>In Class Code:</strong> <a href=""in_class_code_2016/05_distributions_power.R"">Distributions and Power</a><br />
-<strong>Homework:</strong> <a href=""./homework_2018/04_hypothesis_power.html"">hypothesis testing and power</a></p>
+<!-- __In Class Code:__  [Distributions and Power](in_class_code_2018/05_distributions_power.R)\ --> <strong>Homework:</strong> <a href=""./homework_2018/04_hypothesis_power.html"">hypothesis testing and power</a></p>
 </div>
 <div id=""week-5."" class=""section level3"">
 <h3>Week 5.</h3>
@@ -290,7 +289,8 @@ <h3>Week 5.</h3>
 <strong>Discussion Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Wasserstein_Lazar_2016_The_American_Statistician.pdf"">ASA Statement on P-Values</a>, And choose one of the accompanying <a href=""http://byrneslab.net/classes/biol607/readings/p_value_statements.zip"">rejoinders</a> (sign up in <a href=""https://etherpad.wikimedia.org/p/607-t_tests-2018"">here</a>) (also feel free to read them all)<br />
 <strong>Additional Readings on P-Values</strong>: <a href=""https://errorstatistics.com/2016/10/01/a-new-front-in-the-statistics-wars-peaceful-negotiation-in-the-face-of-so-called-methodological-terrorism/"">Peaceful negotiation in the face of so-called ‘methodological terrorism’</a>, <a href=""https://errorstatistics.com/2015/10/10/p-value-madness-a-puzzle-about-the-latest-test-ban-or-dont-ask-dont-tell/"">P-value madness: A puzzle about the latest test ban (or ‘don’t ask, don’t tell’)</a>, <a href=""https://errorstatistics.com/2015/08/31/the-paradox-of-replication-and-the-vindication-of-the-p-value-but-she-can-go-deeper-i/"">The Paradox of Replication, and the vindication of the P-value (but she can go deeper)</a><br />
 <strong>Etherpad</strong>: <a href=""https://etherpad.wikimedia.org/p/607-t_tests-2018"" class=""uri"">https://etherpad.wikimedia.org/p/607-t_tests-2018</a><br />
-<!-- __In Class Code:__ [t and chi square](in_class_code_2016/06_t_chisq.R) --></p>
+<strong>In Class Code:</strong> <a href=""in_class_code_2018/scripts/chisq_t.R"">t and chi square</a><br />
+<strong>Homework:</strong> <a href=""./homework_2018/05_t_chisq_power.html"">Chi Square and T Tests</a></p>
 </div>
 <div id=""week-6."" class=""section level3"">
 <h3>Week 6.</h3>"
biol607,biol607.github.io,3a3a1fd708cfeacd704a725d6f1e739aaad3de56,Jarrett Byrnes,jarrett.byrnes@csmxum00014844.isc3.umb.edu,2018-10-03T16:05:47Z,Jarrett Byrnes,jarrett.byrnes@csmxum00014844.isc3.umb.edu,2018-10-03T16:05:47Z,fixing chisq errors,lectures/10_chisq.Rmd;lectures/10_chisq.html;lectures/10_chisq_files/figure-revealjs/chisq-1.jpeg,True,False,True,False,588,58,646,"---FILE: lectures/10_chisq.Rmd---
@@ -19,6 +19,7 @@ output:
 <div style=""backround:white; font-size:1.5em"">Quantifying Goodness of Fit: the $$\chi^2$$</div>
 ```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
 library(knitr)
+library(kableExtra)
 opts_chunk$set(fig.height=4.5, comment=NA, 
                warning=FALSE, message=FALSE, 
                dev=""jpeg"", echo=FALSE)
@@ -34,12 +35,15 @@ Consider the following data generating process:
 - We have a number of categories  
 \
 - We expect some number of observations in each category  
-\
-<span class=""fragment"">Then add this error generating process:  
-\
-> - Small random errors generating variation in observed values  
-\ 
-> - This error is *normal*
+
+
+<div class=""fragment""><br><br>Then add this error generating process:
+  
+- Small random errors generating variation in observed values  
+   
+- This error is *normal*</div>
+  
+
 
 ## Do our observed values fit our expectations?
 > - $H_0$: Observations = Expectations  
@@ -71,7 +75,7 @@ qplot(x, dchisq(x, df=df), data=chdata, geom=""line"", color=factor(df), group=df,
 ```
 
 ## Birth Days
-![](images/10/baby-home-frame.jpg){width = 80%}
+![](images/10/baby-home-frame.jpg){width=""80%""}
 
 Are births evenly spread across the week?
 
@@ -80,17 +84,18 @@ Are births evenly spread across the week?
 library(tibble)
 days <- c(""Sunday"", ""Monday"", ""Tuesday"", ""Wednesday"", ""Thursday"", ""Friday"", ""Saturday"")
 birth_vec <- c(33,	41,	63,	63,	47,	56,	47)
-births <- data.frame(`Day of the Week` = days, Births = birth_vec)
+births <- tibble(`Day of the Week` = days, Births = birth_vec)
 
-births
+kable(births, ""html"") %>% kable_styling(bootstrap_options = ""striped"")
 ```
 
 ## Even Expectations
 
 ```{r even_births}
 births$Expectation <- sum(births$Births)/7
 
-births
+kable(births, ""html"") %>% kable_styling(bootstrap_options = ""striped"")
+
 ```
 <br>  
 <span class=""fragment"">$\chi^2$ = `r with(births, sum((Births-Expectation)^2/Expectation))` with 6 DF<span><br><br>

---FILE: lectures/10_chisq.html---
@@ -7,11 +7,11 @@
   <meta name=""apple-mobile-web-app-capable"" content=""yes"">
   <meta name=""apple-mobile-web-app-status-bar-style"" content=""black-translucent"">
   <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"">
-  <link rel=""stylesheet"" href=""libs/reveal.js-3.3.0/css/reveal.css""/>
+  <link rel=""stylesheet"" href=""libs/reveal.js-3.3.0.1/css/reveal.css""/>
 
 
 
-<link rel=""stylesheet"" href=""libs/reveal.js-3.3.0/css/theme/white.css"" id=""theme"">
+<link rel=""stylesheet"" href=""libs/reveal.js-3.3.0.1/css/theme/white.css"" id=""theme"">
 
 
   <!-- some tweaks to reveal css -->
@@ -54,54 +54,429 @@
       border-color: gray;
     }
 
+
   </style>
 
     <style type=""text/css"">code{white-space: pre;}</style>
 
     <link rel=""stylesheet"" href=""style.css""/>
-    <!-- Printing and PDF exports -->
-    <script>
-      var link = document.createElement( 'link' );
-      link.rel = 'stylesheet';
-      link.type = 'text/css';
-      link.href = window.location.search.match( /print-pdf/gi ) ? 'libs/reveal.js-3.3.0/css/print/pdf.css' : 'libs/reveal.js-3.3.0/css/print/paper.css';
-      document.getElementsByTagName( 'head' )[0].appendChild( link );
-    </script>
-    <!--[if lt IE 9]>
-    <script src=""libs/reveal.js-3.3.0/lib/js/html5shiv.js""></script>
-    <![endif]-->
 
+<!-- Printing and PDF exports -->
+<script id=""paper-css"" type=""application/dynamic-css"">
+
+/* Default Print Stylesheet Template
+   by Rob Glazebrook of CSSnewbie.com
+   Last Updated: June 4, 2008
+
+   Feel free (nay, compelled) to edit, append, and
+   manipulate this file as you see fit. */
+
+
+@media print {
+
+	/* SECTION 1: Set default width, margin, float, and
+	   background. This prevents elements from extending
+	   beyond the edge of the printed page, and prevents
+	   unnecessary background images from printing */
+	html {
+		background: #fff;
+		width: auto;
+		height: auto;
+		overflow: visible;
+	}
+	body {
+		background: #fff;
+		font-size: 20pt;
+		width: auto;
+		height: auto;
+		border: 0;
+		margin: 0 5%;
+		padding: 0;
+		overflow: visible;
+		float: none !important;
+	}
+
+	/* SECTION 2: Remove any elements not needed in print.
+	   This would include navigation, ads, sidebars, etc. */
+	.nestedarrow,
+	.controls,
+	.fork-reveal,
+	.share-reveal,
+	.state-background,
+	.reveal .progress,
+	.reveal .backgrounds {
+		display: none !important;
+	}
+
+	/* SECTION 3: Set body font face, size, and color.
+	   Consider using a serif font for readability. */
+	body, p, td, li, div {
+		font-size: 20pt!important;
+		font-family: Georgia, ""Times New Roman"", Times, serif !important;
+		color: #000;
+	}
+
+	/* SECTION 4: Set heading font face, sizes, and color.
+	   Differentiate your headings from your body text.
+	   Perhaps use a large sans-serif for distinction. */
+	h1,h2,h3,h4,h5,h6 {
+		color: #000!important;
+		height: auto;
+		line-height: normal;
+		font-family: Georgia, ""Times New Roman"", Times, serif !important;
+		text-shadow: 0 0 0 #000 !important;
+		text-align: left;
+		letter-spacing: normal;
+	}
+	/* Need to reduce the size of the fonts for printing */
+	h1 { font-size: 28pt !important;  }
+	h2 { font-size: 24pt !important; }
+	h3 { font-size: 22pt !important; }
+	h4 { font-size: 22pt !important; font-variant: small-caps; }
+	h5 { font-size: 21pt !important; }
+	h6 { font-size: 20pt !important; font-style: italic; }
+
+	/* SECTION 5: Make hyperlinks more usable.
+	   Ensure links are underlined, and consider appending
+	   the URL to the end of the link for usability. */
+	a:link,
+	a:visited {
+		color: #000 !important;
+		font-weight: bold;
+		text-decoration: underline;
+	}
+	/*
+	.reveal a:link:after,
+	.reveal a:visited:after {
+		content: "" ("" attr(href) "") "";
+		color: #222 !important;
+		font-size: 90%;
+	}
+	*/
+
+
+	/* SECTION 6: more reveal.js specific additions by @skypanther */
+	ul, ol, div, p {
+		visibility: visible;
+		position: static;
+		width: auto;
+		height: auto;
+		display: block;
+		overflow: visible;
+		margin: 0;
+		text-align: left !important;
+	}
+	.reveal pre,
+	.reveal table {
+		margin-left: 0;
+		margin-right: 0;
+	}
+	.reveal pre code {
+		padding: 20px;
+		border: 1px solid #ddd;
+	}
+	.reveal blockquote {
+		margin: 20px 0;
+	}
+	.reveal .slides {
+		position: static !important;
+		width: auto !important;
+		height: auto !important;
+
+		left: 0 !important;
+		top: 0 !important;
+		margin-left: 0 !important;
+		margin-top: 0 !important;
+		padding: 0 !important;
+		zoom: 1 !important;
+
+		overflow: visible !important;
+		display: block !important;
+
+		text-align: left !important;
+		-webkit-perspective: none;
+		   -moz-perspective: none;
+		    -ms-perspective: none;
+		        perspective: none;
+
+		-webkit-perspective-origin: 50% 50%;
+		   -moz-perspective-origin: 50% 50%;
+		    -ms-perspective-origin: 50% 50%;
+		        perspective-origin: 50% 50%;
+	}
+	.reveal .slides section {
+		visibility: visible !important;
+		position: static !important;
+		width: auto !important;
+		height: auto !important;
+		display: block !important;
+		overflow: visible !important;
+
+		left: 0 !important;
+		top: 0 !important;
+		margin-left: 0 !important;
+		margin-top: 0 !important;
+		padding: 60px 20px !important;
+		z-index: auto !important;
+
+		opacity: 1 !important;
+
+		page-break-after: always !important;
+
+		-webkit-transform-style: flat !important;
+		   -moz-transform-style: flat !important;
+		    -ms-transform-style: flat !important;
+		        transform-style: flat !important;
+
+		-webkit-transform: none !important;
+		   -moz-transform: none !important;
+		    -ms-transform: none !important;
+		        transform: none !important;
+
+		-webkit-transition: none !important;
+		   -moz-transition: none !important;
+		    -ms-transition: none !important;
+		        transition: none !important;
+	}
+	.reveal .slides section.stack {
+		padding: 0 !important;
+	}
+	.reveal section:last-of-type {
+		page-break-after: avoid !important;
+	}
+	.reveal section .fragment {
+		opacity: 1 !important;
+		visibility: visible !important;
+
+		-webkit-transform: none !important;
+		   -moz-transform: none !important;
+		    -ms-transform: none !important;
+		        transform: none !important;
+	}
+	.reveal section img {
+		display: block;
+		margin: 15px 0px;
+		background: rgba(255,255,255,1);
+		border: 1px solid #666;
+		box-shadow: none;
+	}
+
+	.reveal section small {
+		font-size: 0.8em;
+	}
+
+}  
+</script>
+
+
+<script id=""pdf-css"" type=""application/dynamic-css"">
+    
+/**
+ * This stylesheet is used to print reveal.js
+ * presentations to PDF.
+ *
+ * https://github.com/hakimel/reveal.js#pdf-export
+ */
+
+* {
+	-webkit-print-color-adjust: exact;
+}
+
+body {
+	margin: 0 auto !important;
+	border: 0;
+	padding: 0;
+	float: none !important;
+	overflow: visible;
+}
+
+html {
+	width: 100%;
+	height: 100%;
+	overflow: visible;
+}
+
+/* Remove any elements not needed in print. */
+.nestedarrow,
+.reveal .controls,
+.reveal .progress,
+.reveal .playback,
+.reveal.overview,
+.fork-reveal,
+.share-reveal,
+.state-background {
+	display: none !important;
+}
+
+h1, h2, h3, h4, h5, h6 {
+	text-shadow: 0 0 0 #000 !important;
+}
+
+.reveal pre code {
+	overflow: hidden !important;
+	font-family: Courier, 'Courier New', monospace !important;
+}
+
+ul, ol, div, p {
+	visibility: visible;
+	position: static;
+	width: auto;
+	height: auto;
+	display: block;
+	overflow: visible;
+	margin: auto;
+}
+.reveal {
+	width: auto !important;
+	height: auto !important;
+	overflow: hidden !important;
+}
+.reveal .slides {
+	position: static;
+	width: 100%;
+	height: auto;
+
+	left: auto;
+	top: auto;
+	margin: 0 !important;
+	padding: 0 !important;
+
+	overflow: visible;
+	display: block;
+
+	-webkit-perspective: none;
+	   -moz-perspective: none;
+	    -ms-perspective: none;
+	        perspective: none;
+
+	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
+	   -moz-perspective-origin: 50% 50%;
+	    -ms-perspective-origin: 50% 50%;
+	        perspective-origin: 50% 50%;
+}
+
+.reveal .slides section {
+	page-break-after: always !important;
+
+	visibility: visible !important;
+	position: relative !important;
+	display: block !important;
+	position: relative !important;
+
+	margin: 0 !important;
+	padding: 0 !important;
+	box-sizing: border-box !important;
+	min-height: 1px;
+
+	opacity: 1 !important;
+
+	-webkit-transform-style: flat !important;
+	   -moz-transform-style: flat !important;
+	    -ms-transform-style: flat !important;
+	        transform-style: flat !important;
+
+	-webkit-transform: none !important;
+	   -moz-transform: none !important;
+	    -ms-transform: none !important;
+	        transform: none !important;
+}
+
+.reveal section.stack {
+	margin: 0 !important;
+	padding: 0 !important;
+	page-break-after: avoid !important;
+	height: auto !important;
+	min-height: auto !important;
+}
+
+.reveal img {
+	box-shadow: none;
+}
+
+.reveal .roll {
+	overflow: visible;
+	line-height: 1em;
+}
+
+/* Slide backgrounds are placed inside of their slide when exporting to PDF */
+.reveal section .slide-background {
+	display: block !important;
+	position: absolute;
+	top: 0;
+	left: 0;
+	width: 100%;
+	z-index: -1;
+}
+
+/* All elements should be above the slide-background */
+.reveal section>* {
+	position: relative;
+	z-index: 1;
+}
+
+/* Display slide speaker notes when 'showNotes' is enabled */
+.reveal .speaker-notes-pdf {
+	display: block;
+	width: 100%;
+	max-height: none;
+	left: auto;
+	top: auto;
+	z-index: 100;
+}
+
+/* Display slide numbers when 'slideNumber' is enabled */
+.reveal .slide-number-pdf {
+	display: block;
+	position: absolute;
+	font-size: 14px;
+}
+
+</script>
+
+
+<script>
+var style = document.createElement( 'style' );
+style.type = 'text/css';
+var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
+var style_script = document.getElementById(style_script_id).text;
+style.innerHTML = style_script;
+document.getElementsByTagName('head')[0].appendChild(style);
+</script>
+
+    <script src=""libs/kePrint-0.0.1/kePrint.js""></script>
 </head>
 <body>
   <div class=""reveal"">
     <div class=""slides"">
 
 
 <section id=""section"" class=""slide level2"" data-background=""images/10/bacteria-in-a-petri-dish-compressed.jpg"">
-<h1></h1>
+<h2></h2>
 <!-- next year add contingency tables -->
 <br><br><br><br>
 <div style=""backround:white; font-size:1.5em"">
 Quantifying Goodness of Fit: the <span class=""math display"">\[\chi^2\]</span>
 </div>
 </section>
 <section id=""number-of-observations-in-categories"" class=""slide level2"">
-<h1>Number of Observations in Categories</h1>
+<h2>Number of Observations in Categories</h2>
 <p>Consider the following data generating process:</p>
 <ul>
 <li>We have a number of categories<br />
 <br />
 </li>
-<li>We expect some number of observations in each category<br />
-<br />
-<span class=""fragment"">Then add this error generating process:<br />
-<br />
-&gt; - Small random errors generating variation in observed values<br />
-  &gt; - This error is <em>normal</em></li>
+<li>We expect some number of observations in each category</li>
+</ul>
+<div class=""fragment"">
+<p><br><br>Then add this error generating process:</p>
+<ul>
+<li><p>Small random errors generating variation in observed values</p></li>
+<li>This error is <em>normal</em>
+</div></li>
 </ul>
 </section>
 <section id=""do-our-observed-values-fit-our-expectations"" class=""slide level2"">
-<h1>Do our observed values fit our expectations?</h1>
+<h2>Do our observed values fit our expectations?</h2>
 <ul>
 <li class=""fragment""><span class=""math inline"">\(H_0\)</span>: Observations = Expectations<br />
 <br />
@@ -121,41 +496,189 @@ <h1>Do our observed values fit our expectations?</h1>
 </ul>
 </section>
 <section id=""the-chi2-distribution"" class=""slide level2"">
-<h1>The <span class=""math inline"">\(\chi^2\)</span> Distribution</h1>
+<h2>The <span class=""math inline"">\(\chi^2\)</span> Distribution</h2>
 <p><span class=""math display"">\[\chi^2 = \sum\frac{\displaystyle(O_i-E_i)^2}{E_i}\]</span></p>
-<p><img src=""10_chisq_files/figure-revealjs/chisq-1.jpeg"" title="""" alt="""" width=""768"" /></p>
+<p><img src=""10_chisq_files/figure-revealjs/chisq-1.jpeg"" width=""768"" /></p>
 </section>
 <section id=""birth-days"" class=""slide level2"">
-<h1>Birth Days</h1>
-<p><img src=""images/10/baby-home-frame.jpg"" />{width = 80%}</p>
+<h2>Birth Days</h2>
+<p><img src=""images/10/baby-home-frame.jpg"" style=""width:80.0%"" /></p>
 <p>Are births evenly spread across the week?</p>
 </section>
 <section id=""birth-days-1"" class=""slide level2"">
-<h1>Birth Days</h1>
-<pre><code>  Day.of.the.Week Births
-1          Sunday     33
-2          Monday     41
-3         Tuesday     63
-4       Wednesday     63
-5        Thursday     47
-6          Friday     56
-7        Saturday     47</code></pre>
+<h2>Birth Days</h2>
+<table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;"">
+<thead>
+<tr>
+<th style=""text-align:left;"">
+Day of the Week
+</th>
+<th style=""text-align:right;"">
+Births
+</th>
+</tr>
+</thead>
+<tbody>
+<tr>
+<td style=""text-align:left;"">
+Sunday
+</td>
+<td style=""text-align:right;"">
+33
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Monday
+</td>
+<td style=""text-align:right;"">
+41
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Tuesday
+</td>
+<td style=""text-align:right;"">
+63
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Wednesday
+</td>
+<td style=""text-align:right;"">
+63
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Thursday
+</td>
+<td style=""text-align:right;"">
+47
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Friday
+</td>
+<td style=""text-align:right;"">
+56
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Saturday
+</td>
+<td style=""text-align:right;"">
+47
+</td>
+</tr>
+</tbody>
+</table>
 </section>
 <section id=""even-expectations"" class=""slide level2"">
-<h1>Even Expectations</h1>
-<pre><code>  Day.of.the.Week Births Expectation
-1          Sunday     33          50
-2          Monday     41          50
-3         Tuesday     63          50
-4       Wednesday     63          50
-5        Thursday     47          50
-6          Friday     56          50
-7        Saturday     47          50</code></pre>
+<h2>Even Expectations</h2>
+<table class=""table table-striped"" style=""margin-left: auto; margin-right: auto;"">
+<thead>
+<tr>
+<th style=""text-align:left;"">
+Day of the Week
+</th>
+<th style=""text-align:right;"">
+Births
+</th>
+<th style=""text-align:right;"">
+Expectation
+</th>
+</tr>
+</thead>
+<tbody>
+<tr>
+<td style=""text-align:left;"">
+Sunday
+</td>
+<td style=""text-align:right;"">
+33
+</td>
+<td style=""text-align:right;"">
+50
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Monday
+</td>
+<td style=""text-align:right;"">
+41
+</td>
+<td style=""text-align:right;"">
+50
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Tuesday
+</td>
+<td style=""text-align:right;"">
+63
+</td>
+<td style=""text-align:right;"">
+50
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Wednesday
+</td>
+<td style=""text-align:right;"">
+63
+</td>
+<td style=""text-align:right;"">
+50
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Thursday
+</td>
+<td style=""text-align:right;"">
+47
+</td>
+<td style=""text-align:right;"">
+50
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Friday
+</td>
+<td style=""text-align:right;"">
+56
+</td>
+<td style=""text-align:right;"">
+50
+</td>
+</tr>
+<tr>
+<td style=""text-align:left;"">
+Saturday
+</td>
+<td style=""text-align:right;"">
+47
+</td>
+<td style=""text-align:right;"">
+50
+</td>
+</tr>
+</tbody>
+</table>
 <p><br><br />
 <span class=""fragment""><span class=""math inline"">\(\chi^2\)</span> = 15.24 with 6 DF<span><br><br> <span class=""fragment"">p = 0.01847</span></p>
 </section>
 <section id=""assumptions-of-chi2-test"" class=""slide level2"">
-<h1>Assumptions of <span class=""math inline"">\(\chi^2\)</span> test</h1>
+<h2>Assumptions of <span class=""math inline"">\(\chi^2\)</span> test</h2>
 <p align=""left"">
 Given that the goal is to detect deviations from expectations given normal error, this test has a few assumptions:
 </p>
@@ -181,8 +704,8 @@ <h1>Assumptions of <span class=""math inline"">\(\chi^2\)</span> test</h1>
     </div>
   </div>
 
-  <script src=""libs/reveal.js-3.3.0/lib/js/head.min.js""></script>
-  <script src=""libs/reveal.js-3.3.0/js/reveal.js""></script>
+  <script src=""libs/reveal.js-3.3.0.1/lib/js/head.min.js""></script>
+  <script src=""libs/reveal.js-3.3.0.1/js/reveal.js""></script>
 
   <script>
 
@@ -202,6 +725,8 @@ <h1>Assumptions of <span class=""math inline"">\(\chi^2\)</span> test</h1>
         // Transition style for full page slide backgrounds
         backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom
 
+
+
         // Optional reveal.js plugins
         dependencies: [
         ]
@@ -212,7 +737,7 @@ <h1>Assumptions of <span class=""math inline"">\(\chi^2\)</span> test</h1>
     (function () {
       var script = document.createElement(""script"");
       script.type = ""text/javascript"";
-      script.src  = ""https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"";
+      script.src  = ""https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"";
       document.getElementsByTagName(""head"")[0].appendChild(script);
     })();
   </script>"
biol607,biol607.github.io,77f8d33bd3ef966accac8220640db47e8252af64,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-29T19:59:06Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-29T19:59:06Z,Fixed typos and VIF,lectures/23_general_linear_model.Rmd;lectures/23_general_linear_model.html,True,False,True,False,98,20,118,"---FILE: lectures/23_general_linear_model.Rmd---
@@ -69,15 +69,22 @@ $$\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} $$
 
 ## Analysis of Covariance
 
-$$\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} $$  
+$$\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon}$$  
 \
-$$Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon$$  
 \
--   ANOVA + a continuous predictor
+$$Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon$$  
+
+
+## Analysis of Covariance
+
+$$Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon$$  
 
--   Often used to correct for a gradient or some continuous variable affecting outcome
 
--   OR used to correct a regression due to additional groups that may throw off slope estimates
+-   ANOVA + a continuous predictor\
+\
+-   Often used to correct for a gradient or some continuous variable affecting outcome\
+\
+-   OR used to correct a regression due to additional groups that may throw off slope estimates\
       - e.g. Simpson's Paradox: A positive relationship between test scores and academic performance can be masked by gender differences
 
 
@@ -182,10 +189,12 @@ residualPlots(neand_lm, cex.lab=1.4, test=FALSE)
 We test a model where
 $$Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \sum_{k}^{i=1}\beta_{j}x + \epsilon$$
 <div class=""fragment"">
+
 ```{r parallel_slopes}
 neand_lm_int <- lm(lnbrain ~ species * lnmass, data=neand)
 knitr::kable(Anova(neand_lm_int))
 ```
+
 </div>
 \
 <div class=""fragment"">If you have an interaction, welp, that's a different model - slopes vary by group!</div>
@@ -280,7 +289,7 @@ variance/covariance matrix of all *Independent variables*\
 
 ## What causes species richness?
 
-- Distance from fire patch, 
+- Distance from fire patch 
 - Elevation
 - Abiotic index
 - Patch age
@@ -336,7 +345,18 @@ Beyond this, are you getting unique information from each variable?</div>
 
 ## Checking for Multicollinearity: Variance Inflation Factor
 
-$$VIF = \frac{1}{1-R^2_{j}}$$ `
+> - Consider $y = \beta_{0} + \beta_{1}x_{1}  + \beta_{2}x_{2} + \epsilon$ \
+\
+> - And $X_{1} = \alpha_{0} + \alpha_{2}x_{2} + \epsilon_j$ \
+\
+> - $var(\beta_{1}) = \frac{\sigma^2}{(n-1)\sigma^2_{X_1}}\frac{1}{1-R^{2}_1}$
+\
+\
+<span class=""fragment"">$$VIF = \frac{1}{1-R^2_{1}}$$ </span>
+
+## Checking for Multicollinearity: Variance Inflation Factor
+$$VIF_1 = \frac{1}{1-R^2_{1}}$$ 
+
 
 ```{r klm_vif}
 vif(klm)

---FILE: lectures/23_general_linear_model.html---
@@ -159,17 +159,23 @@ <h1>Models with Many Predictors</h1>
 </section>
 <section id=""analysis-of-covariance"" class=""slide level2"">
 <h1>Analysis of Covariance</h1>
-<p><span class=""math display"">\[\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} \]</span><br />
+<p><span class=""math display"">\[\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon}\]</span><br />
 <br />
-<span class=""math display"">\[Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon\]</span><br />
 <br />
-- ANOVA + a continuous predictor</p>
-<ul>
-<li><p>Often used to correct for a gradient or some continuous variable affecting outcome</p></li>
-<li>OR used to correct a regression due to additional groups that may throw off slope estimates
+<span class=""math display"">\[Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon\]</span></p>
+</section>
+<section id=""analysis-of-covariance-1"" class=""slide level2"">
+<h1>Analysis of Covariance</h1>
+<p><span class=""math display"">\[Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon\]</span></p>
 <ul>
-<li>e.g. Simpson’s Paradox: A positive relationship between test scores and academic performance can be masked by gender differences</li>
-</ul></li>
+<li>ANOVA + a continuous predictor<br />
+<br />
+</li>
+<li>Often used to correct for a gradient or some continuous variable affecting outcome<br />
+<br />
+</li>
+<li>OR used to correct a regression due to additional groups that may throw off slope estimates<br />
+- e.g. Simpson’s Paradox: A positive relationship between test scores and academic performance can be masked by gender differences</li>
 </ul>
 </section>
 <section id=""neanderthals-and-ancova"" class=""slide level2"">
@@ -233,9 +239,47 @@ <h1>Group Residuals</h1>
 <h1>Test for Parallel Slopes</h1>
 We test a model where <span class=""math display"">\[Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \sum_{k}^{i=1}\beta_{j}x + \epsilon\]</span>
 <div class=""fragment"">
-<pre><code>                 Sum Sq   Df     F value      Pr(&gt;F)</code></pre>
-<hr />
-<p>species 0.0275528 1 6.220266 0.0175024 lnmass 0.1300183 1 29.352684 0.0000045 species:lnmass 0.0048452 1 1.093849 0.3027897 Residuals 0.1550332 35 NA NA</p>
+<table>
+<thead>
+<tr class=""header"">
+<th></th>
+<th style=""text-align: right;"">Sum Sq</th>
+<th style=""text-align: right;"">Df</th>
+<th style=""text-align: right;"">F value</th>
+<th style=""text-align: right;"">Pr(&gt;F)</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td>species</td>
+<td style=""text-align: right;"">0.0275528</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">6.220266</td>
+<td style=""text-align: right;"">0.0175024</td>
+</tr>
+<tr class=""even"">
+<td>lnmass</td>
+<td style=""text-align: right;"">0.1300183</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">29.352684</td>
+<td style=""text-align: right;"">0.0000045</td>
+</tr>
+<tr class=""odd"">
+<td>species:lnmass</td>
+<td style=""text-align: right;"">0.0048452</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">1.093849</td>
+<td style=""text-align: right;"">0.3027897</td>
+</tr>
+<tr class=""even"">
+<td>Residuals</td>
+<td style=""text-align: right;"">0.1550332</td>
+<td style=""text-align: right;"">35</td>
+<td style=""text-align: right;"">NA</td>
+<td style=""text-align: right;"">NA</td>
+</tr>
+</tbody>
+</table>
 </div>
 <br />
 
@@ -347,7 +391,7 @@ <h1></h1>
 <section id=""what-causes-species-richness"" class=""slide level2"">
 <h1>What causes species richness?</h1>
 <ul>
-<li>Distance from fire patch,</li>
+<li>Distance from fire patch</li>
 <li>Elevation</li>
 <li>Abiotic index</li>
 <li>Patch age</li>
@@ -404,7 +448,21 @@ <h1>Checking for Multicollinearity: Correlation Matrices</h1>
 </section>
 <section id=""checking-for-multicollinearity-variance-inflation-factor"" class=""slide level2"">
 <h1>Checking for Multicollinearity: Variance Inflation Factor</h1>
-<p><span class=""math display"">\[VIF = \frac{1}{1-R^2_{j}}\]</span> `</p>
+<ul>
+<li class=""fragment"">Consider <span class=""math inline"">\(y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \epsilon\)</span><br />
+<br />
+</li>
+<li class=""fragment"">And <span class=""math inline"">\(X_{1} = \alpha_{0} + \alpha_{2}x_{2} + \epsilon_j\)</span><br />
+<br />
+</li>
+<li class=""fragment""><span class=""math inline"">\(var(\beta_{1}) = \frac{\sigma^2}{(n-1)\sigma^2_{X_1}}\frac{1}{1-R^{2}_1}\)</span><br />
+<br />
+<span class=""fragment""><span class=""math display"">\[VIF = \frac{1}{1-R^2_{1}}\]</span> </span></li>
+</ul>
+</section>
+<section id=""checking-for-multicollinearity-variance-inflation-factor-1"" class=""slide level2"">
+<h1>Checking for Multicollinearity: Variance Inflation Factor</h1>
+<p><span class=""math display"">\[VIF_1 = \frac{1}{1-R^2_{1}}\]</span></p>
 <pre><code>  cover firesev  hetero 
  1.2949  1.2617  1.0504 </code></pre>
 <div style=""text-align:left"">"
biol607,biol607.github.io,5095b84ac2d5685ada23e9cf9f0462d1e7152395,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-29T15:15:58Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-29T15:15:58Z,Multiple Regression Lecture,lectures/23_general_linear_model.Rmd;lectures/23_general_linear_model.html;lectures/23_general_linear_model_files/figure-revealjs/cr_ancova-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/crazy-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/int_heatmap-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/int_visreg-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/int_visreg_2-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/keeley_int_plot-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/keeley_int_plot2-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/keeley_int_plot3d-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/keeley_int_pred-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/keeley_int_pred_nodata-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/keeley_pairs-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_avplot-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_crplot-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_diag-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_diag2-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_diag2_int-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_diag_int-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_see_effects-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/klm_visreg-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/neand_boxplot-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/neand_boxplot2-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/neand_plot-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/neand_plot_fit-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/surf_int-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/zoop_assumptions-1.jpeg;lectures/23_general_linear_model_files/figure-revealjs/zoop_group_assumptions-1.jpeg;lectures/3dplotting.R;lectures/data/23/18q09NeanderthalBrainSize.csv;lectures/data/23/Keeley_rawdata_select4.csv;lectures/images/23/2way_anova.png;lectures/images/23/anova.png;lectures/images/23/blockonly.png;lectures/images/23/fires.jpg;lectures/images/23/gosling_multicollinearity.jpg;lectures/images/23/interesting_mult_models.jpg;lectures/images/23/linear_reg_agression.jpg;lectures/images/23/matrix_regression.jpg;lectures/images/23/neanlooking.jpeg;lectures/images/23/outliers_cartoon.jpg;lectures/images/23/redo_analysis.jpg;lectures/images/23/regression1.png;lectures/images/23/regression2.png;lectures/images/23/regression_depression.jpg;lectures/images/23/responseonly.png;lectures/images/23/semipartial.png;lectures/images/23/willywonka_ancova.jpg;lectures/images/23/wonka_mult_regression.jpg;schedule.Rmd;schedule.html,True,False,True,False,1721,19,1740,"---FILE: lectures/23_general_linear_model.Rmd---
@@ -0,0 +1,686 @@
+---
+title:
+output:
+  revealjs::revealjs_presentation:
+    reveal_options:
+      slideNumber: true
+      previewLinks: true
+    theme: white
+    center: false
+    transition: fade
+    self_contained: false
+    lib_dir: libs
+    css: style.css
+---
+
+## 
+```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
+library(knitr)
+opts_chunk$set(fig.height=4.5, comment=NA, 
+               warning=FALSE, message=FALSE, 
+               dev=""jpeg"", echo=FALSE)
+library(dplyr)
+library(tidyr)
+library(broom)
+library(ggplot2)
+library(car)
+library(visreg)
+library(lsmeans)
+
+```
+![](images/23/wonka_mult_regression.jpg)
+<h2> Multiple Predictor Variables in General Linear Models</h2>
+
+##
+\ 
+\
+\
+<h3>https://etherpad.wikimedia.org/p/607-mlr</h3>
+
+## Data Generating Processes Until Now
+
+-   One predictor with one response\
+    \
+
+-   Except...ANOVA had multiple possible treatment levels
+
+## The General Linear Model
+
+$$\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} $$  
+
+-   This equation is huge. X can be anything - categorical,
+    continuous, squared, sine, etc.
+
+-   There can be straight additivity, or interactions
+
+-   So far, the only model we've used with >1 predictor is ANOVA
+
+
+## Models with Many Predictors
+
+0.   ANOVA
+
+1.   ANCOVA
+
+2.   Multiple Linear Regression
+
+3.   MLR with Interactions
+
+
+## Analysis of Covariance
+
+$$\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} $$  
+\
+$$Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon$$  
+\
+-   ANOVA + a continuous predictor
+
+-   Often used to correct for a gradient or some continuous variable affecting outcome
+
+-   OR used to correct a regression due to additional groups that may throw off slope estimates
+      - e.g. Simpson's Paradox: A positive relationship between test scores and academic performance can be masked by gender differences
+
+
+
+## Neanderthals and ANCOVA
+
+![image](./images/23/neanlooking.jpeg){width=""60.00000%""}
+
+Who had a bigger brain: Neanderthals or us?
+
+
+
+
+
+## The Means Look the Same...
+
+```{r neand_boxplot}
+neand <- read.csv(""./data/23/18q09NeanderthalBrainSize.csv"")
+neand_plot_box <- qplot(species, lnbrain, data=neand, fill=species, geom=""boxplot"")  + theme_bw()
+neand_plot_box
+```
+
+
+
+## But there appears to be a Relationship Between Body and Brain Mass
+
+```{r neand_plot}
+neand_plot <- qplot(lnmass, lnbrain, data=neand, color=species, size=I(3))  + theme_bw()
+neand_plot
+```
+
+
+
+## And Mean Body Mass is Different
+
+```{r neand_boxplot2}
+neand_plot_box2 <- qplot(species, lnmass, data=neand, fill=species, geom=""boxplot"")  + theme_bw()
+neand_plot_box2
+```
+
+## 
+\
+\
+![](images/23/redo_analysis.jpg)
+
+## Analysis of Covariance (control for a covariate)
+
+```{r neand_model, echo=FALSE}
+neand_lm <- lm(lnbrain ~ species + lnmass, data=neand)
+
+```
+
+
+```{r neand_plot_fit, fig.height=5, fig.width=7}
+neand <- cbind(neand, predict(neand_lm, interval=""confidence""))
+
+neand_plot +
+  geom_line(data=neand, aes(y=fit)) + 
+  geom_ribbon(data=neand, aes(ymin=lwr, 
+                              ymax=upr), 
+              fill=""lightgrey"", 
+              alpha=0.5) 
+```
+
+ANCOVA: Evaluate a categorical effect(s), controlling for a *covariate* (parallel lines)\
+ 
+Groups modify the *intercept*.
+
+
+## The Steps of Statistical Modeling
+1. What is your question?
+2. What model of the world matches your question?
+3. Build a test
+4. Evaluate test assumptions
+5. Evaluate test results
+6. Visualize
+
+
+## Assumptions of Multiway Anova
+-   Independence of data points
+
+-   Normality and homoscedacticity within groups (of residuals)
+
+-   No relationship between fitted and residual values
+
+-   Additivity of Treatment and Covariate (Parallel Slopes)
+
+
+## The Usual Suspects of Assumptions
+```{r zoop_assumptions, fig.height=7}
+par(mfrow=c(2,2))
+plot(neand_lm, which=c(1,2,5), cex.lab=1.4)
+par(mfrow=c(1,1))
+```
+
+## Group Residuals
+```{r zoop_group_assumptions, fig.height=7}
+residualPlots(neand_lm, cex.lab=1.4, test=FALSE)
+```
+
+## Test for Parallel Slopes
+We test a model where
+$$Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \sum_{k}^{i=1}\beta_{j}x + \epsilon$$
+<div class=""fragment"">
+```{r parallel_slopes}
+neand_lm_int <- lm(lnbrain ~ species * lnmass, data=neand)
+knitr::kable(Anova(neand_lm_int))
+```
+</div>
+\
+<div class=""fragment"">If you have an interaction, welp, that's a different model - slopes vary by group!</div>
+
+## Ye Olde F-Test
+```{r F_ANCOVA}
+knitr::kable(Anova(neand_lm))
+```
+
+<span class=""fragment"">What type of Sums of Squares?</span>\
+\
+<span class=""fragment"">II!</span>
+
+## Vsualizing Result
+```{r neand_plot_fit, fig.height=6}
+```
+
+## Parcelling Out Covariate
+
+```{r cr_ancova}
+crPlots(neand_lm)
+```
+
+## Parcelling Out Covariate with PostHocs & Least Square Menas
+```{r cr_lsmanes}
+options(digits=3)
+contrast(lsmeans(neand_lm, spec=""species""), ""tukey"")
+options(digits=5)
+```
+
+Evluated at mean of covariate
+
+
+## Models with Many Predictors
+
+0.   ANOVA
+
+1.   ANCOVA
+
+2.   <font color=""red"">Multiple Linear Regression</font>
+
+3.   MLR with Interactions
+
+## One-Way ANOVA Graphically
+
+![image](./images/23/anova.png){width=""60.00000%""}
+
+
+
+## Multiple Linear Regression?
+
+![image](./images/23/regression1.png){width=""60.00000%""}
+
+
+<div style=""text-align:left"">Note no connection between predictors, as in ANOVA. This is ONLY true ifwe have manipulated variables so that there is no relationship between
+the two. This is not often the case!</div>
+
+
+
+## Multiple Linear Regression
+
+![image](./images/23/regression2.png){width=""60.00000%""}
+
+
+<div style=""text-align:left"">Curved double-headed arrow indicates COVARIANCE between predictors that we must account for.  
+\
+MLR controls for the correlation - estimates unique contribution of each predictor.
+</div>
+
+
+## Calculating Multiple Regression Coefficients with OLS
+
+$$\boldsymbol{Y} = \boldsymbol{b X} + \boldsymbol{\epsilon}$$
+
+<div style=""text-align:left"">
+Remember in Simple Linear Regression $b = \frac{cov_{xy}}{var_{x}}$?\
+\
+In Multiple Linear Regression
+$\boldsymbol{b} = \boldsymbol{cov_{xy}}\boldsymbol{S_{x}^{-1}}$\
+\
+where $\boldsymbol{cov_{xy}}$ is the covariances of $\boldsymbol{x_i}$
+with $\boldsymbol{y}$ and $\boldsymbol{S_{x}^{-1}}$ is the
+variance/covariance matrix of all *Independent variables*\
+</div>
+
+
+##  {data-background=""images/23/fires.jpg""}
+<div style=""bottom:100%; text-align:left; background:goldenrod"">Five year study of wildfires & recovery in Southern California shur- blands in 1993. 90 plots (20 x 50m)
+(data from Jon Keeley et al.)</div>
+
+
+
+## What causes species richness?
+
+- Distance from fire patch, 
+- Elevation
+- Abiotic index
+- Patch age
+- Patch heterogeneity
+- Severity of last fire
+- Plant cover
+
+## Many Things may Influence Species Richness
+
+```{r keeley_pairs}
+keeley <- read.csv(""data/23/Keeley_rawdata_select4.csv"")
+pairs(keeley)
+```
+
+## Our Model
+\
+\
+$$Richness =\beta_{0} ̃+ \beta_{1} cover +\beta_{2} firesev + \beta_{3}hetero +\epsilon$$
+\
+\
+<div class=""fragment"">
+
+```{r mlr, echo=TRUE}
+
+klm <- lm(rich ~ cover + firesev + hetero, data=keeley)
+```
+</div>
+
+## Testing Assumptions
+> - Data Generating Process: Linearity \
+\
+> - Error Generating Process: Normality & homoscedasticity of residuals  
+\
+> - Data: Outliers not influencing residuals  
+\
+> - Predictors: Minimal multicollinearity
+
+## 
+![](./images/23/gosling_multicollinearity.jpg)
+
+## Checking for Multicollinearity: Correlation Matrices
+
+```{r klm_cor, size=""normalsize""}
+with(keeley, cor(cbind(cover, firesev, hetero)))
+``` 
+
+<div style=""text-align:left"">Correlations over 0.4 can
+be problematic, but, they may be OK even as high as 0.8. \
+\
+Beyond this, are you getting unique information from each variable?</div>
+
+
+
+## Checking for Multicollinearity: Variance Inflation Factor
+
+$$VIF = \frac{1}{1-R^2_{j}}$$ `
+
+```{r klm_vif}
+vif(klm)
+``` 
+
+<div style=""text-align:left"">VIF $>$ 5 or 10 can be problematic and indicate an unstable solution. \
+\
+Solution: evaluate correlation and drop a predictor
+</div>
+
+
+## Other Diagnostics as Usual!
+
+```{r klm_diag, fig.height=6}
+par(mfrow=c(2,2))
+plot(klm, which=c(1,2,5))
+par(mfrow=c(1,1))
+```
+
+
+
+## Examine Residuals With Respect to Each Predictor
+
+```{r klm_diag2}
+residualPlots(klm, test=FALSE)
+```
+
+
+
+## Which Variables Explained Variation: Type II Marginal SS
+
+```{r keeley_anova, size=""normalsize""}
+knitr::kable(Anova(klm), digits=2)
+``` 
+
+
+If order of entry matters, can use type I. Remember, what models are you comparing?
+
+
+
+## What’s Going On: Type I and II Sums of Squares
+
+  ------------ ------------ ------------ --
+                  Type I      Type II    
+   Test for A     A v. 1     A + B v. B  
+   Test for B   A + B v. A   A + B v. A  
+  ------------ ------------ ------------ --
+
+\
+\
+
+-   Type II more conservative for A
+
+
+
+## The coefficients
+
+```{r keeley_coe}
+knitr::kable(coef(summary(klm)), digits=2)
+``` 
+
+R<sup>2</sup> = `r summary(klm)$r.square`
+
+
+## Comparing Coefficients on the Same Scale
+
+$$r_{xy} = b_{xy}\frac{sd_{x}}{sd_{y}}$$ 
+
+```{r keeley_std, size=""normalsize""}
+library(QuantPsyc) 
+lm.beta(klm)
+```
+
+
+## Visualization of 4-D Multivariate Models is Difficult!
+
+```{r klm_see_effects}
+
+qplot(cover, rich, data=keeley, colour=firesev, size=firesev^2) +
+  theme_bw(base_size=14) + 
+  scale_color_gradient(low=""yellow"", high=""purple"") +
+  scale_size_continuous(range=c(1,10))
+```
+
+
+
+## Component-Residual Plots Aid in Visualization
+
+```{r klm_crplot, size=""normalsize""}
+crPlots(klm, smoother=F)
+```
+
+Takes effect of predictor + residual of response
+
+## Added Variable Plot to Show Unique Contributions when Holding Others Constant
+```{r klm_avplot}
+avPlots(klm)
+```
+
+## Or Show Regression at Median of Other Variables
+
+```{r klm_visreg, fig.height=6}
+par(mfrow=c(2,2))
+visreg::visreg(klm, cex.lab=1.3)
+par(mfrow=c(1,1))
+``` 
+
+## Or Show Predictions Overlaid on Data
+
+```{r crazy}
+pred_info <- crossing(cover = seq(0,1.5, length.out=100),
+                      firesev=c(2,5,8)) %>%
+  crossing(hetero=c(0.5, 0.8)) %>%
+  modelr::add_predictions(klm, var=""rich"") %>%
+  dplyr::mutate(hetero_split = hetero)
+
+ggplot(pred_info, mapping=aes(x=cover, y=rich)) +
+  geom_line(lwd=1.5, mapping=aes(color=factor(firesev), group=paste(firesev, hetero))) +
+    facet_wrap(~hetero_split) +
+  geom_point(data=keeley %>% 
+               dplyr::mutate(hetero_split = ifelse(hetero<mean(hetero), 0.5, 0.8))) +
+  theme_bw(base_size=14)
+```
+
+
+## Models with Many Predictors
+
+0.   ANOVA
+
+1.   ANCOVA
+
+2.   Multiple Linear Regression
+
+3.   <font color=""red"">MLR with Interactions</font>
+
+
+
+## Problem: What if Continuous Predictors are Not Additive?
+
+```{r keeley_int_plot3d, fig.height=6, fig.width=7}
+source(""./3dplotting.R"")
+with(keeley, scatterPlot3d(age,elev,firesev, 
+                           col=""black"", xlab=""age"", ylab=""elev"", zlab=""firesev"",
+                           phi=20, theta=-25))
+
+```
+
+
+
+
+## Problem: What if Continuous Predictors are Not Additive?
+
+```{r keeley_int_plot}
+keeley$egroup <- keeley$elev<600
+k_plot <- qplot(age, firesev, data=keeley, color=elev, size=elev)  + theme_bw() +
+  scale_color_continuous(low=""blue"", high=""red"")
+k_plot 
+```
+
+
+## Problem: What if Continuous Predictors are Not Additive?
+
+```{r keeley_int_plot2}
+k_plot + stat_smooth(method=""lm"", aes(group=egroup))
+```
+
+##
+\
+\
+![](./images/23/regression_depression.jpg)
+
+## Model For Age Interacting with Elevation to Influence Fire Severity
+$$y = \beta_0 + \beta_{1}x_{1} + \beta_{2}x_{2}+ \beta_{3}x_{1}x_{2}$$
+\
+\
+```{r keeley_mod_int, echo=TRUE}
+keeley_lm_int <- lm(firesev ~ age*elev, data=keeley)
+```
+
+## Testing Assumptions
+> - Data Generating Process: Linearity \
+\
+> - Error Generating Process: Normality & homoscedasticity of residuals  
+\
+> - Data: Outliers not influencing residuals  \
+\
+> - Predictors: Minimal collinearity
+
+## Other Diagnostics as Usual!
+
+```{r klm_diag_int}
+par(mfrow=c(2,2))
+plot(keeley_lm_int, which=c(1,2,5))
+par(mfrow=c(1,1))
+```
+
+
+
+## Examine Residuals With Respect to Each Predictor
+
+```{r klm_diag2_int}
+residualPlots(keeley_lm_int, test=FALSE)
+```
+
+
+## Interactions, VIF, and Centering
+```{r int_vif}
+vif(keeley_lm_int)
+```
+\
+<div style=""text-align:left"">
+This isn't that bad. But it can be. \
+\
+Often, interactions or nonlinear derived predictors are collinear with one or more of their predictors. \
+\
+To remove, this, we **center** predictors - i.e., $X_i - mean(X)$
+</div>
+
+## Interpretation of Centered Coefficients
+$$\huge X_i - \bar{X}$$
+
+
+> - Additive coefficients are the effect of a predictor at the mean value of the other predictors \
+\
+> -   Intercepts are at the mean value of all predictors \
+\
+> -   Visualization will keep you from getting confused! \
+
+## Interactions, VIF, and Centering
+$$y = \beta_0 + \beta_{1}(x_{1}-\bar{x_{1}}) + \beta_{2}(x_{2}-\bar{x_{2}})+ \beta_{3}(x_{1}-\bar{x_{1}})(x_{2}-\bar{x_{2}})$$
+
+\
+\
+<p align=""left"">Variance Inflation Factors for Centered Model:</p>
+```{r keeley_vif_cent}
+keeley <- keeley %>%
+  mutate(age_c = age-mean(age),
+         elev_c = elev - mean(elev))
+
+keeley_lm_int_cent <- lm(firesev ~ age_c*elev_c, data=keeley)
+
+vif(keeley_lm_int_cent)
+```
+
+## F-Tests to Evaluate Model
+What type of Sums of Squares??
+
+<div class=""fragment"">
+```{r int_anova}
+knitr::kable(Anova(keeley_lm_int))
+```
+
+
+## Coefficients!
+```{r int_coef}
+knitr::kable(coef(summary(keeley_lm_int)))
+```
+
+\
+\
+R<sup>2</sup> = `r summary(keeley_lm_int)$r.square`\
+\
+Note that additive coefficients signify the effect of one predictor in the abscence of all others.
+
+## Centered Coefficients!
+```{r int_coef_cent}
+knitr::kable(coef(summary(keeley_lm_int_cent)))
+```
+
+\
+\
+R<sup>2</sup> = `r summary(keeley_lm_int_cent)$r.square`\
+\
+Note that additive coefficients signify the effect of one predictor at the average level of all others.
+
+## Interpretation
+> - What the heck does an interaction effect mean?\
+\
+> - We can look at the effect of one variable at different levels of the other\
+\
+> - We can look at a surface \
+\
+> - We can construct *counterfactual* plots showing how changing both variables influences our outcome
+
+## Age at Different Levels of Elevation
+```{r int_visreg}
+visreg(keeley_lm_int, ""age"", by=""elev"")
+```
+
+## Elevation at Different Levels of Age
+```{r int_visreg_2}
+visreg(keeley_lm_int, ""elev"", by=""age"")
+```
+
+## Surfaces and Other 3d Objects
+```{r surf_int, fig.height=8, fig.width=10}
+abcSurf(keeley_lm_int, phi=20, theta=-65, col=""lightblue"") -> p 
+
+with(keeley, scatterPlot3d(age,elev,firesev, add=T, background=p, col=""black"", alpha=0.4))
+```
+
+## Or all in one plot
+```{r keeley_int_pred}
+k_pred <- crossing(elev = 100:1200, age = quantile(keeley$age)) %>%
+  modelr::add_predictions(keeley_lm_int, var=""firesev"") %>%
+  mutate(age_levels = paste(""Age = "", age, sep=""""))
+
+ggplot() +
+  geom_point(keeley, mapping=aes(x=elev, y=firesev, color=age, size=age)) +
+  geom_line(data=k_pred, mapping=aes(x=elev, y=firesev, color=age, group=age)) +
+  scale_color_continuous(low=""blue"", high=""red"") +
+  theme_bw(base_size=17)
+```
+
+## Without Data and Including CIs
+```{r keeley_int_pred_nodata}
+
+k <- predict(keeley_lm_int, newdata=k_pred, se.fit=TRUE, interval=""confidence"")
+k_pred$lwr = k$fit[,2]
+k_pred$upr = k$fit[,3]
+
+ggplot() +
+  geom_ribbon(data=k_pred, mapping=aes(x=elev, y=firesev, group=age, ymin = lwr, ymax=upr),
+              alpha=0.1) +
+  geom_line(data=k_pred, mapping=aes(x=elev, y=firesev, color=age, group=age)) +
+  scale_color_continuous(low=""blue"", high=""red"") +
+  theme_bw(base_size=17)
+```
+
+## A Heatmap Approach
+```{r int_heatmap}
+k_pred_grid <- crossing(elev = 100:1200, age = 3:60) %>%
+  modelr::add_predictions(keeley_lm_int, var=""firesev"")
+
+ggplot(k_pred_grid, mapping=aes(x=age, y=elev, colour=firesev)) +
+  geom_tile() +
+#  scale_color_continuous(low=""lightblue"", high=""red"")
+  scale_color_gradient2(low = ""blue"", high=""red"", mid=""white"", midpoint=5)
+```
+
+## 
+\
+\
+![](images/23/matrix_regression.jpg)
+\
+<span class=""fragment"">We can do a *lot* with the general linear model!<br></span>
+<span class=""fragment"">You are only limited by the biological models you can imagine.</span>

---FILE: lectures/23_general_linear_model.html---
@@ -0,0 +1,909 @@
+<!DOCTYPE html>
+<html>
+<head>
+  <meta charset=""utf-8"">
+  <meta name=""generator"" content=""pandoc"">
+  <title></title>
+  <meta name=""apple-mobile-web-app-capable"" content=""yes"">
+  <meta name=""apple-mobile-web-app-status-bar-style"" content=""black-translucent"">
+  <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"">
+  <link rel=""stylesheet"" href=""libs/reveal.js-3.3.0/css/reveal.css""/>
+
+
+<style type=""text/css"">
+div.sourceCode { overflow-x: auto; }
+table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
+  margin: 0; padding: 0; vertical-align: baseline; border: none; }
+table.sourceCode { width: 100%; line-height: 100%; }
+td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
+td.sourceCode { padding-left: 5px; }
+code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
+code > span.dt { color: #902000; } /* DataType */
+code > span.dv { color: #40a070; } /* DecVal */
+code > span.bn { color: #40a070; } /* BaseN */
+code > span.fl { color: #40a070; } /* Float */
+code > span.ch { color: #4070a0; } /* Char */
+code > span.st { color: #4070a0; } /* String */
+code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
+code > span.ot { color: #007020; } /* Other */
+code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
+code > span.fu { color: #06287e; } /* Function */
+code > span.er { color: #ff0000; font-weight: bold; } /* Error */
+code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
+code > span.cn { color: #880000; } /* Constant */
+code > span.sc { color: #4070a0; } /* SpecialChar */
+code > span.vs { color: #4070a0; } /* VerbatimString */
+code > span.ss { color: #bb6688; } /* SpecialString */
+code > span.im { } /* Import */
+code > span.va { color: #19177c; } /* Variable */
+code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
+code > span.op { color: #666666; } /* Operator */
+code > span.bu { } /* BuiltIn */
+code > span.ex { } /* Extension */
+code > span.pp { color: #bc7a00; } /* Preprocessor */
+code > span.at { color: #7d9029; } /* Attribute */
+code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
+code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
+code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
+code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
+</style>
+
+<link rel=""stylesheet"" href=""libs/reveal.js-3.3.0/css/theme/white.css"" id=""theme"">
+
+
+  <!-- some tweaks to reveal css -->
+  <style type=""text/css"">
+    .reveal h1 { font-size: 2.0em; }
+    .reveal h2 { font-size: 1.5em;  }
+    .reveal h3 { font-size: 1.25em;	}
+    .reveal h4 { font-size: 1em;	}
+
+    .reveal .slides>section,
+    .reveal .slides>section>section {
+      padding: 0px 0px;
+    }
+
+
+
+    .reveal table {
+      border-width: 1px;
+      border-spacing: 2px;
+      border-style: dotted;
+      border-color: gray;
+      border-collapse: collapse;
+      font-size: 0.7em;
+    }
+
+    .reveal table th {
+      border-width: 1px;
+      padding-left: 10px;
+      padding-right: 25px;
+      font-weight: bold;
+      border-style: dotted;
+      border-color: gray;
+    }
+
+    .reveal table td {
+      border-width: 1px;
+      padding-left: 10px;
+      padding-right: 25px;
+      border-style: dotted;
+      border-color: gray;
+    }
+
+  </style>
+
+    <style type=""text/css"">code{white-space: pre;}</style>
+
+    <link rel=""stylesheet"" href=""style.css""/>
+    <!-- Printing and PDF exports -->
+    <script>
+      var link = document.createElement( 'link' );
+      link.rel = 'stylesheet';
+      link.type = 'text/css';
+      link.href = window.location.search.match( /print-pdf/gi ) ? 'libs/reveal.js-3.3.0/css/print/pdf.css' : 'libs/reveal.js-3.3.0/css/print/paper.css';
+      document.getElementsByTagName( 'head' )[0].appendChild( link );
+    </script>
+    <!--[if lt IE 9]>
+    <script src=""libs/reveal.js-3.3.0/lib/js/html5shiv.js""></script>
+    <![endif]-->
+
+</head>
+<body>
+  <div class=""reveal"">
+    <div class=""slides"">
+
+
+<section id=""section"" class=""slide level2"">
+<h1></h1>
+<img src=""images/23/wonka_mult_regression.jpg"" />
+<h2>
+Multiple Predictor Variables in General Linear Models
+</h2>
+</section>
+<section id=""section-1"" class=""slide level2"">
+<h1></h1>
+ <br />
+<br />
+
+<h3>
+<a href=""https://etherpad.wikimedia.org/p/607-mlr"" class=""uri"">https://etherpad.wikimedia.org/p/607-mlr</a>
+</h3>
+</section>
+<section id=""data-generating-processes-until-now"" class=""slide level2"">
+<h1>Data Generating Processes Until Now</h1>
+<ul>
+<li>One predictor with one response<br />
+<br />
+</li>
+<li>Except…ANOVA had multiple possible treatment levels</li>
+</ul>
+</section>
+<section id=""the-general-linear-model"" class=""slide level2"">
+<h1>The General Linear Model</h1>
+<p><span class=""math display"">\[\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} \]</span></p>
+<ul>
+<li><p>This equation is huge. X can be anything - categorical, continuous, squared, sine, etc.</p></li>
+<li><p>There can be straight additivity, or interactions</p></li>
+<li><p>So far, the only model we’ve used with &gt;1 predictor is ANOVA</p></li>
+</ul>
+</section>
+<section id=""models-with-many-predictors"" class=""slide level2"">
+<h1>Models with Many Predictors</h1>
+<ol start=""0"" type=""1"">
+<li><p>ANOVA</p></li>
+<li><p>ANCOVA</p></li>
+<li><p>Multiple Linear Regression</p></li>
+<li><p>MLR with Interactions</p></li>
+</ol>
+</section>
+<section id=""analysis-of-covariance"" class=""slide level2"">
+<h1>Analysis of Covariance</h1>
+<p><span class=""math display"">\[\Large \boldsymbol{Y} = \boldsymbol{\beta X} + \boldsymbol{\epsilon} \]</span><br />
+<br />
+<span class=""math display"">\[Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \epsilon\]</span><br />
+<br />
+- ANOVA + a continuous predictor</p>
+<ul>
+<li><p>Often used to correct for a gradient or some continuous variable affecting outcome</p></li>
+<li>OR used to correct a regression due to additional groups that may throw off slope estimates
+<ul>
+<li>e.g. Simpson’s Paradox: A positive relationship between test scores and academic performance can be masked by gender differences</li>
+</ul></li>
+</ul>
+</section>
+<section id=""neanderthals-and-ancova"" class=""slide level2"">
+<h1>Neanderthals and ANCOVA</h1>
+<p><img src=""images/23/neanlooking.jpeg"" alt=""image"" style=""width:60.0%"" /></p>
+<p>Who had a bigger brain: Neanderthals or us?</p>
+</section>
+<section id=""the-means-look-the-same"" class=""slide level2"">
+<h1>The Means Look the Same…</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/neand_boxplot-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""but-there-appears-to-be-a-relationship-between-body-and-brain-mass"" class=""slide level2"">
+<h1>But there appears to be a Relationship Between Body and Brain Mass</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/neand_plot-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""and-mean-body-mass-is-different"" class=""slide level2"">
+<h1>And Mean Body Mass is Different</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/neand_boxplot2-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""section-2"" class=""slide level2"">
+<h1></h1>
+<p><br />
+<br />
+<img src=""images/23/redo_analysis.jpg"" /></p>
+</section>
+<section id=""analysis-of-covariance-control-for-a-covariate"" class=""slide level2"">
+<h1>Analysis of Covariance (control for a covariate)</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/neand_plot_fit-1.jpeg"" width=""672"" /></p>
+<p>ANCOVA: Evaluate a categorical effect(s), controlling for a <em>covariate</em> (parallel lines)<br />
+Groups modify the <em>intercept</em>.</p>
+</section>
+<section id=""the-steps-of-statistical-modeling"" class=""slide level2"">
+<h1>The Steps of Statistical Modeling</h1>
+<ol type=""1"">
+<li>What is your question?</li>
+<li>What model of the world matches your question?</li>
+<li>Build a test</li>
+<li>Evaluate test assumptions</li>
+<li>Evaluate test results</li>
+<li>Visualize</li>
+</ol>
+</section>
+<section id=""assumptions-of-multiway-anova"" class=""slide level2"">
+<h1>Assumptions of Multiway Anova</h1>
+<ul>
+<li><p>Independence of data points</p></li>
+<li><p>Normality and homoscedacticity within groups (of residuals)</p></li>
+<li><p>No relationship between fitted and residual values</p></li>
+<li><p>Additivity of Treatment and Covariate (Parallel Slopes)</p></li>
+</ul>
+</section>
+<section id=""the-usual-suspects-of-assumptions"" class=""slide level2"">
+<h1>The Usual Suspects of Assumptions</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/zoop_assumptions-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""group-residuals"" class=""slide level2"">
+<h1>Group Residuals</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/zoop_group_assumptions-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""test-for-parallel-slopes"" class=""slide level2"">
+<h1>Test for Parallel Slopes</h1>
+We test a model where <span class=""math display"">\[Y = \beta_0 + \beta_{1}x  + \sum_{j}^{i=1}\beta_j + \sum_{k}^{i=1}\beta_{j}x + \epsilon\]</span>
+<div class=""fragment"">
+<pre><code>                 Sum Sq   Df     F value      Pr(&gt;F)</code></pre>
+<hr />
+<p>species 0.0275528 1 6.220266 0.0175024 lnmass 0.1300183 1 29.352684 0.0000045 species:lnmass 0.0048452 1 1.093849 0.3027897 Residuals 0.1550332 35 NA NA</p>
+</div>
+<br />
+
+<div class=""fragment"">
+If you have an interaction, welp, that’s a different model - slopes vary by group!
+</div>
+</section>
+<section id=""ye-olde-f-test"" class=""slide level2"">
+<h1>Ye Olde F-Test</h1>
+<table>
+<thead>
+<tr class=""header"">
+<th></th>
+<th style=""text-align: right;"">Sum Sq</th>
+<th style=""text-align: right;"">Df</th>
+<th style=""text-align: right;"">F value</th>
+<th style=""text-align: right;"">Pr(&gt;F)</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td>species</td>
+<td style=""text-align: right;"">0.0275528</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">6.204092</td>
+<td style=""text-align: right;"">0.0174947</td>
+</tr>
+<tr class=""even"">
+<td>lnmass</td>
+<td style=""text-align: right;"">0.1300183</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">29.276363</td>
+<td style=""text-align: right;"">0.0000043</td>
+</tr>
+<tr class=""odd"">
+<td>Residuals</td>
+<td style=""text-align: right;"">0.1598784</td>
+<td style=""text-align: right;"">36</td>
+<td style=""text-align: right;"">NA</td>
+<td style=""text-align: right;"">NA</td>
+</tr>
+</tbody>
+</table>
+<p><span class=""fragment"">What type of Sums of Squares?</span><br />
+<br />
+<span class=""fragment"">II!</span></p>
+</section>
+<section id=""vsualizing-result"" class=""slide level2"">
+<h1>Vsualizing Result</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/neand_plot_fit-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""parcelling-out-covariate"" class=""slide level2"">
+<h1>Parcelling Out Covariate</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/cr_ancova-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""parcelling-out-covariate-with-posthocs-least-square-menas"" class=""slide level2"">
+<h1>Parcelling Out Covariate with PostHocs &amp; Least Square Menas</h1>
+<pre><code> contrast             estimate     SE df t.ratio p.value
+ neanderthal - recent  -0.0703 0.0282 36   -2.49  0.0175</code></pre>
+<p>Evluated at mean of covariate</p>
+</section>
+<section id=""models-with-many-predictors-1"" class=""slide level2"">
+<h1>Models with Many Predictors</h1>
+<ol start=""0"" type=""1"">
+<li><p>ANOVA</p></li>
+<li><p>ANCOVA</p></li>
+<li><p><font color=""red"">Multiple Linear Regression</font></p></li>
+<li><p>MLR with Interactions</p></li>
+</ol>
+</section>
+<section id=""one-way-anova-graphically"" class=""slide level2"">
+<h1>One-Way ANOVA Graphically</h1>
+<p><img src=""images/23/anova.png"" alt=""image"" style=""width:60.0%"" /></p>
+</section>
+<section id=""multiple-linear-regression"" class=""slide level2"">
+<h1>Multiple Linear Regression?</h1>
+<p><img src=""images/23/regression1.png"" alt=""image"" style=""width:60.0%"" /></p>
+<div style=""text-align:left"">
+Note no connection between predictors, as in ANOVA. This is ONLY true ifwe have manipulated variables so that there is no relationship between the two. This is not often the case!
+</div>
+</section>
+<section id=""multiple-linear-regression-1"" class=""slide level2"">
+<h1>Multiple Linear Regression</h1>
+<p><img src=""images/23/regression2.png"" alt=""image"" style=""width:60.0%"" /></p>
+<div style=""text-align:left"">
+<p>Curved double-headed arrow indicates COVARIANCE between predictors that we must account for.<br />
+<br />
+MLR controls for the correlation - estimates unique contribution of each predictor.</p>
+</div>
+</section>
+<section id=""calculating-multiple-regression-coefficients-with-ols"" class=""slide level2"">
+<h1>Calculating Multiple Regression Coefficients with OLS</h1>
+<p><span class=""math display"">\[\boldsymbol{Y} = \boldsymbol{b X} + \boldsymbol{\epsilon}\]</span></p>
+<div style=""text-align:left"">
+Remember in Simple Linear Regression <span class=""math inline"">\(b = \frac{cov_{xy}}{var_{x}}\)</span>?<br />
+<br />
+In Multiple Linear Regression <span class=""math inline"">\(\boldsymbol{b} = \boldsymbol{cov_{xy}}\boldsymbol{S_{x}^{-1}}\)</span><br />
+<br />
+where <span class=""math inline"">\(\boldsymbol{cov_{xy}}\)</span> is the covariances of <span class=""math inline"">\(\boldsymbol{x_i}\)</span> with <span class=""math inline"">\(\boldsymbol{y}\)</span> and <span class=""math inline"">\(\boldsymbol{S_{x}^{-1}}\)</span> is the variance/covariance matrix of all <em>Independent variables</em><br />
+
+</div>
+</section>
+<section id=""section-3"" class=""slide level2"" data-background=""images/23/fires.jpg"">
+<h1></h1>
+<div style=""bottom:100%; text-align:left; background:goldenrod"">
+Five year study of wildfires &amp; recovery in Southern California shur- blands in 1993. 90 plots (20 x 50m) (data from Jon Keeley et al.)
+</div>
+</section>
+<section id=""what-causes-species-richness"" class=""slide level2"">
+<h1>What causes species richness?</h1>
+<ul>
+<li>Distance from fire patch,</li>
+<li>Elevation</li>
+<li>Abiotic index</li>
+<li>Patch age</li>
+<li>Patch heterogeneity</li>
+<li>Severity of last fire</li>
+<li>Plant cover</li>
+</ul>
+</section>
+<section id=""many-things-may-influence-species-richness"" class=""slide level2"">
+<h1>Many Things may Influence Species Richness</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/keeley_pairs-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""our-model"" class=""slide level2"">
+<h1>Our Model</h1>
+<br />
+<br />
+<span class=""math display"">\[Richness =\beta_{0} ̃+ \beta_{1} cover +\beta_{2} firesev + \beta_{3}hetero +\epsilon\]</span><br />
+<br />
+
+<div class=""fragment"">
+<div class=""sourceCode""><pre class=""sourceCode r""><code class=""sourceCode r"">klm &lt;-<span class=""st""> </span><span class=""kw"">lm</span>(rich ~<span class=""st""> </span>cover +<span class=""st""> </span>firesev +<span class=""st""> </span>hetero, <span class=""dt"">data=</span>keeley)</code></pre></div>
+</div>
+</section>
+<section id=""testing-assumptions"" class=""slide level2"">
+<h1>Testing Assumptions</h1>
+<ul>
+<li class=""fragment"">Data Generating Process: Linearity<br />
+<br />
+</li>
+<li class=""fragment"">Error Generating Process: Normality &amp; homoscedasticity of residuals<br />
+<br />
+</li>
+<li class=""fragment"">Data: Outliers not influencing residuals<br />
+<br />
+</li>
+<li class=""fragment"">Predictors: Minimal multicollinearity</li>
+</ul>
+</section>
+<section id=""section-4"" class=""slide level2"">
+<h1></h1>
+<p><img src=""images/23/gosling_multicollinearity.jpg"" /></p>
+</section>
+<section id=""checking-for-multicollinearity-correlation-matrices"" class=""slide level2"">
+<h1>Checking for Multicollinearity: Correlation Matrices</h1>
+<pre><code>           cover   firesev    hetero
+cover    1.00000 -0.437135 -0.168378
+firesev -0.43713  1.000000 -0.052355
+hetero  -0.16838 -0.052355  1.000000</code></pre>
+<div style=""text-align:left"">
+Correlations over 0.4 can be problematic, but, they may be OK even as high as 0.8.<br />
+<br />
+Beyond this, are you getting unique information from each variable?
+</div>
+</section>
+<section id=""checking-for-multicollinearity-variance-inflation-factor"" class=""slide level2"">
+<h1>Checking for Multicollinearity: Variance Inflation Factor</h1>
+<p><span class=""math display"">\[VIF = \frac{1}{1-R^2_{j}}\]</span> `</p>
+<pre><code>  cover firesev  hetero 
+ 1.2949  1.2617  1.0504 </code></pre>
+<div style=""text-align:left"">
+<p>VIF <span class=""math inline"">\(&gt;\)</span> 5 or 10 can be problematic and indicate an unstable solution.<br />
+<br />
+Solution: evaluate correlation and drop a predictor</p>
+</div>
+</section>
+<section id=""other-diagnostics-as-usual"" class=""slide level2"">
+<h1>Other Diagnostics as Usual!</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_diag-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""examine-residuals-with-respect-to-each-predictor"" class=""slide level2"">
+<h1>Examine Residuals With Respect to Each Predictor</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_diag2-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""which-variables-explained-variation-type-ii-marginal-ss"" class=""slide level2"">
+<h1>Which Variables Explained Variation: Type II Marginal SS</h1>
+<table>
+<thead>
+<tr class=""header"">
+<th></th>
+<th style=""text-align: right;"">Sum Sq</th>
+<th style=""text-align: right;"">Df</th>
+<th style=""text-align: right;"">F value</th>
+<th style=""text-align: right;"">Pr(&gt;F)</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td>cover</td>
+<td style=""text-align: right;"">1674.18</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">12.01</td>
+<td style=""text-align: right;"">0.00</td>
+</tr>
+<tr class=""even"">
+<td>firesev</td>
+<td style=""text-align: right;"">635.65</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">4.56</td>
+<td style=""text-align: right;"">0.04</td>
+</tr>
+<tr class=""odd"">
+<td>hetero</td>
+<td style=""text-align: right;"">4864.52</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">34.91</td>
+<td style=""text-align: right;"">0.00</td>
+</tr>
+<tr class=""even"">
+<td>Residuals</td>
+<td style=""text-align: right;"">11984.57</td>
+<td style=""text-align: right;"">86</td>
+<td style=""text-align: right;"">NA</td>
+<td style=""text-align: right;"">NA</td>
+</tr>
+</tbody>
+</table>
+<p>If order of entry matters, can use type I. Remember, what models are you comparing?</p>
+</section>
+<section id=""whats-going-on-type-i-and-ii-sums-of-squares"" class=""slide level2"">
+<h1>What’s Going On: Type I and II Sums of Squares</h1>
+<table>
+<tbody>
+<tr class=""odd"">
+<td></td>
+<td style=""text-align: center;"">Type I</td>
+<td style=""text-align: center;"">Type II</td>
+<td></td>
+</tr>
+<tr class=""even"">
+<td>Test for A</td>
+<td style=""text-align: center;"">A v. 1</td>
+<td style=""text-align: center;"">A + B v. B</td>
+<td></td>
+</tr>
+<tr class=""odd"">
+<td>Test for B</td>
+<td style=""text-align: center;"">A + B v. A</td>
+<td style=""text-align: center;"">A + B v. A</td>
+<td></td>
+</tr>
+</tbody>
+</table>
+<p><br />
+<br />
+- Type II more conservative for A</p>
+</section>
+<section id=""the-coefficients"" class=""slide level2"">
+<h1>The coefficients</h1>
+<table>
+<thead>
+<tr class=""header"">
+<th></th>
+<th style=""text-align: right;"">Estimate</th>
+<th style=""text-align: right;"">Std. Error</th>
+<th style=""text-align: right;"">t value</th>
+<th style=""text-align: right;"">Pr(&gt;|t|)</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td>(Intercept)</td>
+<td style=""text-align: right;"">1.68</td>
+<td style=""text-align: right;"">10.67</td>
+<td style=""text-align: right;"">0.16</td>
+<td style=""text-align: right;"">0.88</td>
+</tr>
+<tr class=""even"">
+<td>cover</td>
+<td style=""text-align: right;"">15.56</td>
+<td style=""text-align: right;"">4.49</td>
+<td style=""text-align: right;"">3.47</td>
+<td style=""text-align: right;"">0.00</td>
+</tr>
+<tr class=""odd"">
+<td>firesev</td>
+<td style=""text-align: right;"">-1.82</td>
+<td style=""text-align: right;"">0.85</td>
+<td style=""text-align: right;"">-2.14</td>
+<td style=""text-align: right;"">0.04</td>
+</tr>
+<tr class=""even"">
+<td>hetero</td>
+<td style=""text-align: right;"">65.99</td>
+<td style=""text-align: right;"">11.17</td>
+<td style=""text-align: right;"">5.91</td>
+<td style=""text-align: right;"">0.00</td>
+</tr>
+</tbody>
+</table>
+<p>R<sup>2</sup> = 0.40986</p>
+</section>
+<section id=""comparing-coefficients-on-the-same-scale"" class=""slide level2"">
+<h1>Comparing Coefficients on the Same Scale</h1>
+<p><span class=""math display"">\[r_{xy} = b_{xy}\frac{sd_{x}}{sd_{y}}\]</span></p>
+<pre><code>   cover  firesev   hetero 
+ 0.32673 -0.19872  0.50160 </code></pre>
+</section>
+<section id=""visualization-of-4-d-multivariate-models-is-difficult"" class=""slide level2"">
+<h1>Visualization of 4-D Multivariate Models is Difficult!</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_see_effects-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""component-residual-plots-aid-in-visualization"" class=""slide level2"">
+<h1>Component-Residual Plots Aid in Visualization</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_crplot-1.jpeg"" width=""768"" /></p>
+<p>Takes effect of predictor + residual of response</p>
+</section>
+<section id=""added-variable-plot-to-show-unique-contributions-when-holding-others-constant"" class=""slide level2"">
+<h1>Added Variable Plot to Show Unique Contributions when Holding Others Constant</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_avplot-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""or-show-regression-at-median-of-other-variables"" class=""slide level2"">
+<h1>Or Show Regression at Median of Other Variables</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_visreg-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""or-show-predictions-overlaid-on-data"" class=""slide level2"">
+<h1>Or Show Predictions Overlaid on Data</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/crazy-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""models-with-many-predictors-2"" class=""slide level2"">
+<h1>Models with Many Predictors</h1>
+<ol start=""0"" type=""1"">
+<li><p>ANOVA</p></li>
+<li><p>ANCOVA</p></li>
+<li><p>Multiple Linear Regression</p></li>
+<li><p><font color=""red"">MLR with Interactions</font></p></li>
+</ol>
+</section>
+<section id=""problem-what-if-continuous-predictors-are-not-additive"" class=""slide level2"">
+<h1>Problem: What if Continuous Predictors are Not Additive?</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/keeley_int_plot3d-1.jpeg"" width=""672"" /></p>
+</section>
+<section id=""problem-what-if-continuous-predictors-are-not-additive-1"" class=""slide level2"">
+<h1>Problem: What if Continuous Predictors are Not Additive?</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/keeley_int_plot-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""problem-what-if-continuous-predictors-are-not-additive-2"" class=""slide level2"">
+<h1>Problem: What if Continuous Predictors are Not Additive?</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/keeley_int_plot2-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""section-5"" class=""slide level2"">
+<h1></h1>
+<p><br />
+<br />
+<img src=""images/23/regression_depression.jpg"" /></p>
+</section>
+<section id=""model-for-age-interacting-with-elevation-to-influence-fire-severity"" class=""slide level2"">
+<h1>Model For Age Interacting with Elevation to Influence Fire Severity</h1>
+<p><span class=""math display"">\[y = \beta_0 + \beta_{1}x_{1} + \beta_{2}x_{2}+ \beta_{3}x_{1}x_{2}\]</span><br />
+<br />
+</p>
+<div class=""sourceCode""><pre class=""sourceCode r""><code class=""sourceCode r"">keeley_lm_int &lt;-<span class=""st""> </span><span class=""kw"">lm</span>(firesev ~<span class=""st""> </span>age*elev, <span class=""dt"">data=</span>keeley)</code></pre></div>
+</section>
+<section id=""testing-assumptions-1"" class=""slide level2"">
+<h1>Testing Assumptions</h1>
+<ul>
+<li class=""fragment"">Data Generating Process: Linearity<br />
+<br />
+</li>
+<li class=""fragment"">Error Generating Process: Normality &amp; homoscedasticity of residuals<br />
+<br />
+</li>
+<li class=""fragment"">Data: Outliers not influencing residuals<br />
+<br />
+</li>
+<li class=""fragment"">Predictors: Minimal collinearity</li>
+</ul>
+</section>
+<section id=""other-diagnostics-as-usual-1"" class=""slide level2"">
+<h1>Other Diagnostics as Usual!</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_diag_int-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""examine-residuals-with-respect-to-each-predictor-1"" class=""slide level2"">
+<h1>Examine Residuals With Respect to Each Predictor</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/klm_diag2_int-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""interactions-vif-and-centering"" class=""slide level2"">
+<h1>Interactions, VIF, and Centering</h1>
+<pre><code>     age     elev age:elev 
+  3.2001   5.5175   8.2871 </code></pre>
+<br />
+
+<div style=""text-align:left"">
+<p>This isn’t that bad. But it can be.<br />
+<br />
+Often, interactions or nonlinear derived predictors are collinear with one or more of their predictors.<br />
+<br />
+To remove, this, we <strong>center</strong> predictors - i.e., <span class=""math inline"">\(X_i - mean(X)\)</span></p>
+</div>
+</section>
+<section id=""interpretation-of-centered-coefficients"" class=""slide level2"">
+<h1>Interpretation of Centered Coefficients</h1>
+<p><span class=""math display"">\[\huge X_i - \bar{X}\]</span></p>
+<ul>
+<li class=""fragment"">Additive coefficients are the effect of a predictor at the mean value of the other predictors<br />
+<br />
+</li>
+<li class=""fragment"">Intercepts are at the mean value of all predictors<br />
+<br />
+</li>
+<li class=""fragment"">Visualization will keep you from getting confused!<br />
+</li>
+</ul>
+</section>
+<section id=""interactions-vif-and-centering-1"" class=""slide level2"">
+<h1>Interactions, VIF, and Centering</h1>
+<p><span class=""math display"">\[y = \beta_0 + \beta_{1}(x_{1}-\bar{x_{1}}) + \beta_{2}(x_{2}-\bar{x_{2}})+ \beta_{3}(x_{1}-\bar{x_{1}})(x_{2}-\bar{x_{2}})\]</span></p>
+<br />
+<br />
+
+<p align=""left"">
+Variance Inflation Factors for Centered Model:
+</p>
+<pre><code>       age_c       elev_c age_c:elev_c 
+      1.0167       1.0418       1.0379 </code></pre>
+</section>
+<section id=""f-tests-to-evaluate-model"" class=""slide level2"">
+<h1>F-Tests to Evaluate Model</h1>
+<p>What type of Sums of Squares??</p>
+<div class=""fragment"">
+
+<table>
+<thead>
+<tr class=""header"">
+<th></th>
+<th style=""text-align: right;"">Sum Sq</th>
+<th style=""text-align: right;"">Df</th>
+<th style=""text-align: right;"">F value</th>
+<th style=""text-align: right;"">Pr(&gt;F)</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td>age</td>
+<td style=""text-align: right;"">52.9632</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">27.7092</td>
+<td style=""text-align: right;"">0.00000</td>
+</tr>
+<tr class=""even"">
+<td>elev</td>
+<td style=""text-align: right;"">6.2531</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">3.2715</td>
+<td style=""text-align: right;"">0.07399</td>
+</tr>
+<tr class=""odd"">
+<td>age:elev</td>
+<td style=""text-align: right;"">22.3045</td>
+<td style=""text-align: right;"">1</td>
+<td style=""text-align: right;"">11.6693</td>
+<td style=""text-align: right;"">0.00097</td>
+</tr>
+<tr class=""even"">
+<td>Residuals</td>
+<td style=""text-align: right;"">164.3797</td>
+<td style=""text-align: right;"">86</td>
+<td style=""text-align: right;"">NA</td>
+<td style=""text-align: right;"">NA</td>
+</tr>
+</tbody>
+</table>
+</section>
+<section id=""coefficients"" class=""slide level2"">
+<h1>Coefficients!</h1>
+<table>
+<thead>
+<tr class=""header"">
+<th></th>
+<th style=""text-align: right;"">Estimate</th>
+<th style=""text-align: right;"">Std. Error</th>
+<th style=""text-align: right;"">t value</th>
+<th style=""text-align: right;"">Pr(&gt;|t|)</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td>(Intercept)</td>
+<td style=""text-align: right;"">1.81322</td>
+<td style=""text-align: right;"">0.61561</td>
+<td style=""text-align: right;"">2.9454</td>
+<td style=""text-align: right;"">0.00415</td>
+</tr>
+<tr class=""even"">
+<td>age</td>
+<td style=""text-align: right;"">0.12063</td>
+<td style=""text-align: right;"">0.02086</td>
+<td style=""text-align: right;"">5.7823</td>
+<td style=""text-align: right;"">0.00000</td>
+</tr>
+<tr class=""odd"">
+<td>elev</td>
+<td style=""text-align: right;"">0.00309</td>
+<td style=""text-align: right;"">0.00133</td>
+<td style=""text-align: right;"">2.3146</td>
+<td style=""text-align: right;"">0.02302</td>
+</tr>
+<tr class=""even"">
+<td>age:elev</td>
+<td style=""text-align: right;"">-0.00015</td>
+<td style=""text-align: right;"">0.00004</td>
+<td style=""text-align: right;"">-3.4160</td>
+<td style=""text-align: right;"">0.00097</td>
+</tr>
+</tbody>
+</table>
+<p><br />
+<br />
+R<sup>2</sup> = 0.32352<br />
+<br />
+Note that additive coefficients signify the effect of one predictor in the abscence of all others.</p>
+</section>
+<section id=""centered-coefficients"" class=""slide level2"">
+<h1>Centered Coefficients!</h1>
+<table>
+<thead>
+<tr class=""header"">
+<th></th>
+<th style=""text-align: right;"">Estimate</th>
+<th style=""text-align: right;"">Std. Error</th>
+<th style=""text-align: right;"">t value</th>
+<th style=""text-align: right;"">Pr(&gt;|t|)</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td>(Intercept)</td>
+<td style=""text-align: right;"">4.60913</td>
+<td style=""text-align: right;"">0.14630</td>
+<td style=""text-align: right;"">31.5040</td>
+<td style=""text-align: right;"">0.00000</td>
+</tr>
+<tr class=""even"">
+<td>age_c</td>
+<td style=""text-align: right;"">0.05811</td>
+<td style=""text-align: right;"">0.01176</td>
+<td style=""text-align: right;"">4.9419</td>
+<td style=""text-align: right;"">0.00000</td>
+</tr>
+<tr class=""odd"">
+<td>elev_c</td>
+<td style=""text-align: right;"">-0.00068</td>
+<td style=""text-align: right;"">0.00058</td>
+<td style=""text-align: right;"">-1.1716</td>
+<td style=""text-align: right;"">0.24460</td>
+</tr>
+<tr class=""even"">
+<td>age_c:elev_c</td>
+<td style=""text-align: right;"">-0.00015</td>
+<td style=""text-align: right;"">0.00004</td>
+<td style=""text-align: right;"">-3.4160</td>
+<td style=""text-align: right;"">0.00097</td>
+</tr>
+</tbody>
+</table>
+<p><br />
+<br />
+R<sup>2</sup> = 0.32352<br />
+<br />
+Note that additive coefficients signify the effect of one predictor at the average level of all others.</p>
+</section>
+<section id=""interpretation"" class=""slide level2"">
+<h1>Interpretation</h1>
+<ul>
+<li class=""fragment"">What the heck does an interaction effect mean?<br />
+<br />
+</li>
+<li class=""fragment"">We can look at the effect of one variable at different levels of the other<br />
+<br />
+</li>
+<li class=""fragment"">We can look at a surface<br />
+<br />
+</li>
+<li class=""fragment"">We can construct <em>counterfactual</em> plots showing how changing both variables influences our outcome</li>
+</ul>
+</section>
+<section id=""age-at-different-levels-of-elevation"" class=""slide level2"">
+<h1>Age at Different Levels of Elevation</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/int_visreg-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""elevation-at-different-levels-of-age"" class=""slide level2"">
+<h1>Elevation at Different Levels of Age</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/int_visreg_2-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""surfaces-and-other-3d-objects"" class=""slide level2"">
+<h1>Surfaces and Other 3d Objects</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/surf_int-1.jpeg"" width=""960"" /></p>
+</section>
+<section id=""or-all-in-one-plot"" class=""slide level2"">
+<h1>Or all in one plot</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/keeley_int_pred-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""without-data-and-including-cis"" class=""slide level2"">
+<h1>Without Data and Including CIs</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/keeley_int_pred_nodata-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""a-heatmap-approach"" class=""slide level2"">
+<h1>A Heatmap Approach</h1>
+<p><img src=""23_general_linear_model_files/figure-revealjs/int_heatmap-1.jpeg"" width=""768"" /></p>
+</section>
+<section id=""section-6"" class=""slide level2"">
+<h1></h1>
+<p><br />
+<br />
+<img src=""images/23/matrix_regression.jpg"" /><br />
+<span class=""fragment"">We can do a <em>lot</em> with the general linear model!<br></span> <span class=""fragment"">You are only limited by the biological models you can imagine.</span></p>
+</section>
+    </div>
+  </div>
+
+  <script src=""libs/reveal.js-3.3.0/lib/js/head.min.js""></script>
+  <script src=""libs/reveal.js-3.3.0/js/reveal.js""></script>
+
+  <script>
+
+      // Full list of configuration options available at:
+      // https://github.com/hakimel/reveal.js#configuration
+      Reveal.initialize({
+        // Display the page number of the current slide
+        slideNumber: true,
+        // Push each slide change to the browser history
+        history: true,
+        // Vertical centering of slides
+        center: false,
+        // Opens links in an iframe preview overlay
+        previewLinks: true,
+        // Transition style
+        transition: 'fade', // none/fade/slide/convex/concave/zoom
+        // Transition style for full page slide backgrounds
+        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom
+
+        // Optional reveal.js plugins
+        dependencies: [
+        ]
+      });
+    </script>
+  <!-- dynamically load mathjax for compatibility with self-contained -->
+  <script>
+    (function () {
+      var script = document.createElement(""script"");
+      script.type = ""text/javascript"";
+      script.src  = ""https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"";
+      document.getElementsByTagName(""head"")[0].appendChild(script);
+    })();
+  </script>
+
+<script>
+  (function() {
+    if (window.jQuery) {
+      Reveal.addEventListener( 'slidechanged', function(event) {  
+        window.jQuery(event.previousSlide).trigger('hidden');
+        window.jQuery(event.currentSlide).trigger('shown');
+      });
+    }
+  })();
+</script>
+
+
+  </body>
+</html>

---FILE: lectures/3dplotting.R---
@@ -0,0 +1,60 @@
+
+#just create a range of values
+grdVals <- function(x, bound=0.01) seq(range(x)[1]-bound, range(x)[2]+bound, length.out=50)
+
+#instantiate a new persp object for a scatterplot
+makeScatterBox <- function(x,y,z, bound=0, ...){
+  x <- grdVals(x, bound=bound)
+  y <- grdVals(y, bound=bound)
+  z <- grdVals(z, bound=bound)
+  z <- matrix(rep(z, 50), nrow=50)
+  p <- persp(x,y,z, col=NA, border=NA, ...)
+  
+  p
+  
+}
+
+#make a scatterplot of points
+#and either create a new persp object
+#or use one already existing
+scatterPlot3d <- function(x,y,z, pch=19, cex=1, col=""black"", bound=1, background=NA, ...){
+  if(!is.na(background[1])){
+    p <- background
+  }else{
+    p <- makeScatterBox(x,y,z, background=background, ...)
+  }
+  points(trans3d(x,y, z, p), pch=pch, col=col, cex=cex)
+  
+  
+}
+
+#Create a surface from an LM or GLM with no more than 2 predictors
+#need to add an a, b, c
+abcSurf <- function(obj, type=""response"", xlab=names(obj$model)[2], ylab=names(obj$model)[3], zlab=names(obj$model)[1], 
+                    ticktype=""detailed"", ...){
+  x <- grdVals(obj$model[2])
+  y <- grdVals(obj$model[3])
+
+  ndf <- expand.grid(x,y)
+  names(ndf) <- names(obj$model)[-1]
+  
+  z <- predict(obj, ndf)
+  z <- matrix(z, nrow=length(x))
+  
+  persp(x,y,z, xlab=xlab, ylab=ylab, zlab=zlab,  ticktype=ticktype, ...)
+}
+
+# 
+# ###### Demo Code
+# x <- 1:100
+# y <- runif(100, 0,100)
+# z <- rnorm(100, x*3+y+x*y)
+# 
+# scatterPlot3d(x,y,z)
+# 
+# myFit <- lm(z ~ x*y)
+# 
+# abcSurf(myFit) -> p
+# scatterPlot3d(x,y,z, add=T, background=p)
+
+

---FILE: lectures/data/23/18q09NeanderthalBrainSize.csv---
@@ -0,0 +1,40 @@
+lnmass,lnbrain,species
+4,7.19,recent
+3.96,7.23,recent
+3.95,7.26,recent
+4.04,7.23,recent
+4.11,7.22,recent
+4.1,7.24,recent
+4.05,7.27,recent
+4.03,7.26,recent
+4.03,7.27,recent
+4.05,7.35,recent
+4.09,7.35,recent
+4.12,7.35,recent
+4.14,7.34,recent
+4.17,7.33,recent
+4.19,7.31,recent
+4.21,7.27,recent
+4.26,7.28,recent
+4.28,7.32,recent
+4.37,7.33,recent
+4.27,7.33,recent
+4.21,7.33,recent
+4.17,7.36,recent
+4.21,7.38,recent
+4.25,7.35,recent
+4.26,7.37,recent
+4.26,7.39,recent
+4.23,7.42,recent
+4.17,7.44,recent
+4.39,7.54,recent
+4.43,7.48,recent
+4.15,7.15,neanderthal
+4.21,7.17,neanderthal
+4.27,7.21,neanderthal
+4.23,7.35,neanderthal
+4.25,7.47,neanderthal
+4.35,7.4,neanderthal
+4.39,7.38,neanderthal
+4.43,7.35,neanderthal
+4.44,7.43,neanderthal

---FILE: lectures/data/23/Keeley_rawdata_select4.csv---
@@ -0,0 +1 @@
+distance,elev,abiotic,age,hetero,firesev,cover,rich53.409,1225,60.67102574,40,0.757065,3.5,1.03879744,5137.03745,60,40.9429135,25,0.49134,4.05,0.47759241,3153.69565,200,50.98805383,15,0.844485,2.6,0.94893572,7153.69565,200,61.15633116,15,0.690847,2.9,1.19490019,6451.95985,970,46.66807061,23,0.545628,4.3,1.29818904,6851.95985,970,39.82356516,24,0.652895,4,1.17348657,3451.95985,950,56.42359736,35,0.742142,4.8,0.86158595,3951.4478,740,46.97990839,14,0.82198,4.8,0.41906623,6637.03745,170,42.08805646,45,0.602799,7.25,0.12851131,2537.8594,190,32.59386543,35,0.805082,6.2,0.3062645,3139.4598,210,42.36410299,45,0.450604,8.05,0.68247702,2839.4598,185,45.61543356,35,0.72762,7.55,0.5310083,4639.4598,185,44.67795209,35,0.745032,7.25,0.2957547,3639.4598,210,49.18702112,30,0.621105,7.3,0.98468564,3859.07485,400,45.80336337,15,0.712214,3.8,0.69869688,4059.07485,400,39.39693407,40,0.651254,5.7,1.16663393,5256.84925,200,64.01331567,21,0.726413,4.3,0.84068273,7340.23825,90,51.05547687,39,0.384182,4.2,1.11949189,4940.23825,95,50.64962673,39,0.683119,4.25,0.41204136,4658.60545,640,58.05778638,29,0.577675,5.1,0.84594715,3658.8474,700,58.03854987,12,0.465738,1.95,1.17590928,3837.8594,110,41.87261476,19,0.623248,4.4,0.66100507,3737.8594,120,43.70487257,20,0.716368,4.45,0.6517726,3637.8594,160,61.84835278,21,0.778771,3,0.41172415,4037.03745,75,45.55794644,3,0.631893,1.2,0.53282988,3639.4598,100,48.86042408,17,0.577784,6.4,0.49739296,3137.03745,210,45.36713827,40,0.630827,8.2,0.20203482,2539.4598,235,44.16061746,40,0.443006,6.85,0.05557657,1537.8594,160,39.75716486,57,0.60485,6.15,0.12124606,2437.03745,180,43.13406326,52,0.720007,7.7,0.08297883,2953.86305,750,47.85938774,35,0.670668,6.1,0.54920017,6153.86305,880,47.31453699,40,0.821218,4.3,0.54646221,5037.8594,90,46.38316209,5,0.632909,3.75,1.27310233,5837.8594,80,48.63162457,5,0.501772,3.7,0.84805791,4452.76225,490,50.52500555,28,0.877938,5.7,0.37941846,5052.76225,470,44.21998229,33,0.672676,5.4,0.35841475,2242.4866,475,45.02599153,31,0.768474,4.85,0.63674533,4542.4866,425,43.52531357,48,0.810168,4.85,0.290822,4542.4866,450,50.83791467,55,0.685509,4.3,0.50717877,3837.03745,120,38.13316317,22,0.681703,3.25,0.63749067,2737.03745,120,40.73619569,21,0.832805,2.5,1.06473979,5251.2534,410,37.06725706,13,0.44362,3.85,0.76755405,3751.77085,500,40.28145203,13,0.655193,2.55,1.32067202,6149.61405,215,43.06370771,25,0.760627,3.3,1.07900425,8550.175,390,46.13870708,15,0.524506,4,0.9377264,5150.175,390,44.8427679,12,0.662972,3.4,0.69915774,5549.61405,240,49.88035458,28,0.717096,3.9,0.30582847,5049.61405,215,40.87125833,16,0.837892,4,0.55437438,7139.4598,600,41.39449819,25,0.458076,7.5,0.83649961,2553.2362,275,45.05330846,28,0.665893,4.9,0.8507124,6953.2362,400,55.1638656,38,0.618283,4.7,0.74700881,3660.3042,665,64.6648058,10,0.799485,1.7,0.51896384,8160.5018,650,64.16075168,26,0.65456,3,1.12002246,6360.5018,645,56.79128184,23,0.818051,6,0.56923477,6460.5018,660,48.47836418,16,0.773888,6.7,0.79578273,6860.5018,650,65.49188037,19,0.732976,2,0.9167156,6460.41025,675,48.2127047,15,0.866607,6.05,0.57708428,5260.41025,670,52.28453281,27,0.695984,3.9,0.67706467,6860.41025,680,53.71627773,30,0.670694,2.9,0.61410302,5760.6414,600,52.98931288,30,0.737869,3.7,0.62828578,6253.69565,275,44.13341085,3,0.748379,1.2,1.1074659,5053.69565,300,37.69876959,6,0.501253,1.3,0.84054394,4060.6414,590,57.79974128,25,0.839612,3.6,0.90855822,7460.68945,550,58.06811815,28,0.784895,4.4,0.73840393,5960.68945,550,57.720386,27,0.731926,3.7,0.27628054,6660.68945,480,59.86913918,19,0.81423,4.85,0.77771328,6360.68945,480,56.65693466,25,0.802434,4.85,0.59338111,6760.723,475,57.95677978,11,0.848965,5.6,0.58821905,6860.723,475,51.36570665,11,0.784076,4.1,0.51846397,6157.7926,450,50.5556375,15,0.768052,5.7,0.34806766,3637.8594,450,57.24555024,36,0.652508,4.2,1.09761107,6537.03745,350,43.4833858,24,0.731988,4.8,0.54498063,3437.03745,365,46.32455146,24,0.665088,7.2,0.67605775,3043.913,260,51.03277982,26,0.738192,9.2,0.28858364,5243.913,250,48.48387036,26,0.504874,6.2,0.68021606,4243.20705,210,54.10154175,6,0.645109,2.1,1.12022933,4942.4866,170,51.49772343,31,0.763785,3.85,0.44701614,6539.4598,130,45.30344428,20,0.657726,3.9,0.48445559,3359.83965,475,46.64319769,15,0.62856,4.85,1.535408,4859.83965,475,58.05232263,15,0.819453,5.3,0.92860808,5859.83965,980,70.45628615,15,0.853594,4.6,0.46919542,6751.77085,350,62.83870344,16,0.653353,3.3,0.45107384,6251.77085,335,44.15408265,20,0.59628,5.4,0.51761156,5851.77085,300,48.98806208,33,0.47335,4.3,0.5572632,5153.2362,650,39.85517866,13,0.667834,3.6,0.5144298,4453.2362,750,42.7387406,20,0.701633,3,1.16590727,4241.0022,730,55.9113579,48,0.674635,3.8,1.08351251,5541.0022,700,53.14061917,35,0.544894,4.5,0.56744869,5553.86305,870,39.94216851,60,0.615237,3.9,0.5808883,4753.86305,575,44.29654736,36,0.810698,4.6,0.37861547,44
\ No newline at end of file

---FILE: schedule.Rmd---
@@ -34,7 +34,7 @@ __Homework:__ https://github.com/biol607/2016_homework_03_ggplot2
 ### Week 4.   
 __Lecture:__ [Frequentist Hypothesis Testing](lectures/07_probability_hypotheses.html), [NHST, Z-Tests, and Power](lectures/08_testing_nhst_power.html) \
 __Lab Topic:__ [Distributions in R, Frequentist Hypothesis testing via simulation](lab/05_hypothesis_power.html)  \
-__Reading:__ W&S 5-7, G&W Chapter 7, 16  
+__Reading:__ W&S 5-7, G&W Chapter 7, 16, [Abraham Lincoln and Confidence Intervals](http://andrewgelman.com/2016/11/23/abraham-lincoln-confidence-intervals/) and links therein    
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-hypotheses  
 __In Class Code:__ [Distributions and Power](in_class_code_2016/05_distributions_power.R)  
 __Quiz:__ http://tinyurl.com/hyp-pre-quiz  
@@ -97,11 +97,11 @@ __Reading:__ W&S 18, [Hurlbert 1984](http://byrneslab.net/classes/biol607/readin
   
   
 ### Week 12.   
-__Lecture:__ Multiple Regression and Interaction Effects, Information Theoretic Approaches  
+__Lecture:__ [The General Linear Model: ANCOVA, Multiple Regression, and Interaction Effects](./lectures/23_general_linear_model.html), Information Theoretic Approaches  
 __Lab Topic:__ Multiple Regression, Multimodel Inference  
 __Readings:__ [Symonds and Moussalli 2010](http://byrneslab.net/classes/biol607/readings/Symonds_and_Moussalli_2010_behav_ecol.pdf)  
 __Optional Readings:__ The whole [Ecology Special Section on P Values](http://byrneslab.net/classes/biol607/readings/ecology_forum_on_p_values.pdf) is incredible reading.  
-  
+__Etherpad:__ https://etherpad.wikimedia.org/p/607-mlr  
   
 ### Week 13.   
 __Lecture:__ Entering a non-normal world - Modeling count data with Genearlized linear models. Overdispersed continuous data.  
@@ -111,7 +111,7 @@ __Reading:__ [O'Hara 2009](http://byrneslab.net/classes/biol607/readings/O'Hara_
   
 ### Week 14.   
 __Lecture:__ Class's Choice  
-__Lab Topic:__ Class's Choice, Final Presentation Open __Lab  
+__Lab Topic:__ Class's Choice, Final Presentation Open Lab  
   
   
 ### Week 15.  "
biol607,biol607.github.io,6e3bc9262669687a995f8ac6e9045ce5e2669aa1,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-23T16:17:28Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-23T16:17:28Z,Typo fix,lab/10_anova.Rmd;lab/10_anova.html,True,False,True,False,20,20,40,"---FILE: lab/10_anova.Rmd---
@@ -398,12 +398,12 @@ intertidal <- read.csv(""./data/10/18e3IntertidalAlgae.csv"")
 ________(herbivores, sqrtarea, data=____, geom=""____"")
 
 #fit
-mouse_lm <- __(______ ~ ______ + _____, data=________)
+intertidal_lm <- __(______ ~ ______ + _____, data=________)
 
 #assumptions
 ________(______, which=c(1,2,4,5))
 
-residualPlots(bee_lm)
+residualPlots(intertidal_lm)
 
 #ANOVA
 ________(______)
@@ -415,14 +415,14 @@ ________(________(______, spec = ""______""), method = ""________"")
 Did that last one pass the test of non-additivity?
 
 
-## 2. Factorial ANOVA
+## 3. Factorial ANOVA
 Going with that last mouse example, if you really looked, it was a factorial design, with multiple treatments and conditions.
 
 ```{r plot_mice}
 qplot(herbivores, sqrtarea, data=intertidal, fill=height, geom=""boxplot"")
 ```
 
-##### 2.1 Fit and Assumption Evaluation
+##### 3.1 Fit and Assumption Evaluation
 We fit factorial models using one of two different notations - both expand to the same thing
 
 ```{r int_fact}
@@ -435,7 +435,7 @@ Both mean the same thing as `:` is the interaction. `*` just means, expand all t
 
 But, after that's done...all of the assumption tests are the same. Try them out.
 
-##### 2.2 Type II and III Sums of Squares
+##### 3.2 Type II and III Sums of Squares
 Now, we can choose type II or III SS once we have >n=1 for simple effects. Let's see the difference. Both are from `Anova()` from the car package.
 
 ```{r Anova_compare}
@@ -444,7 +444,7 @@ Anova(intertidal_lm)
 Anova(intertidal_lm, method=""III"")
 ```
 
-##### 2.3 Post-Hocs
+##### 3.3 Post-Hocs
 Post-hocs are a bit funnier. But not by much. As we have an interaction, let's look at the simple effects:
 
 ```{r tukey_simple}
@@ -455,7 +455,7 @@ Well, that's it, actually. You could begin to look at one of the main effects or
 
 And, yeah, it works the same in BANOVA as all the way back in 1-way ANOVA. Only you have to hand-code it.
 
-##### 2.3 A Kelpy example
+##### 3.3 A Kelpy example
 
 Let's just jump right in with an example, as you should have all of this well in your bones by now. This was from a kelp, predator-diversity experiment I ran ages ago. Note, some things that you want to be factors might be loaded as 
 ```{r echo=FALSE}
@@ -487,7 +487,7 @@ ________(______)
 ________(________(______, spec = ""______""), method = ""________"")
 ```
 
-###### 2.3.1 The Cost of Tukey
+###### 3.3.1 The Cost of Tukey
 So, the kelp example is an interesting one, as this standard workflow is *not* what I wanted when I ran this experiment. I was not interested in a Tukey test of all possible treatments. Run it with no adjustement - what do you see?
 
 ```{r no_adjust, eval=FALSE}
@@ -507,7 +507,7 @@ ________(________(______, spec = ""______""), method = ""________"", ref=____)
 
 What did you learn? 
 
-###### 2.3.2 Replicated Regression
+###### 3.3.2 Replicated Regression
 
 So.... this was actually a replicated regression design. There are a few ways to deal with this. Note the column `Predator_Diversity`
 
@@ -516,7 +516,7 @@ Try this whole thing as a regression. What do you see?
 
 Make a new column that is `Predator_Diversity` as a factor. Refit the factorial ANOVA with this as your treatment. NOW try a Tukey test. What do you see?
 
-###### 2.3.3 A Priori Contrast F tests
+###### 3.3.3 A Priori Contrast F tests
 
 OK, one more way to look at this. What we're actually asking in comparing monocultures and polycultures is, do we explain more variation with a monoculture v. poyculture split than if not?
 

---FILE: lab/10_anova.html---
@@ -530,12 +530,12 @@ <h5>2.4 Faded Examples</h5>
 ________(herbivores, sqrtarea, data=____, geom=&quot;____&quot;)
 
 #fit
-mouse_lm &lt;- __(______ ~ ______ + _____, data=________)
+intertidal_lm &lt;- __(______ ~ ______ + _____, data=________)
 
 #assumptions
 ________(______, which=c(1,2,4,5))
 
-residualPlots(bee_lm)
+residualPlots(intertidal_lm)
 
 #ANOVA
 ________(______)
@@ -546,12 +546,12 @@ <h5>2.4 Faded Examples</h5>
 </div>
 </div>
 <div id=""factorial-anova"" class=""section level2"">
-<h2>2. Factorial ANOVA</h2>
+<h2>3. Factorial ANOVA</h2>
 <p>Going with that last mouse example, if you really looked, it was a factorial design, with multiple treatments and conditions.</p>
 <pre class=""r""><code>qplot(herbivores, sqrtarea, data=intertidal, fill=height, geom=&quot;boxplot&quot;)</code></pre>
 <p><img src=""data:image/png;base64,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"" title alt width=""672"" /></p>
 <div id=""fit-and-assumption-evaluation-1"" class=""section level5"">
-<h5>2.1 Fit and Assumption Evaluation</h5>
+<h5>3.1 Fit and Assumption Evaluation</h5>
 <p>We fit factorial models using one of two different notations - both expand to the same thing</p>
 <pre class=""r""><code>intertidal_lm &lt;- lm(sqrtarea ~ herbivores + height + herbivores:height, data=intertidal)
 
@@ -560,7 +560,7 @@ <h5>2.1 Fit and Assumption Evaluation</h5>
 <p>But, after that’s done…all of the assumption tests are the same. Try them out.</p>
 </div>
 <div id=""type-ii-and-iii-sums-of-squares"" class=""section level5"">
-<h5>2.2 Type II and III Sums of Squares</h5>
+<h5>3.2 Type II and III Sums of Squares</h5>
 <p>Now, we can choose type II or III SS once we have &gt;n=1 for simple effects. Let’s see the difference. Both are from <code>Anova()</code> from the car package.</p>
 <pre class=""r""><code>Anova(intertidal_lm)</code></pre>
 <pre><code>## Anova Table (Type II tests)
@@ -586,7 +586,7 @@ <h5>2.2 Type II and III Sums of Squares</h5>
 ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
 </div>
 <div id=""post-hocs-1"" class=""section level5"">
-<h5>2.3 Post-Hocs</h5>
+<h5>3.3 Post-Hocs</h5>
 <p>Post-hocs are a bit funnier. But not by much. As we have an interaction, let’s look at the simple effects:</p>
 <pre class=""r""><code>contrast(lsmeans(intertidal_lm, spec=c(&quot;herbivores&quot;, &quot;height&quot;)), &quot;tukey&quot;)</code></pre>
 <pre><code>##  contrast                estimate       SE df t.ratio p.value
@@ -602,7 +602,7 @@ <h5>2.3 Post-Hocs</h5>
 <p>And, yeah, it works the same in BANOVA as all the way back in 1-way ANOVA. Only you have to hand-code it.</p>
 </div>
 <div id=""a-kelpy-example"" class=""section level5"">
-<h5>2.3 A Kelpy example</h5>
+<h5>3.3 A Kelpy example</h5>
 <p>Let’s just jump right in with an example, as you should have all of this well in your bones by now. This was from a kelp, predator-diversity experiment I ran ages ago. Note, some things that you want to be factors might be loaded as</p>
 <pre class=""r""><code>kelp &lt;- read.csv(&quot;./data/10/kelp_pred_div_byrnesetal2006.csv&quot;)
 
@@ -627,7 +627,7 @@ <h5>2.3 A Kelpy example</h5>
 #Tukey's HSD
 ________(________(______, spec = &quot;______&quot;), method = &quot;________&quot;)</code></pre>
 <div id=""the-cost-of-tukey"" class=""section level6"">
-<h6>2.3.1 The Cost of Tukey</h6>
+<h6>3.3.1 The Cost of Tukey</h6>
 <p>So, the kelp example is an interesting one, as this standard workflow is <em>not</em> what I wanted when I ran this experiment. I was not interested in a Tukey test of all possible treatments. Run it with no adjustement - what do you see?</p>
 <pre class=""r""><code>#Pariwise Comparison without P-Value adjustment - The LSD test
 ________(________(______, spec = &quot;______&quot;), method = &quot;________&quot;, adjust=&quot;_____&quot;)</code></pre>
@@ -638,13 +638,13 @@ <h6>2.3.1 The Cost of Tukey</h6>
 <p>What did you learn?</p>
 </div>
 <div id=""replicated-regression"" class=""section level6"">
-<h6>2.3.2 Replicated Regression</h6>
+<h6>3.3.2 Replicated Regression</h6>
 <p>So…. this was actually a replicated regression design. There are a few ways to deal with this. Note the column <code>Predator_Diversity</code></p>
 <p>Try this whole thing as a regression. What do you see?</p>
 <p>Make a new column that is <code>Predator_Diversity</code> as a factor. Refit the factorial ANOVA with this as your treatment. NOW try a Tukey test. What do you see?</p>
 </div>
 <div id=""a-priori-contrast-f-tests"" class=""section level6"">
-<h6>2.3.3 A Priori Contrast F tests</h6>
+<h6>3.3.3 A Priori Contrast F tests</h6>
 <p>OK, one more way to look at this. What we’re actually asking in comparing monocultures and polycultures is, do we explain more variation with a monoculture v. poyculture split than if not?</p>
 <pre class=""r""><code>kelp_contr &lt;- lm(Change_g ~ C(factor(Predator_Diversity), c(0,1,-1))*Trial, data=kelp)
 "
biol607,biol607.github.io,c7247ccdce28b8232ad390a471b886d16861b4bd,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-10T22:55:04Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-10T22:55:04Z,Fixing LaTeX errors,lectures/19_expts_anova.Rmd;lectures/19_expts_anova.html,True,False,True,False,40,24,64,"---FILE: lectures/19_expts_anova.Rmd---
@@ -16,6 +16,7 @@ output:
 ## {data-background=""images/19/urchin_diet_expt.jpg""}
 <br>
 <!-- review this slide for continuity and some missing conclusion slides -->
+<!-- explain what levene test and Kruskal-Wallace test are doing -->
 <h1 style=""background-color:white; font-size:68pt"">Experiments and ANOVA</h1>
 
 ```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
@@ -160,10 +161,10 @@ BIAS as well.</span>\
 
 -   Ensure that your effect is not a fluke10
 
--   $  frac{p^{3/2}}{n}$ should approach 0 for Likelihood 
+-   $frac{p^{3/2}}{n}$ should approach 0 for Likelihood 
       - <span style=""font-size:12pt"">Portnoy 1988 Annals of Statistics</span>
 
--   i.e.,$  sim$ 5-10 samples per paramter (1 treatment = 1 parameter, but
+-   i.e.,$sim$ 5-10 samples per paramter (1 treatment = 1 parameter, but
     this is total \# of samples)
 
 ## Outline
@@ -316,7 +317,7 @@ $$3) y_{j} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{j}, \qquad x_{i} = 0,1$
 
 ## Partioning Model
 
-$$  large y_{ij} = \bar{y} + (\bar{y}_{i} - \bar{y}) + ({y}_{ij} - \bar{y}_{i})$$
+$$\large y_{ij} = \bar{y} + (\bar{y}_{i} - \bar{y}) + ({y}_{ij} - \bar{y}_{i})$$
 \
 <div style=""text-align:left"">
 <li> Shows partitioning of variation  
@@ -325,7 +326,8 @@ $$  large y_{ij} = \bar{y} + (\bar{y}_{i} - \bar{y}) + ({y}_{ij} - \bar{y}_{i})$
 </div>
 
 ## Means Model
-$$  large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N(0, \sigma^{2} )$$
+$$\large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}$$  
+$$\epsilon_{ij} \sim N(0, \sigma^{2} )$$
 \
 <div style=""text-align:left"">
 <li> Different mean for each group  
@@ -334,8 +336,8 @@ $$  large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N
 </div>
 
 ## Linear Dummy Variable Model
-$$  large y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}, \qquad x_{i} = 0,1$$  
-$$  epsilon_{ij} \sim N(0, \sigma^{2})$$
+$$\large y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}, \qquad x_{i} = 0,1$$  
+$$\epsilon_{ij} \sim N(0, \sigma^{2})$$
 \ 
 
 - $x_{i}$ inidicates presence/abscence (1/0) of a category\
@@ -390,14 +392,14 @@ $$H_{0} = \mu_{1} = \mu{2} = \mu{3} = ...$$
 
 OR
 
-$$  beta_{0} = \mu, \qquad \beta_{i} = 0$$
+$$\beta_{0} = \mu, \qquad \beta_{i} = 0$$
 
 
 ## Linking your Model to Your Question
 <div style=""text-align:left"">
-Data Generating Process:$$  beta_{0} + \sum \beta_{i}x_{i}$$
+Data Generating Process:$$\beta_{0} + \sum \beta_{i}x_{i}$$
 <br><br>VERSUS<br><br>
-Error Generating Process $$  epsilon_i \sim N(0,\sigma)$$ 
+Error Generating Process $$\epsilon_i \sim N(0,\sigma)$$ 
 
 <div class=""fragment"">If groups are a meaningful explanatory variable, what does that imply about variability in th data?</div>
 </div>
@@ -524,7 +526,7 @@ Levene’s test robust to departures from normality
 ## Kruskal Wallace Test
 
 ```{r brainGene_kruska}
-kruskal.test(PLP1.expression ~ group, data=brainGene)
+knitr::kable(broom::tidy(kruskal.test(PLP1.expression ~ group, data=brainGene)), digits=4)
 ```
 
 

---FILE: lectures/19_expts_anova.html---
@@ -79,7 +79,7 @@
 
 <section id=""section"" class=""slide level2"" data-background=""images/19/urchin_diet_expt.jpg"">
 <h1></h1>
-<p><br></p>
+<br> <!-- review this slide for continuity and some missing conclusion slides -->
 <h1 style=""background-color:white; font-size:68pt"">
 Experiments and ANOVA
 </h1>
@@ -178,11 +178,11 @@ <h1>Ensuring our Signal is Real</h1>
 <ul>
 <li><p>How many points to fit a probability distribution?</p></li>
 <li><p>Ensure that your effect is not a fluke10</p></li>
-<li>$ frac{p^{3/2}}{n}$ should approach 0 for Likelihood
+<li><span class=""math inline"">\(frac{p^{3/2}}{n}\)</span> should approach 0 for Likelihood
 <ul>
 <li><span style=""font-size:12pt"">Portnoy 1988 Annals of Statistics</span></li>
 </ul></li>
-<li><p>i.e.,$ sim$ 5-10 samples per paramter (1 treatment = 1 parameter, but this is total # of samples)</p></li>
+<li><p>i.e.,<span class=""math inline"">\(sim\)</span> 5-10 samples per paramter (1 treatment = 1 parameter, but this is total # of samples)</p></li>
 </ul>
 </section>
 <section id=""outline-1"" class=""slide level2"">
@@ -302,7 +302,7 @@ <h1>Different Ways to Write a Categorical Model</h1>
 </section>
 <section id=""partioning-model"" class=""slide level2"">
 <h1>Partioning Model</h1>
-<span class=""math display"">\[  large y_{ij} = \bar{y} + (\bar{y}_{i} - \bar{y}) + ({y}_{ij} - \bar{y}_{i})\]</span><br />
+<span class=""math display"">\[\large y_{ij} = \bar{y} + (\bar{y}_{i} - \bar{y}) + ({y}_{ij} - \bar{y}_{i})\]</span><br />
 
 <div style=""text-align:left"">
 <li>
@@ -315,7 +315,8 @@ <h1>Partioning Model</h1>
 </section>
 <section id=""means-model"" class=""slide level2"">
 <h1>Means Model</h1>
-<span class=""math display"">\[  large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N(0, \sigma^{2} )\]</span><br />
+<span class=""math display"">\[\large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}\]</span><br />
+<span class=""math display"">\[\epsilon_{ij} \sim N(0, \sigma^{2} )\]</span><br />
 
 <div style=""text-align:left"">
 <li>
@@ -328,8 +329,8 @@ <h1>Means Model</h1>
 </section>
 <section id=""linear-dummy-variable-model"" class=""slide level2"">
 <h1>Linear Dummy Variable Model</h1>
-<p><span class=""math display"">\[  large y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}, \qquad x_{i} = 0,1\]</span><br />
-<span class=""math display"">\[  epsilon_{ij} \sim N(0, \sigma^{2})\]</span>  </p>
+<p><span class=""math display"">\[\large y_{ij} = \beta_{0} + \sum \beta_{i}x_{i} + \epsilon_{ij}, \qquad x_{i} = 0,1\]</span><br />
+<span class=""math display"">\[\epsilon_{ij} \sim N(0, \sigma^{2})\]</span>  </p>
 <ul>
 <li><span class=""math inline"">\(x_{i}\)</span> inidicates presence/abscence (1/0) of a category<br />
 <br />
@@ -387,12 +388,12 @@ <h1>Introducing ANOVA: Comparing Variation</h1>
 <h1>Hypothesis Testing with a Categorical Model: ANOVA</h1>
 <p><span class=""math display"">\[H_{0} = \mu_{1} = \mu{2} = \mu{3} = ...\]</span></p>
 <p>OR</p>
-<p><span class=""math display"">\[  beta_{0} = \mu, \qquad \beta_{i} = 0\]</span></p>
+<p><span class=""math display"">\[\beta_{0} = \mu, \qquad \beta_{i} = 0\]</span></p>
 </section>
 <section id=""linking-your-model-to-your-question"" class=""slide level2"">
 <h1>Linking your Model to Your Question</h1>
 <div style=""text-align:left"">
-<p>Data Generating Process:<span class=""math display"">\[  beta_{0} + \sum \beta_{i}x_{i}\]</span> <br><br>VERSUS<br><br> Error Generating Process <span class=""math display"">\[  epsilon_i \sim N(0,\sigma)\]</span></p>
+<p>Data Generating Process:<span class=""math display"">\[\beta_{0} + \sum \beta_{i}x_{i}\]</span> <br><br>VERSUS<br><br> Error Generating Process <span class=""math display"">\[\epsilon_i \sim N(0,\sigma)\]</span></p>
 <div class=""fragment"">
 If groups are a meaningful explanatory variable, what does that imply about variability in th data?
 </div>
@@ -526,11 +527,24 @@ <h1>What do I do if I Violate Assumptions?</h1>
 </section>
 <section id=""kruskal-wallace-test"" class=""slide level2"">
 <h1>Kruskal Wallace Test</h1>
-<pre><code>
-    Kruskal-Wallis rank sum test
-
-data:  PLP1.expression by group
-Kruskal-Wallis chi-squared = 13.198, df = 2, p-value = 0.001361</code></pre>
+<table>
+<thead>
+<tr class=""header"">
+<th style=""text-align: right;"">statistic</th>
+<th style=""text-align: right;"">p.value</th>
+<th style=""text-align: right;"">parameter</th>
+<th style=""text-align: left;"">method</th>
+</tr>
+</thead>
+<tbody>
+<tr class=""odd"">
+<td style=""text-align: right;"">13.1985</td>
+<td style=""text-align: right;"">0.0014</td>
+<td style=""text-align: right;"">2</td>
+<td style=""text-align: left;"">Kruskal-Wallis rank sum test</td>
+</tr>
+</tbody>
+</table>
 </section>
     </div>
   </div>"
biol607,biol607.github.io,3592fb178b6e2939ab206184635f49ccc0b56b22,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-10T22:48:39Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-10T22:48:39Z,LaTeX typ fix,lectures/20_anova_2.Rmd;lectures/20_anova_2.html,True,False,True,False,46,39,85,"---FILE: lectures/20_anova_2.Rmd---
@@ -15,7 +15,7 @@ output:
 
 ## {data-background=""images/19/urchin_diet_expt.jpg""}
 <br>
-<!-- review this slide for continuity and some missing conclusion slides -->
+<!-- review this slide for continuity and some re-ordering in posthocs. That section is a bit of a mess Also Bayes needs more detail. And likelihood tables are ugly. Broom?-->
 <h1 style=""background-color:white; font-size:68pt"">After the ANOVA</h1>
 
 ```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
@@ -73,7 +73,7 @@ ggplot(brainGene, aes(x=group, y=PLP1.expression, color = group)) +
 
 
 ## Means Model
-$$  large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N(0, \sigma^{2} )$$
+$$\large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N(0, \sigma^{2} )$$
 \
 <div style=""text-align:left"">
 <li> Different mean for each group  
@@ -117,7 +117,7 @@ k = number of groups, n = sample size
 \
 3.  How much variation is retained by the model?  
 \
-4.  How confident can you be in model predictions?
+4.  How confident can you be in model predictions?  
 \
 5. Are groups different from each other
 
@@ -129,7 +129,7 @@ k = number of groups, n = sample size
 \
 3.  How much variation is retained by the model?  
 \
-4.  How confident can you be in model predictions?
+4.  How confident can you be in model predictions?  
 \
 5. Are groups different from each other
 
@@ -148,7 +148,7 @@ knitr::kable(anova(lm(PLP1.expression ~ group, data=brainGene)))
 \
 3.  How much variation is retained by the model?  
 \
-4.  How confident can you be in model predictions?
+4.  How confident can you be in model predictions?  
 \
 5. Are groups different from each other
 
@@ -161,7 +161,7 @@ knitr::kable(anova(lm(PLP1.expression ~ group, data=brainGene)))
 \
 3.  How much variation is retained by the model?  
 \
-4.  How confident can you be in model predictions?
+4.  How confident can you be in model predictions?  
 \
 5. <font color=""red"">Are groups different from each other</font>  
 
@@ -421,13 +421,19 @@ summary(bg_tukey_lik)
 ```
 
 ## BANOVA
-```{r banova, echo=TRUE, cache=TRUE}
+```{r banova, echo=FALSE, cache=TRUE}
 brain_bayes <- stan_glm(PLP1.expression ~ group - 1,
                  family=gaussian(),
                  data = brainGene)
 ```
 
-```{r banova_2, echo=TRUE, cache=TRUE}
+
+```{r banova_show, echo=TRUE, eval=FALSE}
+brain_bayes <- stan_glm(PLP1.expression ~ group - 1,
+                 family=gaussian(),
+                 data = brainGene)
+```
+```{r banova_2, echo=FALSE, cache=TRUE}
 brain_bayes_lmer <- stan_lmer(PLP1.expression ~ 1+ (1|group),
                 # family=gaussian(),
                  data = brainGene)

---FILE: lectures/20_anova_2.html---
@@ -116,7 +116,7 @@
 
 <section id=""section"" class=""slide level2"" data-background=""images/19/urchin_diet_expt.jpg"">
 <h1></h1>
-<br> <!-- review this slide for continuity and some missing conclusion slides -->
+<br> <!-- review this slide for continuity and some re-ordering in posthocs. That section is a bit of a mess Also Bayes needs more detail. And likelihood tables are ugly. Broom?-->
 <h1 style=""background-color:white; font-size:68pt"">
 After the ANOVA
 </h1>
@@ -147,7 +147,7 @@ <h1>Comparison of Means</h1>
 </section>
 <section id=""means-model"" class=""slide level2"">
 <h1>Means Model</h1>
-<span class=""math display"">\[  large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N(0, \sigma^{2} )\]</span><br />
+<span class=""math display"">\[\large y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N(0, \sigma^{2} )\]</span><br />
 
 <div style=""text-align:left"">
 <li>
@@ -192,6 +192,7 @@ <h1>Questions we could ask</h1>
 <br />
 </li>
 <li>How confident can you be in model predictions?<br />
+<br />
 </li>
 <li>Are groups different from each other</li>
 </ol>
@@ -209,6 +210,7 @@ <h1>Questions we could ask</h1>
 <br />
 </li>
 <li>How confident can you be in model predictions?<br />
+<br />
 </li>
 <li>Are groups different from each other</li>
 </ol>
@@ -259,6 +261,7 @@ <h1>Questions we could ask</h1>
 <br />
 </li>
 <li>How confident can you be in model predictions?<br />
+<br />
 </li>
 <li>Are groups different from each other</li>
 </ol>
@@ -276,6 +279,7 @@ <h1>Questions we could ask</h1>
 <br />
 </li>
 <li>How confident can you be in model predictions?<br />
+<br />
 </li>
 <li><font color=""red"">Are groups different from each other</font></li>
 </ol>
@@ -627,9 +631,6 @@ <h1>Likelihood and Posthocs</h1>
 </section>
 <section id=""banova"" class=""slide level2"">
 <h1>BANOVA</h1>
-<div class=""sourceCode""><pre class=""sourceCode r""><code class=""sourceCode r"">brain_bayes &lt;-<span class=""st""> </span><span class=""kw"">stan_glm</span>(PLP1.expression ~<span class=""st""> </span>group -<span class=""st""> </span><span class=""dv"">1</span>,
-                 <span class=""dt"">family=</span><span class=""kw"">gaussian</span>(),
-                 <span class=""dt"">data =</span> brainGene)</code></pre></div>
 <pre><code>
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1).
 
@@ -645,9 +646,9 @@ <h1>BANOVA</h1>
 Chain 1, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 1, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 0.069421 seconds (Warm-up)
-               0.081888 seconds (Sampling)
-               0.151309 seconds (Total)
+ Elapsed Time: 0.080866 seconds (Warm-up)
+               0.08541 seconds (Sampling)
+               0.166276 seconds (Total)
 
 
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2).
@@ -664,9 +665,9 @@ <h1>BANOVA</h1>
 Chain 2, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 2, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 0.072502 seconds (Warm-up)
-               0.085696 seconds (Sampling)
-               0.158198 seconds (Total)
+ Elapsed Time: 0.068858 seconds (Warm-up)
+               0.063094 seconds (Sampling)
+               0.131952 seconds (Total)
 
 
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3).
@@ -683,9 +684,9 @@ <h1>BANOVA</h1>
 Chain 3, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 3, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 0.082055 seconds (Warm-up)
-               0.075523 seconds (Sampling)
-               0.157578 seconds (Total)
+ Elapsed Time: 0.054466 seconds (Warm-up)
+               0.096178 seconds (Sampling)
+               0.150644 seconds (Total)
 
 
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4).
@@ -702,11 +703,11 @@ <h1>BANOVA</h1>
 Chain 4, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 4, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 0.051171 seconds (Warm-up)
-               0.061668 seconds (Sampling)
-               0.112839 seconds (Total)</code></pre>
-<div class=""sourceCode""><pre class=""sourceCode r""><code class=""sourceCode r"">brain_bayes_lmer &lt;-<span class=""st""> </span><span class=""kw"">stan_lmer</span>(PLP1.expression ~<span class=""st""> </span><span class=""dv"">1</span>+<span class=""st""> </span>(<span class=""dv"">1</span>|group),
-                <span class=""co""># family=gaussian(),</span>
+ Elapsed Time: 0.079157 seconds (Warm-up)
+               0.088166 seconds (Sampling)
+               0.167323 seconds (Total)</code></pre>
+<div class=""sourceCode""><pre class=""sourceCode r""><code class=""sourceCode r"">brain_bayes &lt;-<span class=""st""> </span><span class=""kw"">stan_glm</span>(PLP1.expression ~<span class=""st""> </span>group -<span class=""st""> </span><span class=""dv"">1</span>,
+                 <span class=""dt"">family=</span><span class=""kw"">gaussian</span>(),
                  <span class=""dt"">data =</span> brainGene)</code></pre></div>
 <pre><code>
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1).
@@ -723,9 +724,9 @@ <h1>BANOVA</h1>
 Chain 1, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 1, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 1, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 1.40348 seconds (Warm-up)
-               0.726771 seconds (Sampling)
-               2.13025 seconds (Total)
+ Elapsed Time: 1.34429 seconds (Warm-up)
+               0.844418 seconds (Sampling)
+               2.18871 seconds (Total)
 
 
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2).
@@ -742,9 +743,9 @@ <h1>BANOVA</h1>
 Chain 2, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 2, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 2, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 0.929048 seconds (Warm-up)
-               0.847096 seconds (Sampling)
-               1.77614 seconds (Total)
+ Elapsed Time: 0.917152 seconds (Warm-up)
+               1.10817 seconds (Sampling)
+               2.02533 seconds (Total)
 
 
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3).
@@ -761,9 +762,9 @@ <h1>BANOVA</h1>
 Chain 3, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 3, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 3, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 0.957706 seconds (Warm-up)
-               0.654124 seconds (Sampling)
-               1.61183 seconds (Total)
+ Elapsed Time: 0.95174 seconds (Warm-up)
+               0.753277 seconds (Sampling)
+               1.70502 seconds (Total)
 
 
 SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4).
@@ -780,9 +781,9 @@ <h1>BANOVA</h1>
 Chain 4, Iteration: 1600 / 2000 [ 80%]  (Sampling)
 Chain 4, Iteration: 1800 / 2000 [ 90%]  (Sampling)
 Chain 4, Iteration: 2000 / 2000 [100%]  (Sampling)
- Elapsed Time: 1.00292 seconds (Warm-up)
-               1.97456 seconds (Sampling)
-               2.97748 seconds (Total)</code></pre>
+ Elapsed Time: 1.10924 seconds (Warm-up)
+               2.09969 seconds (Sampling)
+               3.20893 seconds (Total)</code></pre>
 </section>
 <section id=""banova-1"" class=""slide level2"">
 <h1>BANOVA</h1>"
biol607,biol607.github.io,12aeaccb09d72a0f5df93cb3f23ed698d7d8de17,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-01T14:46:16Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-01T14:46:16Z,Fix cover image,index.html,False,False,False,False,10,12,22,
biol607,biol607.github.io,b1a5a1dfc72e908ee09397dd3ae7e1a1dae0a87f,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-01T14:44:29Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-01T14:44:29Z,Updating with R Errors page,README.html;error.html;extra.html;final.html;index.html;links.html;overview.html;resources.html;schedule.html;texts.html,False,False,False,False,73,38,111,"---FILE: README.html---
@@ -185,6 +185,9 @@
 <li>
   <a href=""schedule.html"">Schedule</a>
 </li>
+<li>
+  <a href=""error.html"">R Errors</a>
+</li>
 <li>
   <a href=""texts.html"">Texts</a>
 </li>

---FILE: extra.html---
@@ -185,6 +185,9 @@
 <li>
   <a href=""schedule.html"">Schedule</a>
 </li>
+<li>
+  <a href=""error.html"">R Errors</a>
+</li>
 <li>
   <a href=""texts.html"">Texts</a>
 </li>

---FILE: final.html---
@@ -185,6 +185,9 @@
 <li>
   <a href=""schedule.html"">Schedule</a>
 </li>
+<li>
+  <a href=""error.html"">R Errors</a>
+</li>
 <li>
   <a href=""texts.html"">Texts</a>
 </li>

---FILE: links.html---
@@ -185,6 +185,9 @@
 <li>
   <a href=""schedule.html"">Schedule</a>
 </li>
+<li>
+  <a href=""error.html"">R Errors</a>
+</li>
 <li>
   <a href=""texts.html"">Texts</a>
 </li>

---FILE: overview.html---
@@ -185,6 +185,9 @@
 <li>
   <a href=""schedule.html"">Schedule</a>
 </li>
+<li>
+  <a href=""error.html"">R Errors</a>
+</li>
 <li>
   <a href=""texts.html"">Texts</a>
 </li>

---FILE: texts.html---
@@ -185,6 +185,9 @@
 <li>
   <a href=""schedule.html"">Schedule</a>
 </li>
+<li>
+  <a href=""error.html"">R Errors</a>
+</li>
 <li>
   <a href=""texts.html"">Texts</a>
 </li>"
biol607,biol607.github.io,a60e9d0aee8b27bb736f86a6a3d7d0b356f3b2c9,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-01T14:41:55Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-11-01T14:41:55Z,Error message wiki,_site.yml;error.Rmd;error.html,True,False,True,False,289,0,289,"---FILE: _site.yml---
@@ -9,6 +9,8 @@ navbar:
     href: overview.html
   - text: ""Schedule""
     href: schedule.html
+  - text: ""R Errors""
+    href: error.html
   - text: ""Texts""
     href: texts.html
   - text: ""Final Project""

---FILE: error.Rmd---
@@ -0,0 +1,23 @@
+---
+title: ""R Error Messages Ye Have Found""
+---
+
+```{r setup, include=FALSE}
+knitr::opts_chunk$set(echo = TRUE)
+```
+
+We all run into error messages. In class, during a homework, during an exam. This is a page with an embedded etherpad. You can work on it here, or at https://etherpad.wikimedia.org/p/607-error-messages if you would prefer.
+
+For each error:  
+
+0) State who you are
+1) Report the error message.  
+2) Make a reproducible example that generates the error.
+3) Provide any additional comments.  
+4) Email the class that you have put up a new error (or email jarrett.byrnes@umb.edu and I'll forward your notice).
+5) Once you answer an error, put your name by it. +1 point on your exam per error message troubleshot.
+
+See the example at the top as a template.
+
+<iframe src='https://etherpad.wikimedia.org/p/607-error-messages' width=600 height=400>
+"
biol607,biol607.github.io,d1dc63e732e8c6612e0d011038fd7dfbef0d6ae9,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-24T16:37:46Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-24T16:37:46Z,spacing issues,schedule.Rmd,True,False,True,False,0,1,1,"---FILE: schedule.Rmd---
@@ -76,7 +76,6 @@ __Additional Reading on rstanarm__: [How to use it](https://cran.r-project.org/w
 ### Week 9.   
 __Lecture:__ Functions, Tidy data for statistical analysis  
 __Reading:__ Borer et al. 2009. [10 Commandments for Good Data Managament](https://dynamicecology.wordpress.com/2016/08/22/ten-commandments-for-good-data-management/), G&W Chapters on [tidy data](http://r4ds.had.co.nz/tidy-data.html), [Strings](http://r4ds.had.co.nz/strings.html), and [Dates](http://r4ds.had.co.nz/dates-and-times.html)     
-    
   
 ### Week 10.   
 __Lecture:__ Experimental design and ANOVA  "
biol607,biol607.github.io,8c5db5a1d2d5163221790170b73bec23b6bba10b,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-20T16:35:59Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-20T16:35:59Z,Fix,schedule.Rmd,True,False,True,False,1,1,2,"---FILE: schedule.Rmd---
@@ -61,7 +61,7 @@ __In Class Code:__ [lm](in_class_code_2016/06_lm_inclass.R)
   
 ### Week 7.   
 __Lecture:__ [Linear Model Power Analysis](lab/06a_power_analysis.html), [Likelihood](lectures/13_likelihood.html), [Fitting a line with Likelihood](lectures/14_likelihood_2.html)  
-__Lab Topic:__ [Calculating and visualizing Likelihoods, fitting a line with bbmle](lab/07_likelihood.html)  
+__Lab Topic:__ [Calculating and visualizing Likelihoods, fitting a line with bbmle](lab/07_likelihood.html)\
 __Reading:__ W&S 20, G&W Chapter [Iteration](http://r4ds.had.co.nz/iteration.html)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-likelihood  
 __In Class Code:__ [power analysis](in_class_code_2016/06a_power_lm.R)"
biol607,biol607.github.io,210ab21d6e01126ea86783dcf08971a7fb1152ae,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-20T16:35:16Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-20T16:35:16Z,Fixed lab 7 link,schedule.Rmd;schedule.html,True,False,True,False,3,2,5,"---FILE: schedule.Rmd---
@@ -61,7 +61,7 @@ __In Class Code:__ [lm](in_class_code_2016/06_lm_inclass.R)
   
 ### Week 7.   
 __Lecture:__ [Linear Model Power Analysis](lab/06a_power_analysis.html), [Likelihood](lectures/13_likelihood.html), [Fitting a line with Likelihood](lectures/14_likelihood_2.html)  
-__Lab Topic:__ [Calculating and visualizing Likelihoods, fitting a line with bbmle](lab/07_likelihood.Rmd) 
+__Lab Topic:__ [Calculating and visualizing Likelihoods, fitting a line with bbmle](lab/07_likelihood.html)  
 __Reading:__ W&S 20, G&W Chapter [Iteration](http://r4ds.had.co.nz/iteration.html)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-likelihood  
 __In Class Code:__ [power analysis](in_class_code_2016/06a_power_lm.R)

---FILE: schedule.html---
@@ -278,7 +278,8 @@ <h3>Week 6.</h3>
 <div id=""week-7."" class=""section level3"">
 <h3>Week 7.</h3>
 <p><strong>Lecture:</strong> <a href=""lab/06a_power_analysis.html"">Linear Model Power Analysis</a>, <a href=""lectures/13_likelihood.html"">Likelihood</a>, <a href=""lectures/14_likelihood_2.html"">Fitting a line with Likelihood</a><br />
-<strong>Lab Topic:</strong> <a href=""lab/07_likelihood.Rmd"">Calculating and visualizing Likelihoods, fitting a line with bbmle</a> <strong>Reading:</strong> W&amp;S 20, G&amp;W Chapter <a href=""http://r4ds.had.co.nz/iteration.html"">Iteration</a><br />
+<strong>Lab Topic:</strong> <a href=""lab/07_likelihood.html"">Calculating and visualizing Likelihoods, fitting a line with bbmle</a><br />
+<strong>Reading:</strong> W&amp;S 20, G&amp;W Chapter <a href=""http://r4ds.had.co.nz/iteration.html"">Iteration</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-likelihood"" class=""uri"">https://etherpad.wikimedia.org/p/607-likelihood</a><br />
 <strong>In Class Code:</strong> <a href=""in_class_code_2016/06a_power_lm.R"">power analysis</a></p>
 </div>"
biol607,biol607.github.io,720a7c89ded22e2ab6b043b010b207ec9c617772,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-20T14:49:57Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-20T14:49:57Z,Fixed profile likelihood section,lectures/13_likelihood.Rmd;lectures/13_likelihood.html;lectures/13_likelihood_files/figure-revealjs/profileBrute_mean_all-1.jpeg;lectures/13_likelihood_files/figure-revealjs/profileBrute_mean_all_2-1.jpeg,True,False,True,False,86,57,143,"---FILE: lectures/13_likelihood.Rmd---
@@ -12,6 +12,14 @@ output:
     lib_dir: libs
     css: style.css
 ---
+## Announcements
+1. Schedule a meeting with me this week or next to discuss final project\
+\
+2. Exam questions starting to post Thursday\
+\
+3. Three weeks for exam! \
+\
+4. https://etherpad.wikimedia.org/p/607-likelihood
 
 ## Likelihood!
 ![](images/13/i_can_haz_statz_fb_lolstatz.png){width=45%}
@@ -28,6 +36,7 @@ library(ggplot2)
 
 ```
 
+
 ## Review
 
 -   We test hypotheses using $P(x \le Data | H)$
@@ -522,6 +531,13 @@ Estimates: mean = `r round(m_seals, 2)`, SD = `r round(s_seals, 2)`
 
 4.  Specification of Likelihood Function Unwieldy
 
+## Profile CIs
+
+1. For each value of the parameter of interest, get the MLE of the other paramter  
+\
+2. Use this as your **profile** likelihood for your parameter  
+\ 
+3. Values of your parameter with a Log Likelihood 1.92 from the peak are in your CI
 
 
 ## Likelihood Profile of One Coefficient Along ML Estimates of the Other
@@ -543,64 +559,46 @@ ggplot(data = sealSurf %>% filter(m %in% m_levs)) +
 ```
 
 ## Likelihood Profile of One Coefficient Along ML Estimates of the Other
+```{r profileBrute_mean_all}
+mean_prof <- sealSurf %>%
+  group_by(m) %>%
+  filter(ll == max(ll)) %>%
+  ungroup()
 
-```{r profileBrute_SD, cache=TRUE}
-s_levs <- seal_s[c(100,300,500)]
-s_names <- paste(""SD = "", round(s_levs, 2))
-names(s_names) <- s_levs
-
-ggplot(data = sealSurf %>% filter(s %in% s_levs)) +
-  geom_line(mapping = aes(x=m, y=ll)) +
-  facet_wrap(~s, labeller = labeller(s = s_names)) +
-  ggtitle(""ML Curve for Mean at Different SD Values"") +
+mean_prof_plot <- ggplot(data = mean_prof) +
+  geom_line(aes(x=m, y=ll))+
   ylab(""Log Likelihood"") + xlab(""Mean"") +
   theme_bw(base_size=17) +
-  geom_hline(data = sealSurf %>% filter(s %in% s_levs) %>% 
-               group_by(s) %>% summarise(ll = max(ll)) %>% ungroup(),
-             mapping=aes(yintercept=ll), lty=2, col=""red"")
-```
-
-## Likelihood Profile of Mean  Along ML Estimates of the SD
-```{r showMeanLik}
-sealSurf_M <- sealSurf %>% 
-  group_by(s) %>%
-  summarise( m = m[which(ll == max(ll))],
-             ll = max(ll)
-           ) %>%
-  ungroup()
+  ggtitle(""ML Curve for Mean Optimized for SD"")
 
+mean_prof_plot
+```
 
-qplot(m, s, size=as.numeric(ll), data=sealSurf_M, geom=""line"") + 
-  geom_line(mapping = aes(size=as.numeric(ll)),color=""blue"") +
-  xlab(""Mean"") + ylab(""S"") +
-  scale_size_continuous(guide=guide_legend(title=""Log Likelihood"")) +
-  theme_bw(base_size=17)
+## Likelihood Profile of One Coefficient Along ML Estimates of the Other
+```{r profileBrute_mean_all_2}
 
+mean_prof_plot +
+  geom_hline(yintercept = max(mean_prof$ll)-1.92, color=""blue"", lty=2, lwd=2)
 ```
 
-## Likelihood Profile of the SD  Along ML Estimates of the Mean
 
-```{r profile_mean}
+## Likelihood Profile of One Coefficient Along ML Estimates of the Other
+<font color=""red"">Mean profile</font>, <font color=""blue"">SD Profile</font>
+```{r profileBrute2}
 sealSurf_S <- sealSurf %>% 
   group_by(m) %>%
   summarise( s = s[which(ll == max(ll))],
              ll = max(ll)
-           ) %>%
+  ) %>%
   ungroup()
 
+sealSurf_M <- sealSurf %>% 
+  group_by(s) %>%
+  summarise( m = m[which(ll == max(ll))],
+             ll = max(ll)
+  ) %>%
+  ungroup()
 
-
-qplot(m, s, size=as.numeric(ll), data=sealSurf_S, geom=""line"") + 
-  geom_line(mapping = aes(size=as.numeric(ll)), color=""red"") +
-  xlab(""Mean"") + ylab(""SD"") +
-  scale_size_continuous(guide=guide_legend(title=""Log Likelihood"")) +
-  theme_bw(base_size=17)
-
-```
-
-## Likelihood Profile of One Coefficient Along ML Estimates of the Other
-<font color=""red"">Mean profile</font>, <font color=""blue"">SD Profile</font>
-```{r profileBrute2}
 ll_contour 
 ``` 
 
@@ -672,7 +670,7 @@ We use <span style=""color:red"">Algorithms</span>
 
 > -   Newtown-Raphson (algorithmicly implemented in <span>nlm</span> and
     <span>BFGS</span> method) uses derivatives
->     - good for smooth surfaces & good start values
+>     - good for smooth surfaces & good start values  
 \
 > -   Brent’s Method - for single parameter fits  
 \ 
@@ -682,7 +680,9 @@ We use <span style=""color:red"">Algorithms</span>
 > -   Simulated Annealing (<span>SANN</span>) uses Metropolis Algorithm search
 >     - global solution, but slow  
 
-**If your algorithm fails to converge, you cannot evaluate your model or
-coefficients**
+## Final Note on Searching Likelihood Space
+<br><br><br><br>
+<h4>**<font color=""red"">If your algorithm fails to converge, you cannot evaluate your model or
+coefficients</font>**</h4>
 
 

---FILE: lectures/13_likelihood.html---
@@ -77,6 +77,21 @@
     <div class=""slides"">
 
 
+<section id=""announcements"" class=""slide level2"">
+<h1>Announcements</h1>
+<ol type=""1"">
+<li>Schedule a meeting with me this week or next to discuss final project<br />
+<br />
+</li>
+<li>Exam questions starting to post Thursday<br />
+<br />
+</li>
+<li>Three weeks for exam!<br />
+<br />
+</li>
+<li><a href=""https://etherpad.wikimedia.org/p/607-likelihood"" class=""uri"">https://etherpad.wikimedia.org/p/607-likelihood</a></li>
+</ol>
+</section>
 <section id=""likelihood"" class=""slide level2"">
 <h1>Likelihood!</h1>
 <p><img src=""images/13/i_can_haz_statz_fb_lolstatz.png"" style=""width:45.0%"" /></p>
@@ -391,32 +406,39 @@ <h1>New Issues with Multiple Parameters</h1>
 <li><p>Specification of Likelihood Function Unwieldy</p></li>
 </ol>
 </section>
+<section id=""profile-cis"" class=""slide level2"">
+<h1>Profile CIs</h1>
+<ol type=""1"">
+<li>For each value of the parameter of interest, get the MLE of the other paramter<br />
+<br />
+</li>
+<li>Use this as your <strong>profile</strong> likelihood for your parameter<br />
+ </li>
+<li>Values of your parameter with a Log Likelihood 1.92 from the peak are in your CI</li>
+</ol>
+</section>
 <section id=""likelihood-profile-of-one-coefficient-along-ml-estimates-of-the-other"" class=""slide level2"">
 <h1>Likelihood Profile of One Coefficient Along ML Estimates of the Other</h1>
 <p><img src=""13_likelihood_files/figure-revealjs/profileBrute_mean-1.jpeg"" title="""" alt="""" width=""768"" /></p>
 </section>
 <section id=""likelihood-profile-of-one-coefficient-along-ml-estimates-of-the-other-1"" class=""slide level2"">
 <h1>Likelihood Profile of One Coefficient Along ML Estimates of the Other</h1>
-<p><img src=""13_likelihood_files/figure-revealjs/profileBrute_SD-1.jpeg"" title="""" alt="""" width=""768"" /></p>
-</section>
-<section id=""likelihood-profile-of-mean-along-ml-estimates-of-the-sd"" class=""slide level2"">
-<h1>Likelihood Profile of Mean Along ML Estimates of the SD</h1>
-<p><img src=""13_likelihood_files/figure-revealjs/showMeanLik-1.jpeg"" title="""" alt="""" width=""768"" /></p>
-</section>
-<section id=""likelihood-profile-of-the-sd-along-ml-estimates-of-the-mean"" class=""slide level2"">
-<h1>Likelihood Profile of the SD Along ML Estimates of the Mean</h1>
-<p><img src=""13_likelihood_files/figure-revealjs/profile_mean-1.jpeg"" title="""" alt="""" width=""768"" /></p>
+<p><img src=""13_likelihood_files/figure-revealjs/profileBrute_mean_all-1.jpeg"" title="""" alt="""" width=""768"" /></p>
 </section>
 <section id=""likelihood-profile-of-one-coefficient-along-ml-estimates-of-the-other-2"" class=""slide level2"">
 <h1>Likelihood Profile of One Coefficient Along ML Estimates of the Other</h1>
-<p><font color=""red"">Mean profile</font>, <font color=""blue"">SD Profile</font> <img src=""13_likelihood_files/figure-revealjs/profileBrute2-1.jpeg"" title="""" alt="""" width=""768"" /></p>
+<p><img src=""13_likelihood_files/figure-revealjs/profileBrute_mean_all_2-1.jpeg"" title="""" alt="""" width=""768"" /></p>
 </section>
 <section id=""likelihood-profile-of-one-coefficient-along-ml-estimates-of-the-other-3"" class=""slide level2"">
 <h1>Likelihood Profile of One Coefficient Along ML Estimates of the Other</h1>
+<p><font color=""red"">Mean profile</font>, <font color=""blue"">SD Profile</font> <img src=""13_likelihood_files/figure-revealjs/profileBrute2-1.jpeg"" title="""" alt="""" width=""768"" /></p>
+</section>
+<section id=""likelihood-profile-of-one-coefficient-along-ml-estimates-of-the-other-4"" class=""slide level2"">
+<h1>Likelihood Profile of One Coefficient Along ML Estimates of the Other</h1>
 <p><font color=""red"">Mean profile</font>, <font color=""blue"">SD Profile</font></p>
 <p><img src=""13_likelihood_files/figure-revealjs/profileBrute3-1.jpeg"" title="""" alt="""" width=""768"" /></p>
 </section>
-<section id=""likelihood-profile-of-one-coefficient-along-ml-estimates-of-the-other-4"" class=""slide level2"">
+<section id=""likelihood-profile-of-one-coefficient-along-ml-estimates-of-the-other-5"" class=""slide level2"">
 <h1>Likelihood Profile of One Coefficient Along ML Estimates of the Other</h1>
 <p><font color=""red"">Mean profile</font>, <font color=""blue"">SD Profile</font></p>
 <p><img src=""13_likelihood_files/figure-revealjs/profileBrute4-1.jpeg"" title="""" alt="""" width=""768"" /></p>
@@ -451,6 +473,7 @@ <h1>Searching Likelihood Space</h1>
 <li class=""fragment"">Newtown-Raphson (algorithmicly implemented in <span>nlm</span> and <span>BFGS</span> method) uses derivatives
 <ul>
 <li class=""fragment"">good for smooth surfaces &amp; good start values<br />
+<br />
 </li>
 </ul></li>
 <li class=""fragment"">Brent’s Method - for single parameter fits<br />
@@ -465,7 +488,13 @@ <h1>Searching Likelihood Space</h1>
 <li class=""fragment"">global solution, but slow</li>
 </ul></li>
 </ul>
-<p><strong>If your algorithm fails to converge, you cannot evaluate your model or coefficients</strong></p>
+</section>
+<section id=""final-note-on-searching-likelihood-space"" class=""slide level2"">
+<h1>Final Note on Searching Likelihood Space</h1>
+<br><br><br><br>
+<h4>
+<strong><font color=""red"">If your algorithm fails to converge, you cannot evaluate your model or coefficients</font></strong>
+</h4>
 </section>
     </div>
   </div>"
biol607,biol607.github.io,89c9d126460ace3d6382bfcff4f95c0f5ddbb4be,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-13T16:39:21Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-13T16:39:21Z,Fixed linebreaks,schedule.Rmd;schedule.html,True,False,True,False,3,2,5,"---FILE: schedule.Rmd---
@@ -54,7 +54,7 @@ __In Class Code:__ [t and chi square](in_class_code_2016/06_t_chisq.R)
   
 ### Week 6.   
 __Lecture:__ Least Squares Linear Regression: [Correlation and Regression](lectures/11_cor_linear_model.html), [Fit and Power](lectures/12_linear_model_fit.html)  
-__Lab Topic:__ [Linear regression, diagnostics, visualization](lab/06_lm.html), and [data](lab/data_06.zip) 
+__Lab Topic:__ [Linear regression, diagnostics, visualization](lab/06_lm.html), and [data](lab/data_06.zip)\
 __Reading:__ W&S 16-17, G&W on [model basics](http://r4ds.had.co.nz/model-basics.html), [model building](http://r4ds.had.co.nz/model-building.html)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-lm  
   

---FILE: schedule.html---
@@ -270,7 +270,8 @@ <h3>Week 5.</h3>
 <div id=""week-6."" class=""section level3"">
 <h3>Week 6.</h3>
 <p><strong>Lecture:</strong> Least Squares Linear Regression: <a href=""lectures/11_cor_linear_model.html"">Correlation and Regression</a>, <a href=""lectures/12_linear_model_fit.html"">Fit and Power</a><br />
-<strong>Lab Topic:</strong> <a href=""lab/06_lm.html"">Linear regression, diagnostics, visualization</a>, and <a href=""lab/data_06.zip"">data</a> <strong>Reading:</strong> W&amp;S 16-17, G&amp;W on <a href=""http://r4ds.had.co.nz/model-basics.html"">model basics</a>, <a href=""http://r4ds.had.co.nz/model-building.html"">model building</a><br />
+<strong>Lab Topic:</strong> <a href=""lab/06_lm.html"">Linear regression, diagnostics, visualization</a>, and <a href=""lab/data_06.zip"">data</a><br />
+<strong>Reading:</strong> W&amp;S 16-17, G&amp;W on <a href=""http://r4ds.had.co.nz/model-basics.html"">model basics</a>, <a href=""http://r4ds.had.co.nz/model-building.html"">model building</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-lm"" class=""uri"">https://etherpad.wikimedia.org/p/607-lm</a></p>
 </div>
 <div id=""week-7."" class=""section level3"">"
biol607,biol607.github.io,458804da66cf169e22e405b93aa90c5d48c0a3f8,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-06T15:05:30Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-06T15:05:30Z,Fixing links,schedule.Rmd;schedule.html,True,False,True,False,2,2,4,"---FILE: schedule.Rmd---
@@ -44,7 +44,7 @@ __Homework:__ https://github.com/biol607/2016_homework_04_hypothesis_power
   
   
 ### Week 5.   
-__Lecture:__ [T tests](lectures/09_the_t_test.html),  [χ2 tests](lectures/10_chisquare.html), and p  
+__Lecture:__ [T tests](lectures/09_the_t_test.html),  [χ2 tests](lectures/10_chisq.html), and p  
 __Lab Topic:__ [Statistical analysis functions for t and $\chi^2$ in R](lab/05_chisq_t.html)   
 __Reading:__ W&S 8-12, G&W Chapter 10, 20  
 __Discussion Reading:__ [ASA Statement on P-Values](http://byrneslab.net/classes/biol607/readings/Wasserstein_Lazar_2016_The_American_Statistician.pdf), And choose one of the accompanying [rejoinders](http://byrneslab.net/classes/biol607/readings/p_value_statements.zip)

---FILE: schedule.html---
@@ -261,7 +261,7 @@ <h3>Week 4.</h3>
 </div>
 <div id=""week-5."" class=""section level3"">
 <h3>Week 5.</h3>
-<p><strong>Lecture:</strong> <a href=""lectures/09_the_t_test.html"">T tests</a>, <a href=""lectures/10_chisquare.html"">χ2 tests</a>, and p<br />
+<p><strong>Lecture:</strong> <a href=""lectures/09_the_t_test.html"">T tests</a>, <a href=""lectures/10_chisq.html"">χ2 tests</a>, and p<br />
 <strong>Lab Topic:</strong> <a href=""lab/05_chisq_t.html"">Statistical analysis functions for t and <span class=""math inline"">\(\chi^2\)</span> in R</a><br />
 <strong>Reading:</strong> W&amp;S 8-12, G&amp;W Chapter 10, 20<br />
 <strong>Discussion Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Wasserstein_Lazar_2016_The_American_Statistician.pdf"">ASA Statement on P-Values</a>, And choose one of the accompanying <a href=""http://byrneslab.net/classes/biol607/readings/p_value_statements.zip"">rejoinders</a> (sign up in <a href=""https://etherpad.wikimedia.org/p/607-t_tests"">here</a>) (also feel free to read them all) <strong>Etherpad</strong>: <a href=""https://etherpad.wikimedia.org/p/607-t_tests"" class=""uri"">https://etherpad.wikimedia.org/p/607-t_tests</a></p>"
biol607,biol607.github.io,ad2f3dda28c13670212aeda6e80973f759a61820,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-06T15:03:59Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-06T15:03:59Z,Fixed link to lecture 10,lectures/images/10/baby-home-frame.jpg;lectures/images/10/bacteria-in-a-petri-dish-compressed.jpg;lectures/lecture_10_chisq.Rmd;lectures/lecture_10_chisq.html,True,False,True,False,5,2,7,"---FILE: lectures/lecture_10_chisq.Rmd---
@@ -16,7 +16,7 @@ output:
 ## {data-background=""images/10/bacteria-in-a-petri-dish-compressed.jpg""}
 <!-- next year add contingency tables -->
 <br><br><br><br>
-<div style=""backround:white; fonti-size:1.5em"">Quantifying Goodness of Fit: the $$\chi^2$$</div>
+<div style=""backround:white; font-size:1.5em"">Quantifying Goodness of Fit: the $$\chi^2$$</div>
 ```{r prep, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
 library(knitr)
 opts_chunk$set(fig.height=4.5, comment=NA, 

---FILE: lectures/lecture_10_chisq.html---
@@ -79,8 +79,9 @@
 
 <section id=""section"" class=""slide level2"" data-background=""images/10/bacteria-in-a-petri-dish-compressed.jpg"">
 <h1></h1>
+<!-- next year add contingency tables -->
 <br><br><br><br>
-<div style=""backround:white; fonti-size:1.5em"">
+<div style=""backround:white; font-size:1.5em"">
 Quantifying Goodness of Fit: the <span class=""math display"">\[\chi^2\]</span>
 </div>
 </section>
@@ -155,7 +156,9 @@ <h1>Even Expectations</h1>
 </section>
 <section id=""assumptions-of-chi2-test"" class=""slide level2"">
 <h1>Assumptions of <span class=""math inline"">\(\chi^2\)</span> test</h1>
+<p align=""left"">
 Given that the goal is to detect deviations from expectations given normal error, this test has a few assumptions:
+</p>
 <div class=""fragment"">
 <ol type=""1"">
 <li>No expected values less that 1<br />"
biol607,biol607.github.io,90629b242a146c70ca6565d13ac0c3aeecc6f0c1,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-05T16:22:00Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-05T16:22:00Z,Fixed in class code links,schedule.Rmd;schedule.html,True,False,True,False,6,5,11,"---FILE: schedule.Rmd---
@@ -28,7 +28,7 @@ __Lab Topic:__ [Data import](lab/03_read_data_libraries.html) and [introduction
 __Reading:__ W&S Chapter 2, [Unwin 2008](http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf), G&W Chapters on [Data Vizualization](http://r4ds.had.co.nz/data-visualisation.html) and [Graphics for Communication](http://r4ds.had.co.nz/graphics-for-communication.html), [Ggplot2 cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)  
 __Optional Reading:__  [Friendly 2008 on History of Data Viz](http://byrneslab.net/classes/biol607/readings/Friendly_2008_dataviz_history.pdf)\
 __Etherpad:__ https://etherpad.wikimedia.org/p/dataviz  
-__In Class Code:__ [Loading Data](in_class_code_2016/03_load_data.R), [Intro to ggplot2](in_class_code/04_ggplot2_intro.R)\
+__In Class Code:__ [Loading Data](in_class_code_2016/03_load_data.R), [Intro to ggplot2](in_class_code_2016/04_ggplot2_intro.R)\
 __Homework:__ https://github.com/biol607/2016_homework_03_ggplot2  
   
 ### Week 4.   
@@ -38,7 +38,7 @@ __Reading:__ W&S 5-7, G&W Chapter 7, 16
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-hypotheses  
 __In Class Code:__ [Distributions and Power](in_class_code_2016/05_distributions_power.R)  
 __Quiz:__ http://tinyurl.com/hyp-pre-quiz  
-__In Class Code:__  [Distributions and Power](in_class_code/05_distributions_power.R)\
+__In Class Code:__  [Distributions and Power](in_class_code_2016/05_distributions_power.R)\
 __Homework:__ https://github.com/biol607/2016_homework_04_hypothesis_power
 
   
@@ -52,6 +52,7 @@ __Discussion Reading:__ [ASA Statement on P-Values](http://byrneslab.net/classes
 __Etherpad__: https://etherpad.wikimedia.org/p/607-t_tests
   
 ### Week 6.   
+__Lecture:__ Least Squares Linear Regression
 __Lab Topic:__ Linear regression, diagnostics, visualization  
 __Reading:__ W&S 16-17, G&W Chapter 21, 22  
   

---FILE: schedule.html---
@@ -245,7 +245,7 @@ <h3>Week 3.</h3>
 <strong>Reading:</strong> W&amp;S Chapter 2, <a href=""http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf"">Unwin 2008</a>, G&amp;W Chapters on <a href=""http://r4ds.had.co.nz/data-visualisation.html"">Data Vizualization</a> and <a href=""http://r4ds.had.co.nz/graphics-for-communication.html"">Graphics for Communication</a>, <a href=""https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf"">Ggplot2 cheat sheet</a><br />
 <strong>Optional Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Friendly_2008_dataviz_history.pdf"">Friendly 2008 on History of Data Viz</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/dataviz"" class=""uri"">https://etherpad.wikimedia.org/p/dataviz</a><br />
-<strong>In Class Code:</strong> <a href=""in_class_code_2016/03_load_data.R"">Loading Data</a>, <a href=""in_class_code/04_ggplot2_intro.R"">Intro to ggplot2</a><br />
+<strong>In Class Code:</strong> <a href=""in_class_code_2016/03_load_data.R"">Loading Data</a>, <a href=""in_class_code_2016/04_ggplot2_intro.R"">Intro to ggplot2</a><br />
 <strong>Homework:</strong> <a href=""https://github.com/biol607/2016_homework_03_ggplot2"" class=""uri"">https://github.com/biol607/2016_homework_03_ggplot2</a></p>
 </div>
 <div id=""week-4."" class=""section level3"">
@@ -256,7 +256,7 @@ <h3>Week 4.</h3>
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-hypotheses"" class=""uri"">https://etherpad.wikimedia.org/p/607-hypotheses</a><br />
 <strong>In Class Code:</strong> <a href=""in_class_code_2016/05_distributions_power.R"">Distributions and Power</a><br />
 <strong>Quiz:</strong> <a href=""http://tinyurl.com/hyp-pre-quiz"" class=""uri"">http://tinyurl.com/hyp-pre-quiz</a><br />
-<strong>In Class Code:</strong> <a href=""in_class_code/05_distributions_power.R"">Distributions and Power</a><br />
+<strong>In Class Code:</strong> <a href=""in_class_code_2016/05_distributions_power.R"">Distributions and Power</a><br />
 <strong>Homework:</strong> <a href=""https://github.com/biol607/2016_homework_04_hypothesis_power"" class=""uri"">https://github.com/biol607/2016_homework_04_hypothesis_power</a></p>
 </div>
 <div id=""week-5."" class=""section level3"">
@@ -268,7 +268,7 @@ <h3>Week 5.</h3>
 </div>
 <div id=""week-6."" class=""section level3"">
 <h3>Week 6.</h3>
-<p><strong>Lab Topic:</strong> Linear regression, diagnostics, visualization<br />
+<p><strong>Lecture:</strong> Least Squares Linear Regression <strong>Lab Topic:</strong> Linear regression, diagnostics, visualization<br />
 <strong>Reading:</strong> W&amp;S 16-17, G&amp;W Chapter 21, 22</p>
 </div>
 <div id=""week-7."" class=""section level3"">"
biol607,biol607.github.io,14d35ddc28eac5c735b5c395745272ac7ebfe57c,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-04T14:29:52Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-10-04T14:29:52Z,Fixed typos,lectures/08_testing_nhst_power.Rmd;lectures/08_testing_nhst_power.html,True,False,True,False,9,9,18,"---FILE: lectures/08_testing_nhst_power.Rmd---
@@ -279,7 +279,7 @@ p = `r sum(samp>=14.5)/length(samp)`
 
 1.  Null Hypothesis Tests & Other Distibrutions\
 \
-2.  <span style=""color=""red"">Test Statistics: Z-Tests</span> \
+2.  <span style=""color:red"">Test Statistics: Z-Tests</span> \
 \
 3.  Null Hypothesis Sigificance Testing\
 \
@@ -434,7 +434,7 @@ Egon Pearson
 \
 3.  Null Hypothesis Sigificance Testing\
 \
-4.  <span style=""color=""red""Statistical Power in a NHST framework</span> \
+4.  <span style=""color:red"">Statistical Power in a NHST framework</span> \
 \
 5. Power via Simulation\
 \
@@ -495,7 +495,7 @@ assuming a given $\alpha$!
 
 1.  Null Hypothesis Tests & Other Distibrutions\
 \
-2.  <span style=""color=""red"">Test Statistics: Z-Tests</span> \
+2.  <span style=""color:red"">Test Statistics: Z-Tests</span> \
 \
 3.  Null Hypothesis Sigificance Testing\
 \

---FILE: lectures/08_testing_nhst_power.html---
@@ -311,15 +311,15 @@ <h1>Simulated Distribution</h1>
 <section id=""testing-a-tapir"" class=""slide level2"">
 <h1>Testing a “Tapir”</h1>
 <p><img src=""08_testing_nhst_power_files/figure-revealjs/tapir_test-1.jpeg"" title="""" alt="""" width=""768"" /></p>
-<p>p = 0.01026</p>
+<p>p = 0.01042</p>
 </section>
 <section id=""today-1"" class=""slide level2"">
 <h1>Today</h1>
 <ol type=""1"">
 <li>Null Hypothesis Tests &amp; Other Distibrutions<br />
 <br />
 </li>
-<li>&lt;span style=“color=”red“&gt;Test Statistics: Z-Tests</span><br />
+<li><span style=""color:red"">Test Statistics: Z-Tests</span><br />
 <br />
 </li>
 <li>Null Hypothesis Sigificance Testing<br />
@@ -413,8 +413,8 @@ <h1>Our Corgis and Z</h1>
 <h1>Our Corgis and Z</h1>
 <p><img src=""08_testing_nhst_power_files/figure-revealjs/withTest-1.jpeg"" title="""" alt="""" width=""768"" /></p>
 <ul>
-<li>Z = 3/0.258 = 19.365</li>
-<li>p = 1.52354910^{-83}</li>
+<li>Z = 3/0.258 = 11.619</li>
+<li>p = 3.299758910^{-31}</li>
 </ul>
 </section>
 <section id=""today-2"" class=""slide level2"">
@@ -475,7 +475,7 @@ <h3>
 <li>Null Hypothesis Sigificance Testing<br />
 <br />
 </li>
-<li>&lt;span style=“color=”red“Statistical Power in a NHST framework</span><br />
+<li><span style=""color:red"">Statistical Power in a NHST framework</span><br />
 <br />
 </li>
 <li>Power via Simulation<br />
@@ -552,7 +552,7 @@ <h1>Today</h1>
 <li>Null Hypothesis Tests &amp; Other Distibrutions<br />
 <br />
 </li>
-<li>&lt;span style=“color=”red“&gt;Test Statistics: Z-Tests</span><br />
+<li><span style=""color:red"">Test Statistics: Z-Tests</span><br />
 <br />
 </li>
 <li>Null Hypothesis Sigificance Testing<br />"
biol607,biol607.github.io,434924ceed2cf76a8a11ef628039e64bfc849e87,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-30T21:45:59Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-30T21:45:59Z,Fixed spacing,schedule.Rmd;schedule.html,True,False,True,False,7,6,13,"---FILE: schedule.Rmd---
@@ -36,8 +36,8 @@ __Lecture:__ [Frequentist Hypothesis Testing](lectures/07_probability_hypotheses
 __Lab Topic:__ [Distributions in R, Frequentist Hypothesis testing via simulation](lab/05_hypothesis_power.html)  \
 __Reading:__ W&S 5-7, G&W Chapter 7, 16  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-hypotheses  
-__In Class Code:__ [Distributions and Power](in_class_code_2016/05_distributions_power.R)\
-__Quiz:__ http://tinyurl.com/hyp-pre-quiz\
+__In Class Code:__ [Distributions and Power](in_class_code_2016/05_distributions_power.R)  
+__Quiz:__ http://tinyurl.com/hyp-pre-quiz  
 __Homework:__ https://github.com/biol607/2016_homework_04_hypothesis_power
 
   
@@ -47,7 +47,8 @@ __Lecture:__ T,  χ2 tests, and p
 __Lab Topic:__ Simple hypothesis tests with data  
 __Reading:__ W&S 8-12, G&W Chapter 10, 20  
 __Discussion Reading:__ [ASA Statement on P-Values](http://byrneslab.net/classes/biol607/readings/Wasserstein_Lazar_2016_The_American_Statistician.pdf), And choose one of the accompanying [rejoinders](http://byrneslab.net/classes/biol607/readings/p_value_statements.zip)
-(sign up in class)  
+(sign up in [here](https://etherpad.wikimedia.org/p/607-t_tests))  (also feel free to read them all)
+__Etherpad__: https://etherpad.wikimedia.org/p/607-t_tests
   
 ### Week 6.   
 __Lab Topic:__ Linear regression, diagnostics, visualization  
@@ -59,7 +60,6 @@ __Lecture:__ Likelihoodist Inference, Fitting a line with Likelihood, Model Sele
 __Lab Topic:__ Calculating and visualizing Likelihoods, fitting a line with bbmle and glm  
 __Reading:__ W&S 20, G&W Chapter 18  
   
-  
 ### Week 8.   
 __Lecture:__ Bayesian Inference, Fitting a line with Bayesian techniques  
 __Lab Topic:__ Bayesian computation in R, Fitting a line with Bayesian techniques  

---FILE: schedule.html---
@@ -255,14 +255,15 @@ <h3>Week 4.</h3>
 <strong>Reading:</strong> W&amp;S 5-7, G&amp;W Chapter 7, 16<br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-hypotheses"" class=""uri"">https://etherpad.wikimedia.org/p/607-hypotheses</a><br />
 <strong>In Class Code:</strong> <a href=""in_class_code_2016/05_distributions_power.R"">Distributions and Power</a><br />
-<strong>Quiz:</strong> <a href=""http://tinyurl.com/hyp-pre-quiz\"" class=""uri"">http://tinyurl.com/hyp-pre-quiz\</a> <strong>Homework:</strong> <a href=""https://github.com/biol607/2016_homework_04_hypothesis_power"" class=""uri"">https://github.com/biol607/2016_homework_04_hypothesis_power</a></p>
+<strong>Quiz:</strong> <a href=""http://tinyurl.com/hyp-pre-quiz"" class=""uri"">http://tinyurl.com/hyp-pre-quiz</a><br />
+<strong>Homework:</strong> <a href=""https://github.com/biol607/2016_homework_04_hypothesis_power"" class=""uri"">https://github.com/biol607/2016_homework_04_hypothesis_power</a></p>
 </div>
 <div id=""week-5."" class=""section level3"">
 <h3>Week 5.</h3>
 <p><strong>Lecture:</strong> T, χ2 tests, and p<br />
 <strong>Lab Topic:</strong> Simple hypothesis tests with data<br />
 <strong>Reading:</strong> W&amp;S 8-12, G&amp;W Chapter 10, 20<br />
-<strong>Discussion Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Wasserstein_Lazar_2016_The_American_Statistician.pdf"">ASA Statement on P-Values</a>, And choose one of the accompanying <a href=""http://byrneslab.net/classes/biol607/readings/p_value_statements.zip"">rejoinders</a> (sign up in class)</p>
+<strong>Discussion Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Wasserstein_Lazar_2016_The_American_Statistician.pdf"">ASA Statement on P-Values</a>, And choose one of the accompanying <a href=""http://byrneslab.net/classes/biol607/readings/p_value_statements.zip"">rejoinders</a> (sign up in <a href=""https://etherpad.wikimedia.org/p/607-t_tests"">here</a>) (also feel free to read them all) <strong>Etherpad</strong>: <a href=""https://etherpad.wikimedia.org/p/607-t_tests"" class=""uri"">https://etherpad.wikimedia.org/p/607-t_tests</a></p>
 </div>
 <div id=""week-6."" class=""section level3"">
 <h3>Week 6.</h3>"
biol607,biol607.github.io,c548344cf38097c959b3b1a89d97f1fa75fbd8e8,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-29T16:30:33Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-29T16:30:33Z,Fixed tpo,lectures/08_testing_nhst_power.Rmd;lectures/08_testing_nhst_power.html;lectures/08_testing_nhst_power_files/figure-revealjs/tapir-1.jpeg;lectures/08_testing_nhst_power_files/figure-revealjs/tapir_test-1.jpeg,True,False,True,False,4,4,8,"---FILE: lectures/08_testing_nhst_power.Rmd---
@@ -368,13 +368,13 @@ normRef <- ggplot(mapping = aes(x=vals, y=dnorm(vals))) +
   xlab(""Z"") +
   ylab(""Proability Density\n"") +
   theme_bw(base_size=17)
-z <- round(5/(1/sqrt(15)),3)
+z <- round(3/(1/sqrt(15)),3)
 normRef
 ```
 
 > - Corgis: $\mu = 7, \sigma=1$, N=15 so $s_{\bar{Y}}=$ `r round(1/sqrt(15),3)`
 > - Sample, $\bar{Y} = 10$, 10-7 = 3
-> - Z = 3/`r round(1/sqrt(15),3)` = `r round(3/(1/sqrt(15)),3)`
+> - Z = 3/`r round(1/sqrt(15),3)` = `r z`
 
 ## Our Corgis and Z
 ```{r withTest}
@@ -576,7 +576,7 @@ ggplot(sim_data) +
   #scale_y_log10() +
   theme_bw(base_size=17) + 
   xlab(""Sample Size"") + 
-  geom_hline(yintercept=0.05)
+  geom_hline(yintercept=0.05, lwd=2, color=""red"", lty=2)
 ```
 
 ## Turn Many P-Values into Power!

---FILE: lectures/08_testing_nhst_power.html---
@@ -311,7 +311,7 @@ <h1>Simulated Distribution</h1>
 <section id=""testing-a-tapir"" class=""slide level2"">
 <h1>Testing a “Tapir”</h1>
 <p><img src=""08_testing_nhst_power_files/figure-revealjs/tapir_test-1.jpeg"" title="""" alt="""" width=""768"" /></p>
-<p>p = 0.01047</p>
+<p>p = 0.01026</p>
 </section>
 <section id=""today-1"" class=""slide level2"">
 <h1>Today</h1>"
biol607,biol607.github.io,cb31391d60bda05588ce886fdb8d7273f3d0ab19,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-27T15:03:13Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-27T15:03:13Z,Fixed title,lectures/07_probability_hypotheses.Rmd;lectures/07_probability_hypotheses.html,True,False,True,False,3,2,5,"---FILE: lectures/07_probability_hypotheses.Rmd---
@@ -1,5 +1,5 @@
 ---
-title: ""Probability and Frequentist Hypothesis Testing""
+title: 
 output:
   revealjs::revealjs_presentation:
     reveal_options:
@@ -22,6 +22,7 @@ output:
   2) Grok the frequentist paradigm versus likelihoodist v. bayesian
   3) Understand how we use integrated probability to calculate a p-value
   -->
+<!-- for next year, insert scientific method slides -->
 </center>
 <p align=""left"" style=""font-size:7pt; font-color:black;"">https://xkcd.com/882/</p>
 

---FILE: lectures/07_probability_hypotheses.html---
@@ -91,7 +91,7 @@ <h1>
   1) Understand what a point versus integrated probability means.
   2) Grok the frequentist paradigm versus likelihoodist v. bayesian
   3) Understand how we use integrated probability to calculate a p-value
-  -->
+  --> <!-- for next year, insert scientific method slides -->
 </center>
 <p align=""left"" style=""font-size:7pt; font-color:black;"">
 <a href=""https://xkcd.com/882/"" class=""uri"">https://xkcd.com/882/</a>"
biol607,biol607.github.io,753849aa9ca5d9d7160ae2988cbac020ce368969,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-22T14:54:42Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-22T14:54:42Z,fix cheatsheet link,schedule.Rmd;schedule.html,True,False,True,False,2,3,5,"---FILE: schedule.Rmd---
@@ -25,8 +25,7 @@ __Homework:__ https://github.com/biol607/2016_homework_02_sampling
 ###Week 3.   
 __Lecture:__ [Data visualization](lectures/05_data_viz_principles.pptx).\
 __Lab Topic:__ [Data import](lab/03_read_data_libraries.html) and [introduction to ggplot2](lab/04_ggplot_intro.html). Forcats and factors.  Data for lab [here](lab/data_03_04.zip).  
-__Reading:__ W&S Chapter 2, [Unwin 2008](http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf), G&W Chapters on [Data Vizualization](http://r4ds.had.co.nz/data-visualisation.html) and [Graphics for Communication](http://r4ds.had.co.nz/graphics-for-communication.html), [Ggplot2 cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfPaperpile
-)  
+__Reading:__ W&S Chapter 2, [Unwin 2008](http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf), G&W Chapters on [Data Vizualization](http://r4ds.had.co.nz/data-visualisation.html) and [Graphics for Communication](http://r4ds.had.co.nz/graphics-for-communication.html), [Ggplot2 cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)  
 __Optional Reading:__  [Friendly 2008 on History of Data Viz](http://byrneslab.net/classes/biol607/readings/Friendly_2008_dataviz_history.pdf)\
 __Etherpad:__ https://etherpad.wikimedia.org/p/dataviz  
 __Homework:__ https://github.com/biol607/2016_homework_03_ggplot2  

---FILE: schedule.html---
@@ -242,7 +242,7 @@ <h3>Week 2.</h3>
 <h3>Week 3.</h3>
 <p><strong>Lecture:</strong> <a href=""lectures/05_data_viz_principles.pptx"">Data visualization</a>.<br />
 <strong>Lab Topic:</strong> <a href=""lab/03_read_data_libraries.html"">Data import</a> and <a href=""lab/04_ggplot_intro.html"">introduction to ggplot2</a>. Forcats and factors. Data for lab <a href=""lab/data_03_04.zip"">here</a>.<br />
-<strong>Reading:</strong> W&amp;S Chapter 2, <a href=""http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf"">Unwin 2008</a>, G&amp;W Chapters on <a href=""http://r4ds.had.co.nz/data-visualisation.html"">Data Vizualization</a> and <a href=""http://r4ds.had.co.nz/graphics-for-communication.html"">Graphics for Communication</a>, <a href=""https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfPaperpile"">Ggplot2 cheat sheet</a><br />
+<strong>Reading:</strong> W&amp;S Chapter 2, <a href=""http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf"">Unwin 2008</a>, G&amp;W Chapters on <a href=""http://r4ds.had.co.nz/data-visualisation.html"">Data Vizualization</a> and <a href=""http://r4ds.had.co.nz/graphics-for-communication.html"">Graphics for Communication</a>, <a href=""https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf"">Ggplot2 cheat sheet</a><br />
 <strong>Optional Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Friendly_2008_dataviz_history.pdf"">Friendly 2008 on History of Data Viz</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/dataviz"" class=""uri"">https://etherpad.wikimedia.org/p/dataviz</a><br />
 <strong>Homework:</strong> <a href=""https://github.com/biol607/2016_homework_03_ggplot2"" class=""uri"">https://github.com/biol607/2016_homework_03_ggplot2</a></p>"
biol607,biol607.github.io,3e019af979644a26c136944db577d2fbdb06a681,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-22T02:49:30Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-22T02:49:30Z,Fixed typo,lectures/05_data_viz_principles.pptx,False,False,False,False,0,0,0,
biol607,biol607.github.io,989f6307ac16309b8871622523bed6e7a9da8db0,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-20T13:57:10Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-20T13:57:10Z,Fix line break,schedule.Rmd;schedule.html,True,False,True,False,3,2,5,"---FILE: schedule.Rmd---
@@ -21,7 +21,7 @@ __Etherpad:__ https://etherpad.wikimedia.org/p/sampling
 __Homework:__ https://github.com/biol607/2016_homework_02_sampling
 
 ###Week 3.   
-__Lecture:__ [Data visualization](lectures/05_data_viz_principles.pptx). 
+__Lecture:__ [Data visualization](lectures/05_data_viz_principles.pptx).  
 __Lab Topic:__ Data import and visualization, Introduction to ggplot2   
 __Reading:__ W&S Chapter 2, [Unwin 2008](http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf), G&W Chapters on [Data Vizualization](http://r4ds.had.co.nz/data-visualisation.html) and [Graphics for Communication](http://r4ds.had.co.nz/graphics-for-communication.html), [Ggplot2 cheat sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfPaperpile
 )  

---FILE: schedule.html---
@@ -238,7 +238,8 @@ <h3>Week 2.</h3>
 </div>
 <div id=""week-3."" class=""section level3"">
 <h3>Week 3.</h3>
-<p><strong>Lecture:</strong> <a href=""lectures/05_data_viz_principles.pptx"">Data visualization</a>. <strong>Lab Topic:</strong> Data import and visualization, Introduction to ggplot2<br />
+<p><strong>Lecture:</strong> <a href=""lectures/05_data_viz_principles.pptx"">Data visualization</a>.<br />
+<strong>Lab Topic:</strong> Data import and visualization, Introduction to ggplot2<br />
 <strong>Reading:</strong> W&amp;S Chapter 2, <a href=""http://byrneslab.net/classes/biol607/readings/Unwin_2008_dataviz.pdf"">Unwin 2008</a>, G&amp;W Chapters on <a href=""http://r4ds.had.co.nz/data-visualisation.html"">Data Vizualization</a> and <a href=""http://r4ds.had.co.nz/graphics-for-communication.html"">Graphics for Communication</a>, <a href=""https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfPaperpile"">Ggplot2 cheat sheet</a><br />
 <strong>Optional Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Friendly_2008_dataviz_history.pdf"">Friendly 2008 on History of Data Viz</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/dataviz"" class=""uri"">https://etherpad.wikimedia.org/p/dataviz</a></p>"
biol607,biol607.github.io,7dc70f946c83f9f391ff1c57757824b1212ef9ef,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-19T14:27:42Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-19T14:27:42Z,Fixed linebreaks in resources.,resources.Rmd;resources.html,True,False,True,False,6,4,10,"---FILE: resources.Rmd---
@@ -10,9 +10,9 @@ title: ""Resources, Blogs, and More""
 [STAT545](http://stat545.com/) UBC Course by Jenny Bryan that covers many similar topics to us - probably better - and more!
 
 ### Github
-[Using Git and Github with Rstudio](http://www.molecularecologist.com/2013/11/using-github-with-r-and-rstudio/)
-[Git and Github in Rstudio](http://www.datasurg.net/2015/07/13/rstudio-and-github/)
-[Happy with Git](http://happygitwithr.com/index.html). 2006. Jenny Bryan. Introduction to Git and Github for her class. Very detailed and walks you through each step.
+[Using Git and Github with Rstudio](http://www.molecularecologist.com/2013/11/using-github-with-r-and-rstudio/)\
+[Git and Github in Rstudio](http://www.datasurg.net/2015/07/13/rstudio-and-github/)\
+[Happy with Git](http://happygitwithr.com/index.html). 2006. Jenny Bryan. Introduction to Git and Github for her class. Very detailed and walks you through each step.  
 
 ### Offline Books
 

---FILE: resources.html---
@@ -228,7 +228,9 @@ <h3>R</h3>
 </div>
 <div id=""github"" class=""section level3"">
 <h3>Github</h3>
-<p><a href=""http://www.molecularecologist.com/2013/11/using-github-with-r-and-rstudio/"">Using Git and Github with Rstudio</a> <a href=""http://www.datasurg.net/2015/07/13/rstudio-and-github/"">Git and Github in Rstudio</a> <a href=""http://happygitwithr.com/index.html"">Happy with Git</a>. 2006. Jenny Bryan. Introduction to Git and Github for her class. Very detailed and walks you through each step.</p>
+<p><a href=""http://www.molecularecologist.com/2013/11/using-github-with-r-and-rstudio/"">Using Git and Github with Rstudio</a><br />
+<a href=""http://www.datasurg.net/2015/07/13/rstudio-and-github/"">Git and Github in Rstudio</a><br />
+<a href=""http://happygitwithr.com/index.html"">Happy with Git</a>. 2006. Jenny Bryan. Introduction to Git and Github for her class. Very detailed and walks you through each step.</p>
 </div>
 <div id=""offline-books"" class=""section level3"">
 <h3>Offline Books</h3>"
biol607,biol607.github.io,9b37f7a59e29ce67b44102fbea34d324681ad0fe,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-19T14:24:52Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-19T14:24:52Z,Fixing index.,index.html;index_files/figure-html/demoplot-1.png,False,False,False,False,10,12,22,
biol607,biol607.github.io,1b297bcf4c6374974994849c9827b46011cc88c3,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-19T14:23:14Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-19T14:23:14Z,Fix frontpage image.,index_files/figure-html/demoplot-1.png,False,False,False,False,0,0,0,
biol607,biol607.github.io,5165aa7530237fc4dd8920a623bce42850c04754,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-15T12:34:11Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-15T12:34:11Z,Fixed bad link,schedule.Rmd;schedule.html,True,False,True,False,2,2,4,"---FILE: schedule.Rmd---
@@ -16,7 +16,7 @@ __Etherpad:__ https://etherpad.wikimedia.org/p/607-intro
 __Lecture:__ [Sampling](lectures/03_sampling_lecture.html) and [Simulation for Estimation](lectures/04_simulation_estimation.html). Descriptive statistics, and the creation of good observational sampling designs.   
 __Lab Topic:__ Libraries in R. R as a Data Importing Tool, Dplyr. Forcats.  
 __Reading:__ W&S 1,3-4, G&W Chapter 5, 11, 18, Dplyr cheat sheet  
-__Optional Reading:__ [Cumming et al. 2007 on SDs, SEs, and CIs](http://byrneslab.net/classes/biol-607/readings/Cumming_2007_error.pdf), [Simpler Coding with Pipes](https://www.r-statistics.com/2014/08/simpler-r-coding-with-pipes-the-present-and-future-of-the-magrittr-package/), [Managing Data Frames with the Dplyr package](https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html)  
+__Optional Reading:__ [Cumming et al. 2007 on SDs, SEs, and CIs](http://byrneslab.net/classes/biol607/readings/Cumming_2007_error.pdf), [Simpler Coding with Pipes](https://www.r-statistics.com/2014/08/simpler-r-coding-with-pipes-the-present-and-future-of-the-magrittr-package/), [Managing Data Frames with the Dplyr package](https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/sampling  
   
 ###Week 3.   

---FILE: schedule.html---
@@ -228,7 +228,7 @@ <h3>Week 2.</h3>
 <p><strong>Lecture:</strong> <a href=""lectures/03_sampling_lecture.html"">Sampling</a> and <a href=""lectures/04_simulation_estimation.html"">Simulation for Estimation</a>. Descriptive statistics, and the creation of good observational sampling designs.<br />
 <strong>Lab Topic:</strong> Libraries in R. R as a Data Importing Tool, Dplyr. Forcats.<br />
 <strong>Reading:</strong> W&amp;S 1,3-4, G&amp;W Chapter 5, 11, 18, Dplyr cheat sheet<br />
-<strong>Optional Reading:</strong> <a href=""http://byrneslab.net/classes/biol-607/readings/Cumming_2007_error.pdf"">Cumming et al. 2007 on SDs, SEs, and CIs</a>, <a href=""https://www.r-statistics.com/2014/08/simpler-r-coding-with-pipes-the-present-and-future-of-the-magrittr-package/"">Simpler Coding with Pipes</a>, <a href=""https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html"">Managing Data Frames with the Dplyr package</a><br />
+<strong>Optional Reading:</strong> <a href=""http://byrneslab.net/classes/biol607/readings/Cumming_2007_error.pdf"">Cumming et al. 2007 on SDs, SEs, and CIs</a>, <a href=""https://www.r-statistics.com/2014/08/simpler-r-coding-with-pipes-the-present-and-future-of-the-magrittr-package/"">Simpler Coding with Pipes</a>, <a href=""https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html"">Managing Data Frames with the Dplyr package</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/sampling"" class=""uri"">https://etherpad.wikimedia.org/p/sampling</a></p>
 </div>
 <div id=""week-3."" class=""section level3"">"
biol607,biol607.github.io,719921a779c0d8f5f20161d08ea208a205b85601,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-12T15:55:24Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-12T15:55:24Z,Fixing lecture numbers,lectures/02a_r_intro_list_matrix_df.Rmd;lectures/02a_r_intro_list_matrix_df.html;schedule.Rmd;schedule.html,True,False,True,False,2,2,4,"---FILE: schedule.Rmd---
@@ -8,7 +8,7 @@
       
 ### Week 1.  
 __Lecture:__ [How do we use data to understand how the world works?](lectures/01_intro.html)
-__Lab:__ [Intro to R](lectures/02_r_intro.html). [Matrices, Lists and Data Frames](lectures/03_r_intro_list_matrix_df.html). [Introduction to Markdown](lab/01_markdown_intro.html)    
+__Lab:__ [Intro to R](lectures/02_r_intro.html). [Matrices, Lists and Data Frames](lectures/02a_r_intro_list_matrix_df.html). [Introduction to Markdown](lab/01_markdown_intro.html)    
 __Reading:__ G&W  [Preface](http://r4ds.had.co.nz/introduction.html), [Intro](http://r4ds.had.co.nz/explore-intro.html), [Workflow basics](http://r4ds.had.co.nz/workflow-basics.html), [Vectors](http://r4ds.had.co.nz/vectors.html), and [Markdown](http://r4ds.had.co.nz/r-markdown.html) Chapters, [RMarkdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)  
 __Etherpad:__ https://etherpad.wikimedia.org/p/607-intro
   

---FILE: schedule.html---
@@ -219,7 +219,7 @@ <h1 class=""title toc-ignore"">Course Schedule and Readings</h1>
 <p>W&amp;S = Whitlock and Schluter, G&amp;W = <a href=""http://r4ds.had.co.nz"">Grolemund and Wickham</a>, U/P for linked pdfs = biol607</p>
 <div id=""week-1."" class=""section level3"">
 <h3>Week 1.</h3>
-<p><strong>Lecture:</strong> <a href=""lectures/01_intro.html"">How do we use data to understand how the world works?</a> <strong>Lab:</strong> <a href=""lectures/02_r_intro.html"">Intro to R</a>. <a href=""lectures/03_r_intro_list_matrix_df.html"">Matrices, Lists and Data Frames</a>. <a href=""lab/01_markdown_intro.html"">Introduction to Markdown</a><br />
+<p><strong>Lecture:</strong> <a href=""lectures/01_intro.html"">How do we use data to understand how the world works?</a> <strong>Lab:</strong> <a href=""lectures/02_r_intro.html"">Intro to R</a>. <a href=""lectures/02a_r_intro_list_matrix_df.html"">Matrices, Lists and Data Frames</a>. <a href=""lab/01_markdown_intro.html"">Introduction to Markdown</a><br />
 <strong>Reading:</strong> G&amp;W <a href=""http://r4ds.had.co.nz/introduction.html"">Preface</a>, <a href=""http://r4ds.had.co.nz/explore-intro.html"">Intro</a>, <a href=""http://r4ds.had.co.nz/workflow-basics.html"">Workflow basics</a>, <a href=""http://r4ds.had.co.nz/vectors.html"">Vectors</a>, and <a href=""http://r4ds.had.co.nz/r-markdown.html"">Markdown</a> Chapters, <a href=""https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf"">RMarkdown Cheat Sheet</a><br />
 <strong>Etherpad:</strong> <a href=""https://etherpad.wikimedia.org/p/607-intro"" class=""uri"">https://etherpad.wikimedia.org/p/607-intro</a></p>
 </div>"
biol607,biol607.github.io,7f27008287f053bafa753a4ca18625e2b0c87310,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-08T02:44:34Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-08T02:44:34Z,"fixing error in 1st R intro`
`",lectures/02_r_intro.Rmd;lectures/02_r_intro.html,True,False,True,False,3,153,156,"---FILE: lectures/02_r_intro.Rmd---
@@ -269,69 +269,3 @@ Last, often you just want some summary information about your vector. You'll wan
 ```{r}
 summary(my_numbers)
 ```
-
-
-###I'm all vectorized. What does this have to do with data?
-
-We've talked before about how vectors are like the columns in a spreadsheet. Those 'sheets' themselves within R care called data frames. Let's look at a sample data frame.
-
-```{r}
-#load the data
-data(mtcars)
-
-#look at the top 6 rows
-head(mtcars)
-```
-
-So, I've loaded a data set about cars. And now we're looking at the first six lines using the head function. If you want, try typing `mtcars` without anything else. It runs off the screen! Hence, `head` is a useful function.
-
-There are other ways we can get information about the data frame that you will use time and time and time again to diagnose what's going wrong.
-
-```{r}
-summary(mtcars)
-
-#str is life
-str(mtcars)
-```
-
-Summary gives you some nice information about each column of `mtcars`. But `str` is the real star of the show. Any time your code borks on you and you think it's because of one of the objects you are passing to it, use `str` as it will give you a rich set of information about object types, values, etc. Often your object is not doing what you think it's doing.
-
-###So, I've got data. How do I use it?
-
-First off, how do we work with individual columns?  We have a few ways to access them.
-
-```{r}
-mtcars[[""mpg""]]
-```
-
-This is kind of like the [] notation of before, but instead we use [[]] with a quoted variable name.
-
-```{r}
-mtcars$mpg
-```
-
-This is a way we can not worry about quotes (unless you have spaces in column names, but you won't will you?) to also get the values in a column.  Last, as we see in a spreadsheet, there are rows and columns - a matrix if you will. We can take advantage of this, as r uses the `[row, column]` formulation of indexing matrices.  So
-
-```{r}
-mtcars[1,1]
-```
-
-gives us that first value of mtcars. To get the second row and then second column, we can use similar notation.
-
-```{r}
-#columns
-mtcars[,2]
-
-#rows
-mtcars[2,]
-```
-
-By leaving out a row or column number, it means, give *all* of the values in that row or column to us.  We can of course be more specific.
-
-```{r}
-mtcars[1:5,1]
-```
-
-Yes, we can use vectors of numbers as rows and columns to get many of them.
-
-Great! Now that you've got those basics, futz around with the rows and columns of mtcars. Get information about each of them, try applying arithmatic and functions, and see what shakes out!

---FILE: lectures/02_r_intro.html---
@@ -278,105 +278,21 @@ <h3>Combining values into larger objects</h3>
 <pre><code>##  [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0</code></pre>
 <p>Second, 10 random numbers between 0 and 100</p>
 <pre class=""r""><code>runif(10, min = 0, max = 100)</code></pre>
-<pre><code>##  [1] 32.359810 26.084500 97.131662 49.051413  2.539534 80.607729 93.982156
-##  [8] 33.973858 85.676577 98.077677</code></pre>
+<pre><code>##  [1]  5.0449549 53.6507961 73.7152073 91.3051059 84.0075868 86.7884400
+##  [7] 37.2419118 34.7859719  0.8265922 41.9767806</code></pre>
 <p>Now, vectors are neat, as they allow us to introduce two more concepts. First, some functions take vectors as input, and return other types of objects. For example, let’s say we wanted to sum everything in <code>my_vector</code> above. And then get the average of a bunch of random numbers between 0 and 100</p>
 <pre class=""r""><code>sum(my_vector)</code></pre>
 <pre><code>## [1] 5050</code></pre>
 <pre class=""r""><code>#a function in a function!
 #oh my!
 mean(runif(10, min = 0, max = 100))</code></pre>
-<pre><code>## [1] 48.2402</code></pre>
+<pre><code>## [1] 43.65547</code></pre>
 <p>OH! Notice I nested a function inside of a function. YES! You can do that. But only when you <strong>really</strong> need to. To keep track of things, it’s often better practice to create an object with a variable name that has meaning to you, and <em>then</em> feed that as an argument to another function.</p>
 <p>Last, often you just want some summary information about your vector. You’ll want to do this for many more complex objects in the future as well. Fortunately, there’s a function for that. Summary!</p>
 <pre class=""r""><code>summary(my_numbers)</code></pre>
 <pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 ##    5.00    6.25    7.50    7.50    8.75   10.00</code></pre>
 </div>
-<div id=""im-all-vectorized.-what-does-this-have-to-do-with-data"" class=""section level3"">
-<h3>I’m all vectorized. What does this have to do with data?</h3>
-<p>We’ve talked before about how vectors are like the columns in a spreadsheet. Those ‘sheets’ themselves within R care called data frames. Let’s look at a sample data frame.</p>
-<pre class=""r""><code>#load the data
-data(mtcars)
-
-#look at the top 6 rows
-head(mtcars)</code></pre>
-<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
-## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
-## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
-## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
-## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
-## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
-## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
-<p>So, I’ve loaded a data set about cars. And now we’re looking at the first six lines using the head function. If you want, try typing <code>mtcars</code> without anything else. It runs off the screen! Hence, <code>head</code> is a useful function.</p>
-<p>There are other ways we can get information about the data frame that you will use time and time and time again to diagnose what’s going wrong.</p>
-<pre class=""r""><code>summary(mtcars)</code></pre>
-<pre><code>##       mpg             cyl             disp             hp       
-##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  
-##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  
-##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  
-##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  
-##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  
-##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  
-##       drat             wt             qsec             vs        
-##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  
-##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  
-##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  
-##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  
-##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  
-##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  
-##        am              gear            carb      
-##  Min.   :0.0000   Min.   :3.000   Min.   :1.000  
-##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  
-##  Median :0.0000   Median :4.000   Median :2.000  
-##  Mean   :0.4062   Mean   :3.688   Mean   :2.812  
-##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  
-##  Max.   :1.0000   Max.   :5.000   Max.   :8.000</code></pre>
-<pre class=""r""><code>#str is life
-str(mtcars)</code></pre>
-<pre><code>## 'data.frame':    32 obs. of  11 variables:
-##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
-##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
-##  $ disp: num  160 160 108 258 360 ...
-##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
-##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
-##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
-##  $ qsec: num  16.5 17 18.6 19.4 17 ...
-##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
-##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
-##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
-##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>
-<p>Summary gives you some nice information about each column of <code>mtcars</code>. But <code>str</code> is the real star of the show. Any time your code borks on you and you think it’s because of one of the objects you are passing to it, use <code>str</code> as it will give you a rich set of information about object types, values, etc. Often your object is not doing what you think it’s doing.</p>
-</div>
-<div id=""so-ive-got-data.-how-do-i-use-it"" class=""section level3"">
-<h3>So, I’ve got data. How do I use it?</h3>
-<p>First off, how do we work with individual columns? We have a few ways to access them.</p>
-<pre class=""r""><code>mtcars[[&quot;mpg&quot;]]</code></pre>
-<pre><code>##  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2
-## [15] 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4
-## [29] 15.8 19.7 15.0 21.4</code></pre>
-<p>This is kind of like the [] notation of before, but instead we use [[]] with a quoted variable name.</p>
-<pre class=""r""><code>mtcars$mpg</code></pre>
-<pre><code>##  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2
-## [15] 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4
-## [29] 15.8 19.7 15.0 21.4</code></pre>
-<p>This is a way we can not worry about quotes (unless you have spaces in column names, but you won’t will you?) to also get the values in a column. Last, as we see in a spreadsheet, there are rows and columns - a matrix if you will. We can take advantage of this, as r uses the <code>[row, column]</code> formulation of indexing matrices. So</p>
-<pre class=""r""><code>mtcars[1,1]</code></pre>
-<pre><code>## [1] 21</code></pre>
-<p>gives us that first value of mtcars. To get the second row and then second column, we can use similar notation.</p>
-<pre class=""r""><code>#columns
-mtcars[,2]</code></pre>
-<pre><code>##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4</code></pre>
-<pre class=""r""><code>#rows
-mtcars[2,]</code></pre>
-<pre><code>##               mpg cyl disp  hp drat    wt  qsec vs am gear carb
-## Mazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4</code></pre>
-<p>By leaving out a row or column number, it means, give <em>all</em> of the values in that row or column to us. We can of course be more specific.</p>
-<pre class=""r""><code>mtcars[1:5,1]</code></pre>
-<pre><code>## [1] 21.0 21.0 22.8 21.4 18.7</code></pre>
-<p>Yes, we can use vectors of numbers as rows and columns to get many of them.</p>
-<p>Great! Now that you’ve got those basics, futz around with the rows and columns of mtcars. Get information about each of them, try applying arithmatic and functions, and see what shakes out!</p>
-</div>
 
 
 "
biol607,biol607.github.io,ac4785d8133c14f9df302006209cac3127000813,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-08T02:34:55Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-08T02:34:55Z,fixing cover image,index.html,False,False,False,False,10,12,22,
biol607,biol607.github.io,7a3770b87c237283cb57b5eb81c055fdd702c068,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-06T14:29:33Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-06T14:29:33Z,Fixed course name,index.Rmd;index.html,True,False,True,False,3,3,6,"---FILE: index.Rmd---
@@ -1,5 +1,5 @@
 ---
-title: ""Intro to Data Science for Biology""
+title: ""Biol 607: Intro to Computational Data Analysis for Biology""
 ---
 
 ```{r demoplot, fig.align='center', echo=FALSE}"
biol607,biol607.github.io,ab7eab846a67c26fe80b996689ffbe27c33b1576,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-06T04:08:05Z,Jarrett Byrnes,jarrett.byrnes@umb.edu,2016-09-06T04:08:05Z,Fixing image size problems,lectures/01_intro.Rmd;lectures/01_intro.html;lectures/images/01/Life Cycle DiagramA.png,True,False,True,False,7,6,13,"---FILE: lectures/01_intro.Rmd---
@@ -9,6 +9,7 @@ output:
     center: false
     transition: fade
     self_contained: false
+    lib_dir: libs
     css: style.css
 
 ---

---FILE: lectures/01_intro.html---
@@ -7,7 +7,7 @@
   <meta name=""apple-mobile-web-app-capable"" content=""yes"">
   <meta name=""apple-mobile-web-app-status-bar-style"" content=""black-translucent"">
   <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"">
-  <link rel=""stylesheet"" href=""01_intro_files/reveal.js-3.3.0/css/reveal.css""/>
+  <link rel=""stylesheet"" href=""libs/reveal.js-3.3.0/css/reveal.css""/>
 
 
 <style type=""text/css"">
@@ -48,7 +48,7 @@
 code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
 </style>
 
-<link rel=""stylesheet"" href=""01_intro_files/reveal.js-3.3.0/css/theme/white.css"" id=""theme"">
+<link rel=""stylesheet"" href=""libs/reveal.js-3.3.0/css/theme/white.css"" id=""theme"">
 
 
   <!-- some tweaks to reveal css -->
@@ -101,11 +101,11 @@
       var link = document.createElement( 'link' );
       link.rel = 'stylesheet';
       link.type = 'text/css';
-      link.href = window.location.search.match( /print-pdf/gi ) ? '01_intro_files/reveal.js-3.3.0/css/print/pdf.css' : '01_intro_files/reveal.js-3.3.0/css/print/paper.css';
+      link.href = window.location.search.match( /print-pdf/gi ) ? 'libs/reveal.js-3.3.0/css/print/pdf.css' : 'libs/reveal.js-3.3.0/css/print/paper.css';
       document.getElementsByTagName( 'head' )[0].appendChild( link );
     </script>
     <!--[if lt IE 9]>
-    <script src=""01_intro_files/reveal.js-3.3.0/lib/js/html5shiv.js""></script>
+    <script src=""libs/reveal.js-3.3.0/lib/js/html5shiv.js""></script>
     <![endif]-->
 
 </head>
@@ -712,8 +712,8 @@ <h1>Questions</h1>
     </div>
   </div>
 
-  <script src=""01_intro_files/reveal.js-3.3.0/lib/js/head.min.js""></script>
-  <script src=""01_intro_files/reveal.js-3.3.0/js/reveal.js""></script>
+  <script src=""libs/reveal.js-3.3.0/lib/js/head.min.js""></script>
+  <script src=""libs/reveal.js-3.3.0/js/reveal.js""></script>
 
   <script>
 "
