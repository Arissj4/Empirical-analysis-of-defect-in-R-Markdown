repo_owner,repo_name,commit_hash,author_name,author_email,author_date,committer_name,committer_email,committer_date,message,filenames,touches_rmd,touches_r,touches_r_or_rmd,is_merge,added,deleted,changed,diff
nipraxis,textbook,9da53ac9c478f5f02b2cc31de0fc6156039d8e45,Matthew Brett,matthew.brett@gmail.com,2023-09-13T19:08:19Z,Matthew Brett,matthew.brett@gmail.com,2023-09-13T19:08:39Z,Fix link to FMRI love article,glm_intro.Rmd,True,False,True,False,1,1,2,"---FILE: glm_intro.Rmd---
@@ -18,7 +18,7 @@ jupyter:
 
 For more detail on the General Linear Model, see [The general linear model and
 fMRI: Does love last
-forever?](http://matthew.dynevor.org/_downloads/does_glm_love.pdf).
+forever?](http://matthew.dynevor.org/research/articles/does_glm_love.pdf).
 
 ```{python}
 # Import numerical and plotting libraries"
nipraxis,textbook,8fff4e0ca346ee63c0aaeb6a441eaf454ebb842f,Chris Markiewicz,effigies@gmail.com,2023-08-25T23:14:56Z,Chris Markiewicz,effigies@gmail.com,2023-08-25T23:14:56Z,"FIX: Recursively update, rather than override, default theme options",_config.yml,False,False,False,False,1,0,1,"---FILE: _config.yml---
@@ -120,6 +120,7 @@ launch_buttons:
   thebe: true
 
 sphinx:
+  recursive_update: true
   config:
     html_theme_options:
       logo:"
nipraxis,textbook,1e38861e1ac995ede25c4bffe60cb8d67bd175a1,Matthew Brett,matthew.brett@gmail.com,2023-04-24T13:57:25Z,Matthew Brett,matthew.brett@gmail.com,2023-04-24T21:49:59Z,"Fix for new Scipy, affine_transform",optimizing_space.Rmd,True,False,True,False,7,3,10,"---FILE: optimizing_space.Rmd---
@@ -7,9 +7,9 @@ jupyter:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.10.3
+      jupytext_version: 1.14.5
   kernelspec:
-    display_name: Python 3
+    display_name: Python 3 (ipykernel)
     language: python
     name: python3
 ---
@@ -390,7 +390,11 @@ def fancy_x_trans_slice(img_slice, x_vox_trans):
         positive or negative, and does not need to be integer value.
     """"""
     # Resample image using bilinear interpolation (order=1)
-    trans_slice = snd.affine_transform(img_slice, [1, 1], [-x_vox_trans, 0], order=1)
+    trans_slice = snd.affine_transform(
+        img_slice,
+        [1, 1],
+        [float(-x_vox_trans), 0],
+        order=1)
     return trans_slice
 ```
 "
nipraxis,textbook,f66ae67f7d4a03d6672d71be1a3dccc0a1a7ce6a,Matthew Brett,matthew.brett@gmail.com,2023-03-20T11:30:08Z,Matthew Brett,matthew.brett@gmail.com,2023-03-20T11:30:08Z,Fix notation,regression_notation.Rmd,True,False,True,False,1,1,2,"---FILE: regression_notation.Rmd---
@@ -231,7 +231,7 @@ $$
 \vec{\hat{y}} = [ \hat{y}_1, \hat{y}_2, ..., \hat{y}_n ]
 $$
 
-Read the $|hat{ }$ over the $y$ as ""estimated"", so $\hat{y}_1$ is our
+Read the $\hat{ }$ over the $y$ as ""estimated"", so $\hat{y}_1$ is our
 *estimate* for $y_1$, given the model.
 
 Our regression model says each fitted value comes about by multiplying the"
nipraxis,textbook,87dcf16c4a252422112a4e7cccc748c2fb666a9b,Matthew Brett,matthew.brett@gmail.com,2022-09-24T23:12:23Z,Matthew Brett,matthew.brett@gmail.com,2022-09-24T23:12:23Z,Fix links and latex markup in rotations page,rotation_2d_3d.md,False,False,False,False,15,11,26,"---FILE: rotation_2d_3d.md---
@@ -16,7 +16,9 @@ jupyter:
 
 ## Rotations in two dimensions
 
-See: [rotation in 2d] and [Wikipedia on rotation matrices].
+See: [rotation in
+2d](https://matthew-brett.github.io/teaching/rotation_2d.html) and [Wikipedia
+on rotation matrices](https://en.wikipedia.org/wiki/Rotation_matrix).
 
 In two dimensions, rotating a vector $\theta$ around the origin can be
 expressed as a 2 by 2 transformation matrix:
@@ -54,12 +56,14 @@ See [rotation in 2D] for a visual proof.
 
 ## Rotations in three dimensions
 
-Rotations in three dimensions extend simply from two dimensions.  
-Consider a [right-handed] set of x, y, z axes, maybe forming the x axis with your right thumb, the y axis with your index finger, and the z axis with your middle
-finger.
-Now look down the z axis, from positive z toward negative z.
-You see the x and y axes pointing right and up respectively, on a plane in front of you.
-A rotation around z leaves z unchanged, but changes x and y according to the 2D rotation formula above:
+Rotations in three dimensions extend simply from two dimensions.
+
+Consider a [right-handed] set of x, y, z axes, maybe forming the x axis with
+your right thumb, the y axis with your index finger, and the z axis with your
+middle finger. Now look down the z axis, from positive z toward negative z. You
+see the x and y axes pointing right and up respectively, on a plane in front of
+you. A rotation around z leaves z unchanged, but changes x and y according to
+the 2D rotation formula above:
 
 $$
 R_z(\theta) =
@@ -88,8 +92,8 @@ R_x(\theta) = \begin{bmatrix}
 $$
 
 Now consider a rotation around the y axis.   We look from positive y down the
-y axis to the z and x axes, pointing right and up respectively.  \$z\$ replaces
-\$x\$ in the 2D formula, and \$x\$ replaces \$y\$:
+y axis to the z and x axes, pointing right and up respectively.  $z$ replaces
+$x$ in the 2D formula, and $x$ replaces $y$:
 
 $$
 z' = z \cos \theta - x \sin \theta \\
@@ -105,7 +109,7 @@ R_y(\theta) = \begin{bmatrix}
 $$
 
 We can combine rotations with matrix multiplication. For example, here is an
-rotation of \$gamma\$ radians around the x axis:
+rotation of $gamma$ radians around the x axis:
 
 $$
 \begin{bmatrix}
@@ -125,7 +129,7 @@ z\\
 \end{bmatrix}
 $$
 
-We could then apply a rotation of \$phi\$ radians around the y axis:
+We could then apply a rotation of $phi$ radians around the y axis:
 
 $$
 \begin{bmatrix}"
nipraxis,textbook,942d8da99ca4bfe1ef1ec682d3b5f7070eb8acd4,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:58:57Z,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:58:57Z,Another attempted fix to nans link,whole_image_statistics.Rmd,True,False,True,False,1,1,2,"---FILE: whole_image_statistics.Rmd---
@@ -201,7 +201,7 @@ plt.imshow(t_3d[:, :, 15], cmap='gray')
 
 Notice the white areas at the edge of the image.  These are voxels where the t
 value is `nan` – [Not a number](https://en.wikipedia.org/wiki/NaN).  See also
-[Not a number](nans.md).  `nan` values arise when all the scans have 0 at this
+[Not a number](nans).  `nan` values arise when all the scans have 0 at this
 voxel, so the numerator and denominator of the t statistic are both 0.
 
 ```{python}"
nipraxis,textbook,9b87d847c84c31080afa024ea33ae092e4b77680,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:43:56Z,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:43:56Z,Fix links to nans page again,whole_image_statistics.Rmd,True,False,True,False,1,1,2,"---FILE: whole_image_statistics.Rmd---
@@ -201,7 +201,7 @@ plt.imshow(t_3d[:, :, 15], cmap='gray')
 
 Notice the white areas at the edge of the image.  These are voxels where the t
 value is `nan` – [Not a number](https://en.wikipedia.org/wiki/NaN).  See also
-[Not a number](nans.Rmd).  `nan` values arise when all the scans have 0 at this
+[Not a number](nans.md).  `nan` values arise when all the scans have 0 at this
 voxel, so the numerator and denominator of the t statistic are both 0.
 
 ```{python}"
nipraxis,textbook,cdb9af547710c2c2e31e639c77a7290ec21012a9,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:38:43Z,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:38:43Z,Rework slice time correction,slice_timing.Rmd,True,False,True,False,25,24,49,"---FILE: slice_timing.Rmd---
@@ -71,9 +71,9 @@ plt.xlabel('x axis')
 plt.ylabel('z axis');
 ```
 
-The scanner acquired the slices in interleaved order, first acquiring slice
-index 0, 2, 4, … 22 (where 0 is the bottom slice) then acquiring slices 1, 3,
-5, .. 23 [^how-we-know].
+The scanner acquired the slices (planes) in interleaved order, first acquiring
+slice index 0, 2, 4, … 22 (where 0 is the bottom slice) then acquiring slices
+1, 3, 5, .. 23 [^how-we-know].
 
 [^how-we-know]: We could not find the slice acquisition order noted in the
 paper about the [relevant
@@ -125,8 +125,8 @@ time_for_single_slice = TR / n_z_slices
 time_for_single_slice
 ```
 
-The times of acquisition of first and second slices (slice 0 and slice 1) will
-be:
+The *times of acquisition* of first and second slices in *space* (slice 0 and
+slice 1) will be:
 
 ```{python}
 time_for_slice_0 = 0
@@ -147,24 +147,24 @@ How can we make a new 4D time series, where all the slices in each volume
 correspond to our best guess at what these slices would have looked like, if we
 had acquired them all at the same time?
 
-This is the job of the *slice timing correction*.
+This is the job of *slice timing correction*.
 
 
 ## Slice timing is interpolation in time
 
 
-Let’s first get a time series from the bottom slice.  Here’s what the bottom
-slice looks like, for the first volume:
+Let’s first get a time series from the bottom slice (in space).  Here’s what
+the bottom slice looks like, for the first volume:
 
 ```{python}
 plt.imshow(vol0[:, :, 0])
 plt.title('Vol 0, z slice 0');
 ```
 
-We are going to collect a time series from a sample voxel from this slice, and
-the slice above it (slice 1):
+We are going to collect a voxel time series from a sample voxel from this
+slice, and the slice above it (slice 1):
 
-Our sample voxel coordinates
+Our sample voxel coordinates:
 
 ```{python}
 vox_x = 28  # voxel coordinate in first dimension
@@ -190,8 +190,8 @@ time_course_slice_0 = data[vox_x, vox_y, 0, :]
 time_course_slice_1 = data[vox_x, vox_y, 1, :]
 ```
 
-The times of acquisition of the voxels for slice 0 are at the beginning of each
-TR:
+The *times* of acquisition of the voxels for slice 0 are at the beginning of
+each TR:
 
 ```{python}
 vol_nos = np.arange(data.shape[-1])
@@ -273,11 +273,10 @@ Now we need to work out where these lines cross the slice 1 time course.
 
 This is where we can use [Linear
 interpolation](https://matthew-brett.github.io/teaching/linear_interpolation.html).
-This is *interpolation* because we are estimating a value from the slice 1 time
-course, that is between two points that we have values for (inter == between).
-It is *linear* interpolation because we are getting our estimate by assuming a
-straight line between to the two known points in order to estimate our new
-value.
+This is *inter*-polation because we are estimating a value from the slice 1
+time course, that is *between* two points we do have values for. It is *linear*
+interpolation because we are getting our estimate by assuming a straight line
+between to the two known points in order to estimate our new value.
 
 In the general case of linear interpolation (see [Linear
 interpolation](https://matthew-brett.github.io/teaching/linear_interpolation.html)),
@@ -317,18 +316,19 @@ It is inconvenient to have to do this calculation for every point. We also
 need a good way of deciding what to do about values at the beginning and the
 end.
 
-Luckily `scipy` has a sub-package called `scipy.interpolate` that takes
-care of this for us.
+Luckily Scipy has a sub-package called `scipy.interpolate` that takes care of
+this for us.
 
-We use it by first creating an interpolation object, that will do the
-interpolation:
+We use it by first creating an *interpolation object*, that will do the
+interpolation.  We create this object using the `InterpolatedUnivariateSpline`
+class from `scipy.interpolate`.
 
 ```{python}
 from scipy.interpolate import InterpolatedUnivariateSpline as Interp
 ```
 
-This `Interp` class can do more fancy interpolation, but we will use it
-for linear interpolation (`k=1` argument below):
+This class can do more fancy interpolation, but we will use it for linear
+interpolation (`k=1` argument below):
 
 ```{python}
 lin_interper = Interp(times_slice_1, time_course_slice_1, k=1)
@@ -371,6 +371,7 @@ plt.xlabel('time (seconds)');
 
 ## Slice time correction
 
+
 We can do this for each time course in each slice, and make a new 4D image,
 that has a copy of the values in slice 0, but the interpolated values for all
 the other slices.  This new 4D image has been *slice time corrected*."
nipraxis,textbook,a7df53befea05c68eeac44ac819f265aba50fe06,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:28:23Z,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:28:23Z,Fix reference to nans page.,whole_image_statistics.Rmd,True,False,True,False,1,1,2,"---FILE: whole_image_statistics.Rmd---
@@ -201,7 +201,7 @@ plt.imshow(t_3d[:, :, 15], cmap='gray')
 
 Notice the white areas at the edge of the image.  These are voxels where the t
 value is `nan` – [Not a number](https://en.wikipedia.org/wiki/NaN).  See also
-[Not a number](nan.Rmd).  `nan` values arise when all the scans have 0 at this
+[Not a number](nans.Rmd).  `nan` values arise when all the scans have 0 at this
 voxel, so the numerator and denominator of the t statistic are both 0.
 
 ```{python}"
nipraxis,textbook,cf335aa265b0816c0b7b38ed427a047f1abaf908,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:13:00Z,Matthew Brett,matthew.brett@gmail.com,2022-09-21T03:13:00Z,Ref marker for Sidak correction,bonferroni_correction.Rmd,True,False,True,False,2,0,2,"---FILE: bonferroni_correction.Rmd---
@@ -38,6 +38,8 @@ $\le \alpha$ that *any* p value will be $\lt \theta$, then
 the threshold $\theta$ can be said to control the *family-wise
 error rate* at level $\alpha$.
 
+(sidak-correction)=
+
 ## Not the Bonferroni correction
 
 The inequality used for the Bonferroni is harder to explain than a"
nipraxis,textbook,1da46030ef03d295d4e4fdae70ea15da06d10ea9,Matthew Brett,matthew.brett@gmail.com,2022-09-14T11:22:42Z,Matthew Brett,matthew.brett@gmail.com,2022-09-14T11:22:42Z,More fixes and rewrites,regression_notation.Rmd,True,False,True,False,21,9,30,"---FILE: regression_notation.Rmd---
@@ -122,6 +122,7 @@ b = res.slope
 c = res.intercept
 ```
 
+
 ## Fitted values and errors
 
 Remember, our predicted or *fitted* values are given by multiplying the `x`
@@ -151,6 +152,10 @@ plt.plot(x, fitted, 'ro', label='Values predicted from line')
 # Plot the distance between predicted and actual, for all points.
 for i in range(n):
     plt.plot([x[i], x[i]], [fitted[i], y[i]], 'k:')
+# The following code line is just to trick Matplotlib into making a new
+# a single legend entry for the dotted lines.
+plt.plot([], [], 'k:', label='Errors')
+# Show the legend
 plt.legend()
 ```
 
@@ -163,7 +168,7 @@ with, let's think about the actual set of values we have for $\xvec$.  We could
 write the actual values in mathematical notation as:
 
 $$
-$\xvec = [ 0.389,  0.2  ,  0.241,  0.463, \\
+\xvec = [ 0.389,  0.2  ,  0.241,  0.463, \\
      4.585,  1.097,  1.642,  4.972, \\
      7.957,  5.585,  5.527,  6.964 ]
 $$
@@ -173,15 +178,22 @@ could write $\xvec$ in a more general way, to be *any* 12 values, like this:
 
 $$
 \xvec = [ x_1, x_2, x_3, x_4, x_5, x_6,\\
-           x_7, x_8, x_9, x_{10}, x_{11}, x_{12} ]
+          x_7, x_8, x_9, x_{10}, x_{11}, x_{12} ]
 $$
 
 This means that $\xvec$ consists of 12 numbers, $x_1, x_2 ..., x_{12}$, where
 $x_1$ can be any number, $x_2$ can be any number, and so on.
 
 $x_1$ is the value for the first student, $x_2$ is the value for the second student, and so on.
 
-In our *particular case*, $x_1 = 0.389,  x_2 = 0.2$ and so on.
+In our *particular case*:
+
+$$
+x_1 = 0.389 \\
+x_2 = 0.2 \\
+... \\
+x_{12} = 6.964
+$$
 
 We can make $\xvec$ be even more general, by writing it like this:
 
@@ -200,13 +212,13 @@ $$
 15.19 ,  11.902,  22.721,  22.324 ]
 $$
 
-More generally we can write a vector of any $n$ numbers as:
+More generally we can write $\yvec$ as a vector of any $n$ numbers:
 
 $$
 \yvec = [ y_1, y_2, ..., y_{n} ]
 $$
 
-If we have $n$ values in $\xvec$ and $\yvec$ then we have $n$ *fitted* values.  The fitted value for the first student as $f_1$, that for the second as $f_2$ and so on:
+If we have $n$ values in $\xvec$ and $\yvec$ then we have $n$ *fitted* values.  Write the fitted value for the first student as $f_1$, that for the second as $f_2$, and so on:
 
 $$
 \vec{f} = [ f_1, f_2, ..., f_n ]
@@ -242,10 +254,10 @@ we use $i$ to mean any whole number from 1 through $n$, and so:
 
 $$
 f_i = b x_i + c \\
-\text{where} i \in [ 1, 2, ... n]
+\text{for } i \in [ 1, 2, ... n]
 $$.
 
-For the second line above, read $\in$ as ""in"", and the whole line as ""Where $i$
+For the second line above, read $\in$ as ""in"", and the whole line as ""For $i$ in 1 through $n$"", or as ""Where $i$
 can take any value from 1 through $n$ inclusive"".
 
 We mean here, that for any whole number $i$ from 1 through $n$, the fitted
@@ -265,14 +277,14 @@ $$
 = [ y_1 - (b x_1 + c), y_2 - (b x_2 + c), ..., y_n - (b x_n + c) ] \\
 $$
 
-Again, we could also write this same idea with the general index $i$ as:
+Again, we could write this same idea with the general index $i$ as:
 
 $$
 e_i = y_i - (b x_i + c)
 $$
 
 
-## The regression model
+## Approximation and errors
 
 Our straight line model says that the $y_i$ values are approximately predicted
 by the fitted values $f_i = b x_i + c$."
nipraxis,textbook,2dcd5f34c8408dc61a140107c5da3590def2764c,Matthew Brett,matthew.brett@gmail.com,2022-09-12T16:21:40Z,Matthew Brett,matthew.brett@gmail.com,2022-09-12T16:21:40Z,Regression notation,_toc.yml;on_regression.Rmd;regression_notation.Rmd,True,False,True,False,269,3,272,"---FILE: _toc.yml---
@@ -55,6 +55,7 @@ parts:
   - file: convolution_background
   - file: on_regression
   - file: regress_one_voxel
+  - file: regression_notation
   - file: glm_intro
   - file: mean_test_example
   - file: model_one_voxel

---FILE: on_regression.Rmd---
@@ -16,12 +16,13 @@ jupyter:
 
 # Introducing regression
 
-These are some notes on simple regression, multiple regression, and the general
-linear model.
+These are some notes to introduce simple regression.
 
 ## The example regression problem
 
-[So far](voxels_by_time.Rmd) we have been looking at the relationship between a neural predictor and a voxel time course.  We have thus far used *correlation* to look for a relationship.
+[So far](voxels_by_time.Rmd) we have been looking at the relationship between a
+neural predictor and a voxel time course.  We have thus far used *correlation*
+to look for a relationship.
 
 To make things simpler, for illustration, lets look the relationship between
 two short example arrays.  We will get back to the voxel and predictor problem

---FILE: regression_notation.Rmd---
@@ -0,0 +1,264 @@
+---
+jupyter:
+  jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
+    text_representation:
+      extension: .Rmd
+      format_name: rmarkdown
+      format_version: '1.2'
+      jupytext_version: 1.10.3
+  kernelspec:
+    display_name: Python 3
+    language: python
+    name: python3
+---
+
+# Notation for regression models
+
+```{python}
+# Import numerical and plotting libraries
+import numpy as np
+import numpy.linalg as npl
+import matplotlib.pyplot as plt
+# Only show 6 decimals when printing
+np.set_printoptions(precision=6)
+import scipy.stats as sps
+```
+
+This page starts with the model for simple regression, that you have already
+see in the [regression page](on_regression.Rmd).
+
+Our purpose is to introduce the *mathematical notation* for regression models.
+You will need this notation to understand the [General Linear Model
+(GLM)](glm_intro.Rmd), a standard statistical generalization of *multiple
+regression*.  The GLM is the standard model for statistical analysis of
+functional imaging data.
+
+## Sweaty palms, dangerous students
+
+You have already seen the data we are using here, in the [regression
+page](on_regression.Rmd).
+
+We have measured scores for a “psychopathy” personality trait in 12 students.
+We also measured how much sweat each student had on their palms, and we call
+this a “clammy” score.  We are looking for a straight-line (linear)
+relationship that allows us to use the ""clammy"" score to predict the
+""psychopathy"" score.
+
+Here are our particular ""psychopathy"" and ""clammy"" scores as arrays:
+
+```{python}
+# The values we are trying to predict.
+psychopathy = np.array(
+    [11.416,   4.514,  12.204,  14.835,
+      8.416,   6.563,  17.343,  13.02,
+     15.19 ,  11.902,  22.721,  22.324])
+
+# The values we will predict with.
+clammy = np.array(
+    [0.389,  0.2  ,  0.241,  0.463,
+     4.585,  1.097,  1.642,  4.972,
+     7.957,  5.585,  5.527,  6.964])
+
+# The number of values
+n = len(clammy)
+```
+
+Here is a plot of the two variables, with `clammy` on the x-axis and
+`psychopathy` on the y-axis.  We could refer to this plot as ""psychopathy"" *as
+a function of* ""clammy"", where ""clammy"" is on the x-axis.  By convention we put
+the predictor on the x-axis.
+
+```{python}
+plt.plot(clammy, psychopathy, '+')
+plt.xlabel('Clamminess of handshake')
+plt.ylabel('Psychopathy score')
+```
+
+We will call the sequence of 12 ""clammy"" scores — the ""clammy"" *variable* or
+*vector*.  A vector is a sequence of values — in this case the sequence of 12
+""clammy"" scores, one for each student.  Similarly, we have a ""psychopathy""
+vector of 12 values.
+
+$\newcommand{\yvec}{\vec{y}} \newcommand{\xvec}{\vec{x}} \newcommand{\evec}{\vec{\varepsilon}}$
+
+We could call the ""clammy"" vector a *predictor*, but other names for a
+predicting vector are *regressor*, *covariate*, *explanatory variable*,
+*independent variable* or *exogenous* variable.  We also often use the
+mathematical notation $\xvec$ to refer to a predicting vector.  The arrow over
+the top of $\xvec$ reminds us that $\xvec$ refers to a vector (array, sequence)
+of values, rather than a single value.
+
+Just to remind us, let's give the name `x` to the array (vector) of predicting values:
+
+```{python}
+x = clammy
+```
+
+We could call the vector we are trying to predict the *predicted variable*,
+*response variable*, *regressand*, *dependent variable* or *endogenous*
+variable. We also often use the mathematical notation $\yvec$ to refer to a
+predicted vector.
+
+```{python}
+y = psychopathy
+```
+
+You have already seen the best (minimal mean square error) line that relates
+the `clammy` (`x`) scores to the `psychopathy` (`y`) scores.
+
+```{python}
+res = sps.linregress(x, y)
+res
+```
+
+Call the slope of the line `b` and the intercept `c` (C for Constant).
+
+```{python}
+b = res.slope
+c = res.intercept
+```
+
+Remember, our predicted or *fitted* values are given by multiplying the `x`
+(`clammy`) values by `b` (the slope) and then adding `c` (the intercept).  In
+Numpy that looks like this:
+
+```{python}
+fitted = b * x + c
+fitted
+```
+
+The *errors* are the differences between the *fitted* and *actual* (`y`) values:
+
+```{python}
+errors = y - fitted
+errors
+```
+
+The dashed lines in the plot below represent the *errors*.  The `errors` values
+above represent the (positive and negative) lengths of these dashed lines.
+
+```{python}
+# Plot the data
+plt.plot(x, y, '+', label='Actual values')
+# Plot the predicted values
+plt.plot(x, fitted, 'ro', label='Values predicted from line')
+# Plot the distance between predicted and actual, for all points.
+for i in range(n):
+    plt.plot([x[i], x[i]], [fitted[i], y[i]], 'k:')
+plt.legend()
+```
+
+Our next step is to write out this model more generally and more formally in
+mathematical symbols, so we can think about *any* vector (sequence) of x values
+$\xvec$, and any sequence (vector) of matching y values $\yvec$.  But to start
+with, let's think about the actual set of values we have for $\xvec$.  We could
+write the actual values in mathematical notation as:
+
+$$
+$\xvec = [ 0.389,  0.2  ,  0.241,  0.463, \\
+     4.585,  1.097,  1.642,  4.972, \\
+     7.957,  5.585,  5.527,  6.964 ]
+$$
+
+This means that $\xvec$ is a sequence of these specific 12 values.  But we
+could write $\xvec$ in a more general way, to be *any* 12 values, like this:
+
+$$
+\xvec = [ x_1, x_2, x_3, x_4, x_5, x_6,\\
+           x_7, x_8, x_9, x_{10}, x_{11}, x_{12} ]
+$$
+
+This means that $\xvec$ consists of 12 numbers, $x_1, x_2 ..., x_{12}$, where
+$x_1$ can be any number, $x_2$ can be any number, and so on.
+
+$x_1$ is the value for the first student, $x_2$ is the value for the second student, and so on.
+
+In our *particular case*, $x_1 = 0.389,  x_2 = 0.2$ and so on.
+
+We can make $\xvec$ be even more general, by writing it like this:
+
+$$
+\xvec = [ x_1, x_2, ..., x_{n} ]
+$$
+
+This means that $\xvec$ is a sequence of any $n$ numbers, where $n$ can be any
+whole number, such as 1, 2, 3 ...  In our specific case, $n = 12$.
+
+Similarly, for our `psychopathy` ($\yvec$) values, we can write:
+
+$$
+\yvec = [ 11.416,   4.514,  12.204,  14.835, \\
+8.416,   6.563,  17.343,  13.02, \\
+15.19 ,  11.902,  22.721,  22.324 ]
+$$
+
+More generally we can write a vector of any $n$ numbers as:
+
+$$
+\yvec = [ y_1, y_2, ..., y_{n} ]
+$$
+
+If we have $n$ values in $\xvec$ and $\yvec$ then we have $n$ *fitted* values.  The fitted value for the first student as $f_1$, that for the second as $f_2$ and so on:
+
+$$
+\vec{f} = [ f_1, f_2, ..., f_n ]
+$$
+
+Our regression model says each fitted value comes about by multiplying the corresponding $x$ value by $b$ (the slope) and then adding $c$ (the intercept).
+
+So, the fitted values are:
+
+$$
+f_1 = b x_1 + c \\
+f_2 = b x_2 + c \\
+... \\
+f_n = b x_n + c
+$$
+
+More compactly:
+
+$$
+\vec{f} = [ b x_1 + c, b x_2 + c, ..., b x_n + c ]
+$$
+
+We often use $i$ as a general *index* into the vectors.  So, instead of writing:
+
+$$
+f_1 = b x_1 + c \\
+f_2 = b x_2 + c \\
+... \\
+f_n = b x_n + c
+$$
+
+we use $i$ to mean any whole number from 1 through $n$, and so:
+
+$$
+f_i = b x_i + c \\
+$$
+
+where $i \in [ 1, 2, ... n]$.  Read $\in$ as ""in"".
+
+We mean here, that for any whole number $i$ from 1 through $n$, the fitted
+value $f_i$ (e.g. $f_3$, where $i=3$) is given by $b$ times the corresponding
+$x$ value ($x_3$ where $i=3$) plus $c$.
+
+The error vector $\vec{e}$ is:
+
+$$
+\vec{e} = [ e_2, e_2, ..., e_n ]
+$$
+
+For our linear model, the errors are:
+
+$$
+\vec{e} = [ y_1 - f_1, y_2 - f_2, ..., y_n - f_n ] \\
+= [ y_1 - (b x_1 + c), y_2 - (b x_2 + c), ..., y_n - (b x_n + c) ] \\
+$$
+
+Again, we could also write this same idea with the general index $i$ as:
+
+$$
+e_i = y_i - (b x_i + c)
+$$"
nipraxis,textbook,1b2f1b72937fd06757689abd4a26ebe0785d2a63,Matthew Brett,matthew.brett@gmail.com,2022-09-02T19:09:04Z,Matthew Brett,matthew.brett@gmail.com,2022-09-02T19:09:04Z,Fix 1/sigma,why_mse_correlation.Rmd,True,False,True,False,3,3,6,"---FILE: why_mse_correlation.Rmd---
@@ -418,9 +418,9 @@ With the results above, we can prove z-scores have a sum of 0:
 
 $$
 \Sigma z_x = \Sigma ((x_i - \xbar) / \sigma_x) \\
-= \sigma_x \Sigma (x_i - \xbar) \\
-= \sigma_x (\Sigma x_i - \Sigma \xbar) \\
-= \sigma_x (n \xbar - n \xbar) \\
+= \frac{1}{\sigma_x} \Sigma (x_i - \xbar) \\
+= \frac{1}{\sigma_x} (\Sigma x_i - \Sigma \xbar) \\
+= \frac{1}{\sigma_x} (n \xbar - n \xbar) \\
 = 0
 $$
 "
nipraxis,textbook,af3a3bc554eb40dad19a02b3446dd15b083bce86,Matthew Brett,matthew.brett@gmail.com,2022-09-01T11:12:28Z,Matthew Brett,matthew.brett@gmail.com,2022-09-01T11:12:28Z,Fix up validating_data for current task,assert.Rmd;validating_data.Rmd,True,False,True,False,21,19,40,"---FILE: assert.Rmd---
@@ -1,11 +1,17 @@
 ---
 jupyter:
   jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
     text_representation:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.11.5
+      jupytext_version: 1.13.7
+  kernelspec:
+    display_name: Python 3
+    language: python
+    name: python3
 ---
 
 # Using `assert` for testing

---FILE: validating_data.Rmd---
@@ -24,33 +24,29 @@ jupyter:
 Go to your fork of your diagnostics repository, and follow the instructions in
 the `README.md` file *Get the data section*.
 
-- Your `data` directory should now a directory called `group-0?` where `?` ia a
+- Your `data` directory should now a directory called `group-0?` where `?` is a
   number from 0 through 2.  This directory in turn contains 10 subdirectories
   of form `sub-0?`, where `?` is a number between 1 and 10. Each of these
   directories contain a `func` directory, which each contain two `.nii.gz`
   files.  These are the FMRI data files.  There are matching `.tsv` files that contain the event onset data for the task during each scanning run.  You will also see a file of form `group-0?/hash_list.txt`.
-- Now do `git status`.  You will see that note of the files you have just
-  unpackaged show up in Git's listing of untracked files. This is because we
-  put a clever `.gitignore` file in the `data` directory, to tell git to ignore
-  all files.
-- You can see the file by opening it in your text editor, or with `more
-  data/group-0?/.gitignore` at the terminal (where `?` is the number that matches your data, from 0 through 2).
-- In fact, you do want to put the `data/group-0?/hash_list.txt` file into Git
-  version control.  To do this, make a new branch, checkout that branch, and
-  then run `git add -f data/group-0?/hash_list.txt`  The `-f` tells Git to add
-  the file, even though the `.gitignore` file says to ignore it.  Run `git
-  status` to check that you did add the file to the staging area.  Commit your
-  change, push up your branch and make a PR to the main repo.  Someone should
-  merge this.
+- Now do `git status`.  You will see that directory of the files you have just
+  unpackaged show up in Git's listing of untracked files.
+- Next put the `data/group-0?/hash_list.txt` file into Git version control, so
+  you are keeping a record of what the data hashes ought to be.  To do this,
+  make a new branch, maybe called `add-hashes`, checkout that branch, and then
+  run `git add data/group-0?/hash_list.txt`  Run `git status` to check that you
+  did add the file to the staging area.  Commit your change, push up your
+  branch and make a Pull Request (PR) to the main repo.  Someone should merge
+  this.  As it is simple, that person could be you.
 - Now have a look at `data/group-0?/hash_list.txt`. For each of the files,
   `hash_list.txt` has a line with the SHA1 hash for that file, and the
   filename, separated by a space;
 - You want to be able to confirm that your data has not been overwritten or
   corrupted since you downloaded it.  To do this, you need to calculate the
   current hash for each of the unpacked `.nii.gz` and `.tsv` files and compare
   it to the hash value in `hash_list.txt`;
-- Now run `python3 scripts/validate_data.py data`.  When you first run this
-  file, it will fail;
+- Now run `python3 scripts/validate_data.py`.  When you first run this file, it
+  will fail;
 - In due course, you will edit `scripts/validate_data.py` in your text editor
   to fix.  See below.
 
@@ -124,8 +120,8 @@ if (even_no % 2) != 0:   # Oh no, it's not an even number
     raise ValueError(f'Oh no, {even_no} is not an even number')
 ```
 
-## On to the exercise
+## On to the validation
 
-Now run `python3 scripts/validate_data.py data`.  It will fail.  Use the code
+Now run `python3 scripts/validate_data.py`.  It will fail.  Use the code
 suggestions above to edit the `validate_data.py` script and fix it, so it
 correctly checks all the hashes of the listed data files."
nipraxis,textbook,3b69f23260337a6fb4502eccea57e05a122479d6,Matthew Brett,matthew.brett@gmail.com,2022-08-29T18:05:59Z,Matthew Brett,matthew.brett@gmail.com,2022-08-29T18:05:59Z,Put error back into example,on_modules.Rmd,True,False,True,False,1,1,2,"---FILE: on_modules.Rmd---
@@ -69,7 +69,7 @@ def detect_outliers(some_values, n_stds=2):
     overall_mean = np.mean(some_values)
     overall_std = np.std(some_values)
     thresh = overall_std * n_stds
-    is_outlier = np.abs(some_values - overall_mean) > thresh
+    is_outlier = (some_values - overall_mean) < -thresh
     return np.where(is_outlier)[0]
 ```
 "
nipraxis,textbook,f1564abc6e55fa2285a57a1c488ad3e6bdce5b1a,Matthew Brett,matthew.brett@gmail.com,2022-08-29T14:21:40Z,Matthew Brett,matthew.brett@gmail.com,2022-08-29T14:22:01Z,"Small edits, including fixed link.",reshape_and_3d.Rmd;reshape_and_4d.Rmd,True,False,True,False,2,1,3,"---FILE: reshape_and_3d.Rmd---
@@ -30,6 +30,7 @@ columns.
 A three-dimensional array takes a little bit more work to visualize, and get
 used to.
 
+
 ## Reshaping a three-dimensional array
 
 You have already seen the algorithm for [reshaping in one and two

---FILE: reshape_and_4d.Rmd---
@@ -28,7 +28,7 @@ We often find ourselves doing complicated reshape operations when we are
 dealing with images.   For example, we may find ourselves reshaping the first
 few dimensions, but leaving the last intact.
 
-Let's start with a [familiar 3D array](reshape_and_4d.Rmd):
+Let's start with a [familiar 3D array](reshape_and_3d.Rmd):
 
 ```{python}
 arr_3d = np.reshape(np.arange(24), (2, 3, 4))"
nipraxis,textbook,b87543c53e1a209a52c0a28b256e8f3b05eb0c21,Matthew Brett,matthew.brett@gmail.com,2022-08-24T22:22:24Z,Matthew Brett,matthew.brett@gmail.com,2022-08-24T22:22:24Z,Fix error for Sympy 1.11,glm_intro.Rmd,True,False,True,False,3,3,6,"---FILE: glm_intro.Rmd---
@@ -7,9 +7,9 @@ jupyter:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.13.7
+      jupytext_version: 1.11.5
   kernelspec:
-    display_name: Python 3
+    display_name: Python 3 (ipykernel)
     language: python
     name: python3
 ---
@@ -193,7 +193,7 @@ $$
 ```{python tags=c(""hide-input"")}
 # Ignore this cell.
 sy_b, sy_c = symbols('b, c')
-sy_c_mat = MatrixSymbol('c', 1, 1)
+sy_c_mat = MatrixSymbol('c', 12, 1)
 rhs = MatAdd(MatMul(sy_b, sy_x_val), sy_c_mat, sy_e_val)
 Eq(sy_y_val, rhs, evaluate=False)
 ```"
nipraxis,textbook,032bd79902331eb712be346c6acbbd04179e4ad2,Matthew Brett,matthew.brett@gmail.com,2022-08-24T21:38:26Z,Matthew Brett,matthew.brett@gmail.com,2022-08-24T21:38:26Z,Upload reports on failure,.github/workflows/book.yml,False,False,False,False,6,0,6,"---FILE: .github/workflows/book.yml---
@@ -47,6 +47,12 @@ jobs:
       run: |
         make html
 
+    - uses: actions/upload-artifact@v3
+      if: failure()
+      with:
+        name: error_logs
+        path: ./_build/html/reports
+
     # Push the book's HTML to github-pages
     - name: GitHub Pages action
       if: ${{ github.ref == 'refs/heads/main' }}"
nipraxis,textbook,9fcc046f69ffb27eb1da5453541440fb86946451,Matthew Brett,matthew.brett@gmail.com,2022-08-15T13:03:46Z,Matthew Brett,matthew.brett@gmail.com,2022-08-15T13:03:46Z,Fix links to 2D reshape page,reshape_and_3d.Rmd,True,False,True,False,2,2,4,"---FILE: reshape_and_3d.Rmd---
@@ -33,7 +33,7 @@ used to.
 ## Reshaping a three-dimensional array
 
 You have already seen the algorithm for [reshaping in one and two
-dimensions](arrays_2d_reshape.Rmd).
+dimensions](reshape_and_2d.Rmd).
 
 NumPy uses the same algorithm for reshaping a three-dimensional array.
 
@@ -118,7 +118,7 @@ the array from both perspectives.
 
 
 We can reshape to one dimension in the [same way as we did for the 2D
-arrays](arrays_2d_reshape.Rmd):
+arrays](reshape_and_2d.Rmd):
 
 ```{python}
 # Reshape to 1D array with 24 elements."
nipraxis,textbook,ff89c3f46214ac068735eaa858f160342ad87235,Matthew Brett,matthew.brett@gmail.com,2022-08-15T12:51:06Z,Matthew Brett,matthew.brett@gmail.com,2022-08-15T12:51:06Z,Fix link in reshape page,reshape_and_2d.Rmd,True,False,True,False,2,2,4,"---FILE: reshape_and_2d.Rmd---
@@ -123,8 +123,8 @@ Our array above has 15 * 2 = 30 elements.  Numpy can change make any shape array
 For example, one common move is to take a 2D array and flatten it out into a 1D
 array.  The corresponding 1D array would have one axis of length 30.
 
-We could write the new shape as a [single element tuple](length_one_tuple.Rmd),
-like this:
+We could write the new shape as a [single element
+tuple](length_one_tuples.Rmd), like this:
 
 ```{python}
 new_shape = (30,)"
nipraxis,textbook,d7a6edf6397b48daa51eac61dda1852a5811a971,Matthew Brett,matthew.brett@gmail.com,2022-08-15T12:39:43Z,Matthew Brett,matthew.brett@gmail.com,2022-08-15T12:39:43Z,"Fix 2D reshape, add diagram",reshape_and_2d.Rmd,True,False,True,False,10,2,12,"---FILE: reshape_and_2d.Rmd---
@@ -153,9 +153,17 @@ a = first_15_rows
 Then the first eight values of the 1D reshaped version comes from:
 
 ```{python}
-[a[0,0], a[0,1], a[1,0], a[1,1], a[2,0], a[2,1], a[3,0], a[3,1]]
+[a[0,0], a[0,1],
+ a[1,0], a[1,1],
+ a[2,0], a[2,1],
+ a[3,0], a[3,1]]
 ```
 
+The image below shows the order in which `np.reshape` fetches the elements from the array:
+
+![](images/2d_reshape.svg)
+
+
 We can reverse the process, by taking the flattened array, and putting it back
 into its original two dimensions:
 
@@ -205,7 +213,7 @@ arr_2d_strs
 Back to 1D:
 
 ```{python}
-np.reshape(arr_2_strs, (9,))
+np.reshape(arr_2d_strs, (8,))
 ```
 
 As before, NumPy fetches the data across the columns, then the rows, to fill"
nipraxis,textbook,fb758e34493dafbba6487f0d3f6c7a0001efdb06,Matthew Brett,matthew.brett@gmail.com,2022-08-10T23:18:45Z,Matthew Brett,matthew.brett@gmail.com,2022-08-10T23:18:45Z,Fix holdover from os.path page.,pathlib.Rmd,True,False,True,False,6,5,11,"---FILE: pathlib.Rmd---
@@ -19,8 +19,8 @@ jupyter:
 
 The `pathlib` module is one of two ways of manipulating and using file paths in
 Python.  See the [path manipulation](path_manipulation.Rmd) page for more
-discussion, and [os.path](os_path.Rmd) page for a tutorial on the alternative
-`os.path` approach.
+discussion, and [os.path](os_path.Rmd) page for a tutorial on an alternative
+approach.
 
 The primary documentation for `pathlib` is
 <https://docs.python.org/3/library/pathlib.html>.
@@ -73,9 +73,10 @@ We can always convert the `Path` object to a simple string, using the
 str(abs_p)
 ```
 
-The first function we will use from `os.path` is `dirname`.  To avoid
-typing `os.path` all the time, we import `os.path` with the shortened name
-`op`:
+Sometimes we want to get a path referring the directory *containing* a path.
+The Path object has a `parent` attribute attached to it (an attribute is data
+attached to an object).  The `parent` attribute is a Path object for the
+containing directory:
 
 ```{python}
 abs_p.parent"
nipraxis,textbook,02ef76be75287630e50162b6f680e79f16d617c6,Matthew Brett,matthew.brett@gmail.com,2022-08-10T22:53:27Z,Matthew Brett,matthew.brett@gmail.com,2022-08-10T22:53:27Z,Fix what_is_an_image pathlib line,what_is_an_image.Rmd,True,False,True,False,2,2,4,"---FILE: what_is_an_image.Rmd---
@@ -7,7 +7,7 @@ jupyter:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.11.5
+      jupytext_version: 1.13.7
   kernelspec:
     display_name: Python 3
     language: python
@@ -127,7 +127,7 @@ module](pathlib.Rmd):
 # Open a file, read in binary bytes.
 from pathlib import Path
 
-contents = Path(structural_fname).read_binary()
+contents = Path(structural_fname).read_bytes()
 ```
 
 How do I find out what `type` of object is attached to this variable called"
nipraxis,textbook,2d8e3c091cb6ad8344885e49699d418c0858643d,Matthew Brett,matthew.brett@gmail.com,2022-08-10T21:45:59Z,Matthew Brett,matthew.brett@gmail.com,2022-08-10T21:45:59Z,Fix up numpy intro page,numpy_intro.Rmd,True,False,True,False,23,15,38,"---FILE: numpy_intro.Rmd---
@@ -89,8 +89,9 @@ There is one row per trial, where each row records:
   'spacebar')
 * `response_time` — the reaction time for their response (milliseconds after
   the stimulus, 0 if no response)
-* `trial_ISI` — the time to wait until the *next* stimulus (the Interstimulus
-  Interval)
+* `trial_ISI` — the time between the *previous* stimulus and this one (the
+  Interstimulus Interval).  For the first stimulus this is the time from the
+  start of the experimental software.
 * `trial_shape` — the name of the stimulus ('red_star', 'red_circle' and so
   on).
 
@@ -100,7 +101,10 @@ Here we open the file as text, and load the lines of the file into memory as a l
 
 ```{python}
 # Load the lines in the file as a list.
-lines = open(stim_fname, 'rt').readlines()
+from pathlib import Path
+
+text = Path(stim_fname).read_text()
+lines = text.splitlines()
 # Show the first 5 lines.
 lines[:5]
 ```
@@ -263,6 +267,8 @@ my_list[1] = 'some_text'
 my_list
 ```
 
+## Reading directly into arrays
+
 There is another way we could have collected the information from the file, and put it directly into arrays.
 
 We could have started with an array of the right length and type, by using the `np.zeros` function to make an array with all 0.0 values.
@@ -420,21 +426,21 @@ Remember {ref}`Boolean values <true-and-false>`, and
 Let's start by looking at the first 15 reaction times.
 
 ```{python}
-first_rts = rt_arr[:15]
-first_rts
+first_15_rts = rt_arr[:15]
+first_15_rts
 ```
 
 Remember that comparisons are operators that give answers to a *comparison
 question*.  This is how comparisons work on individual values:
 
 ```{python}
-first_rts[0] > 0
+first_15_rts[0] > 0
 ```
 
 What do you think will happen if we do the comparison on the whole array, like this?
 
 ```python
-first_rts > 0
+first_15_rts > 0
 ```
 
 You have seen how Numpy works when adding a single number to an array — it
@@ -444,21 +450,22 @@ array*.
 Comparisons work the same way:
 
 ```{python}
-first_rts_not_zero = first_rts > 0
-first_rts_not_zero
+first_15_rts_not_zero = first_15_rts > 0
+first_15_rts_not_zero
 ```
 
 This is the result of asking the comparison question `> 0` of *every element in
 the array*.
 
-So the values that end up in the `first_rts_not_zero` array come from these comparisons:
+So the values that end up in the `first_15_rts_not_zero` array come from these
+comparisons:
 
 ```{python}
-print('Position 0:', first_rts[0] > 0)
-print('Position 1:', first_rts[1] > 0)
+print('Position 0:', first_15_rts[0] > 0)
+print('Position 1:', first_15_rts[1] > 0)
 print(' ... and so on, up to ...')
-print('Position 13:', first_rts[13] > 0)
-print('Position 14:', first_rts[14] > 0)
+print('Position 13:', first_15_rts[13] > 0)
+print('Position 14:', first_15_rts[14] > 0)
 ```
 Here is the equivalent array for all the reaction times:
 
@@ -472,7 +479,8 @@ We will [soon see](boolean_indexing) that we can use these arrays to select elem
 
 Specifically, if we put a Boolean array like `rts_not_zero` between square brackets for another array, that will have the effect of selecting the elements at positions where `rts_not_zero` has True, and throwing away elements where `rts_not_zero` has False.
 
-For0 example, rushing ahead, we can select the values in `rt_arr` corresponding to reaction times greater than zero with:
+For example, rushing ahead, we can select the values in `rt_arr` corresponding
+to reaction times greater than zero with:
 
 ```{python}
 rt_arr[rts_not_zero]"
nipraxis,textbook,d91030538be4b3d357a72e4cbc8995847d1a303c,Matthew Brett,matthew.brett@gmail.com,2022-08-10T16:05:14Z,Matthew Brett,matthew.brett@gmail.com,2022-08-10T16:05:14Z,"Fix links, again",pathlib.Rmd,True,False,True,False,4,1,5,"---FILE: pathlib.Rmd---
@@ -17,7 +17,10 @@ jupyter:
 
 # Using the `pathlib` module
 
-The `pathlib` module is one of two ways of manipulating and using file paths in Python.  See the {doc}`path manipulation <path_manipulation.Rmd>` page for more discussion, and {doc}`os_path.Rmd` page for a tutorial on the alternative `os.path` approach.
+The `pathlib` module is one of two ways of manipulating and using file paths in
+Python.  See the [path manipulation](path_manipulation.Rmd) page for more
+discussion, and [os.path](os_path.Rmd) page for a tutorial on the alternative
+`os.path` approach.
 
 The primary documentation for `pathlib` is
 <https://docs.python.org/3/library/pathlib.html>."
nipraxis,textbook,2154be04482af5e34828233152e434161d4e6274,Matthew Brett,matthew.brett@gmail.com,2022-08-10T15:53:25Z,Matthew Brett,matthew.brett@gmail.com,2022-08-10T15:53:25Z,Fix page links,path_manipulation.Rmd,True,False,True,False,2,2,4,"---FILE: path_manipulation.Rmd---
@@ -31,8 +31,8 @@ os.getcwd()
 
 There are two standard ways of manipulating pathnames in Python.
 
-* {doc}`pathlib.Rmd`
-* {doc}`os_path.Rmd`
+* [The pathlib module](pathlib.Rmd)
+* [The os.path module](os_path.Rmd)
 
 Of the two techniques, the `os.path` way is rather simpler, but it covers a
 smaller range of tasks.  It can also be more verbose. `pathlib` does more, and"
nipraxis,textbook,7d744fd465a5180cc07d565f7ded9e429128ab1b,Matthew Brett,matthew.brett@gmail.com,2022-08-10T15:34:03Z,Matthew Brett,matthew.brett@gmail.com,2022-08-10T15:34:03Z,More fixes,os_path.Rmd;pathlib.Rmd,True,False,True,False,36,13,49,"---FILE: os_path.Rmd---
@@ -15,7 +15,10 @@ jupyter:
 
 # Using the `os.path` module
 
-The `os.path` module is one of two ways of manipulating and using file paths in Python.  See the {doc}`path manipulation <path_manipulation.Rmd>` page for more discussion, and {doc}`pathlib.Rmd` page for a tutorial on the alternative `pathlib` approach.
+The `os.path` module is one of two ways of manipulating and using file
+paths in Python.  See the [path manipulation](path_manipulation.Rmd) page
+for more discussion, and [pathlib](pathlib.Rmd) page for a tutorial on the
+alternative `pathlib` approach.
 
 The primary documentation for `os.path` is
 <https://docs.python.org/3/library/os.path.html>

---FILE: pathlib.Rmd---
@@ -30,9 +30,9 @@ from pathlib import Path
 
 In Jupyter or IPython, you can tab complete on `Path` to list the methods (functions) and attributes attached to it.
 
-An object (value) of type `Path` represents a pathname.  As you {doc}`remember
-<path_manipulation.Rmd>`, a pathname is a string that identifies a particular
-file or directory on a computer filesystem.
+An object (value) of type `Path` represents a pathname.  As you [remember
+](path_manipulation.Rmd), a pathname is a string that identifies a
+particular file or directory on a computer filesystem.
 
 Let us start by making a default object from the `Path` class, like this:
 
@@ -41,21 +41,35 @@ p = Path()
 p
 ```
 
+By default, the path object, here `p`, refers to our current working directory, or `.` for short.   `.` is a *relative path*, meaning that we specify where we are relative to our current directory.  `.` means we are exactly in our current directory.
+
+Because the `.` is a *relative path*, it does not tell us where we are in
+the filesystem, only where we are relative to the current directory.
+
+Path objects have an `absolute` function attached to them.  Another way of
+saying this is that Path objects have an `absolute` *method*.  Calling
+this method gives us the *absolute* location of the path, meaning, the
+filesystem position relative to the base location of the disk the file is
+on.
+
 ```{python}
 abs_p = p.absolute()
 abs_p
 ```
 
-We can always convert the `Path` object to a simple string, using the `str` function. `str()` converts anything to a string, if possible:
+Notice the `/` in front of the absolute filename (on Unix), meaning the
+base location for all files.  You will see a drive location like `C:` or
+similar, at the front of the absolute path, if you are on Windows.
+
+
+We can always convert the `Path` object to a simple string, using the
+`str` function. `str()` converts anything to a string, if it can:
 
 ```{python}
 # The path, as a string
 str(abs_p)
 ```
 
-`read_bytes`
-`read_text`
-
 The first function we will use from `os.path` is `dirname`.  To avoid
 typing `os.path` all the time, we import `os.path` with the shortened name
 `op`:
@@ -64,17 +78,19 @@ typing `os.path` all the time, we import `os.path` with the shortened name
 abs_p.parent
 ```
 
-The `dirname` function gives the directory name from a full file path. It
-works correctly for Unix paths on Unix machines, and Windows paths on Windows
-machines:
+The `parent` attribute of the Path object gives the directory name from a
+full file path. It works correctly for Unix paths on Unix machines, and
+Windows paths on Windows machines.
 
 ```{python}
 # On Unix
-Path('/a/full/path/then_filename.txt').parent
+a_path = Path('/a/full/path/then_filename.txt')
+# Show the directory containing the file.
+a_path.parent
 ```
 
 You'll see something like this as output if you run something similar on
-Windows.  So `op.dirname('c:\\a\\full\\path\\then_filename.txt') ` will give:
+Windows.
 
 ```
 'c:\\a\\full\\path'
@@ -128,3 +144,7 @@ extension:
 ```{python}
 Path('relative/path/then_filename.txt').suffix
 ```
+
+`read_bytes`
+`read_text`
+"
nipraxis,textbook,31b5e35d254195dd5333376ef32c6cd58df986a2,Matthew Brett,matthew.brett@gmail.com,2022-08-10T15:23:26Z,Matthew Brett,matthew.brett@gmail.com,2022-08-10T15:23:26Z,Update pathblib page to avoid error,pathlib.Rmd,True,False,True,False,20,15,35,"---FILE: pathlib.Rmd---
@@ -1,16 +1,18 @@
 ---
 jupyter:
-  orphan: true
   jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
     text_representation:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.11.5
+      jupytext_version: 1.10.3
   kernelspec:
-    display_name: Python 3 (ipykernel)
+    display_name: Python 3
     language: python
     name: python3
+  orphan: true
 ---
 
 # Using the `pathlib` module
@@ -39,11 +41,16 @@ p = Path()
 p
 ```
 
+```{python}
+abs_p = p.absolute()
+abs_p
+```
+
 We can always convert the `Path` object to a simple string, using the `str` function. `str()` converts anything to a string, if possible:
 
 ```{python}
-# The filename, as a string
-str(p)
+# The path, as a string
+str(abs_p)
 ```
 
 `read_bytes`
@@ -54,8 +61,7 @@ typing `os.path` all the time, we import `os.path` with the shortened name
 `op`:
 
 ```{python}
-import os.path as op
-op.dirname
+abs_p.parent
 ```
 
 The `dirname` function gives the directory name from a full file path. It
@@ -64,7 +70,7 @@ machines:
 
 ```{python}
 # On Unix
-op.dirname('/a/full/path/then_filename.txt')
+Path('/a/full/path/then_filename.txt').parent
 ```
 
 You'll see something like this as output if you run something similar on
@@ -85,14 +91,14 @@ of the root of the file system:
 
 ```{python}
 # On Unix
-op.dirname('relative/path/then_filename.txt')
+Path('relative/path/then_filename.txt').parent
 ```
 
 Use `basename` to get the filename rather than the directory name:
 
 ```{python}
 # On Unix
-op.basename('/a/full/path/then_filename.txt')
+Path('/a/full/path/then_filename.txt').name
 ```
 
 Sometimes you want to join one or more directory names with a filename to get
@@ -104,7 +110,7 @@ does:
 
 ```{python}
 # On Unix
-op.join('relative', 'path', 'then_filename.txt')
+Path('relative') / 'path' / 'then_filename.txt'
 ```
 
 This also works on Windows.  `op.join('relative', 'path', 'then_filename.txt')` gives output `'relative\\path\\then_filename.txt'`.
@@ -113,13 +119,12 @@ To convert a relative to an absolute path, use `abspath`:
 
 ```{python}
 # Show the current working directory
-os.getcwd()
-op.abspath('relative/path/then_filename.txt')
+Path('relative/path/then_filename.txt').absolute()
 ```
 
-Use `splitext` to split a path into: the path + filename; and the file
+Use `suffix` to split a path into: the path + filename; and the file
 extension:
 
 ```{python}
-op.splitext('relative/path/then_filename.txt')
+Path('relative/path/then_filename.txt').suffix
 ```"
nipraxis,textbook,f562f55eb29b03b4a60f83d722853c665f484900,Matthew Brett,matthew.brett@gmail.com,2022-06-05T07:41:07Z,Matthew Brett,matthew.brett@gmail.com,2022-06-05T07:41:07Z,"Try fix for ""Unknown directive type ""tabbed""""

https://github.com/executablebooks/jupyter-book/issues/977",build_requirements.txt,False,False,False,False,1,0,1,"---FILE: build_requirements.txt---
@@ -1,5 +1,6 @@
 # Build requirements
 jupyter-book
+sphinx-panels
 # Notebook execution problem with older attrs?
 # attrs==21.4
 "
nipraxis,textbook,260287cf53a29511091ed6752b23c6ba0e5e59bf,Matthew Brett,matthew.brett@gmail.com,2022-06-03T11:50:10Z,Matthew Brett,matthew.brett@gmail.com,2022-06-03T11:50:10Z,"Try Sympy trick for pretty printing c

See: https://github.com/sympy/sympy/issues/23552#issuecomment-1145085557",bonferroni_correction.Rmd;glm_intro.Rmd;whole_image_statistics.Rmd,True,False,True,False,10,5,15,"---FILE: bonferroni_correction.Rmd---
@@ -1,5 +1,6 @@
 ---
 jupyter:
+  orphan: true
   jupytext:
     notebook_metadata_filter: all,-language_info
     split_at_heading: true

---FILE: glm_intro.Rmd---
@@ -119,8 +119,11 @@ student $i$ where $i \in 1 .. 12$:
 ```{python tags=c(""hide-input"")}
 # Ignore this cell - it is to give a better display to the mathematics.
 # Import Symbolic mathematics routines.
-from sympy import Eq, Matrix, IndexedBase, symbols
-from sympy.matrices import MatAdd, MatMul
+from sympy import Eq, Matrix, IndexedBase, symbols, init_printing
+from sympy.matrices import MatAdd, MatMul, MatrixSymbol
+
+# Make equations print in definition order.
+init_printing(order='none')
 
 sy_y = symbols(r'\vec{y}')
 sy_y_ind = IndexedBase(""y"")
@@ -190,7 +193,8 @@ $$
 ```{python tags=c(""hide-input"")}
 # Ignore this cell.
 sy_b, sy_c = symbols('b, c')
-rhs = MatAdd(MatMul(sy_b, sy_x_val), sy_c, sy_e_val)
+sy_c_mat = MatrixSymbol('c', 1, 1)
+rhs = MatAdd(MatMul(sy_b, sy_x_val), sy_c_mat, sy_e_val)
 Eq(sy_y_val, rhs, evaluate=False)
 ```
 
@@ -301,7 +305,7 @@ sy_x_1_ind = IndexedBase(""x_1,"")
 sy_x_1_val = Matrix([sy_x_1_ind[i] for i in vector_indices])
 sy_x_2_ind = IndexedBase(""x_2,"")
 sy_x_2_val = Matrix([sy_x_2_ind[i] for i in vector_indices])
-Eq(sy_y_val, MatAdd(MatMul(sy_b_1, sy_x_1_val), MatMul(sy_b_2, sy_x_2_val), sy_c, sy_e_val), evaluate=False)
+Eq(sy_y_val, MatAdd(MatMul(sy_b_1, sy_x_1_val), MatMul(sy_b_2, sy_x_2_val), sy_c_mat, sy_e_val), evaluate=False)
 ```
 
 In this model $\Xmat$ has three columns ($\xvec_1$, $\xvec_2$ and ones), and

---FILE: whole_image_statistics.Rmd---
@@ -314,7 +314,7 @@ plt.imshow(p_3d[:, :, 15], cmap='gray')
 We now have a very large number of t statistics and p values.  We want to find
 to control the family-wise error rate, where the “family” is the set of all of
 the voxel t tests / p values.  See: [Bonferroni
-correction](https://matthew-brett.github.io/teaching/bonferroni_correction.html).
+correction](bonferroni_correction.Rmd).
 
 We start with the Šidák correction, that gives the correct threshold when all
 the test are independent:"
nipraxis,textbook,ade5f26fe31d6f8056be80cb697470fc3390f8c6,Matthew Brett,matthew.brett@gmail.com,2022-05-30T15:46:11Z,Matthew Brett,matthew.brett@gmail.com,2022-05-30T15:46:11Z,Fix notebook remover,Makefile,False,False,False,False,1,1,2,"---FILE: Makefile---
@@ -9,7 +9,7 @@ clean:
 	rm -rf _build
 
 rm-ipynb:
-	rm -rf */*.ipynb
+	rm -rf *.ipynb
 
 github: html
 	cp CNAME $(BUILD_DIR)"
nipraxis,textbook,9f782e45e6df1fc4f5d0b10ec8cc4d1f5ec3f1d6,Matthew Brett,matthew.brett@gmail.com,2022-05-28T14:27:13Z,Matthew Brett,matthew.brett@gmail.com,2022-05-28T14:27:13Z,Plotting fixes,why_mse_correlation.Rmd,True,False,True,False,2,2,4,"---FILE: why_mse_correlation.Rmd---
@@ -42,8 +42,8 @@ clammy = np.array([0.389,  0.2,    0.241,  0.463,
                    4.585,  1.097,  1.642,  4.972,
                    7.957,  5.585,  5.527,  6.964])
 plt.plot(psychopathy, clammy, '+')
-plt.xlabel('Psychopathy')
 plt.xlabel('Clammy')
+plt.ylabel('Psychopathy');
 ```
 
 To get ourselves into a mathematical mood, we will rename our x-axis values to `x` and the y-axis values to `y`.  We will store the number of observations in both as `n`.
@@ -267,7 +267,7 @@ for i in range(n_intercepts):
 
 plt.plot(intercepts, sse_inters_b0p6)
 plt.xlabel('intercept')
-plt.xlabel('SSE for intercept, b=0.6')
+plt.ylabel('SSE for intercept, b=0.6');
 ```
 
 As [before](on_regression.Rmd), it looks as though $c=0$ is the intercept that"
nipraxis,textbook,59036a276457fd076775b0f889a3c7b48b8c511c,Matthew Brett,matthew.brett@gmail.com,2022-05-24T17:36:07Z,Matthew Brett,matthew.brett@gmail.com,2022-05-24T17:36:07Z,Fill out multiple regression model,glm_intro.Rmd,True,False,True,False,17,0,17,"---FILE: glm_intro.Rmd---
@@ -327,6 +327,23 @@ $$
 \yvec = \Xmat \bvec + \evec
 $$
 
+where:
+
+```{python}
+# Ignore this cell.
+sy_xmat3_val = Matrix.hstack(sy_x_1_val, sy_x_2_val, sy_o_val)
+sy_Xmat = symbols(r'\boldsymbol{X}')
+Eq(sy_Xmat, sy_xmat3_val, evaluate=False)
+```
+
+and therefore:
+
+```{python tags=c(""hide-input"")}
+# Ignore this cell.
+sy_xmat3_val = Matrix.hstack(sy_x_1_val, sy_x_2_val, sy_o_val)
+sy_beta2_val = Matrix([sy_b_1, sy_b_2, sy_c])
+Eq(sy_y_val, MatAdd(MatMul(sy_xmat3_val, sy_beta2_val), sy_e_val), evaluate=False)
+```
 
 ### Population, sample, estimate
 "
nipraxis,textbook,2aab6c970b0246f0e9a7b60f2a5b9b0d8d0c3ff6,Matthew Brett,matthew.brett@gmail.com,2022-05-23T22:47:55Z,Matthew Brett,matthew.brett@gmail.com,2022-05-23T22:47:55Z,"Fixes to GLM intro

FIxes for column order.  Hide-input on Sympy cells",glm_intro.Rmd,True,False,True,False,60,55,115,"---FILE: glm_intro.Rmd---
@@ -7,7 +7,7 @@ jupyter:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.13.7
+      jupytext_version: 1.10.3
   kernelspec:
     display_name: Python 3
     language: python
@@ -144,8 +144,9 @@ vectors.
 
 $\yvec$ is the vector of values $y_1 ... y_{12}$.
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
 # Ignore this cell - it is to give a better display to the mathematics.
+# Inport Symbolic mathematics routines.
 from sympy import (Eq, Matrix, IndexedBase, symbols)
 from sympy.matrices import MatAdd, MatMul
 
@@ -156,7 +157,7 @@ Eq(sy_y, sy_y_val, evaluate=False)
 
 $\xvec$ is the vector of values $x_1 ... x_{12}$:
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
 # Ignore this cell.
 sy_x = symbols(r'\vec{x}')
 sy_x_val = Matrix(clammy)
@@ -165,7 +166,7 @@ Eq(sy_x, sy_x_val, evaluate=False)
 
 $\evec$ is the vector of as-yet-unknown errors $e_1 ... e_{12}$:
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
 # Ignore this cell.
 sy_e = symbols(r'\vec{\varepsilon}')
 sy_e_ind = IndexedBase(""e"")
@@ -183,7 +184,7 @@ $$
 Bear with with us for a little trick. We define $\vec{o}$ as a vector of ones,
 like this:
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
 # Ignore this cell.
 sy_o = symbols(r'\vec{o}')
 sy_o_val = Matrix([1 for i in range(1, 13)])
@@ -198,7 +199,7 @@ $$
 
 because $o_i = 1$ and so $co_i = c$.
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
 # Ignore this cell.
 sy_b, sy_c = symbols('b, c')
 rhs = MatAdd(MatMul(sy_b, sy_x_val), MatMul(sy_c, sy_o_val), sy_e_val)
@@ -207,7 +208,7 @@ Eq(sy_y, rhs, evaluate=False)
 
 This evaluates to the result we intend:
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
 # Ignore this cell.
 Eq(sy_y, sy_c * sy_o_val + sy_b * sy_x_val + sy_e_val, evaluate=False)
 ```
@@ -216,26 +217,27 @@ $\newcommand{Xmat}{\boldsymbol X} \newcommand{\bvec}{\vec{\beta}}$
 
 We can now rephrase the calculation in terms of matrix multiplication.
 
-Call $\Xmat$ the matrix of two columns, where the first column is the column
-of ones ($\vec{o}$ above) and the second column is $\xvec$. Call $\bvec$
-the column vector:
+Call $\Xmat$ the matrix of two columns, where the first column is $\xvec$ and 
+the second column is the column of ones ($\vec{o}$ above). Call $\bvec$ the
+column vector:
 
 $$
 \left[
 \begin{array}{\bvec}
-c \\
 b \\
+c \\
 \end{array}
 \right]
 $$
 
-This gives us exactly the same calculation as $\yvec = c + b \xvec + \evec$ above, but in terms of matrix multiplication:
+This gives us exactly the same calculation as $\yvec = b \xvec + c + \evec$
+above, but in terms of matrix multiplication:
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
+# Ignore this cell.
 sy_beta_val = Matrix([[sy_b], [sy_c]])
 sy_xmat_val = Matrix.hstack(sy_x_val, sy_o_val)
-eq = Eq(sy_y_val, MatAdd(MatMul(sy_xmat_val, sy_beta_val), sy_e_val), evaluate=False)
-eq
+Eq(sy_y_val, MatAdd(MatMul(sy_xmat_val, sy_beta_val), sy_e_val), evaluate=False)
 ```
 
 In symbols:
@@ -245,9 +247,11 @@ $$
 $$
 
 
-Because of the way that matrix multiplication works, this again gives us the intended result:
+Because of the way that matrix multiplication works, this again gives us the
+intended result:
 
-```{python tags=c(""hide-cell"")}
+```{python tags=c(""hide-input"")}
+# Ignore this cell.
 Eq(sy_y_val, sy_xmat_val @ sy_beta_val + sy_e_val, evaluate=False)
 ```
 
@@ -278,15 +282,15 @@ $$
 y_i = b_1 x_{1, i} + b_2 x_{2, i} + c + e_i
 $$
 
-In this model $\Xmat$ has three columns (ones, $\xvec_1$, and
-$\xvec_2$), and the $\bvec$ vector has three values $c, b_1,
-b_2$. This gives the same matrix formulation, with our new $\Xmat$ and
-$\bvec$: $\yvec = \Xmat \bvec + \evec$.
+In this model $\Xmat$ has three columns ($\xvec_1$, $\xvec_2$ and ones), and
+the $\bvec$ vector has three values $b_1, b_2, c$. This gives the same matrix
+formulation, with our new $\Xmat$ and $\bvec$: $\yvec = \Xmat \bvec + \evec$.
 
 This is a *linear* model because our model says that the data $y_i$ comes
-from the *sum* of some components ($c, b_1 x_{1, i}, b_2 x_{2, i}, e_i$).
+from the *sum* of some components ($b_1 x_{1, i}, b_2 x_{2, i}, c, e_i$).
 
-We can keep doing this by adding more and more regressors. In general, a linear model with $p$ predictors looks like this:
+We can keep doing this by adding more and more regressors. In general, a linear
+model with $p$ predictors looks like this:
 
 $$
 y_i = b_1 x_{1, i} + b_2 x_{2, i} + ... b_p x_{p, i} + e_i
@@ -326,10 +330,9 @@ Let’s assume that we want an estimate for the line parameters (intercept
 and slope) that gives the smallest “distance” between the estimated
 values (predicted from the line), and the actual values (the data).
 
-We’ll define ‘distance’ as the squared difference of the predicted value
-from the actual value. These are the squared error terms
-$e_1^2, e_2^2 ... e_{n}^2$, where $n$ is the number of
-observations - 12 in our case.
+We’ll define ‘distance’ as the squared difference of the predicted value from
+the actual value. These are the squared error terms $e_1^2, e_2^2 ... e_{n}^2$,
+where $n$ is the number of observations; 12 in our case.
 
 As a reminder: our model is:
 
@@ -402,13 +405,13 @@ Using this matrix algebra, what line do we estimate for `psychopathy`
 and `clammy`?
 
 ```{python}
-X = np.column_stack((np.ones(12), clammy))
+X = np.column_stack((clammy, np.ones(12)))
 X
 ```
 
 ```{python}
 # Use the pseudoinverse to get estimated B
-B = npl.pinv(X).dot(psychopathy)
+B = npl.pinv(X) @ psychopathy
 B
 ```
 
@@ -433,20 +436,20 @@ plt.title('Clammy vs psychopathy with best line')
 plt.legend();
 ```
 
-Our claim was that this estimate for slope and intercept minimize the mean (and therefore sum) of
-squared error:
+Our claim was that this estimate for slope and intercept minimize the sum (and
+therefore mean) of squared error:
 
 ```{python}
-fitted = X.dot(B)
+fitted = X @ B
 errors = psychopathy - fitted
 print(np.sum(errors ** 2))
 ```
 
-Is this sum of squared errors smaller than our earlier guess of an intercept
-of 10 and a slope of 0.9?
+Is this sum of squared errors smaller than our earlier guess of an intercept of
+10 and a slope of 0.9?
 
 ```{python}
-fitted = X.dot([10, 0.9])
+fitted = X @ [0.9, 10]
 errors = psychopathy - fitted
 print(np.sum(errors ** 2))
 ```
@@ -473,15 +476,14 @@ of the best fit line between `clammy` and `psychopathy` would be
 different from zero.
 
 In our model, we have two predictors, the column of ones and `clammy`.
-$p = 2$ and $\bhat$ is length 2. We could choose just the second of
-the values in $\bhat$ (therefore $b_1$ if $b_0$ is the first
-value) with a contrast:
+$p = 2$ and $\bhat$ is length 2. We could choose just the first of
+the values in $\bhat$ with a contrast:
 
 $$
 \left[
 \begin{array}{\cvec}
-0 \\
 1 \\
+0 \\
 \end{array}
 \right]
 $$
@@ -510,22 +512,22 @@ from scipy.stats import t as t_dist
 def t_stat(y, X, c):
     """""" betas, t statistic and significance test given data, design matrix, contrast
 
-    This is OLS estimation; we assume the errors to have independent
-    and identical normal distributions around zero for each $i$ in
+    This is Ordinary Least Square estimation; we assume the errors to have
+    independent and identical normal distributions around zero for each $i$ in
     $\e_i$ (i.i.d).
     """"""
     # Make sure y, X, c are all arrays
     y = np.asarray(y)
     X = np.asarray(X)
     c = np.atleast_2d(c).T  # As column vector
     # Calculate the parameters - b hat
-    beta = npl.pinv(X).dot(y)
+    beta = npl.pinv(X) @ y
     # The fitted values - y hat
-    fitted = X.dot(beta)
+    fitted = X @ beta
     # Residual error
     errors = y - fitted
     # Residual sum of squares
-    RSS = (errors**2).sum(axis=0)
+    RSS = np.sum(errors**2, axis=0)
     # Degrees of freedom is the number of observations n minus the number
     # of independent regressors we have used.  If all the regressor
     # columns in X are independent then the (matrix rank of X) == p
@@ -536,16 +538,18 @@ def t_stat(y, X, c):
     # Mean residual sum of squares
     MRSS = RSS / df
     # calculate bottom half of t statistic
-    SE = np.sqrt(MRSS * c.T.dot(npl.pinv(X.T.dot(X)).dot(c)))
-    t = c.T.dot(beta) / SE
-    # Get p value for t value using cumulative density dunction
-    # (CDF) of t distribution
+    SE = np.sqrt(MRSS * c.T @ npl.pinv(X.T @ X) @ c)
+    t = c.T @ beta / SE
+    # Get p value for t value using cumulative density function
+    # (CDF) of the t distribution.
     ltp = t_dist.cdf(t, df) # lower tail p
     p = 1 - ltp # upper tail p
     return beta, t, df, p
 ```
 
-See [p values from cumulative distribution functions](https://matthew-brett.github.io/teaching/on_cdfs.html) for background on the probability values.
+See [p values from cumulative distribution
+functions](https://matthew-brett.github.io/teaching/on_cdfs.html) for
+background on the probability values.
 
 So, does `clammy` predict `psychopathy`? If it does not, then our
 estimate of the slope will not be convincingly different from 0. The t
@@ -554,16 +558,17 @@ large values mean that the slope is large compared to the error in the
 estimate.
 
 ```{python}
-X = np.column_stack((np.ones(12), clammy))
+X = np.column_stack((clammy, np.ones(12)))
 Y = np.asarray(psychopathy)
-B, t, df, p = t_stat(Y, X, [0, 1])
+B, t, df, p = t_stat(Y, X, [1, 0])
 t, p
 ```
 
 (dummy-coding)=
 
 ## Dummy coding and the general linear model
 
+
 So far we have been doing *multiple regression*. That is, all the columns
 (except the column of ones) are continuous vectors of numbers predicting our
 outcome data `psychopathy`. These type of predictors are often called
@@ -628,14 +633,14 @@ When we estimate these using the least squares method, what estimates
 will we get for $\bhat$?
 
 ```{python}
-B = npl.pinv(X).dot(psychopathy)
+B = npl.pinv(X) @ psychopathy
 B
 ```
 
 ```{python}
-np.mean(psychopathy[:4])
-np.mean(psychopathy[4:8])
-np.mean(psychopathy[8:])
+print('Berkeley mean:', np.mean(psychopathy[:4]))
+print('Stanford mean:', np.mean(psychopathy[4:8]))
+print('MIT mean:', np.mean(psychopathy[8:]))
 ```
 
 It looks like the MIT students are a bit more psychopathic. Are they more"
nipraxis,textbook,f5971f70e7e98935226fff53fe613b1572c77615,Matthew Brett,matthew.brett@gmail.com,2022-05-23T13:49:27Z,Matthew Brett,matthew.brett@gmail.com,2022-05-23T13:49:42Z,Adapt toc to voxel / regression stuff,_toc.yml;model_one_voxel.Rmd;regress_one_voxel.Rmd,True,False,True,False,206,42,248,"---FILE: _toc.yml---
@@ -49,12 +49,13 @@ parts:
   - file: on_convolution
   - file: convolution_background
   - file: on_regression
+  - file: regress_one_voxel
   - file: glm_intro
-  - file: model_one_voxel
   - file: mean_test_example
+  - file: model_one_voxel
+  - file: multi_multiply
   - file: hypothesis_tests
   - file: test_one_voxel
-  - file: multi_multiply
   - file: non_tr_onsets
   - file: whole_image_statistics
 

---FILE: model_one_voxel.Rmd---
@@ -7,7 +7,7 @@ jupyter:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.10.3
+      jupytext_version: 1.13.7
   kernelspec:
     display_name: Python 3
     language: python
@@ -16,12 +16,12 @@ jupyter:
 
 # Modeling a single voxel
 
-Earlier – [Voxel time courses](voxel_time_courses.Rmd) – we were looking
-at a single voxel time course.
+The end of the [voxel regression page](regress_one_voxel.Rmd) has a worked
+example of applying [General Linear
+Model](https://textbook.nipraxis.org/glm_intro.html) (GLM) to do regression
+for a single voxel.
 
-Here we use the [General Linear
-Model](https://matthew-brett.github.io/teaching/glm_intro.html) formulation to
-do a test on a single voxel.
+This page runs the same calculations, but also unpacks the GLM notation that we are using in the calculation.
 
 Let’s get that same voxel time course back again:
 
@@ -33,59 +33,46 @@ import nibabel as nib
 np.set_printoptions(precision=6)
 ```
 
-We load the data, and knock off the first four volumes to remove the
-artefact we discovered in First go at brain activation exercise:
-
 ```{python}
 # Load the function to fetch the data file we need.
 import nipraxis
 # Fetch the data file.
 data_fname = nipraxis.fetch_file('ds114_sub009_t2r1.nii')
-# Show the file name of the fetched data.
-data_fname
-```
-
-```{python}
 img = nib.load(data_fname)
 data = img.get_fdata()
+# Knock off the first four volumes (to avoid artefact).
 data = data[..., 4:]
-```
-
-The voxel coordinate (3D coordinate) that we were looking at in
-Voxel time courses was at (42, 32, 19):
-
-```{python}
+# Get the voxel time course of interest.
 voxel_time_course = data[42, 32, 19]
 plt.plot(voxel_time_course)
 ```
 
-Now we are going to use the convolved regressor from [Convolving with the
-hemodyamic response function](convolution_background) to do a simple
-regression on this voxel time course.
-
-First fetch the text file with the convolved time course:
+Load the convolved time course, and plot the voxel values against the convolved regressor:
 
 ```{python}
 tc_fname = nipraxis.fetch_file('ds114_sub009_t2r1_conv.txt')
 # Show the file name of the fetched data.
-tc_fname
-```
-
-```{python}
 convolved = np.loadtxt(tc_fname)
-# Knock off first 4 elements to match data
+# Knock off first 4 elements to match data.
 convolved = convolved[4:]
-plt.plot(convolved)
+# Plot.
+plt.scatter(convolved, voxel_time_course)
+plt.xlabel('Convolved prediction')
+plt.ylabel('Voxel values')
 ```
 
+As you remember, we apply the GLM by first preparing a design matrix, that has one column corresponding for each *parameter* in the *model*.
+
+In our case we have two parameters, the *slope* and the *intercept*.
+
 First we make our *design matrix*.  It has a column for the convolved
 regressor, and a column of ones:
 
 ```{python}
 N = len(convolved)
 X = np.ones((N, 2))
 X[:, 0] = convolved
-plt.imshow(X, interpolation='nearest', cmap='gray', aspect=0.1)
+plt.imshow(X, cmap='gray', aspect=0.1)
 ```
 
 $\newcommand{\yvec}{\vec{y}}$
@@ -94,14 +81,14 @@ $\newcommand{\evec}{\vec{\varepsilon}}$
 $\newcommand{Xmat}{\boldsymbol X} \newcommand{\bvec}{\vec{\beta}}$
 $\newcommand{\bhat}{\hat{\bvec}} \newcommand{\yhat}{\hat{\yvec}}$
 
-As you will remember from the [introduction to the General Linear Model](https://matthew-brett.github.io/teaching/glm_intro.html), our
-model is:
+Our model is:
 
 $$
 \yvec = \Xmat \bvec + \evec
 $$
 
-We can get our least squares parameter *estimates* for $\bvec$ with:
+We can get our least mean squared error (MSE) parameter *estimates* for
+$\bvec$ with:
 
 $$
 \bhat = \Xmat^+y
@@ -125,16 +112,21 @@ Xp.shape
 We calculate $\bhat$:
 
 ```{python}
-beta_hat = Xp.dot(voxel_time_course)
+beta_hat = Xp @ voxel_time_course
 beta_hat
 ```
 
 We can then calculate $\yhat$ (also called the *fitted data*):
 
 ```{python}
-y_hat = X.dot(beta_hat)
+y_hat = X @ beta_hat
+```
+
+Finally, we may be interested to calculate the MSE of this model:
+
+```{python}
+# Residuals are actual minus fitted.
 e_vec = voxel_time_course - y_hat
-print(np.sum(e_vec ** 2))
-plt.plot(voxel_time_course)
-plt.plot(y_hat)
+mse = np.mean(e_vec ** 2)
+mse
 ```

---FILE: regress_one_voxel.Rmd---
@@ -0,0 +1,171 @@
+---
+jupyter:
+  jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
+    text_representation:
+      extension: .Rmd
+      format_name: rmarkdown
+      format_version: '1.2'
+      jupytext_version: 1.13.7
+  kernelspec:
+    display_name: Python 3
+    language: python
+    name: python3
+---
+
+# Regression for a single voxel
+
+Earlier – [Voxel time courses](voxel_time_courses.Rmd) – we were looking
+at a single voxel time course.
+
+Here we use the [General Linear
+Model](https://textbook.nipraxis.org/glm_intro.html) formulation to
+do a test on a single voxel.
+
+Let’s get that same voxel time course back again:
+
+```{python}
+import numpy as np
+import matplotlib.pyplot as plt
+import nibabel as nib
+# Only show 6 decimals when printing
+np.set_printoptions(precision=6)
+```
+
+We load the data, and knock off the first four volumes to remove the
+artefact we discovered in First go at brain activation exercise:
+
+```{python}
+# Load the function to fetch the data file we need.
+import nipraxis
+# Fetch the data file.
+data_fname = nipraxis.fetch_file('ds114_sub009_t2r1.nii')
+# Show the file name of the fetched data.
+data_fname
+```
+
+```{python}
+img = nib.load(data_fname)
+data = img.get_fdata()
+data = data[..., 4:]
+```
+
+The voxel coordinate (3D coordinate) that we were looking at in
+Voxel time courses was at (42, 32, 19):
+
+```{python}
+voxel_time_course = data[42, 32, 19]
+plt.plot(voxel_time_course)
+```
+
+Now we are going to use the convolved regressor from [Convolving with the
+hemodyamic response function](convolution_background) to do a simple
+regression on this voxel time course.
+
+First fetch the text file with the convolved time course:
+
+```{python}
+tc_fname = nipraxis.fetch_file('ds114_sub009_t2r1_conv.txt')
+# Show the file name of the fetched data.
+tc_fname
+```
+
+```{python}
+convolved = np.loadtxt(tc_fname)
+# Knock off first 4 elements to match data
+convolved = convolved[4:]
+plt.plot(convolved)
+```
+
+Finally, we plot the convolved prediction and the time-course together:
+
+```{python}
+plt.scatter(convolved, voxel_time_course)
+plt.xlabel('Convolved prediction')
+plt.ylabel('Voxel values')
+```
+
+## Using correlation-like calculations
+
+We can get the best-fitting line using the calculations from the [regression page](on_regression.Rmd):
+
+```{python}
+def calc_z_scores(arr):
+    """""" Calculate z-scores for array `arr`
+    """"""
+    return (arr - np.mean(arr)) / np.std(arr)
+```
+
+```{python}
+# Correlation
+r = np.mean(calc_z_scores(convolved) * calc_z_scores(voxel_time_course))
+r
+```
+
+The best fit line is:
+
+```{python}
+best_slope = r * np.std(voxel_time_course) / np.std(convolved)
+print('Best slope:', best_slope)
+best_intercept = np.mean(voxel_time_course) - best_slope * np.mean(convolved)
+print('Best intercept:', best_intercept)
+```
+
+```{python}
+plt.scatter(convolved, voxel_time_course)
+x_vals = np.array([np.min(convolved), np.max(convolved)])
+plt.plot(x_vals, best_intercept + best_slope * x_vals, 'r:')
+plt.xlabel('Convolved prediction')
+plt.ylabel('Voxel values')
+```
+
+Using Scipy:
+
+```{python}
+import scipy.stats as sps
+sps.linregress(convolved, voxel_time_course)
+```
+
+## Using the General Linear Model
+
+This is a leap ahead, past the page about the [General Linear Model](glm_intro.Rmd) (GLM).  To give you a taster, the General Linear Model is just a generalization of the correlation and similar calculations above.
+
+The GLM generalizes the regression calculation so that we can add as many
+regressors as we want.  We do this by forming a *design* matrix, that contains
+the regressors, and then doing some *matrix calculations* that generalize the
+calculations above.   Don't worry about the details of those calculations for
+now.  The rest of this page shows how the calculation works out.
+
+
+First we make our *design matrix*.  It has a column for the convolved
+regressor, and a column of ones:
+
+```{python}
+N = len(convolved)
+X = np.ones((N, 2))
+X[:, 0] = convolved
+plt.imshow(X, cmap='gray', aspect=0.1)
+```
+
+Our regression model has two *parameters* that we want to *estimate*.  These are the *slope* for the convolved regressor, and the intercept - as they were in the calculations above.
+
+You will find in the [introduction to the General Linear
+Model](https://textbook.nipraxis.org/glm_intro.html), that we can calculate
+the best-fit (least mean-squared-error) parameters using the following matrix
+calculation:
+
+```{python}
+import numpy.linalg as npl
+
+# We call the parameters *beta*
+beta = npl.pinv(X) @ voxel_time_course
+beta
+```
+
+You will notice that the GLM, as we would hope, has found the same parameters
+as we found in the correlation and `scipy.stats.linregress` calculations:
+
+```{python}
+assert np.allclose(beta, [best_slope, best_intercept])
+```"
nipraxis,textbook,5bc1f56651372e249bc0ccc98b2bbd539abca45c,Matthew Brett,matthew.brett@gmail.com,2022-05-20T11:54:24Z,Matthew Brett,matthew.brett@gmail.com,2022-05-20T11:54:24Z,"Extend explanation of regression

We probably don't need to go over all this, but wave at the middle, and
then show the end.",on_regression.Rmd,True,False,True,False,122,27,149,"---FILE: on_regression.Rmd---
@@ -289,11 +289,17 @@ x_z = (x - np.mean(x)) / np.std(x)
 y_z = (y - np.mean(y)) / np.std(y)
 ```
 
-While we are here, let's calculate the correlation:
+While we are here, let's calculate the correlation.  We will all the correlation value `r`, because that's the standard letter for the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) that we are using here.  In fact, it is often called ""Pearson's r"" - see the Wikipedia page.
 
 ```{python}
-correlation = np.mean(x_z * y_z)
-correlation
+# Calculate the correlation
+r = np.mean(x_z * y_z)
+r
+```
+
+```{python}
+# Or course this is (within tiny floating-point error) the same as:
+np.corrcoef(x, y)[0, 1]
 ```
 
 When we plot these z-score versions, we see that an intercept of 0 looks about
@@ -308,10 +314,38 @@ Let's try a slope of about 0.6, and an intercept of 0.
 
 ```{python}
 pred_y_z = 0 + 0.6 * x_z
-plt.plot(x_z, y_z, '+')
-plt.plot(x_z, pred_y_z, 'ro')
-plt.title('x, y in z-score form, with predictions')
+plt.plot(x_z, y_z, '+', label='Actual')
+plt.plot(x_z, pred_y_z, 'ro', label='Predicted')
+# Draw the predicting line
+x_lims = np.array([-2, 2])
+plt.plot(x_lims, 0 + x_lims * 0.6, ':')
+# Draw the errors
+for i in np.arange(len(x_z)):
+    plt.plot([x_z[i], x_z[i]], [y_z[i], pred_y_z[i]], 'k:')
+plt.title('x, y in z-score form, with guessed line and predictions')
+plt.legend()
+```
+In fact, let's make that into a function to do some plots later:
+
+```{python}
+def plot_line_data(x, y, slope, inter, x_lims):
+    pred_y = inter + slope * x
+    plt.plot(x, y, '+', label='Actual')
+    plt.plot(x, pred_y, 'ro', label='Predicted')
+    # Draw the predicting line
+    plt.plot(x_lims, inter + slope * np.array(x_lims), ':')
+    # Draw the errors
+    for i in np.arange(len(x)):
+        plt.plot([x[i], x[i]], [y[i], pred_y[i]], 'k:')
+    plt.legend()
+    return pred_y
+```
+
+```{python}
+plot_line_data(x_z, y_z, 0.6, 0, np.array([-2, 2]))
+plt.title('Same graph using function')
 ```
+
 The MSE is:
 
 ```{python}
@@ -356,24 +390,35 @@ plt.ylabel('MSE for intercept')
 plt.title('MSE values for intercepts, slope=0.6')
 ```
 
+This is the minimum (best) value we found for MSE, for all the intercepts we tried.
+
 ```{python}
 np.min(mses_for_intercepts)
 ```
 
+The `np.argmin` function tells us the *index* (position) of this minimum value.
+
 ```{python}
 min_index = np.argmin(mses_for_intercepts)
 min_index
 ```
 
+By the definition of the `np.argmin` function, the value of `mses_for_intercepts` at this index, is the minimum.
+
 ```{python}
 mses_for_intercepts[min_index]
 ```
 
+Now we know the index of the minimum value, we can find the intercept value that corresponds to this MSE value:
+
 ```{python}
 intercepts_to_try[min_index]
 ```
 
-Try a different slope - say 0.2
+Sure enough, as we asserted before, 0 was the intercept value giving the lowest (best) MSE, for all the intercepts we tried.
+
+
+But - maybe that is only true for our particular slope: 0.6.  We will try a different slope - say 0.2
 
 ```{python}
 # Corresponding MSE values
@@ -400,7 +445,10 @@ mses_for_intercepts_0p2[min_index_0p2]
 intercepts_to_try[min_index_0p2]
 ```
 
-Now let's find the best slope.
+Our preliminary guess - that turns out to be right when we analyze this mathematically - is that 0 is *always* the best intercept for z-scored x and y, regardless of slope, and regardless of the x and y that we z-scored.
+
+
+Now let's find the best slope, assuming an intercept of 0.
 
 ```{python}
 # -1 to 1 in steps of 0.001
@@ -427,49 +475,89 @@ best_slope = slopes_to_try[min_index_slope]
 print('Slope giving min MSE', best_slope)
 ```
 
-Wait - but `best_slope` is almost exactly the same as the correlation!.  And in fact, it is not exactly the same only because we are not trying every value for `slope`, but only in steps 0.001 apart.  With the `correlation` value for `slope`, we get value for MSE that is even a tiny bit *lower* than the best value we found by searching.
+Wait - but `best_slope` is almost exactly the same as the correlation!.  And in fact, it is not exactly the same only because we are not trying every value for `slope`, but only in steps 0.001 apart.  With the `r` (correlation) value for `slope`, we get value for MSE that is even a tiny bit *lower* than the best value we found by searching.
+
+```{python}
+calc_mse(x_z, y_z, r, 0)
+```
+
+We have discovered that the r value (correlation) is also the slope of the best-fit (MSE) line for the z-scored x and y values.
+
+
+Here's the best-fit line, using, using `r`:
+
+```{python}
+plot_line_data(x_z, y_z, r, 0, np.array([-2, 2]))
+plt.title('Best-fit (MSE) line to z-scored data')
+```
+
+## Best fit line for the original data
+
+
+We now have the best-fit line for the z-score data.  What does this line look like for the original data, before the z-score transformation?
+
+To answer this question, we gradually undo the z-score transformations, while keeping track on the effect on the best-bit line.  First let's multiply the `y_z` values by the standard deviation of the original `y`s to undo that part of the z-transformation.  The result is the `y` values that just have the mean subtracted.
 
 ```{python}
-calc_mse(x_z, y_z, correlation, 0)
+y_minus_mean = y_z * np.std(y)  # Undo division by standard deviation
+plot_line_data(x_z, y_minus_mean, r * np.std(y), 0, np.array([-2, 2]))
+plt.title('Best-fit (MSE) line to z-scored data')
 ```
 
-The *correlation* is also the slope of the best-fit (MSE) line for the z-scored x and y values.
+Notice that, when we do this, the `y` values all stretch on the `y` axis by a factor of `np.std(y)` How about slope of the best-fit line?
 
-## Regression
+Remember, the slope is the number of units that y increases for a one-unit increase in x.  Because the y values scale by a factor of `np.std(y)`, the number of units of y for a one-unit increase in x also scales by a factor of `np.std(y)`, and the equivalent best-fit line has slope `r * np.std(y)`.
 
 
-For fairly simple reasons, but reasons we won't go into here, we can use the best fit line for the z-scores to give the best fit line for the original values.   First we get the best-fit slope by scaling the correlation by the standard deviations:
+Next we undo the `np.std(x)` transformation the `x_z` values.
 
 ```{python}
-best_original_slope = correlation * np.std(y) / np.std(x)
-best_original_slope
+x_minus_mean = x_z * np.std(x)  # Undo division by standard deviation on x.
+pred_y_minus_mean = plot_line_data(x_minus_mean, y_minus_mean, r * np.std(y) / np.std(x), 0, np.array([-5, 5]))
+plt.title('Best-fit (MSE) line to z-scored data')
 ```
 
-In fact, given we know the z-score best-fit intercept was 0, the slope tells us what the best-fit intecept must be for the original data:
+Now the `x_z` values stretch by a factor of `np.std(x)` along the x-axis.  The previous y increase for one unit in x (the slope) becomes the y increase for a `1 * np.std(x)` increase on x, so, to get the new slope, we divide the previous slope by `np.std(x)`.
+
+
+The last thing we need to do is undo the subtraction of `np.mean(y)` and `np.mean(x)`.   When we do this, we shift all the points `np.mean(y)` up on the y-axis, and `np.mean(x)` right on the x-axis, like this:
 
 ```{python}
-best_original_intercept = np.mean(y) - best_original_slope * np.mean(x)
-best_original_intercept
+y_back_again = y_minus_mean + np.mean(y)  # Obviously also == y
+x_back_again = x_minus_mean + np.mean(x)  # Obviously also == x
+pred_y_back_again = pred_y_minus_mean + np.mean(y)
+plt.plot(x_back_again, y_back_again, '+')
+plt.plot(x_back_again, pred_y_back_again, 'ro')
 ```
 
+It might seem intuitive, and it is in fact correct, that when we shift the points on the graph like this, we can shift the line with them.  That is, the line just moves with the points, as you can see from the updated predictions above.  So, the slope does not change, but the point that the line hits the y axis does - and that is the intercept.
+
+The intercept for the graph before the shift was 0, 0.  After the shift, that point has moved to `np.mean(x), np.mean(y)`.  To track back along the line to the y-axis, we move `-np.mean(x)` units along the line.   So, in terms of y, we start at `np.mean(y)` then move `slope * -np.mean(y)` units as we move along the x-axis.  The new intercept is therefore:
+
 ```{python}
-calc_mse(x, y, best_original_slope, best_original_intercept)
+new_slope = r * np.std(y) / np.std(x)
+new_intercept = np.mean(y) - new_slope * np.mean(x)
+new_intercept
 ```
 
 ```{python}
-best_predicted_y = best_original_intercept + best_original_slope * x
+plot_line_data(x, y, new_slope, new_intercept, np.array([0, 9]))
+plt.title('Best-fit (MSE) line to orinal data, reconstructed from r')
 ```
 
+Thus, the best slope and intercept for the original data are the values we've just calculated:
+
 ```{python}
-# Plot the data with the new line
-plt.plot(x, y, '+', label='Actual values')
-# Plot the predicted values
-plt.plot(x, best_predicted_y, 'ro', label='Values predicted from line')
-# Plot the distance between predicted and actual, for all points.
-for i in range(n):
-    plt.plot([x[i], x[i]], [best_predicted_y[i], y[i]], 'k:')
+best_original_slope = r * np.std(y) / np.std(x)
+best_original_intercept = np.mean(y) - best_original_slope * np.mean(x)
 ```
 
+```{python}
+calc_mse(x, y, best_original_slope, best_original_intercept)
+```
+
+To confirm that is the case, we would have to calculate the MSE for all possible intercept and slope pairs, but here we just confirm that if we assume the intercept is correct, we have the best slope:
+
 ```{python}
 orig_slopes_to_try = np.arange(2000) / 1000
 n_orig_slopes = len(orig_slopes_to_try)
@@ -488,6 +576,8 @@ plt.plot(orig_slopes_to_try, mses_for_orig_slopes)
 orig_slopes_to_try[np.argmin(mses_for_orig_slopes)]
 ```
 
+And, if we assume the slope is correct, we have the best intercept:
+
 ```{python}
 orig_inters_to_try = np.arange(2000) / 100
 n_orig_inters = len(orig_inters_to_try)
@@ -506,6 +596,11 @@ plt.plot(orig_inters_to_try, mses_for_orig_inters)
 orig_inters_to_try[np.argmin(mses_for_orig_inters)]
 ```
 
+**Question**: How would you try a large number of slope, intercept pairs to see which is the best?  **Hint**: You might consider using a 2D array somewhere.
+
+## Automating the best-slope calculation with Scipy
+
+
 We can ask Scipy to use a calculation that is based on the correlation calculation above, to get this best slope and intercept automatically:
 
 ```{python}"
nipraxis,textbook,ecf0657885ae43a85c0fd359a18dae52f879226b,Matthew Brett,matthew.brett@gmail.com,2022-05-19T18:45:13Z,Matthew Brett,matthew.brett@gmail.com,2022-05-19T18:45:28Z,Extend discussion of regression,on_regression.Rmd,True,False,True,False,206,0,206,"---FILE: on_regression.Rmd---
@@ -289,6 +289,13 @@ x_z = (x - np.mean(x)) / np.std(x)
 y_z = (y - np.mean(y)) / np.std(y)
 ```
 
+While we are here, let's calculate the correlation:
+
+```{python}
+correlation = np.mean(x_z * y_z)
+correlation
+```
+
 When we plot these z-score versions, we see that an intercept of 0 looks about
 right:
 
@@ -312,3 +319,202 @@ errors = y_z - pred_y_z
 mse_for_guess = np.mean(errors ** 2)
 mse_for_guess
 ```
+
+Our life will be easier with a function to calculate the MSE for a given slope and intercept.
+
+```{python}
+def calc_mse(x, y, slope, inter):
+    predicted = inter + slope * x
+    errors = y - predicted
+    return np.mean(errors ** 2)
+```
+
+```{python}
+calc_mse(x_z, y_z, 0.6, 0)
+```
+
+We assumed that 0 was the *best* intercept in the sense of giving the smallest MSE, but is that true?
+
+Let's try lots of intercepts, with a given slope of 0.6, and see what MSE values we get.
+
+```{python}
+# -1 to 1 in steps of 0.001
+intercepts_to_try = np.arange(2000) / 1000 - 1
+n_intercepts = len(intercepts_to_try)
+# Corresponding MSE values
+mses_for_intercepts = np.zeros(n_intercepts)
+for i in np.arange(n_intercepts):
+    inter = intercepts_to_try[i]
+    mses_for_intercepts[i] = calc_mse(x_z, y_z, 0.6, inter)
+mses_for_intercepts[:10]
+```
+
+```{python}
+plt.plot(intercepts_to_try, mses_for_intercepts)
+plt.xlabel('intercept')
+plt.ylabel('MSE for intercept')
+plt.title('MSE values for intercepts, slope=0.6')
+```
+
+```{python}
+np.min(mses_for_intercepts)
+```
+
+```{python}
+min_index = np.argmin(mses_for_intercepts)
+min_index
+```
+
+```{python}
+mses_for_intercepts[min_index]
+```
+
+```{python}
+intercepts_to_try[min_index]
+```
+
+Try a different slope - say 0.2
+
+```{python}
+# Corresponding MSE values
+mses_for_intercepts_0p2 = np.zeros(n_intercepts)
+for i in np.arange(n_intercepts):
+    inter = intercepts_to_try[i]
+    mses_for_intercepts_0p2[i] = calc_mse(x_z, y_z, 0.2, inter)
+mses_for_intercepts[:10]
+```
+
+```{python}
+plt.plot(intercepts_to_try, mses_for_intercepts_0p2)
+plt.xlabel('intercept')
+plt.ylabel('MSE for intercept')
+plt.title('MSE values for intercepts, slope=0.2')
+```
+
+```{python}
+min_index_0p2 = np.argmin(mses_for_intercepts_0p2)
+mses_for_intercepts_0p2[min_index_0p2]
+```
+
+```{python}
+intercepts_to_try[min_index_0p2]
+```
+
+Now let's find the best slope.
+
+```{python}
+# -1 to 1 in steps of 0.001
+slopes_to_try = np.arange(2000) / 1000 - 1
+n_slopes = len(slopes_to_try)
+mses_for_slopes = np.zeros(n_slopes)
+for i in np.arange(n_slopes):
+    slope = slopes_to_try[i]
+    mses_for_slopes[i] = calc_mse(x_z, y_z, slope, 0)
+mses_for_slopes[:10]
+```
+
+```{python}
+plt.plot(slopes_to_try, mses_for_slopes)
+plt.xlabel('slope')
+plt.ylabel('MSE for slope')
+plt.title('MSE values for slopes, intercept=0')
+```
+
+```{python}
+min_index_slope = np.argmin(mses_for_slopes)
+print('Min MSE for slopes', mses_for_slopes[min_index_slope])
+best_slope = slopes_to_try[min_index_slope]
+print('Slope giving min MSE', best_slope)
+```
+
+Wait - but `best_slope` is almost exactly the same as the correlation!.  And in fact, it is not exactly the same only because we are not trying every value for `slope`, but only in steps 0.001 apart.  With the `correlation` value for `slope`, we get value for MSE that is even a tiny bit *lower* than the best value we found by searching.
+
+```{python}
+calc_mse(x_z, y_z, correlation, 0)
+```
+
+The *correlation* is also the slope of the best-fit (MSE) line for the z-scored x and y values.
+
+## Regression
+
+
+For fairly simple reasons, but reasons we won't go into here, we can use the best fit line for the z-scores to give the best fit line for the original values.   First we get the best-fit slope by scaling the correlation by the standard deviations:
+
+```{python}
+best_original_slope = correlation * np.std(y) / np.std(x)
+best_original_slope
+```
+
+In fact, given we know the z-score best-fit intercept was 0, the slope tells us what the best-fit intecept must be for the original data:
+
+```{python}
+best_original_intercept = np.mean(y) - best_original_slope * np.mean(x)
+best_original_intercept
+```
+
+```{python}
+calc_mse(x, y, best_original_slope, best_original_intercept)
+```
+
+```{python}
+best_predicted_y = best_original_intercept + best_original_slope * x
+```
+
+```{python}
+# Plot the data with the new line
+plt.plot(x, y, '+', label='Actual values')
+# Plot the predicted values
+plt.plot(x, best_predicted_y, 'ro', label='Values predicted from line')
+# Plot the distance between predicted and actual, for all points.
+for i in range(n):
+    plt.plot([x[i], x[i]], [best_predicted_y[i], y[i]], 'k:')
+```
+
+```{python}
+orig_slopes_to_try = np.arange(2000) / 1000
+n_orig_slopes = len(orig_slopes_to_try)
+mses_for_orig_slopes = np.zeros(n_orig_slopes)
+for i in np.arange(n_orig_slopes):
+    slope = orig_slopes_to_try[i]
+    mses_for_orig_slopes[i] = calc_mse(x, y, slope, best_original_intercept)
+mses_for_orig_slopes[:10]
+```
+
+```{python}
+plt.plot(orig_slopes_to_try, mses_for_orig_slopes)
+```
+
+```{python}
+orig_slopes_to_try[np.argmin(mses_for_orig_slopes)]
+```
+
+```{python}
+orig_inters_to_try = np.arange(2000) / 100
+n_orig_inters = len(orig_inters_to_try)
+mses_for_orig_inters = np.zeros(n_orig_inters)
+for i in np.arange(n_orig_inters):
+    inter = orig_inters_to_try[i]
+    mses_for_orig_inters[i] = calc_mse(x, y, best_original_slope, inter)
+mses_for_orig_inters[:10]
+```
+
+```{python}
+plt.plot(orig_inters_to_try, mses_for_orig_inters)
+```
+
+```{python}
+orig_inters_to_try[np.argmin(mses_for_orig_inters)]
+```
+
+We can ask Scipy to use a calculation that is based on the correlation calculation above, to get this best slope and intercept automatically:
+
+```{python}
+import scipy.stats
+```
+
+```{python}
+results = scipy.stats.linregress(x, y)
+results
+```
+
+Notice the slope, intercept and correlation values are (almost precisely) the same as we found using our correlation calculations above."
nipraxis,textbook,35c43f2516d6a06731518b06a8b73682d20f170e,Christopher J. Markiewicz,markiewicz@stanford.edu,2022-05-19T15:12:43Z,Christopher J. Markiewicz,markiewicz@stanford.edu,2022-05-19T15:12:43Z,"FIX: Good enough header levels

Fixed MyST parser complaints, and tried to eyeball likely hierarchy.
It's probably wrong, so please review.",glm_intro.Rmd,True,False,True,False,9,9,18,"---FILE: glm_intro.Rmd---
@@ -29,12 +29,12 @@ regression. By adding some specially formed regressors, we can also
 express group membership, and therefore do analysis of variance. This
 last step is where multiple regression becomes the general linear model.
 
-### About this page
+## About this page
 
 We go through regression and the general linear model slowly, showing
 how it works in symbols, and in code, with actual numbers.
 
-### The example regression problem
+## The example regression problem
 
 Let’s imagine that we have measured scores for a “psychopathy”
 personality trait in 12 students. We also have some other information
@@ -146,7 +146,7 @@ above) plus some error for each observation:
 
 $y_i = c + bx_i + e_i$
 
-# Simple regression in matrix form
+## Simple regression in matrix form
 
 It turns out it will be useful to rephrase the simple regression model
 in matrix form. Let’s make the data and predictor and errors into
@@ -270,7 +270,7 @@ $$
 \yvec = \Xmat \bvec + \evec
 $$
 
-## Population, sample, estimate
+### Population, sample, estimate
 
 
 $\newcommand{\bhat}{\hat{\bvec}} \newcommand{\yhat}{\hat{\yvec}}$
@@ -283,7 +283,7 @@ we cannot get the true population $\bvec$ vector, we can only
 estimate as $\bhat$ to distinguish it from the true population
 parameters $\bvec$.
 
-## Solving the model with matrix algebra
+### Solving the model with matrix algebra
 
 
 The reason to formulate our problem with matrices is so we can use some
@@ -418,7 +418,7 @@ errors = psychopathy - fitted
 print(np.sum(errors ** 2))
 ```
 
-# Contrasts
+## Contrasts
 
 $\newcommand{cvec}{\mathbf{c}}$ We can combine the values in the
 $\bhat$ vector in different ways by using a *contrast* vector. A
@@ -528,7 +528,7 @@ t, p
 ```
 
 <!-- dummy-coding: -->
-# Dummy coding and the general linear model
+## Dummy coding and the general linear model
 
 So far we have been doing *multiple regression*. That is, all the
 columns (except the column of ones) are continuous vectors of numbers
@@ -621,7 +621,7 @@ The model above expresses the effect of group membership. It is the
 expression of a one-way analysis of variance (ANOVA) model using
 $\yvec = \Xmat \bvec + \evec$.
 
-# ANCOVA in the General Linear Model
+## ANCOVA in the General Linear Model
 
 Our formulation $\yvec = \Xmat \bvec + \evec$ makes it very easy
 to add extra regressors to models with group membership. For example, we
@@ -653,7 +653,7 @@ It looks like there’s not much independent effect of clamminess. The MIT
 students seem to have clammy hands, and once we know that the student is
 from MIT, the clammy score is not as useful.
 
-# Displaying the design matrix as an image
+## Displaying the design matrix as an image
 
 We can show the design as an image, by scaling the values with columns.
 "
nipraxis,textbook,8aa60d905b622aabfb609399cf02e238e2cdd39a,Matthew Brett,matthew.brett@gmail.com,2022-05-19T11:46:35Z,Matthew Brett,matthew.brett@gmail.com,2022-05-19T11:46:35Z,Fix link in convolution page,on_convolution.Rmd,True,False,True,False,5,4,9,"---FILE: on_convolution.Rmd---
@@ -118,10 +118,11 @@ plt.title('Estimated BOLD signal for event at time 0')
 ```
 
 This is the hemodynamic response to a neural impulse.  In signal processing
-terms this is the hemodynamic [impulse response function](impulseresponse).
-It is usually called the hemodynamic response function (HRF), because it is a
-function that gives the predicted hemodynamic response at any given time
-following an impulse at time 0.
+terms this is the hemodynamic [impulse response
+function](https://en.wikipedia.org/wiki/Impulse_response). It is usually called
+the hemodynamic response function (HRF), because it is a function that gives
+the predicted hemodynamic response at any given time following an impulse at
+time 0.
 
 ## Building the hemodynamic output from the neural input
 "
nipraxis,textbook,b29e3c1f4be6d2be63a146df3b9e6d20e31d8729,Matthew Brett,matthew.brett@gmail.com,2022-05-19T11:28:48Z,Matthew Brett,matthew.brett@gmail.com,2022-05-19T11:28:48Z,Maybe fix YaML in _toc,_toc.yml,False,False,False,False,1,0,1,"---FILE: _toc.yml---
@@ -44,6 +44,7 @@ parts:
   - file: on_testing
 
 - caption: Detecting activation
+  chapters:
   - file: voxel_time_courses
   - file: on_convolution
   - file: convolution_background"
nipraxis,textbook,65893309c3f52221e673c9fb54b858798e225025,Matthew Brett,matthew.brett@gmail.com,2022-05-12T16:46:26Z,Matthew Brett,matthew.brett@gmail.com,2022-05-12T16:46:26Z,"Avoid embarrassing error

Python in fact does not require __init__.py files in a directory, to be
importable.  It only requires `.py` files.  Without `__init__.py`, the
directory forms a namespace package.

Via PEP420 (https://peps.python.org/pep-0420/) ""Regular packages will
continue to have an __init__.py and will reside in a single directory.

Namespace packages cannot contain an __init__.py""",module_directories.Rmd,True,False,True,False,6,12,18,"---FILE: module_directories.Rmd---
@@ -80,22 +80,16 @@ Just to confirm, we show the files in `mydirmod`:
 os.listdir('mydirmod')
 ```
 
-If `mydirmod` is a directory module, let's try importing `mydirmod`:
-
-```{python tags=c(""raises-exception"")}
-import mydirmod
-```
-
-Oh dear — `mydirmod` isn't a module yet.  We have to do one more step.
+But, our work here is not yet done.  To make the `mydirmod` into a directory
+module, we have to do one more step.
 
 
 ## Making `mydirmod` into a module
 
-The key step to tell Python that `mydirmod` is a module, is to create an
-`__init__.py` file inside the directory.  Notice the [double
-underscores](dunders.Rmd), indicating that this filename is special for
-Python.  For the moment, let's create a file that has (virtually) nothing in
-it:
+The key step to tell Python that `mydirmod` is a directory module, is to create
+an `__init__.py` file inside the directory.  Notice the [double
+underscores](dunders.Rmd), indicating that this filename is special for Python.
+For the moment, let's create a file that has (virtually) nothing in it:
 
 ```{python}
 # %%file mydirmod/__init__.py"
nipraxis,textbook,10b10c632c5ce8d1178f66ce14c7505b73ca5782,Matthew Brett,matthew.brett@gmail.com,2022-05-09T15:59:43Z,Matthew Brett,matthew.brett@gmail.com,2022-05-09T15:59:43Z,Some attempted fixes,_config.yml;on_modules.Rmd,True,False,True,False,2,1,3,"---FILE: _config.yml---
@@ -89,6 +89,7 @@ exclude_patterns:
   - topics.md
   # Drop?
   - using_pythonpath.Rmd
+  - .pytest_cache/README.md
 
 # Define the name of the latex output file for PDF builds
 latex:

---FILE: on_modules.Rmd---
@@ -175,7 +175,7 @@ means_again = volmeans.vol_means(data_fname)
 volmeans.detect_outliers_fixed(means_again)
 ```
 
-(reloading)=
+(reload)=
 
 ## Changing the module, reloading
 "
nipraxis,textbook,87cc73f893a8e5af18eec0b9eed5f1bb904e983f,Christopher J. Markiewicz,markiewicz@stanford.edu,2022-04-29T19:16:16Z,Christopher J. Markiewicz,markiewicz@stanford.edu,2022-04-29T19:16:16Z,FIX: Use nipraxis to fetch the relevant file in on_modules,on_modules.Rmd,True,False,True,False,3,2,5,"---FILE: on_modules.Rmd---
@@ -182,7 +182,7 @@ I can do this very simply, by adding some stuff to the end of the module.  I'll
 """"""
 
 import numpy as np
-
+import nipraxis
 import nibabel as nib
 
 def vol_means(image_fname):
@@ -205,7 +205,8 @@ def detect_outliers_fixed(some_values, n_stds=2):
     return np.where(is_outlier)[0]
 
 
-means = vol_means('ds107_sub012_t1r2.nii')
+fname = nipraxis.fetch_file('ds107_sub012_t1r2.nii')
+means = vol_means(fname)
 print(detect_outliers_fixed(means))
 ```
 "
nipraxis,textbook,512f0019db8259da8d229350f4b143be7711eeef,Matthew Brett,matthew.brett@gmail.com,2022-04-27T09:38:27Z,Matthew Brett,matthew.brett@gmail.com,2022-04-27T09:38:27Z,Fix link to image,reshape_and_4d.Rmd,True,False,True,False,1,3,4,"---FILE: reshape_and_4d.Rmd---
@@ -118,9 +118,7 @@ voxel_by_time = np.reshape(data, (n_voxels, data.shape[-1]))
 voxel_by_time.shape
 ```
 
-![](image/voxels_by_time.png)
+![](images/voxels_by_time.png)
 
 This is a useful operation when we want to apply some processing on all
 voxels, without regard to their relative spatial position.
-
-"
nipraxis,textbook,5cd811bead648e8308a7865dafdcef718bbd00f8,Matthew Brett,matthew.brett@gmail.com,2022-04-26T14:54:52Z,Matthew Brett,matthew.brett@gmail.com,2022-04-26T14:54:52Z,Fix numpy_logical explanation,numpy_logical.Rmd,True,False,True,False,62,4,66,"---FILE: numpy_logical.Rmd---
@@ -1,13 +1,15 @@
 ---
 jupyter:
   jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
     text_representation:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.11.5
+      jupytext_version: 1.13.7
   kernelspec:
-    display_name: Python 3 (ipykernel)
+    display_name: Python 3
     language: python
     name: python3
 ---
@@ -99,6 +101,62 @@ bool1 | bool2
 ~bool1
 ```
 
+## Bitwise, brackets
+
 **Be careful when using the bitwise operators**.  The bitwise operators have
-relatively low operator precedence, meaning that Python will prefer to apply
-many other operators *before* bitwise operators.
+relatively high operator precedence, meaning that Python will prefer to apply
+the bitwise operator *before* other operators, such as comparison operators.
+
+
+For example, consider these arrays, and the Boolean arrays from their comparison:
+
+```{python}
+first = np.array([1, 0, 1])
+first == 1
+```
+
+```{python}
+second = np.array([0, 1, 1])
+second == 1
+```
+
+```{python tags=c(""raises-exception"")}
+# This will give an error.  Why?
+first == 1 & second == 1
+```
+
+The problem is that Numpy registers `&` as having [higher operator
+preference](https://docs.python.org/3/reference/expressions.html#operator-precedence)
+than `==`, so it does the `&` operation before the `==`, meaning that the code
+above is equivalent to:
+
+```{python tags=c(""raises-exception"")}
+first == (1 & second) == 1
+```
+
+Therefore, you get the error like this:
+
+```{python}
+# Python is doing this under the hood in the statement above.
+res = 1 & second
+```
+
+```{python tags=c(""raises-exception"")}
+# Python next does this, generating the error.
+first == res == second
+```
+
+The fix is to specify that you want the `==` operation done before the `&`
+operation, using parentheses:
+
+```{python}
+# Guarantee the order of operations with parentheses.
+(first == 1) & (second == 1)
+```
+
+To avoid this problem, you might prefer to use `np.logical_and` etc:
+
+```{python}
+# The same operation using logical_and
+np.logical_and(first == 1, second == 1)
+```"
nipraxis,textbook,b56a87d0d1a3d4ce0ac556c3e5124a6291a8352d,Matthew Brett,matthew.brett@gmail.com,2022-04-25T15:56:37Z,Matthew Brett,matthew.brett@gmail.com,2022-04-25T15:56:37Z,Fix _toc,_toc.yml,False,False,False,False,0,1,1,"---FILE: _toc.yml---
@@ -45,7 +45,6 @@ parts:
 
 - caption: Into the fourth dimension
   chapters:
-  - file: voxels_by_time
   - file: voxel_time_courses
   - file: otsu_threshold
   - file: numpy_random"
nipraxis,textbook,62ba9ebcfbfa9a5da69509ac300f63e3297e716b,Matthew Brett,matthew.brett@gmail.com,2022-04-12T17:43:18Z,Matthew Brett,matthew.brett@gmail.com,2022-04-12T17:44:45Z,Error on warning for build,Makefile,False,False,False,False,1,1,2,"---FILE: Makefile---
@@ -3,7 +3,7 @@ BUILD_DIR=_build/html
 html:
 	# Check for ipynb files in source (should all be .Rmd).
 	if compgen -G ""*.ipynb"" 2> /dev/null; then (echo ""ipynb files"" && exit 1); fi
-	jupyter-book build .
+	jupyter-book build -W .
 
 clean:
 	rm -rf _build"
nipraxis,textbook,c126ec1bf48ed0268c451431777fa055555cf693,Oscar Esteban,code@oscaresteban.es,2022-04-12T14:12:40Z,Oscar Esteban,code@oscaresteban.es,2022-04-12T16:17:56Z,fix: make sure dipy lecture builds,.github/workflows/book.yml;requirements.txt,False,False,False,False,4,5,9,"---FILE: .github/workflows/book.yml---
@@ -39,9 +39,8 @@ jobs:
 
     - name: Install dependencies
       run: |
-        pip install -U pip
+        pip install ""pip==22.0.3""
         pip install -r requirements.txt
-        pip install jupyter-book
 
     # Build the page
     - name: Build the book

---FILE: requirements.txt---
@@ -1,12 +1,12 @@
 # Requirements for notebooks / Binderhub
-attrs==19.3.0  # pacify okpy
 matplotlib
 scipy
 nibabel
 ipython
 jupytext
+jupyter-book
 okpy
-dipy
-fury<0.8  # pacify dipy imports
+dipy==1.4.1
+fury==0.7.1  # pacify dipy imports
 scikit-image
 nipraxis"
nipraxis,textbook,dfa1a8af2f5c054193e1c19ef17e7490a651f9e3,Oscar Esteban,code@oscaresteban.es,2022-04-12T13:35:09Z,GitHub,noreply@github.com,2022-04-12T13:35:09Z,fix: amend previous commit,.github/workflows/book.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/book.yml---
@@ -47,7 +47,7 @@ jobs:
     - name: Build the book
       run: |
         make html
-        echo ""textbook.nipraxis.org"" >> ./build/html/CNAME
+        echo ""textbook.nipraxis.org"" >> ./_build/html/CNAME
 
     # Push the book's HTML to github-pages
     - name: GitHub Pages action"
nipraxis,textbook,666b7150b8e2a07cf721773f241cc48a7cd11e3d,Oscar Esteban,code@oscaresteban.es,2022-04-12T13:29:56Z,GitHub,noreply@github.com,2022-04-12T13:29:56Z,fix: add ``CNAME`` file to deployment,.github/workflows/book.yml,False,False,False,False,1,0,1,"---FILE: .github/workflows/book.yml---
@@ -47,6 +47,7 @@ jobs:
     - name: Build the book
       run: |
         make html
+        echo ""textbook.nipraxis.org"" >> ./build/html/CNAME
 
     # Push the book's HTML to github-pages
     - name: GitHub Pages action"
nipraxis,textbook,baaa3bfeac958906e963610010de92ade31b2412,Oscar Esteban,code@oscaresteban.es,2022-04-12T10:30:55Z,GitHub,noreply@github.com,2022-04-12T10:30:55Z,fix: roll-back / improve some of changes in #20,images_3d.Rmd,True,False,True,False,8,14,22,"---FILE: images_3d.Rmd---
@@ -143,19 +143,6 @@ middle_slice = data[:, :, img.shape[-1] // 2 - 1]
 plt.imshow(middle_slice)
 ```
 
-We can also *cut* the image through a different plane, for instance, sagittal:
-
-```{python}
-middle_slice = data[:, img.shape[1] // 2 - 1, :]
-plt.imshow(middle_slice)
-```
-
-NiBabel's Python representation of images offers a helpful utility to look at images through three viewplanes, implemented as a function attached to image objects (hence, a *method* of the object):
-
-```{python}
-img.orthoview()
-```
-
 We might be interested to look at the histogram of voxel values in this 3D block.  In order to do that, we `np.ravel` the 3D volume to 1D, to throw away the spatial arrangement of the voxels.
 
 ```{python}
@@ -190,7 +177,7 @@ from bottom, and so on:
 We can also think of this 3D image as a stack of 2D images where the 2D images are (back to front, bottom to top), like this:
 
 ```{python}
-yz_slice = data[127, :, :]
+yz_slice = data[img.shape[0] // 2 - 1, :, :]
 yz_slice.shape
 ```
 
@@ -215,3 +202,10 @@ plt.imshow(xz_slice)
 ```
 
 ![](images/image_plane_stack_col.png)
+
+
+NiBabel's Python representation of images offers a helpful utility to look at images through three viewplanes, implemented as a function attached to image objects (hence, a *method* of the object):
+
+```{python}
+img.orthoview()
+```"
nipraxis,textbook,6a7fcec20af5295d558d724a14231a92e5342f1b,Oscar Esteban,code@oscaresteban.es,2022-04-12T09:59:38Z,Oscar Esteban,code@oscaresteban.es,2022-04-12T10:14:41Z,maint(gha): fix build on pull-request,.github/workflows/book.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/book.yml---
@@ -7,7 +7,7 @@ on:
     - main
   pull_request:
     branches:
-    - master
+    - main
 
 # This job installs dependencies, build the book, and pushes it to `gh-pages`
 jobs:"
nipraxis,textbook,82374733cd8672d46599097dc7928aa1d44842a6,Oscar Esteban,code@oscaresteban.es,2022-04-12T10:12:01Z,Oscar Esteban,code@oscaresteban.es,2022-04-12T10:12:01Z,fix: bibtex references at build,.github/workflows/book.yml;Makefile;_config.yml;_references.bib,False,False,False,False,3,64,67,"---FILE: .github/workflows/book.yml---
@@ -46,7 +46,7 @@ jobs:
     # Build the page
     - name: Build the book
       run: |
-        jupyter-book build .
+        make html
 
     # Push the book's HTML to github-pages
     # uncomment when the action builds correctly

---FILE: Makefile---
@@ -1,6 +1,6 @@
 BUILD_DIR=_build/html
 
-html: bibliography
+html:
 	# Check for ipynb files in source (should all be .Rmd).
 	if compgen -G ""*.ipynb"" 2> /dev/null; then (echo ""ipynb files"" && exit 1); fi
 	jupyter-book build .
@@ -15,8 +15,3 @@ github: html
 	cp CNAME $(BUILD_DIR)
 	ghp-import -n $(BUILD_DIR) -p -f
 	./_scripts/check_pushed.sh
-
-BIBLIOGRAPHIES = bib/course_refs.bib
-
-bibliography: $(BIBLIOGRAPHIES)
-	cat $(BIBLIOGRAPHIES) > _references.bib

---FILE: _config.yml---
@@ -149,4 +149,4 @@ redirection:
 #   data-types/Ranges: ../arrays/Ranges
 #
 bibtex_bibfiles:
-  - _references.bib
+  - bib/course_refs.bib

---FILE: _references.bib---
@@ -1,56 +0,0 @@
----
----
-
-@inproceedings{holdgraf_evidence_2014,
-	address = {Brisbane, Australia, Australia},
-	title = {Evidence for {Predictive} {Coding} in {Human} {Auditory} {Cortex}},
-	booktitle = {International {Conference} on {Cognitive} {Neuroscience}},
-	publisher = {Frontiers in Neuroscience},
-	author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Knight, Robert T.},
-	year = {2014}
-}
-
-@article{holdgraf_rapid_2016,
-	title = {Rapid tuning shifts in human auditory cortex enhance speech intelligibility},
-	volume = {7},
-	issn = {2041-1723},
-	url = {http://www.nature.com/doifinder/10.1038/ncomms13654},
-	doi = {10.1038/ncomms13654},
-	number = {May},
-	journal = {Nature Communications},
-	author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Rieger, Jochem W. and Crone, Nathan and Lin, Jack J. and Knight, Robert T. and Theunissen, Frédéric E.},
-	year = {2016},
-	pages = {13654},
-	file = {Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:C\:\\Users\\chold\\Zotero\\storage\\MDQP3JWE\\Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:application/pdf}
-}
-
-@inproceedings{holdgraf_portable_2017,
-	title = {Portable learning environments for hands-on computational instruction using container-and cloud-based technology to teach data science},
-	volume = {Part F1287},
-	isbn = {978-1-4503-5272-7},
-	doi = {10.1145/3093338.3093370},
-	abstract = {© 2017 ACM. There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the effciency and ease with which students can learn. This manuscript details recent advances towards using commonly-Available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benets (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility.},
-	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
-	author = {Holdgraf, Christopher Ramsay and Culich, A. and Rokem, A. and Deniz, F. and Alegro, M. and Ushizima, D.},
-	year = {2017},
-	keywords = {Teaching, Bootcamps, Cloud computing, Data science, Docker, Pedagogy}
-}
-
-@article{holdgraf_encoding_2017,
-	title = {Encoding and decoding models in cognitive electrophysiology},
-	volume = {11},
-	issn = {16625137},
-	doi = {10.3389/fnsys.2017.00061},
-	abstract = {© 2017 Holdgraf, Rieger, Micheli, Martin, Knight and Theunissen. Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of “Encoding” models, in which stimulus features are used to model brain activity, and “Decoding” models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aimis to provide a practical understanding of predictivemodeling of human brain data and to propose best-practices in conducting these analyses.},
-	journal = {Frontiers in Systems Neuroscience},
-	author = {Holdgraf, Christopher Ramsay and Rieger, J.W. and Micheli, C. and Martin, S. and Knight, R.T. and Theunissen, F.E.},
-	year = {2017},
-	keywords = {Decoding models, Encoding models, Electrocorticography (ECoG), Electrophysiology/evoked potentials, Machine learning applied to neuroscience, Natural stimuli, Predictive modeling, Tutorials}
-}
-
-@book{ruby,
-  title     = {The Ruby Programming Language},
-  author    = {Flanagan, David and Matsumoto, Yukihiro},
-  year      = {2008},
-  publisher = {O'Reilly Media}
-}"
nipraxis,textbook,089fd78a91440db4fe4b5aa313556a7d03b3d564,Oscar Esteban,code@oscaresteban.es,2022-04-12T09:42:26Z,Oscar Esteban,code@oscaresteban.es,2022-04-12T09:42:26Z,maint(build): fix minor issues on gha and build time,_references.bib;requirements.txt,False,False,False,False,1,0,1,"---FILE: requirements.txt---
@@ -1,4 +1,5 @@
 # Requirements for notebooks / Binderhub
+attrs==19.3.0  # pacify okpy
 matplotlib
 scipy
 nibabel"
nipraxis,textbook,d108a1d0426e463fc3d14b6f7afb5b4528b6b148,Oscar Esteban,code@oscaresteban.es,2022-04-12T09:37:22Z,Oscar Esteban,code@oscaresteban.es,2022-04-12T09:37:22Z,maint(gha): fix jupyter-book installation ordering,.github/workflows/book.yml,False,False,False,False,1,1,2,"---FILE: .github/workflows/book.yml---
@@ -37,8 +37,8 @@ jobs:
     - name: Install dependencies
       run: |
         pip install -U pip
-        pip install jupyter-book
         pip install -r requirements.txt
+        pip install jupyter-book
 
     # Build the page
     - name: Build the book"
nipraxis,textbook,230b1e9a850b2f9b4e5265c230215c1caf9ca00c,Oscar Esteban,code@oscaresteban.es,2022-04-12T07:11:10Z,GitHub,noreply@github.com,2022-04-12T07:11:10Z,"FIX: Revise NIfTI-1 header size (as nibabel reports, it is 348)",images_3d.Rmd,True,False,True,False,1,1,2,"---FILE: images_3d.Rmd---
@@ -47,7 +47,7 @@ with `.nii.gz` instead.
 
 Inside, the file contains:
 
-* 352 bytes of *header* information.  Among other things, the header
+* 348 bytes of *header* information.  Among other things, the header
   gives the 3D or 4D shape of the file, and the data type of the pixel
   (voxel) data.
 * Usually, directly after the header, we have the image data.  If the image"
nipraxis,textbook,7c858b494b394603bacf5f51fff08d57dde61e5a,Matthew Brett,matthew.brett@gmail.com,2022-04-11T20:55:58Z,Matthew Brett,matthew.brett@gmail.com,2022-04-11T20:55:58Z,"Fix col plane, a little",images/image_plane_stack_col.png,False,False,False,False,0,0,0,
nipraxis,textbook,7854a5797f433c10fcbdf3e000919459927108da,pxr687,57416249+pxr687@users.noreply.github.com,2022-04-07T13:38:09Z,pxr687,57416249+pxr687@users.noreply.github.com,2022-04-07T13:38:09Z,fixed image filename so it matches the link on the textbook page,images/two_d_slices_first_plane.png,False,False,False,False,0,0,0,
nipraxis,textbook,0e4d3a1dbb721237aa43a8cfd70dcba310f59fe3,Matthew Brett,matthew.brett@gmail.com,2022-04-07T13:30:28Z,Matthew Brett,matthew.brett@gmail.com,2022-04-07T13:30:28Z,Fix image name from slab -> plan rename,images/two_d_slices_first_plan.png,False,False,False,False,0,0,0,
nipraxis,textbook,1f8d06ec4a156295423e91a918cbf30449953050,Matthew Brett,matthew.brett@gmail.com,2022-04-07T12:25:14Z,Matthew Brett,matthew.brett@gmail.com,2022-04-07T12:25:14Z,Fix reading_text import,reading_text.Rmd,True,False,True,False,6,3,9,"---FILE: reading_text.Rmd---
@@ -1,6 +1,5 @@
 ---
 jupyter:
-  orphan: true
   jupytext:
     notebook_metadata_filter: all,-language_info
     split_at_heading: true
@@ -13,6 +12,7 @@ jupyter:
     display_name: Python 3
     language: python
     name: python3
+  orphan: true
 ---
 
 # Reading data from text files
@@ -58,9 +58,12 @@ numbers_again
 ```
 
 In fact we read these data even more concisely, and quickly, by using
-`np.loadtxt`:
+`np.loadtxt`.
 
 ```{python}
-np.loadtxt('some_numbers.txt')
+import numpy as np
 ```
 
+```{python}
+np.loadtxt('some_numbers.txt')
+```
\ No newline at end of file"
nipraxis,textbook,c5c03a73eda78bd38955d2b1a5d7cb2132456c8d,pxr687,57416249+pxr687@users.noreply.github.com,2022-04-07T12:05:36Z,pxr687,57416249+pxr687@users.noreply.github.com,2022-04-07T12:05:36Z,"a few suggested edits

(Note: these suggested edits will make a lot more sense viewed in notebook form).

- There was an error in the bottom right cell of the second slab in the images (it said '11' rather than '111', I have fixed this.

- I added some extra images and explanation that I think might aid clarity/visualisation. I've exploded some operations over several cells also with the same aim.",arrays_3d.Rmd;images/two_d_second_slice_np.png;images/two_d_slice_np.png;images/two_d_slices.png;images/two_d_slices_first_slab.png;images/two_d_slices_without_annotation.png,True,False,True,False,77,12,89,"---FILE: arrays_3d.Rmd---
@@ -7,9 +7,9 @@ jupyter:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.13.7
+      jupytext_version: 1.10.1
   kernelspec:
-    display_name: Python 3
+    display_name: Python 3 (ipykernel)
     language: python
     name: python3
 ---
@@ -28,7 +28,7 @@ plt.rcParams['image.cmap'] = 'gray'
 np.set_printoptions(precision=4, suppress=True)
 ```
 
-So far we have seen one-dimensional and two-dimensional arrays.  These are easy to think about, the one-dimensional array is like a row from a table or spreadsheet.  A two-dimensional array is has rows and columns, like a table or a spreadsheet.
+So far we have seen one-dimensional and two-dimensional arrays.  These are easy to think about, the one-dimensional array is like a row from a table or spreadsheet.  A two-dimensional array has rows and columns, like a table or a spreadsheet.
 
 A three-dimensional array takes a little bit more work to visualize, and get used to.
 
@@ -39,14 +39,20 @@ One way to think of three-dimensional arrays is as stacks of 2D arrays.
 Here are a couple of two-dimensional arrays.
 
 ```{python}
+# make the first 2D array
 first_1d = np.arange(10, 22)
 first_2d = np.reshape(first_1d, (4, 3))
+
+# show the first 2D array
 first_2d
 ```
 
 ```{python}
+# make the second 2D array
 second_1d = np.arange(100, 112)
 second_2d = np.reshape(second_1d, (4, 3))
+
+# show the second 2D array
 second_2d
 ```
 
@@ -86,50 +92,109 @@ first_2d.size
 
 ## Three dimensions
 
-Here we make a three-dimension array, by stacking the two 2D arrays together.
+Here we make a three-dimensional array, by stacking the two 2D arrays together.
+
+Let's look again at the `first_2d` array: 
+
+```{python}
+first_2d
+```
 
-First we create an empty array of the right shape:
+and the `second_2d` array:
 
 ```{python}
+second_2d
+```
+
+If you imagine the `first_2d` array and the `second_2d` array as physical objects, then when we stack them, we will get the 3D array shown in the image below:
+
+
+![](images/two_d_slices_without_annotation.png)
+
+
+The object in the foreground of the image (with 10, 11, 12 on its topmost row) is the first *slab* of the 3D array, and contains the values from the `first_2d` array. The object in the background of the image (with 100, 101, 102 on its topmost row) is the second slab, and contains the values from the `second_2d` array.
+
+To create the 3D array, first we create an empty array of the right shape:
+
+```{python}
+# make a 3D array
 into_3d = np.zeros((4, 3, 2))
+
+# show the 3D array
+into_3d
+```
+
+The 3D array, as numpy shows it, might not have an obvious resemblance to the image of the two slabs shown above, but bear with us.
+
+The 3D array is of the right shape:
+
+```{python}
 into_3d.shape
 ```
 
-Then we make the first 2D array be the first 4 by 3 slab of the 3D array:
+The shape can be read as 'two slabs, each with 4 rows and 3 columns', which corresponds to what is shown in the image of the slabs above.
+
+Currently the 3D array only contains 0s, so next we make the `first_2d` array be the first 4 by 3 slab of the 3D array:
+
+```{python}
+# show the first_2d array
+first_2d
+```
 
 ```{python}
-# Set all rows, all columns for slab 0:
+# Set all rows, all columns for slab 0 to be the first_2d array
 into_3d[:, :, 0] = first_2d
 ```
 
-and the second 2D array be the second 4 by 3 slab:
+The image below shows how the `into_3d` array now looks, with the `first_2d` array as the first 3 by 4 slab:
+
+
+![](images/two_d_slices_first_slab.png)
+
+
+Next we make the `second_2d` array be the second 4 by 3 slab of the 3D array:
+
+```{python}
+# show the second_2d array
+second_2d
+```
 
 ```{python}
-# Set all rows, all columns for slab 1:
+# Set all rows, all columns for slab 1 to be the second_2d array
 into_3d[:, :, 1] = second_2d
 ```
+We have just stacked the `first_2d` and `second_2d` arrays into the `into_3d` array. The image below shows how the index commands we just used in order to do this relate to the slabs of the `into_3d` array, and to the values that are now in each slab.
+
+`into_3d[:, :, 0]` can be read as *'all rows, all columns of slab 0'*.
+
+`into_3d[:, :, 1]` can be read as *'all rows, all columns of slab 1'*.
+
+
 ![](images/two_d_slices.png)
 
 
-This is how Numpy shows us the contents of the array:
+The output of the cell below is how Numpy shows us the contents of the `into_3d` array:
 
 ```{python}
 into_3d
 ```
 
-This is looking at the array in a different way, where the slices are in the first dimension, like this:
+Relative to the images above, this is a different way of looking at the `into_3d` array, where the slices are in the first dimension, like this:
 
 ```{python}
 # First row, all columns, all slabs.
 into_3d[0, :, :]
 ```
 
+![](images/two_d_slice_np.png)
+
 ```{python}
 # Second row, all columns, all slabs.
 into_3d[1, :, :]
 ```
 
-![](images/two_d_slice_np.png)
+![](images/two_d_second_slice_np.png)
+
 
 Make a prediction of what the values would look like by slicing in the second dimension.
 Check your guess below:"
nipraxis,textbook,2b08adcf78398659639e456857354250257c143f,Zvi Baratz,z.baratz@gmail.com,2022-04-07T07:53:16Z,GitHub,noreply@github.com,2022-04-07T07:53:16Z,Typo fix?,images_3d.Rmd,True,False,True,False,1,1,2,"---FILE: images_3d.Rmd---
@@ -57,7 +57,7 @@ Inside, the file contains:
   64-bit float, which is 8 bytes long, so the image data would be
   `64 * 64 * 32 * 8` bytes long.
 
-To load these images into Python, and the [Nibabel](http://nipy.org/nibabel)
+To load these images into Python, use the [Nibabel](http://nipy.org/nibabel)
 package.
 
 Start by importing the Nibabel library:"
nipraxis,textbook,be2e97ccba5ba70c2ad8f856744b0f8244266b39,Christopher J. Markiewicz,markiewicz@stanford.edu,2022-04-06T21:32:34Z,Christopher J. Markiewicz,markiewicz@stanford.edu,2022-04-06T21:32:34Z,Minor formatting issues,images_3d.Rmd,True,False,True,False,3,3,6,"---FILE: images_3d.Rmd---
@@ -38,7 +38,7 @@ np.set_printoptions(precision=4, suppress=True)
 
 We will spend a lot of time loading data from brain images.
 
-MRI images for functional MRI are usually stored using the [NIfTi
+MRI images for functional MRI are usually stored using the [NIfTI
 format](https://nifti.nimh.nih.gov/nifti-1).
 
 As you've already seen, this is a very simple format that is typically
@@ -54,8 +54,8 @@ Inside, the file contains:
   data is shape (I, J, K), and S is the number of bytes to store the data for
   one pixel (voxel) value, then the image data is `I * J * K * S` in length.
   For example, the image might be shape 64, 64, 32, and the data type might be
-  64-bit float, which is 8 bytes long, so the image data would be `64 * 64 *
-  32 * 8` bytes long.
+  64-bit float, which is 8 bytes long, so the image data would be
+  `64 * 64 * 32 * 8` bytes long.
 
 To load these images into Python, and the [Nibabel](http://nipy.org/nibabel)
 package."
nipraxis,textbook,51d269109117d89edff591286926bb239b187b37,Matthew Brett,matthew.brett@gmail.com,2022-04-05T21:24:58Z,Matthew Brett,matthew.brett@gmail.com,2022-04-05T21:24:58Z,"Fix bug in fetch_file, edit editor page",choosing_editor.md;numpy_intro.Rmd,True,False,True,False,116,54,170,"---FILE: choosing_editor.md---
@@ -14,8 +14,8 @@ orphan: true
 > <p class=""attribution"">-From: ""The pragmatic programmer"" </p>
 
 Being efficient means being able to use your tools well.  If you are fluent in
-using your editor, you will be able think better.  You will do better work
-more quickly and you will make fewer mistakes.
+using your editor, you will be able think better.  You will do better work more
+quickly and you will make fewer mistakes.
 
 This is particularly so for a text file editor.  Bear in mind that you will
 likely spend many hundreds of hours using this editor for various parts of
@@ -31,83 +31,145 @@ tools that waste their time or energy.  Take their impatience seriously when
 you consider choosing your editor.  No programmer would use Windows Notepad
 for writing code, and that is for a very good reason.
 
-The two most well-known and fully-featured cross-platform general text editors
-are [emacs] and [vim] / [gvim].  These run on any platform.  [Atom] is another
-open-source cross-platform general text editor.
+## What are the options?
+
+Choosing an editor is an important, even fateful decision.  You choose an
+editor in order to invest in it; if you choose wisely, your will find your
+investment repaid, handsomely, and daily.  Choose carefully, and choose for
+the long haul.
+
+There are various surveys on the editors that Python developers use.  Some recent examples are:
+
+*   [The Jetbrains 2020
+    Python developer
+    survey](https://www.jetbrains.com/lp/python-developers-survey-2020/)
+    (though note, this survey was by the makers of one of the editors,
+    PyCharm).
+*   [The 2021 StackOverflow developer
+    survey](https://insights.stackoverflow.com/survey/2021#section-most-popular-technologies-integrated-development-environment)
+    (though note, this is for developers in all languages).
+*   [A discussion of recent
+    trends](https://asterisk.dynevor.org/editor-dominance.html).
+
+From these, and from our own experience talking to other people with lots of
+experience writing Python code, we believe your practical long-term options
+are the following.  The editors are in the order from the Jetbrains survey
+above, but with the position of PyCharm estimated from the StackOverflow
+survey, on the basis that the JetBrains survey results are likely to be biased
+to PyCharm users).
+
+* A version of [Visual studio code](https://code.visualstudio.com).  More on
+  what we mean by ""a version"" below.
+* [Vim](https://www.vim.org)
+* [PyCharm](https://www.jetbrains.com/pycharm)
+* [Sublime text](https://www.sublimetext.com)
+* [Emacs](https://emacs.org)
+* [Spyder](https://www.spyder-ide.org)
+
+## Some version of Visual Studio Code
+
+[Visual studio code](https://code.visualstudio.com) is a code editor and
+integrated development environment almost entirely written by programmers at
+Microsoft.  It is entirely free to download (free as in free beer).  See below
+for more discussion about whether it truly open-source (free as in freedom).
+
+It is very popular as a Python editor, and [increasingly
+so](https://asterisk.dynevor.org/editor-dominance.html).
+
+You will find many powerful features for interacting with Python code, and, more recently and experimentally, with Jupyter Notebooks.
+
+It works well and looks good out of the box, but it is highly configurable.
+Configuration takes some getting used to, and can be fiddly, but there is a
+lot of help from others trying to do the same thing, online.
 
-No doubt you want to know what your instructors use for their daily work?  No?
-Well, in any case, the answer is that most of us use Vim.
+There are various controversies about Visual Studio Code (or VSCode for short).
 
-## vim
+Although Microsoft releases the code to build a version of VSCode on your own computer, the installer you download from their website contains and refers to to not-free, not-open-source components, and it sends telemetry data to Microsoft.  See the [VSCodium site](https://vscodium.com) for a discussion, and links to download versions of VSCode without these features, and so, less locked to Microsoft.
 
-Vim is based on a classic Unix editor called [vi].  To quote from the [about
-vim][about vim] page:
+Another controversy is the practical extent to which the community can
+influence the direction of development of the VSCode editor.  In practice,
+Microsoft decides the direction of the project, and drives the changes,
+because they pay the programmers that write it.  See [this post for some
+discussion](https://www.eclipse.org/org/press-release/20200331-theia.php). For
+those reasons, some people prefer to use a build of the more open
+[Theia editor](https://theia-ide.org), that uses much of machinery behind VSCode.
 
-> Vim isn't an editor designed to hold its users' hands. It is a tool, the
-> use of which must be learned.
+## Vim
+
+The two most well-known and fully-featured cross-platform general text editors
+are Emacs and Vim.  These run on any platform.  More about Emacs further down
+the page.
+
+Vim is based on a classic Unix editor called
+[vi](https://en.wikipedia.org/wiki/Vi).  To quote from the [about
+vim](https://www.vim.org/about.php) page:
+
+> Vim isn't an editor designed to hold its users' hands. It is a tool, the use
+> of which must be learned.
 
 To use Vim well, you have to practice using its commands, by following any one
-of several online tutorials. For example, you might try the [openvim tutorial](http://www.openvim.com/tutorial.html).  The trick is to teach your fingers
+of several online tutorials. For example, you might try the [openvim
+tutorial](http://www.openvim.com/tutorial.html) or [Vim
+adventures](https://vim-adventures.com/).  The trick is to teach your fingers
 what to do so you don't think about it any more. This takes a long time,
 budget a week of 30 minutes a day to start to feel comfortable.
 
-If you do invest this effort, vim is an immensely satisfying editor to use,
+If you do invest this effort, Vim is an immensely satisfying editor to use,
 because you quickly find that your fingers remember what to do.  A
-programmer's joke about vim is that ""the cursor follows your eyes"", because
+programmer's joke about Vim is that ""the cursor follows your eyes"", because
 your fingers are moving the cursor around the text without any apparent
 thought or effort on your part.
 
-It is relatively complicated to configure vim to its full potential.  Please
+It is relatively complicated to configure Vim to its full potential.  Please
 ask for help if you are interested to do this.  It is time well spent.
 
-## Emacs
-
-Emacs is a classic and highly configurable text editor originally written by
-the great [Richard Stallman].  It is quicker to learn than vim, probably
-harder to configure, and has great power.  A good place to start is the [emacs
-tour][emacs tour].  About an hour of research and practice gets you far enough to learn
-how to start learning emacs.
-
-As for vim, to use it well, it needs some practice to get used to the
-keystrokes used, and as for vim, this practice is amply repaid by very fluent
-movement and editing.
+## PyCharm
 
-Emacs hard to configure, largely because it is so powerful.  We are very happy
-to help with this if you are interested.
+We have heard good things about PyCharm, from people who know what they are
+talking about.  This is a general text editor with features that allow it to
+be used as an integrated development environment for Python.  There is a free
+""community"" version and a £149 initial annual fee for the professional
+version.  The free version does have Python editing support, and support for
+other languages, but the professional edition supports more languages and
+features.  See the [PyCharm web site](https://www.jetbrains.com/pycharm/) and
+[PyCharm feature
+comparison](https://www.jetbrains.com/products/compare/?product=pycharm&product=pycharm-ce)
+for more detail.
 
-## Atom
+### Sublime Text
 
-[Atom] is another open-source text editor that describes itself as ""A hackable
-text editor for the 21st Century"".  It has installers for OSX, Linux and
-Windows.  It has many ""packages"" that can configure the editor in various
-ways, including setting it to use the same key combinations as [vim]
-([vim-mode], [ex-mode]) and emacs ([emacs-mode]).  It integrates with [git]
-version control out of the box.  It has useful features for development, such
-as the ability to run code from the editor and return the results to the
-editor window (see the [hydrogen] package).
+Some people really like [Sublime text](https://www.sublimetext.com).  It is
+free to try, and it appears the trial version does not expire, but if you
+continue to use it, the authors ask you to buy a license for \$99.
 
-## Other editors that you might consider
+## Emacs
 
-We will support these editors too for anyone who wants to use them.  If you
-prefer another editor, we will help as best we can.
+Emacs is a very powerful classic free, and open-source text editor. The great
+[Richard Stallman](https://en.wikipedia.org/wiki/Richard_Stallman) wrote the
+original version. It is quicker to learn than vim, probably harder to
+configure, and configurable to an extreme degree.  A good place to start is
+the [emacs tour](https://www.gnu.org/software/emacs/tour).  About an hour of
+research and practice gets you far enough to learn how to start learning
+Emacs.
 
-This suggested set of editors comes from the top four editors from [text
-editor competition][text editor competition] with the addition of TextMate and
-PyCharm (because we know at least one very efficient coder who use these).
+As for Vim, to use it well, it needs practice.  You will need to get used to
+the keystrokes, and as for Vim, this practice is amply repaid by very fluent
+movement and editing.
 
-### For any platform
+Emacs is hard to configure, largely because it is so powerful.  We are very
+happy to help with this if you are interested.
 
-Some people really like [Sublime text].  It is free to try, and the trial
-version does not expire, but if you continue to use it, the authors ask you to
-buy a license for 70 USD.
+## Spyder
 
-We have heard good things about PyCharm.  This is a general text editor with
-features that allow it to be used as an integrated development environment for
-Python. See the [PyCharm web site](https://www.jetbrains.com/pycharm/) for more detail.
+[Spyder](https://www.spyder-ide.org) is a code editor and integrated
+development environment that is specific to Python.  It is free, and
+open-source.  You can install it in the same way you install other Python
+packages.  It has a similar interface to, for example, the Matlab graphical
+environment, with separate windows for code, plots, variables and so on.
 
-### For Windows
+## Another option for Windows
 
-[Notepad++] seems to be a Windows favorite.
+[Notepad++](https://notepad-plus-plus.org) only works on Windows, but seems to be a popular choice.
 
 ## Suggestions?
 

---FILE: numpy_intro.Rmd---
@@ -73,7 +73,7 @@ We fetch the text file we will be working on:
 import nipraxis
 
 # Fetch the file.
-stim_fname = nipraxis.fetch_data('24719.f3_beh_CHYM.csv')
+stim_fname = nipraxis.fetch_file('24719.f3_beh_CHYM.csv')
 # Show the filename.
 stim_fname
 ```"
nipraxis,textbook,375bf19bafff6ad3b1fac10bbcc636c1de9f8200,Matthew Brett,matthew.brett@gmail.com,2022-04-02T16:21:27Z,Matthew Brett,matthew.brett@gmail.com,2022-04-02T16:21:27Z,Fix toc link,_toc.yml,False,False,False,False,1,1,2,"---FILE: _toc.yml---
@@ -24,7 +24,7 @@ parts:
   - file: numpy_intro
   - file: boolean_indexing
   - file: arrays_and_images
-  - file: boolea_indexing_nd
+  - file: boolean_indexing_nd
   - file: numpy_logical
   - file: reshape_and_3d
   - file: otsu_threshold"
nipraxis,textbook,d85125fad58465dabcb1f63d89dbbd7a123e3a58,Matthew Brett,matthew.brett@gmail.com,2022-04-02T14:32:55Z,Matthew Brett,matthew.brett@gmail.com,2022-04-02T14:32:55Z,Fix ipynb check for top-level ipynb files,Makefile,False,False,False,False,1,1,2,"---FILE: Makefile---
@@ -2,7 +2,7 @@ BUILD_DIR=_build/html
 
 html: bibliography
 	# Check for ipynb files in source (should all be .Rmd).
-	if compgen -G ""*/*.ipynb"" 2> /dev/null; then (echo ""ipynb files"" && exit 1); fi
+	if compgen -G ""*.ipynb"" 2> /dev/null; then (echo ""ipynb files"" && exit 1); fi
 	jupyter-book build .
 
 clean:"
nipraxis,textbook,6cd8908e4de25a5439113287ccc6cdd1ab60677b,Matthew Brett,matthew.brett@gmail.com,2022-03-31T15:05:36Z,Matthew Brett,matthew.brett@gmail.com,2022-03-31T15:05:36Z,Fixes,24719.f3_beh_CHYM.csv;_config.yml;_toc.yml;choosing_editor.md;data/24719.f3_beh_CHYM.csv;data/README.md;installation.md;installation_on_mac.md;numpy_intro.Rmd,True,False,True,False,698,28,726,"---FILE: 24719.f3_beh_CHYM.csv---
@@ -0,0 +1 @@
+data/24719.f3_beh_CHYM.csv
\ No newline at end of file

---FILE: _config.yml---
@@ -60,19 +60,16 @@ exclude_patterns:
   # Hemodynamic response modeling
   - introducing_nipype.Rmd
   # Code organization and collaboration
-  - choosing_editor.md
   - full_scripting.md
   - git_videos.md
   - git_walk_through.Rmd
   - git_workflow_exercises.md
   - github_dummies_homework.md
   - github_glm_homework.md
   - github_pca_homework.md
-  - installation_on_linux.md
-  - installation_on_mac.md
-  - installation_on_windows.md
   # Meta-course
   - README.md
+  - data/README.md
   - _todo.md
   - _scripts
   - bibliography.md
@@ -85,7 +82,6 @@ exclude_patterns:
   - glossary.md
   - index.md
   - logistics.md
-  - markdown.md
   - mentors.md
   - multi_model_homework.md
   - participate.md

---FILE: _toc.yml---
@@ -16,11 +16,12 @@ parts:
   - file: using_jupyter
   - file: more_on_jupyter
   - file: brisk_python
-  - file: installing
+  - file: installation
 
 - caption: Images and code and images
   chapters:
   - file: what_is_an_image
+  - file: numpy_intro
   - file: arrays_and_images
   - file: slicing_with_booleans
   - file: numpy_logical

---FILE: choosing_editor.md---
@@ -1,3 +1,7 @@
+---
+orphan: true
+---
+
 (choosing-editor)=
 
 # Choosing an editor
@@ -87,9 +91,9 @@ editor window (see the [hydrogen] package).
 We will support these editors too for anyone who wants to use them.  If you
 prefer another editor, we will help as best we can.
 
-This suggested set of editors comes from the top four editors from  [text
-editor competition][text editor competition] with the addition of TextMate and PyCharm (because we know at least
-one very efficient coder who use these).
+This suggested set of editors comes from the top four editors from [text
+editor competition][text editor competition] with the addition of TextMate and
+PyCharm (because we know at least one very efficient coder who use these).
 
 ### For any platform
 
@@ -99,12 +103,7 @@ buy a license for 70 USD.
 
 We have heard good things about PyCharm.  This is a general text editor with
 features that allow it to be used as an integrated development environment for
-Python. See the [PyCharm web site](PyCharm) for more detail.
-
-### For OSX
-
-[textmate] has some very serious enthusiasts among people who know what they
-are doing on OSX. There's a free [textmate download for UC Berkeley].
+Python. See the [PyCharm web site](https://www.jetbrains.com/pycharm/) for more detail.
 
 ### For Windows
 

---FILE: data/24719.f3_beh_CHYM.csv---
@@ -0,0 +1,321 @@
+response,response_time,trial_ISI,trial_shape
+None,0,2000,red_star
+None,0,1000,red_circle
+None,0,2500,green_triangle
+None,0,1500,yellow_square
+None,0,1500,blue_circle
+None,0,2000,green_star
+None,0,2500,blue_square
+None,0,1500,yellow_square
+None,0,2000,yellow_triangle
+None,0,1000,yellow_square
+space,427,1000,red_square
+None,0,1500,yellow_square
+None,0,1500,red_circle
+space,369,2000,red_square
+None,0,1500,yellow_triangle
+space,337,1500,red_square
+None,0,1500,blue_square
+space,308,2000,red_square
+None,0,2000,yellow_star
+None,0,1500,blue_star
+None,0,2500,yellow_circle
+None,0,1000,green_star
+None,0,2500,green_square
+space,375,1000,red_square
+space,478,2500,red_square
+space,300,2000,red_square
+None,0,2000,green_triangle
+None,0,1000,green_star
+None,0,1000,blue_circle
+space,321,1000,red_square
+None,0,1500,yellow_triangle
+space,306,1500,red_square
+None,0,1500,red_star
+None,0,1500,green_star
+None,0,1000,green_star
+None,0,1000,blue_circle
+None,0,2500,yellow_circle
+None,0,1000,blue_circle
+space,370,2000,red_square
+None,0,2000,red_circle
+None,0,2500,yellow_square
+None,0,1500,blue_square
+None,0,1500,green_triangle
+space,372,1000,red_square
+space,372,2500,red_square
+space,342,2000,red_square
+None,0,1000,green_square
+None,0,1500,blue_star
+space,382,2000,red_square
+None,0,1500,blue_square
+None,0,2000,yellow_triangle
+space,318,1000,red_triangle
+None,0,1000,green_square
+None,0,2500,blue_triangle
+None,0,1500,green_circle
+None,0,2000,blue_star
+space,371,1000,red_square
+None,0,1000,red_triangle
+space,450,1000,red_square
+None,0,1000,yellow_triangle
+None,0,2500,green_triangle
+space,442,2500,red_square
+None,0,2000,green_circle
+None,0,2500,yellow_star
+None,0,2500,blue_circle
+None,0,1500,yellow_square
+None,0,2000,green_square
+None,0,1500,yellow_triangle
+None,0,2000,blue_star
+None,0,1000,blue_square
+space,452,1500,red_square
+space,381,2500,red_square
+None,0,1000,green_triangle
+None,0,1000,red_circle
+None,0,2500,yellow_square
+None,0,2000,red_triangle
+None,0,2500,green_square
+None,0,2500,red_circle
+None,0,2500,yellow_circle
+space,351,1500,red_square
+None,0,1500,blue_circle
+None,0,1000,blue_square
+None,0,2500,red_circle
+space,394,2500,red_square
+None,0,2500,red_triangle
+space,353,1500,red_square
+None,0,1000,green_triangle
+None,0,2500,yellow_triangle
+None,0,2000,blue_star
+None,0,1500,yellow_square
+None,0,2000,blue_triangle
+space,380,2500,red_square
+None,0,1500,blue_star
+space,387,1500,red_square
+space,341,2500,red_square
+space,366,2500,red_circle
+space,432,2000,red_square
+space,415,2000,red_square
+None,0,1000,blue_triangle
+space,406,1000,red_square
+None,0,2000,yellow_square
+None,0,2500,green_square
+None,0,1000,green_square
+None,0,2000,yellow_star
+None,0,2000,green_circle
+None,0,1000,blue_triangle
+None,0,2000,blue_triangle
+None,0,2000,blue_triangle
+None,0,1500,red_circle
+None,0,1000,green_square
+None,0,1000,red_star
+None,0,1500,red_circle
+None,0,1500,red_star
+None,0,2500,blue_star
+None,0,2500,green_square
+None,0,2500,blue_square
+None,0,1500,blue_triangle
+None,0,2500,blue_square
+space,466,1500,red_square
+None,0,1000,blue_star
+None,0,1500,blue_star
+space,364,2500,red_square
+space,462,1000,red_square
+space,477,2000,red_square
+None,0,2500,green_circle
+None,0,1000,green_star
+None,0,2000,green_triangle
+None,0,2500,red_circle
+space,374,1500,red_square
+space,384,2000,red_square
+None,0,1000,blue_triangle
+None,0,1000,yellow_square
+None,0,2500,blue_circle
+space,375,1500,red_square
+None,0,2000,red_triangle
+None,0,1000,red_star
+None,0,1000,red_circle
+None,0,2500,red_triangle
+None,0,1000,yellow_square
+space,432,2500,red_square
+None,0,2500,green_square
+None,0,2500,green_square
+None,0,1000,red_star
+None,0,2000,green_circle
+None,0,1000,yellow_star
+None,0,1500,red_star
+None,0,2000,red_star
+None,0,1500,red_circle
+space,369,1500,red_square
+None,0,1500,yellow_star
+None,0,1000,yellow_square
+None,0,1000,yellow_star
+space,354,1500,red_square
+None,0,1000,red_triangle
+space,455,2500,red_square
+None,0,2000,blue_square
+None,0,2500,blue_triangle
+space,338,1500,red_square
+None,0,2000,green_square
+None,0,2500,yellow_triangle
+space,364,1000,red_square
+None,0,1000,green_circle
+None,0,2500,red_star
+space,376,1500,red_square
+space,345,1500,red_square
+space,299,1500,red_square
+space,308,2500,red_square
+space,381,1000,red_square
+None,0,2500,green_circle
+space,375,2000,red_square
+space,438,2000,red_square
+space,325,1000,red_square
+None,0,2000,blue_square
+None,0,2000,blue_square
+None,0,2000,blue_star
+space,371,2500,red_square
+space,377,1500,red_square
+space,314,2000,red_square
+None,0,2000,blue_square
+space,297,2000,red_square
+None,0,1000,red_triangle
+None,0,2000,yellow_circle
+None,0,2000,red_triangle
+None,0,1500,green_circle
+space,349,1000,red_triangle
+None,0,1000,red_star
+space,607,2000,red_square
+space,391,2000,red_square
+space,334,1500,red_square
+None,0,1500,blue_circle
+space,385,1000,red_square
+None,0,2000,yellow_circle
+None,0,1500,blue_square
+None,0,1500,yellow_square
+space,386,1000,red_square
+space,369,2000,red_square
+None,0,1500,blue_square
+None,0,1000,red_triangle
+space,394,1500,red_square
+None,0,2500,red_star
+None,0,2000,yellow_square
+space,361,1000,red_square
+space,480,2000,red_square
+None,0,2000,green_star
+None,0,2000,green_triangle
+None,0,1500,yellow_square
+space,375,1000,red_square
+None,0,1500,green_star
+space,368,1500,red_square
+None,0,1000,yellow_star
+space,325,2000,red_square
+None,0,1000,yellow_square
+None,0,1500,green_circle
+None,0,2500,green_square
+None,0,2000,green_square
+space,410,2500,red_square
+space,328,2500,red_square
+None,0,1500,red_triangle
+None,0,1000,red_star
+None,0,2500,yellow_circle
+None,0,2500,red_circle
+None,0,1000,red_star
+None,0,2500,yellow_square
+None,0,2500,green_square
+space,412,2000,red_square
+None,0,2000,red_star
+space,379,1000,red_square
+None,0,2500,blue_circle
+None,0,2500,yellow_triangle
+None,0,1500,yellow_star
+space,383,2500,red_square
+space,376,2000,red_square
+space,351,2500,red_square
+None,0,2000,yellow_star
+None,0,2500,red_triangle
+None,0,1000,yellow_circle
+space,361,1000,red_triangle
+None,0,1500,green_triangle
+space,403,2500,red_square
+space,411,1500,red_square
+None,0,2500,red_triangle
+None,0,2500,red_triangle
+None,0,1500,blue_circle
+None,0,1000,red_triangle
+None,0,2500,red_circle
+None,0,1500,green_circle
+space,447,2000,red_square
+None,0,1000,red_star
+space,590,2000,red_square
+None,0,2500,yellow_triangle
+space,328,2000,red_square
+None,0,1500,yellow_circle
+space,376,2500,red_square
+space,410,2500,red_square
+None,0,1500,blue_square
+space,433,1500,red_square
+space,370,1000,red_square
+space,353,1500,red_square
+None,0,1500,blue_square
+None,0,1500,yellow_circle
+None,0,1000,yellow_star
+None,0,2500,yellow_circle
+None,0,1500,red_triangle
+None,0,2500,green_triangle
+space,353,2000,red_square
+None,0,2500,blue_star
+space,386,1000,red_square
+space,417,2500,red_square
+space,315,2000,red_square
+None,0,2000,yellow_triangle
+space,298,2000,red_square
+None,0,2500,blue_square
+None,0,2000,blue_triangle
+None,0,1000,green_circle
+space,370,1500,red_square
+None,0,1000,yellow_star
+None,0,2000,yellow_triangle
+None,0,2500,yellow_star
+None,0,2500,red_circle
+space,365,1000,red_square
+None,0,1500,blue_square
+None,0,1500,yellow_square
+space,439,2000,red_square
+space,391,2500,red_square
+None,0,1500,green_triangle
+None,0,2000,red_circle
+None,0,1500,red_star
+None,0,1500,blue_square
+None,0,2500,red_circle
+None,0,2500,blue_star
+None,0,1000,green_triangle
+None,0,2500,green_star
+None,0,2000,blue_triangle
+None,0,1500,green_square
+None,0,2000,green_square
+space,351,2500,red_square
+None,0,2000,yellow_square
+None,0,2000,blue_star
+None,0,1500,green_circle
+None,0,1000,red_circle
+None,0,1000,red_star
+space,464,2500,red_square
+None,0,2000,red_star
+None,0,2000,green_star
+None,0,2000,blue_triangle
+None,0,1500,yellow_circle
+None,0,2000,blue_triangle
+space,357,1000,red_square
+None,0,1000,red_star
+None,0,2000,green_star
+None,0,1000,green_triangle
+None,0,1500,blue_circle
+None,0,1500,yellow_circle
+None,0,2500,green_square
+space,328,2000,red_square
+space,294,1000,red_square
+None,0,2500,green_circle
+None,0,1000,green_star
+space,471,1000,red_circle
+None,0,1000,blue_circle

---FILE: data/README.md---
@@ -0,0 +1,8 @@
+# Very small data files
+
+Anything larger than around 1M should go in the Nipraxis dataset:
+<https://github.com/nipraxis/nipraxis-data>
+
+Chris Gorgolewski provided the data file `24719.f3_beh_CHYM.csv`, and he tells
+me I can release that under a [CC-0
+license](https://creativecommons.org/share-your-work/public-domain/cc0/).

---FILE: installation.md---
@@ -1,8 +1,8 @@
-# Installation
+# Installing on your computer
 
 We are going to be using the following major software for the class:
 
-- [Python] 3 (see {ref}`why-python-3`);
+- [Python] 3 (see {ref}`why-python`);
 - [numpy] (the Python array package);
 - [matplotlib] (Python plotting package);
 - [scipy] (Python scientific library);
@@ -67,13 +67,15 @@ python3 -m jupyter notebook
 This will open your web browser in the Jupyter interface.  Select the file you
 are interested in.
 
+(why-python)=
+
 ## Why Python
 
 Python is well-suited to scientific computing for [many
 reasons](https://github.com/nipy/nipy/blob/master/doc/faq/why.rst#why-python).
 
 Python code is famously easy to read, and Python has become a common choice
-for introductions to programming {{ -- }} see for example the [Berkeley CS61A
+for introductions to programming — see for example the [Berkeley CS61A
 course](http://cs61a.org) and the [MIT introduction to computer science and
 programming](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00sc-introduction-to-computer-science-and-programming-spring-2011/).
 

---FILE: installation_on_mac.md---
@@ -31,7 +31,7 @@ xcode-select --install
 
 in Terminal.app.   If you don't have the command line tools, you will get a dialog box like this:
 
-![](image/xcode_cli_dialog.png)
+![](images/xcode_cli_dialog.png)
 
 Select ""Install"".  You may need to wait a while for that to complete.
 
@@ -54,8 +54,40 @@ To install Homebrew, follow the instructions on the [homebrew home page](https:/
 
 ## Python
 
-In {ref}`terminal.app`, type:
+Mac actually comes with a version of Python for its own use, but it's nearly always better to install your own version, for your use.
+
+First, install with Homebrew.
+
+In {ref}`terminal-app`, type:
 
 ```bash
 brew install python
 ```
+
+Next, open the file `~/.bash_profile` with a text editor, for example, like this:
+
+```
+touch ~/.bash_profile
+open -a TextEdit ~/.bash_profile
+```
+
+Scroll to the end of the file, and add this line:
+
+```
+export PATH=/usr/local/bin:$PATH
+```
+
+Be very careful that TextEdit doesn't capitalize Export for you.  Correct it
+again to lower case if it does.
+
+Save, and close the text editor.  Close Terminal.app  Start Terminal.app again, and confirm you are looking at the right Python:
+
+```bash
+which python3
+```
+
+You should see:
+
+```
+/usr/local/bin/python3
+```

---FILE: numpy_intro.Rmd---
@@ -16,33 +16,343 @@ jupyter:
 
 # Introduction to Numpy
 
+Numpy is the fundamental package for creating and manipulating *arrays* in
+Python.
+
+As for all Python libraries, we need to load the library into Python, in order to use it.  We use the `import` statement to do that:
+
 ```{python}
 import numpy
 ```
 
+`numpy` is now a *module* available for use.  A module is Python's term for a library of code and / or data.
+
+```{python}
+# Show what 'numpy' is
+numpy
+```
+
+Numpy is now ready to use, and has the name `numpy`.  For example, if we want to see the value of pi, according to Numpy, we could run this code:
+
 ```{python}
+numpy.pi
+```
+
+Although it is perfectly reasonable to import Numpy with the simplest statement above, in practice, nearly everyone imports Numpy like this:
+
+```{python}
+# Make numpy available, but give it the name ""np"".
 import numpy as np
 ```
 
+All this is, is a version of the `import` statement where we *rename* the `numpy` module to `np`.
+
+Now, instead of using the longer `numpy` as the name for the module, we can use `np`.
+
+```{python}
+# Show what 'np' is
+np
+```
+
+```{python}
+np.pi
+```
+
+You will see that we nearly always use that `import numpy as np` form, and you
+will also see that almost everyone else in the Python world does the same
+thing.  It's near-universal convention.  That way, everyone knows you mean
+`numpy` when you use `np`.
+
+## Some example data
+
+Let's start with some data, and then go on to process these data with arrays.
+
+Here is a text file with some data about an experiment running in the scanner.
+The subject saw stimuli every 1.75 seconds or so.  Sometimes they press a
+spacebar in response to the stimulus. The file records the subject's data.  There is one row per trial, where each row records:
+
+* `response` — what response the subject make for this trial ('None' or
+  'spacebar')
+* `response_time` — the reaction time for their response (milliseconds after
+  the stimulus, 0 if no response)
+* `trial_ISI` — the time to wait until the *next* stimulus (the Interstimulus
+  Interval)
+* `trial_shape` — the name of the stimulus ('red_star', 'red_circle' and so
+  on).
+
+Here we open the file as text, and load the lines of the file into memory as a list.
+
+```{python}
+# Load the lines in the file as a list.
+lines = open('24719.f3_beh_CHYM.csv', 'rt').readlines()
+# Show the first 5 lines.
+lines[:5]
+```
+
+As you can see, the first line in the file gives the names of the fields in each row.  We will throw away that line to start.
+
+```{python}
+del lines[0]
+lines[:5]
+```
+
+There is one line for each trial, so the number of lines in the file is also the number of trials.
+
+```{python}
+n_trials = len(lines)
+n_trials
+```
+
+Let's look at the first line again:
+
+```{python}
+type(lines[0])
+```
+
+```{python}
+lines[0]
+```
+
+We can use the `split` *method* of the string to split the string at the commas (`,`).  This gives us a list of strings:
+
+```{python}
+parts = lines[0].split(',')
+parts
+```
+
+This in turn suggests how we could use `for` loop to collect the response times and the trial ISIs into lists, with one element per trial.
+
+```{python}
+rt_list = []  # List to hold the response times.
+isi_list = []  # List to hold the trial_isis
+# For each number from 0 up to (not including) the value of n_trials.
+for i in range(n_trials):
+    line = lines[i]  # Get the corresponding line for trial position i
+    parts = line.split(',')  # Split at commas
+    rt_list.append(float(parts[1]))  # Second thing is response time
+    isi_list.append(float(parts[2]))  # Third thing is trialISI
+# Show the first 5 trial ISIs.
+isi_list[:5]
+```
+
+## From lists to arrays
+
+Lists are useful, but we very often need arrays to help us process the data.
+
+We hope you will see why by example.
+
+You can make an array from a list by using the `np.array` function.
+
+```{python}
+isi_arr = np.array(isi_list)
+isi_arr
+```
+
+Let us do the same for the reaction times:
+
+```{python}
+rt_arr = np.array(rt_list)
+# Show the first 15 elements for brevity
+rt_list[:15]
+```
+
+## Arrays have a shape
+
+The array object has `shape` data attached to it:
+
+```{python}
+isi_arr.shape
+```
+
+The shape gives the number of dimensions, and the number of elements for each dimension.  We only have a one-dimensional array, so we see one number, which is the number of elements in the array.  We will get on to two-dimensional arrays later.
+
+## Arrays have a datatype
+
+Notice that the `np.array` function worked out that all the values in the list
+are floating point values, so the array has a *datatype* (`dtype`) of
+`float64` — the standard type of floating point value for Numpy and most other
+numerical packages (such as Matlab and R).   The array `dtype` specifies what
+type of elements the array does and can contain.
+
+```{python}
+isi_arr.dtype
+```
+
+This means that, for example, you cannot put data into this array that can very simply make a floating point value:
+
+```{python tags=c(""raises-exception"")}
+isi_arr[0] = 'some text'
+```
+
+This is in contrast to a list, where the elements can be a mixture of any type of Python value.
+
+```{python}
+my_list = [10.1, 15.3, 0.5]
+my_list[1] = 'some_text'
+my_list
+```
+
+There is another way we could have collected the information from the file, and put it directly into arrays.
+
+We could have started with an array of the right length and type, by using the `np.zeros` function to make an array with all 0.0 values.
+
+```{python}
+rt_arr = np.zeros(n_trials)
+rt_arr
+```
+
+```{python}
+rt_arr = np.zeros(n_trials)
+isi_arr = np.zeros(n_trials)
+for i in range(n_trials):
+    line = lines[i]
+    parts = line.split(',')
+    rt_arr[i] = float(parts[1])
+    isi_arr[i] = float(parts[2])
+isi_arr[:5]
+```
+
+This is a fairly typical use of `np.zeros`.  We make an array of the right size and then fill in the elements later.
+
+## Our task
+
+We want to calculate the relationship between the onsets of the trials, the onsets of the responses, and the onsets of the scans in the scanner.
+
+As you saw above, the inter-stimulus interval is the interval between the
+previous trial and the current one.
+
+For example, the first ISI here, 2000, means that the first trial started at 2000ms from the start of the experiment.
+
+The second ISI, 1000, means that the second trial started at 2000ms + 1000ms from the start of the experiment.
+
+We want to work out how these stimulus times relate to the scanning we did.
+
+Let's think first about the trial onset times, in terms of scans.
+
+The first thing we need to know is that the experimental software started 4 seconds after the start of the first scan.
+
+We know from the first ISI above that the first trial happened 2000ms after the experimental software started.   That means that it happened 4000 + 2000 milliseconds after the scanner started.
+
+The second then we need to know is that the TR (time-to-repeat) for the scanner was 2 seconds.  So, the first scan happens at time 0, the second at time 2000ms, and so on.
+
+
+## Baby steps
+
+Let's proceed in steps.  First let us work out the time for each trial *from
+the start of the experimental software*.   Call this the `cumulative_times`.
+
+
+Because we are careful, we first work out what that would look like if we did it by hand.   Here are the first five ISI values.
+
+```{python}
+isi_arr[:5]
+```
+
+The first value for `cumulative_times` should be 2000.  The second will be 2000 + 1000.  The third will be 2000 + 1000 + 2500.
+
+```{python}
+[2000, 2000 + 1000, 2000 + 1000 + 2500, 2000 + 1000 + 2500 + 1500]
+```
+
+We could do the calculation for every trial with a `for` loop, like this:
+
+```{python}
+cumulative_times = np.zeros(n_trials)
+start_time = 0
+for i in range(0, n_trials):
+    start_time = start_time + isi_arr[i]
+    cumulative_times[i] = start_time
+# Show the first 10 values
+cumulative_times[:10]
+```
+
+That calculation looks right, comparing to our by-hand calculation above.
+
+Luckily, Numpy has a useful function called `np.cumsum` that does the cumulative sum of the elements in the array.  As you can see, this does exactly what we want here:
+
 ```{python}
-rts = [10, 20, 30]
-rts
+# Show the cumulative sum
+cumulative_times = np.cumsum(isi_arr)
+# Show the first 10 values
+cumulative_times[:10]
 ```
 
+Next we need to make these experiment time into times in terms of the scanner start.  To do this, we need to add 4000 to every time.   Of course we could go through and do that with a For loop:
+
 ```{python}
-type(rts)
+scanner_times = np.zeros(n_trials)
+for i in range(n_trials):
+    scanner_times[i] = cumulative_times[i] + 4000
+scanner_times[:10]
 ```
 
+Luckily, however, Numpy will do that for us, because when we add a single number to an array, it has the effect of adding that number to *every element* in the array:
+
 ```{python}
-rts_arr = np.array(rts)
-rts_arr
+scanner_times = cumulative_times + 4000
+scanner_times[:10]
 ```
 
+We now have trial onset times in milliseconds, from the start of the first scan.  We want to recode these numbers in terms of scans since the scanner session started.
+
+For example, the first time is 6000 ms.  The scans each last 2000 ms.  So, in terms of scans, the first time is:
+
+```{python}
+scan_length_ms = 2000
+6000 / scan_length_ms
+```
+
+Luckily — Numpy does division in the same way as it does addition, with a single number against an array:
+
+```{python}
+times_in_scans = scanner_times / scan_length_ms
+times_in_scans[:10]
+```
+
+## 2D arrays
+
+We can also have arrays with more than one dimension.  For example, we can make our original trial and reaction times into a two-dimensional array, like this:
+
 ```{python}
-rts_arr = np.array([10, 20, 30])
-rts_arr
+all_times = np.zeros((n_trials, 2))
+all_times.shape
 ```
 
 ```{python}
-## Making Numpy 
+# Fill all of the rows, first column with the isi_arr values.
+all_times[:, 0] = isi_arr
+# Fill all of the rows, second column with the rt_arr values.
+all_times[:, 1] = rt_arr
+# Show the first 5 rows, and all columns.
+all_times[:5, :]
 ```
+
+## Reshaping
+
+Finally, we often want to change the *shape* of arrays.
+
+One common thing we may want to do is to take a 2D array and flatten it out into a 1D array, or visa versa.  That does not make much sense in this particular case, but bear with us.
+
+```{python}
+flattened = np.reshape(all_times, [n_trials * 2])
+flattened.shape
+```
+
+```{python}
+# The first 10 values.
+flattened[:10]
+```
+
+Here we take the flattened array, and put it back into its original two dimensions:
+
+```{python}
+unflattened = np.reshape(flattened, [n_trials, 2])
+unflattened.shape
+```
+
+```{python}
+# Show the first five rows.
+unflattened[:5, :]
+```
+
+"
nipraxis,textbook,2985495315c8d1b0e4159ca87c83809dc962ae6c,Matthew Brett,matthew.brett@gmail.com,2022-03-26T17:10:07Z,Matthew Brett,matthew.brett@gmail.com,2022-03-26T17:10:24Z,Fixes to build,_toc.yml;arrays_and_images.Rmd;bib/course_refs.bib;images/gui_and_code.png;surviving_computers.md;the_software.md;to_code.md,True,False,True,False,45,11,56,"---FILE: _toc.yml---
@@ -11,10 +11,10 @@ parts:
 - caption: The tools
   chapters:
   - file: to_code
-  - file: intro/surviving_computers
-  - file: intro/the_software
-  - file: intro/using_jupyter
-  - file: intro/more_on_jupyter
+  - file: surviving_computers
+  - file: the_software
+  - file: using_jupyter
+  - file: more_on_jupyter
   - file: brisk_python
 
 - caption: Images and code and images

---FILE: arrays_and_images.Rmd---
@@ -18,10 +18,12 @@ jupyter:
 
 You can consider arrays as images, and images as arrays.
 
-We start off with our usual imports:
+We start off with the imports that we need.
 
 ```{python}
+# The standard library for working with arrays.
 import numpy as np
+# Standard library for plotting.
 import matplotlib.pyplot as plt
 ```
 

---FILE: bib/course_refs.bib---
@@ -174,3 +174,38 @@ @article{feynman1974cargo
   publisher={California Institute of Technology},
   url={http://calteches.library.caltech.edu/51/2/CargoCult.pdf}
 }
+
+@article{mueller2014pen,
+  title={The pen is mightier than the keyboard: Advantages of longhand over laptop note taking},
+  author={Mueller, Pam A and Oppenheimer, Daniel M},
+  journal={Psychological science},
+  volume={25},
+  number={6},
+  pages={1159--1168},
+  year={2014},
+  publisher={Sage Publications Sage CA: Los Angeles, CA}
+}
+
+@article{lui2006pair,
+  title={Pair programming productivity: Novice--novice vs. expert--expert},
+  author={Lui, Kim Man and Chan, Keith CC},
+  journal={International Journal of Human-computer studies},
+  volume={64},
+  number={9},
+  pages={915--925},
+  year={2006},
+  publisher={Elsevier},
+  url={http://www.cs.utexas.edu/users/mckinley/305j/pair-hcs-2006.pdf}
+}
+
+@article{rosen2018correlations,
+  title={Correlations, trends and potential biases among publicly accessible web-based student evaluations of teaching: a large-scale study of RateMyProfessors. com data},
+  author={Rosen, Andrew S},
+  journal={Assessment \& Evaluation in Higher Education},
+  volume={43},
+  number={1},
+  pages={31--44},
+  year={2018},
+  publisher={Taylor \& Francis},
+  url={https://asrosen.com/wp-content/uploads/2018/07/postprint_rmp-1.pdf}
+}

---FILE: surviving_computers.md---
@@ -48,6 +48,6 @@ will find that you can think more quickly and carefully.
 
 ## References
 
-```{bibliography} ../_references.bib
+```{bibliography}
 :filter: docname in docnames
 ```

---FILE: the_software.md---
@@ -1,9 +1,6 @@
 # Our tools
 
-You have already jumped into the technology, but now we step back and
-[name the parts](https://en.wikipedia.org/wiki/Naming_of_Parts).
-
-You have been working in the [Jupyter Notebook](https://jupyter.org).
+We are going to start working in the [Jupyter Notebook](https://jupyter.org).
 This is an *interface* that allows us to interact with the
 [Python](https://www.python.org) programming language.
 

---FILE: to_code.md---
@@ -31,7 +31,7 @@ You will soon find that most work that is worth doing, is
 difficult.  If you want to do something interesting, that has
 not been done before, you will often need code.
 
-![GUIs and code](../images/gui_and_code.png)
+![GUIs and code](images/gui_and_code.png)
 
 We need code more than we did ten years ago, because we have
 much more data, and the data is of many different types.  If"
nipraxis,textbook,f0bbabb689dd19b677fa36a5ca8f526fe5192aa1,pxr687,57416249+pxr687@users.noreply.github.com,2022-03-25T13:55:46Z,pxr687,57416249+pxr687@users.noreply.github.com,2022-03-25T14:15:54Z,"some small suggested edits

- split some lines over separate cells to maybe make it clearer to the reader what is happening (e.g. where the first line  wasn't shown in the output of the cell)

- I think some headings were missing (e.g 'Tuple'), have amended

-  removed comment from 'commented out' code which generates errors e.g. `my_tuple.append(20)`; added 'raises exception' cell tags",brisk_python.Rmd,True,False,True,False,55,13,68,"---FILE: brisk_python.Rmd---
@@ -5,7 +5,11 @@ jupyter:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.11.5
+      jupytext_version: 1.13.0
+  kernelspec:
+    display_name: Python 3
+    language: python
+    name: python3
 ---
 
 # Brisk introduction to Python
@@ -365,6 +369,9 @@ for e in my_list:
 ```{python}
 # Can be indexed
 my_list[1]
+```
+
+```{python}
 # Can be sliced
 my_list[0:2]
 ```
@@ -375,7 +382,12 @@ Indices for Python sequences start at 0.  For Python, the first element is at
 index 0, the second element is at index 1, and so on:
 
 ```{python}
+# the first element
 my_list[0]
+```
+
+```{python}
+# the second element
 my_list[1]
 ```
 
@@ -385,7 +397,9 @@ Negative numbers as indices count back from the end of the list. For
 example, use index `-1` to return the last element in the list:
 
 ```{python}
-my_list
+print(my_list)
+
+# the last element
 my_list[-1]
 ```
 
@@ -485,6 +499,9 @@ You can remove elements from the list with the `pop` method:
 # Remove and return the last element of the list
 my_list.pop()
 my_list
+```
+
+```{python}
 # Remove and return the third element of the list
 my_list.pop(2)
 my_list
@@ -517,6 +534,9 @@ the stop index.
 
 ```{python}
 my_list[2:]
+```
+
+```{python}
 my_list[2:len(my_list)]
 ```
 
@@ -555,7 +575,7 @@ indexing, negative `start` and `stop` values count back from the end of
 the list:
 
 ```{python}
-my_list
+print(my_list)
 my_list[-4:-2]
 ```
 
@@ -571,9 +591,15 @@ the start index defaults to the last element in the list. If you don’t specify
 the stop index, it defaults to one prior to index 0:
 
 ```{python}
-my_list
+print(my_list)
 my_list[-1:1:-1]
+```
+
+```{python}
 my_list[:1:-1]
+```
+
+```{python}
 my_list[-2::-1]
 ```
 
@@ -584,7 +610,7 @@ you a reversed copy of the list:
 my_list[::-1]
 ```
 
-Tuples
+# Tuples
 
 Tuples are almost the same as lists, except they are not mutable. That
 is, you cannot change the elements of a tuple, or change the number of
@@ -595,14 +621,14 @@ my_tuple = (9, 4, 7, 0, 8)
 my_tuple
 ```
 
-```{python}
+```{python tags=c(""raises-exception"")}
 # This raises a TypeError
-# my_tuple[1] = 99
+my_tuple[1] = 99
 ```
 
-```{python}
+```{python tags=c(""raises-exception"")}
 # This raises an AttributeError, because tuples have no append method
-# my_tuple.append(20)
+my_tuple.append(20)
 ```
 
 Here’s an empty tuple:
@@ -665,6 +691,9 @@ Convert other objects to strings using `str`:
 ```{python}
 # Convert integer to string
 str(9)
+```
+
+```{python}
 # Convert floating point value to string
 str(1.2)
 ```
@@ -695,14 +724,19 @@ my_string[1]
 my_string[1:5]
 ```
 
+```{python}
+# Can slice
+my_string[::-1]
+```
+
 # Strings are immutable
 
 Unlike lists, strings are immutable. You cannot change the characters within a
 string:
 
-```{python}
+```{python tags=c(""raises-exception"")}
 # Raises a TypeError
-# my_string[1] = 'N'
+my_string[1] = 'N'
 ```
 
 # Adding strings
@@ -912,6 +946,9 @@ in c`, `in` triggers a test of membership, returning True or False.
 
 ```{python}
 5 in my_set
+```
+
+```{python}
 11 in my_set
 ```
 
@@ -925,6 +962,9 @@ Lists and tuples are also containers:
 
 ```{python}
 9 in [9, 4, 7, 0, 8]
+```
+
+```{python}
 3 in (1, 3, 5)
 ```
 
@@ -1014,10 +1054,13 @@ have a key in a dictionary:
 
 ```{python}
 'MATLAB' in software
+```
+
+```{python}
 'happiness' in software
 ```
 
-“for”, “while”, “continue” and “break”
+# “for”, “while”, “continue” and “break”
 
 `for` statements and `while` statement are *loops*, because Python keeps
 executing the `for` or `while` block until the `for` runs out of
@@ -1029,7 +1072,6 @@ for i in range(10):
     if i == 6:
         break
     print(i)
-
 ```
 
 The `continue` statement short-circuits execution of the current iteration"
nipraxis,textbook,66d022e8c8aae0f9dd7bfb81dbde574e40b64723,Matthew Brett,matthew.brett@gmail.com,2022-03-18T22:53:07Z,Matthew Brett,matthew.brett@gmail.com,2022-03-18T22:53:07Z,"Fix some more refs, add anterios cingulate page",_config.yml;_toc.yml;anterior_cingulate.md;bib/course_refs.bib;dipy_registration.py;markdown.md,False,False,False,False,334,128,462,"---FILE: _config.yml---
@@ -58,7 +58,6 @@ exclude_patterns:
   - pearson_functions.md
   - validate_against_scipy.Rmd
   # Hemodynamic response modeling
-  - anterior_cingulate.md
   - introducing_nipype.Rmd
   # Code organization and collaboration
   - choosing_editor.md

---FILE: _toc.yml---
@@ -49,6 +49,7 @@ parts:
   - file: rotation_2d_3d
   - file: resampling_with_ndimage
   - file: dipy_registration
+  - file: anterior_cingulate
 
 - caption: Statistics and the general linear model
   chapters:

---FILE: anterior_cingulate.md---
@@ -36,7 +36,7 @@ repository](https://github.com/nipraxis/nipraxis-data). Another, more automated
 way of getting the files is using Nipraxis to fetch them from the web, then
 copy the resulting files to your working directory.
 
-```{python}
+```python
 # Automatic download of data files to working directory.
 import os
 import shutil
@@ -161,3 +161,9 @@ We will be using [MRIcron](https://www.nitrc.org/projects/mricron):
   Do you think the cytoarchitecture lines up with where you think it should be,
   given your drawing and the anatomy of the template and the individual subject
   image?
+
+## References
+
+```{bibliography}
+:filter: docname in docnames
+```

---FILE: bib/course_refs.bib---
@@ -174,4 +174,3 @@ @article{feynman1974cargo
   publisher={California Institute of Technology},
   url={http://calteches.library.caltech.edu/51/2/CargoCult.pdf}
 }
-

---FILE: dipy_registration.py---
@@ -0,0 +1,326 @@
+"""""" Run non-linear registration using Dipy
+""""""
+
+import os
+from os.path import split as psplit, join as pjoin, splitext, exists
+import pickle
+
+import numpy as np
+import numpy.linalg as npl
+
+import nibabel as nib
+from nibabel.affines import to_matvec
+
+from scipy.ndimage import affine_transform
+
+from dipy.align.imaffine import (MutualInformationMetric, AffineRegistration)
+from dipy.align.transforms import (TranslationTransform3D,
+                                   RigidTransform3D,
+                                   AffineTransform3D)
+from dipy.align.imwarp import SymmetricDiffeomorphicRegistration
+from dipy.align.metrics import CCMetric
+
+
+TEMPLATE_IMG = 'mni_icbm152_t1_tal_nlin_asym_09a.nii'
+TEMPLATE_MASK = 'mni_icbm152_t1_tal_nlin_asym_09a_mask.nii'
+
+
+def as_image(image):
+    """""" If `image` is string, assume filename and load image, else pass through
+    """"""
+    if isinstance(image, str):
+        image = nib.load(image)
+    return image
+
+
+def apply_mask(brain_img, mask_img):
+    """""" Load brain image, matching mask; apply and return masked image
+
+    Parameters
+    ----------
+    brain_img : str or image
+        string giving image filename or image object
+    mask_img : str or image
+        string giving mask image filename or image object.  Mask must match
+        `brain_img` after mapping with image and mask affine.
+
+    Returns
+    -------
+    mask_img : image object
+        image object where data is data from `brain_img` multiplied elementwise
+        by the data of `mask_img`, where the mask data has[ been resampled into
+        the voxel space of `brain_img` if necessary.
+    """"""
+    brain_img = as_image(brain_img)
+    b_aff = brain_img.affine
+    b_hdr = brain_img.header
+    mask_img = as_image(mask_img)
+    brain_data = brain_img.get_data()
+    mask_data = mask_img.get_data()
+    if not np.allclose(b_aff, mask_img.affine):
+        # Mask and brain have different affines - we need to resample
+        brain2mask = npl.inv(mask_img.affine).dot(brain_img.affine)
+        mat, vec = to_matvec(brain2mask)
+        mask_data = affine_transform(mask_data, mat, vec,
+                                     output_shape=brain_data.shape,
+                                     order=0)  # nearest neighbor
+    return nib.Nifti1Image(brain_data * mask_data, b_aff, b_hdr)
+
+
+def register_affine(t_masked, m_masked, affreg=None,
+                    final_iters=(10000, 1000, 100)):
+    """""" Run affine registration between images `t_masked`, `m_masked`
+
+    Parameters
+    ----------
+    t_masked : image
+        Template image object, with image data masked to set out-of-brain
+        voxels to zero.
+    m_masked : image
+        Moving (individual) image object, with image data masked to set
+        out-of-brain voxels to zero.
+    affreg : None or AffineRegistration instance, optional
+        AffineRegistration with which to register `m_masked` to `t_masked`.  If
+        None, we make an instance with default parameters.
+    final_iters : tuple, optional
+        Length 3 tuple of level iterations to use on final affine pass of the
+        registration.
+
+    Returns
+    -------
+    affine : shape (4, 4) ndarray
+        Final affine mapping from voxels in `t_masked` to voxels in `m_masked`.
+    """"""
+    if affreg is None:
+        metric = MutualInformationMetric(nbins=32,
+                                         sampling_proportion=None)
+        affreg = AffineRegistration(metric=metric)
+    t_data = t_masked.get_data().astype(float)
+    m_data = m_masked.get_data().astype(float)
+    t_aff = t_masked.affine
+    m_aff = m_masked.affine
+    translation = affreg.optimize(t_data, m_data, TranslationTransform3D(),
+                                  None, t_aff, m_aff)
+    rigid = affreg.optimize(t_data, m_data, RigidTransform3D(), None, t_aff,
+                            m_aff, starting_affine=translation.affine)
+    # Maybe bump up iterations for last step
+    if final_iters is not None:
+        affreg.level_iters = list(final_iters)
+    affine = affreg.optimize(t_data, m_data, AffineTransform3D(), None, t_aff,
+                             m_aff, starting_affine=rigid.affine)
+    return affine.affine
+
+
+def register_diffeo(t_masked, m_masked, start_affine, registration=None):
+    """""" Run non-linear registration between `t_masked` and `m_masked`
+
+    Parameters
+    ----------
+    t_masked : image
+        Template image object, with image data masked to set out-of-brain
+        voxels to zero.
+    m_masked : image
+        Moving (individual) image object, with image data masked to set
+        out-of-brain voxels to zero.
+    start_affine : shape (4, 4) ndarray
+        Affine mapping from voxels in `t_masked` to voxels in `m_masked`.
+    registration : None or SymmetricDiffeoMorphicRegistration instance
+        Registration instance we will use to register `t_masked` and
+        `m_masked`.  If None, make a default registration object.
+
+    Returns
+    -------
+    mapping : mapping instance
+        Instance giving affine + non-linear mapping between voxels in
+        `t_masked` and voxels in `m_masked`.
+    """"""
+    if registration is None:
+        registration = SymmetricDiffeomorphicRegistration(
+            metric=CCMetric(3),
+            level_iters=[10, 10, 5])
+    return registration.optimize(t_masked.get_data().astype(float),
+                                 m_masked.get_data().astype(float),
+                                 t_masked.affine,
+                                 m_masked.affine,
+                                 start_affine)
+
+
+def register_save(template_fname, template_mask_fname,
+                  moving_fname, moving_mask_fname):
+    """""" Resister individual image `moving_fname` to template `template_fname`
+
+    Save warped image and mapping object to directory of `moving_fname`.
+
+    Parameters
+    ----------
+    template_fname : str
+        string giving image filename of template image to register to
+    template_mask_fname : str
+        string giving image filename of brain mask corresponding to
+        `template_fname`
+    moving_fname : str
+        string giving image filename of individual image to register
+    moving_mask_fname : str
+        string giving image filename of brain mask corresponding to
+        `moving_fname`
+
+    Returns
+    -------
+    mapping : mapping instance
+        Instance giving affine + non-linear mapping between voxels in
+        `template_fname` and voxels in `moving_fname`.
+    """"""
+    path, basename = psplit(moving_fname)
+    root, ext = splitext(basename)
+    if ext == '.gz':  # ignore .gz suffix
+        root, ext = splitext(root)
+    t_masked = apply_mask(template_fname, template_mask_fname)
+    m_masked = apply_mask(moving_fname, moving_mask_fname)
+    affine = register_affine(t_masked, m_masked)
+    mapping = register_diffeo(t_masked, m_masked, affine)
+    masked_data = m_masked.get_data()
+    warped_moving = nib.Nifti1Image(mapping.transform(masked_data),
+                                    t_masked.affine,
+                                    t_masked.header)
+    nib.save(warped_moving, pjoin(path, 'w_' + basename))
+    with open(pjoin(path, 'map_' + root + '.pkl'), 'wb') as fobj:
+        pickle.dump(mapping, fobj)
+    return mapping
+
+
+def as_mapping(mapping):
+    """""" If `mapping` is string, assume filename, load pickle, else pass through
+    """"""
+    if isinstance(mapping, ):
+        with open(mapping, 'rb') as fobj:
+            mapping = pickle.load(fobj)
+    return mapping
+
+
+def write_warped(fname, mapping, interpolation='nearest', template_header=None):
+    """""" Warp an image in individual space to template space
+
+    Parameters
+    ----------
+    fmame : str
+        Filename of image to warp in template space
+    mapping : mapping instance
+        object containing mapping from individual space to template space
+    interpolation : str, optional
+        interpolation to use when resampling data from `fname`
+    template_header : None or header instance
+        template header with which to save image.  If None, use default header.
+    """"""
+    img = nib.load(fname)
+    mapping = as_mapping(mapping)
+    template_affine = mapping.codomain_grid2world
+    data = img.get_data().astype(float)
+    warped = mapping.transform(data, interpolation=interpolation)
+    warped_img = nib.Nifti1Image(warped, template_affine, template_header)
+    path, basename = psplit(fname)
+    out_fname = pjoin(path, 'w_' + basename)
+    nib.save(warped_img, out_fname)
+
+
+def find_anatomicals(root):
+    """""" Find anatomical image, mask pairs from OpenFMRI directory root `root`
+
+    Parameters
+    ----------
+    root : str
+        root directory of OpenFMRI dataset - e.g. ""ds114""
+
+    Returns
+    -------
+    anatomicals : list
+        List of anatomical, mask image pairs for each subject in `root`
+    """"""
+    anatomicals = []
+    for dirpath, dirnames, filenames in os.walk(root):
+        if not 'highres001.nii.gz' in filenames:
+            continue
+        full_image = pjoin(dirpath, 'highres001.nii.gz')
+        mask_image = pjoin(dirpath, 'highres001_brain_mask.nii.gz')
+        assert exists(full_image)
+        assert exists(mask_image)
+        anatomicals.append((mask_image, full_image))
+    return anatomicals
+
+
+def sub2img_mask(root, sub_no):
+    """""" Return anatomical image, mask pair from OpenFMRI root, subject no
+
+    Parameters
+    ----------
+    root : str
+        root directory of OpenFMRI dataset - e.g. ""ds114""
+    sub_no : int
+        subject no, where 1 is the first subject
+
+    Returns
+    -------
+    anatomical_fname : str
+        filename of anatomical image, beginning with path `root`
+    mask_fname : str
+        filename of anatomical image brain mask, beginning with path `root`
+    """"""
+    anatomical_path = pjoin(root, 'sub{:03d}'.format(sub_no), 'anatomy')
+    ret = (pjoin(anatomical_path, 'highres001.nii.gz'),
+           pjoin(anatomical_path, 'highres001_brain_mask.nii.gz'))
+    if all(exists(p) for p in ret):
+        return ret
+    return ()
+
+
+def write_highres(root):
+    """""" Calculcate parameters, write anatomicals from OpenFMRI directory `root`
+
+    Parameters
+    ----------
+    root : str
+        root directory of OpenFMRI dataset - e.g. ""ds114""
+    """"""
+    for moving_img, moving_mask in find_anatomicals(root):
+        register_save(TEMPLATE_IMG, TEMPLATE_MASK,
+                      moving_img, moving_mask)
+
+
+def write_highres_parallel(root):
+    """""" Calculcate parameters, write anatomicals from OpenFMRI directory `root`
+
+    Use parallel execution.  Careful, this can crash your machine with too many
+    images found at `root`.
+
+    Parameters
+    ----------
+    root : str
+        root directory of OpenFMRI dataset - e.g. ""ds114""
+    """"""
+    import multiprocessing
+    jobs = []
+    for moving_img, moving_mask in find_anatomicals(root):
+        p = multiprocessing.Process(target=register_save, args=(
+            TEMPLATE_IMG, TEMPLATE_MASK, moving_img, moving_mask))
+        jobs.append(p)
+        p.start()
+
+
+def register_subject(root, sub_no):
+    """""" Calculcate parameters, write anatomical for subject at OpenFMRI `root`
+
+    Parameters
+    ----------
+    root : str
+        root directory of OpenFMRI dataset - e.g. ""ds114""
+    sub_no : int
+        subject no, where 1 is the first subject
+
+    Returns
+    -------
+    mapping : mapping instance
+        Instance giving affine + non-linear mapping between voxels in
+        `t_masked` and voxels in `m_masked`.
+    """"""
+    moving_img, moving_mask = sub2img_mask(root, sub_no)
+    return register_save(TEMPLATE_IMG, TEMPLATE_MASK,
+                         moving_img, moving_mask)

---FILE: markdown.md---
@@ -1,125 +0,0 @@
-# Markdown Files
-
-Whether you write your book's content in Jupyter Notebooks (`.ipynb`) or
-in regular markdown files (`.md`), you'll write in the same flavor of markdown
-called **MyST Markdown**.
-
-## What is MyST?
-
-MyST stands for ""Markedly Structured Text"". It
-is a slight variation on a flavor of markdown called ""CommonMark"" markdown,
-with small syntax extensions to allow you to write **roles** and **directives**
-in the Sphinx ecosystem.
-
-## What are roles and directives?
-
-Roles and directives are two of the most powerful tools in Jupyter Book. They
-are kind of like functions, but written in a markup language. They both
-serve a similar purpose, but **roles are written in one line**, whereas
-**directives span many lines**. They both accept different kinds of inputs,
-and what they do with those inputs depends on the specific role or directive
-that is being called.
-
-### Using a directive
-
-At its simplest, you can insert a directive into your book's content like so:
-
-````
-```{mydirectivename}
-My directive content
-```
-````
-
-This will only work if a directive with name `mydirectivename` already exists
-(which it doesn't). There are many pre-defined directives associated with
-Jupyter Book. For example, to insert a note box into your content, you can
-use the following directive:
-
-````
-```{note}
-Here is a note
-```
-````
-
-This results in:
-
-```{note}
-Here is a note
-```
-
-In your built book.
-
-For more information on writing directives, see the
-[MyST documentation](https://myst-parser.readthedocs.io/).
-
-
-### Using a role
-
-Roles are very similar to directives, but they are less-complex and written
-entirely on one line. You can insert a role into your book's content with
-this pattern:
-
-```
-Some content {rolename}`and here is my role's content!`
-```
-
-Again, roles will only work if `rolename` is a valid role's name. For example,
-the `doc` role can be used to refer to another page in your book. You can
-refer directly to another page by its relative path. For example, the
-role syntax `` {doc}`intro` `` will result in: {doc}`intro`.
-
-For more information on writing roles, see the
-[MyST documentation](https://myst-parser.readthedocs.io/).
-
-
-### Adding a citation
-
-You can also cite references that are stored in a `bibtex` file. For example,
-the following syntax: `` {cite}`holdgraf_evidence_2014` `` will render like
-this: {cite}`holdgraf_evidence_2014`.
-
-Moreover, you can insert a bibliography into your page with this syntax:
-The `{bibliography}` directive must be used for all the `{cite}` roles to
-render properly.
-For example, if the references for your book are stored in `references.bib`,
-then the bibliography is inserted with:
-
-````
-```{bibliography}
-```
-````
-
-Resulting in a rendered bibliography that looks like:
-
-```{bibliography}
-```
-
-
-### Executing code in your markdown files
-
-If you'd like to include computational content inside these markdown files,
-you can use MyST Markdown to define cells that will be executed when your
-book is built. Jupyter Book uses *jupytext* to do this.
-
-First, add Jupytext metadata to the file. For example, to add Jupytext metadata
-to this markdown page, run this command:
-
-```
-jupyter-book myst init markdown.md
-```
-
-Once a markdown file has Jupytext metadata in it, you can add the following
-directive to run the code at build time:
-
-````
-```{code-cell}
-print(""Here is some code to execute"")
-```
-````
-
-When your book is built, the contents of any `{code-cell}` blocks will be
-executed with your default Jupyter kernel, and their outputs will be displayed
-in-line with the rest of your content.
-
-For more information about executing computational content with Jupyter Book,
-see [The MyST-NB documentation](https://myst-nb.readthedocs.io/)."
nipraxis,textbook,279576d4da8c072a535b011b5fd40e40ca95c7ec,Matthew Brett,matthew.brett@gmail.com,2022-03-18T22:21:39Z,Matthew Brett,matthew.brett@gmail.com,2022-03-18T22:21:39Z,Clean up for crashes and reference errors,model_one_voxel.Rmd;on_modules.Rmd;otsu_threshold.Rmd;voxel_time_courses.Rmd;whole_image_statistics.Rmd,True,False,True,False,29,8,37,"---FILE: model_one_voxel.Rmd---
@@ -56,7 +56,7 @@ plt.plot(voxel_time_course)
 ```
 
 Now we are going to use the convolved regressor from [Convolving with the
-hemodyamic response function](convolution_backround.Rmd) to do a simple
+hemodyamic response function](convolution_background) to do a simple
 regression on this voxel time course.
 
 First fetch the text file with the convolved time course:

---FILE: on_modules.Rmd---
@@ -183,5 +183,5 @@ sys.argv == ['my_script.py', '1']
 
 Example files at:
 
-* [mymodule.py](mymodule.py)
-* [my_script.py](my_script.py)
+* {download}`mymodule.py`
+* {download}`my_script.py`

---FILE: otsu_threshold.Rmd---
@@ -1,6 +1,8 @@
 ---
 jupyter:
   jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
     text_representation:
       extension: .Rmd
       format_name: rmarkdown
@@ -67,10 +69,19 @@ Conceptually, Otsu’s method proceeds like this:
 Here is Otsu’s threshold in action.  First we load an image:
 
 ```{python}
+# Get the image file from the web.
+import nipraxis
+camera_fname = nipraxis.fetch_file('camera.txt')
+camera_fname
+```
+
+```{python}
+# Loat the file, show the image.
 import numpy as np
 import matplotlib.pyplot as plt
-cameraman = np.loadtxt('camera.txt').reshape((512, 512))
-plt.imshow(cameraman.T, cmap='gray', interpolation='nearest')
+
+cameraman = np.loadtxt(camera_fname).reshape((512, 512))
+plt.imshow(cameraman.T, cmap='gray')
 ```
 
 Make a histogram:

---FILE: voxel_time_courses.Rmd---
@@ -1,6 +1,8 @@
 ---
 jupyter:
   jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
     text_representation:
       extension: .Rmd
       format_name: rmarkdown
@@ -121,12 +123,20 @@ switch off when the task stops (value = 0).
 To get this on-off measure, we will use our pre-packaged function for OpenFMRI
 data:
 
+```{python}
+# Fetch the condition file
+import nipraxis
+
+cond_fname = nipraxis.fetch_file('ds114_sub009_t2r1_cond.txt')
+cond_fname
+```
+
 ```{python}
 # Load the neural time course using pre-packaged function
-from stimuli import events2neural
+from nipraxis.stimuli import events2neural
 TR = 2.5  # time between volumes
 n_trs = img.shape[-1]  # The original number of TRs
-neural = events2neural('ds114_sub009_t2r1_cond.txt', TR, n_trs)
+neural = events2neural(cond_fname, TR, n_trs)
 plt.plot(neural)
 ```
 

---FILE: whole_image_statistics.Rmd---
@@ -242,7 +242,7 @@ np.sum(mask)
 
 We can use the 3D mask to slice into the 4D data matrix.  For every True value
 in the 3D mask, the result has the vector of values over time for that voxel.
-See: [Indexing with Boolean masks](boolean_indexing.Rmd).
+See: [Indexing with Boolean masks](boolean_indexing).
 
 ```{python}
 Y = data[mask]"
nipraxis,textbook,be75dc27a678b3aa4458b0a7ce7ccb288938b3f0,Matthew Brett,matthew.brett@gmail.com,2022-03-18T20:10:58Z,Matthew Brett,matthew.brett@gmail.com,2022-03-18T20:10:58Z,Fix map_coordinates for nipraxis download,map_coordinates.Rmd,True,False,True,False,29,22,51,"---FILE: map_coordinates.Rmd---
@@ -1,6 +1,8 @@
 ---
 jupyter:
   jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
     text_representation:
       extension: .Rmd
       format_name: rmarkdown
@@ -17,19 +19,15 @@ jupyter:
 Requirements:
 
 * [coordinate systems and affine transforms](http://nipy.org/nibabel/coordinate_systems.html);
-
-* Making coordinate arrays with meshgrid;
-
-* numpy.tranpose for swapping axes;
-
-* The nibabel.affines module;
-
-* Applying coordinate transforms with nibabel.affines.apply_affine;
+* Making coordinate arrays with `meshgrid`;
+* `numpy.tranpose` for swapping axes;
+* The `nibabel.affines` module;
+* Applying coordinate transforms with `nibabel.affines.apply_affine`;
 
 <!-- see coordinate_board.jpg for diagram needed about here. -->
-scipy.ndimage.affine_transform is a routine
-that samples between images where there is an affine transform between the
-coordinates of the output image and the input image.
+`scipy.ndimage.affine_transform` is a routine that samples between images where
+there is an affine transform between the coordinates of the output image and
+the input image.
 
 `scipy.ndimage.map_coordinates` is a more general way of resampling between
 images, where we specify the coordinates in the input image, for each voxel
@@ -44,7 +42,6 @@ be expressed as an affine, such as complex non-linear transformations.
 `map_coordinates` accepts:
 
 * `input` – the array to resample from;
-
 * `coordinates` – the array shape (3,) + `output_shape` giving the
   voxel coordinates at which to sample `input`;
 
@@ -62,15 +59,9 @@ it:
 * inserts `v` into `K` with `K[i, j, k] = v`.
 
 This might be clearer with an example. Let’s resample a structural brain image
-to a functional brain image.   See Reslicing with affines exercise for
+to a functional brain image.   See the Reslicing with affines exercise for
 an exercise using `scipy.ndimage.affine_transform` to do this.
 
-You will need the:
-
-* BOLD (functional) image : `ds114_sub009_t2r1.nii`;
-
-* structural image : `ds114_sub009_highres.nii`.
-
 ```{python}
 #: standard imports
 import numpy as np
@@ -84,12 +75,28 @@ plt.rcParams['image.interpolation'] = 'nearest'
 import nibabel as nib
 ```
 
-Load the structural and functional data:
+We will need the:
+
+* BOLD (functional) image : `ds114_sub009_t2r1.nii`;
+* structural image : `ds114_sub009_highres.nii`.
+
+```{python}
+# Load the function to fetch the data file we need.
+import nipraxis
+# Fetch the BOLD image
+bold_fname = nipraxis.fetch_file('ds114_sub009_t2r1.nii')
+# Show the file name.
+print(bold_fname)
+# Fetch structural image
+structural_fname = nipraxis.fetch_file('ds114_sub009_highres.nii')
+# Show the file names
+print(structural_fname)
+```
 
 ```{python}
-bold_img = nib.load('ds114_sub009_t2r1.nii')
+bold_img = nib.load(bold_fname)
 mean_bold_data = bold_img.get_fdata().mean(axis=-1)
-structural_img = nib.load('ds114_sub009_highres.nii')
+structural_img = nib.load(structural_fname)
 structural_data = structural_img.get_fdata()
 ```
 "
nipraxis,textbook,bceef238e75a215078ed21e03555040d93790f9e,Matthew Brett,matthew.brett@gmail.com,2022-03-18T20:02:20Z,Matthew Brett,matthew.brett@gmail.com,2022-03-18T20:02:20Z,Fix up saving images notebook,.gitignore;saving_images.Rmd,True,False,True,False,18,38,56,"---FILE: .gitignore---
@@ -9,3 +9,4 @@ _references.bib
 some_numbers.txt
 ds114_sub009_t2r1_conv.txt
 voxel_time_course.txt
+clipped_image.nii

---FILE: saving_images.Rmd---
@@ -8,9 +8,7 @@ jupyter:
       jupytext_version: 1.11.5
 ---
 
-$\newcommand{L}[1]{\| #1 \|}\newcommand{VL}[1]{\L{ \vec{#1} }}\newcommand{R}[1]{\operatorname{Re}\,(#1)}\newcommand{I}[1]{\operatorname{Im}\, (#1)}$
-
-## Making and saving new images in nibabel
+# Making and saving new images in nibabel
 
 We often want to do some processing on an image, then save the processed image
 back to an image file on disk.
@@ -20,34 +18,43 @@ NIfTI `.nii` image, we get an image object of type `Nifti1Image`.
 
 ```{python}
 import numpy as np
+import nibabel as nib
 ```
 
 ```{python}
-import nibabel as nib
-img = nib.load('ds114_sub009_highres.nii')
+# Load the function to fetch the data file we need.
+import nipraxis
+# Fetch the data file.
+data_fname = nipraxis.fetch_file('ds114_sub009_highres.nii')
+# Show the file name of the fetched data.
+data_fname
+```
+
+```{python}
+img = nib.load(data_fname)
 type(img)
 ```
 
 Maybe we were worried about some very high values in the image, and we wanted
 to clip them down to a more reasonable number:
 
 ```{python}
-data = img.get_data()
-data.max()  
+data = img.get_fdata()
+data.max()
 ```
 
 We might consider clipping the top 5 percent of voxel values:
 
 ```{python}
-data = img.get_data()
+data = img.get_fdata()
 top_95_thresh = np.percentile(data, 95)
 top_95_thresh
 ```
 
 ```{python}
 new_data = data.copy()
 new_data[new_data > top_95_thresh] = top_95_thresh
-new_data.max()  
+new_data.max()
 ```
 
 We can make a new `Nifti1Image` by constructing it directly.  We pass the
@@ -90,33 +97,5 @@ This image has the clipped data:
 
 ```{python}
 clipped_back = nib.load('clipped_image.nii')
-clipped_back.get_data().max()  
+clipped_back.get_fdata().max()
 ```
-
-<!-- vim:ft=rst -->
-<!-- Course -->
-<!-- BIC -->
-<!-- Python distributions -->
-<!-- Version control -->
-<!-- Editors -->
-<!-- Python and common libraries -->
-<!-- IPython -->
-<!-- Virtualenv and helpers -->
-<!-- Pypi and packaging -->
-<!-- Mac development -->
-<!-- Windows development -->
-<!-- Nipy and friends -->
-<!-- FMRI datasets -->
-<!-- Languages -->
-<!-- Imaging software -->
-<!-- Installation -->
-<!-- Tutorials -->
-<!-- MB tutorials -->
-<!-- Ideas -->
-<!-- Psych-214 -->
-<!-- People -->
-<!-- Licenses -->
-<!-- Neuroimaging stuff -->
-<!-- OpenFMRI projects -->
-<!-- Unix -->
-<!-- Substitutions -->"
nipraxis,textbook,07375b8260969425a3a1e1701b0faa227a89c0f2,Matthew Brett,matthew.brett@gmail.com,2022-03-18T19:52:56Z,Matthew Brett,matthew.brett@gmail.com,2022-03-18T19:52:56Z,Fix modules page,my_script.py;mymodule.py;on_modules.Rmd,True,False,True,False,59,7,66,"---FILE: my_script.py---
@@ -0,0 +1,22 @@
+"""""" This is my script
+
+It uses mymodule
+""""""
+
+import sys
+
+import mymodule
+
+
+def main():
+    # This function executed when we are being run as a script
+    print(sys.argv)
+    filename = sys.argv[1]
+    means = mymodule.vol_means(filename)
+    for mean in means:
+        print(mean)
+
+
+if __name__ == '__main__':
+    # We are being run as a script
+    main()

---FILE: mymodule.py---
@@ -0,0 +1,18 @@
+"""""" This is mymodule
+
+It has useful functions in it.
+""""""
+
+# Don't forget to import the other modules used by the code in this module
+import numpy as np
+
+import nibabel as nib
+
+def vol_means(image_fname):
+    img = nib.load(image_fname, mmap=False)
+    data = img.get_data()
+    means = []
+    for i in range(data.shape[-1]):
+        vol = data[..., i]
+        means.append(np.mean(vol))
+    return means

---FILE: on_modules.Rmd---
@@ -1,13 +1,15 @@
 ---
 jupyter:
   jupytext:
+    notebook_metadata_filter: all,-language_info
+    split_at_heading: true
     text_representation:
       extension: .Rmd
       format_name: rmarkdown
       format_version: '1.2'
-      jupytext_version: 1.11.5
+      jupytext_version: 1.10.3
   kernelspec:
-    display_name: Python 3 (ipykernel)
+    display_name: Python 3
     language: python
     name: python3
 ---
@@ -21,6 +23,17 @@ import matplotlib.pyplot as plt
 import nibabel as nib
 ```
 
+We get a 4D volume to work on:
+
+```{python}
+# Load the function to fetch the data file we need.
+import nipraxis
+# Fetch the data file.
+data_fname = nipraxis.fetch_file('ds107_sub012_t1r2.nii')
+# Show the file name of the fetched data.
+data_fname
+```
+
 Let’s say I have a function that loads a 4D image, and returns a list with
 mean values across all voxels in each volume:
 
@@ -38,7 +51,7 @@ def vol_means(image_fname):
 First we check that works:
 
 ```{python}
-my_means = vol_means('ds107_sub012_t1r2.nii')
+my_means = vol_means(data_fname)
 plt.plot(my_means)
 ```
 
@@ -129,7 +142,7 @@ if __name__ == '__main__':
     main()
 ```
 
-See [if `__name__ == '__main__'`](https://docs.python.org/3/library/__main__.html) in the Python
+See [`if __name__ == '__main__'`](https://docs.python.org/3/library/__main__.html) in the Python
 documentation.
 
 # Command line arguments
@@ -170,6 +183,5 @@ sys.argv == ['my_script.py', '1']
 
 Example files at:
 
-* `mymodule.py`;
-
-* `my_script.py`.
+* [mymodule.py](mymodule.py)
+* [my_script.py](my_script.py)"
nipraxis,textbook,e52f9926e65318d14e3d9e568e2a4d156f0f1ea1,Matthew Brett,matthew.brett@gmail.com,2022-03-01T15:12:08Z,Matthew Brett,matthew.brett@gmail.com,2022-03-01T15:12:08Z,"Small refactorings of _toc

Fix link in syllabus",_config.yml;_toc.yml;pages/syllabus.md,False,False,False,False,25,14,39,"---FILE: _config.yml---
@@ -1,9 +1,14 @@
 # Book settings
 # Learn more at https://jupyterbook.org/customize/config.html
 
-title: Practical Neuroimaging
+title: Practice and theory of brain imaging
 author: The nipraxis team
 logo: images/reggie.png
+# >- starts a multiline string, where newlines replaced by spaces, and final
+# newlines are stripped.
+description: >-
+  Textbook for Practice and Theory of Brain Imaging course.
+  https://nipraxis.org for more information on the course.
 
 # Force re-execution of notebooks on each build.
 # See https://jupyterbook.org/content/execute.html

---FILE: _toc.yml---
@@ -7,6 +7,7 @@ parts:
 - caption: The neuroimaging problem
   chapters:
   - file: pages/the_problem
+
 - caption: Images and code and images
   chapters:
   - file: pages/what_is_an_image
@@ -18,11 +19,25 @@ parts:
   - file: pages/reshape_and_3d
   - file: pages/otsu_threshold
   - file: pages/numpy_random
+
+- caption: ""NumPy arrays, Matplotlib plots""
+  chapters:
+  - file: pages/methods_vs_functions
+  - file: pages/reshape_and_4d
+  - file: pages/allclose
+  - file: pages/arange
+  - file: pages/dot_outer
+  - file: pages/newaxis
+  - file: pages/subtract_means
+  - file: pages/plot_lines
+  - file: pages/subplots
+
 - caption: Into the fourth dimension
   chapters:
   - file: pages/intro_to_4d
   - file: pages/voxels_by_time
   - file: pages/voxel_time_courses
+
 - caption: Coordinate systems and spatial transforms
   chapters:
   - file: pages/diagonal_zooms
@@ -34,7 +49,8 @@ parts:
   - file: pages/rotation_2d_3d
   - file: pages/resampling_with_ndimage
   - file: pages/dipy_registration
-- caption: Statistics and the general linear model in fMRI
+
+- caption: Statistics and the general linear model
   chapters:
   - file: pages/mean_test_example
   - file: pages/hypothesis_tests
@@ -44,6 +60,7 @@ parts:
   - file: pages/multi_multiply
   - file: pages/non_tr_onsets
   - file: pages/whole_image_statistics
+
 - caption: Code organization and collaboration
   chapters:
   # - file: pages/git_walk_through
@@ -52,14 +69,3 @@ parts:
   - file: pages/sys_path
   - file: pages/assert
   - file: pages/coding_style
-- caption: ""NumPy and Matplotlib: reshape, dot products, lines and subplots""
-  chapters:
-  - file: pages/methods_vs_functions
-  - file: pages/reshape_and_4d
-  - file: pages/allclose
-  - file: pages/arange
-  - file: pages/dot_outer
-  - file: pages/newaxis
-  - file: pages/subtract_means
-  - file: pages/plot_lines
-  - file: pages/subplots

---FILE: pages/syllabus.md---
@@ -39,7 +39,7 @@ orphan: true
 - collaborating with peers and mentors;
 - role of working practice in quality, reproducibility, collaboration;
 - choosing and learning simple tools;
-- version control with [git];
+- version control with [git](https://git-scm.com);
 - sharing code with <https://github.com>;
 - scripting and coding with Python;
 - pair coding and code review;"
nipraxis,textbook,36155b64262c7ddf91d951b80a83f102b61c3bfc,Matthew Brett,matthew.brett@gmail.com,2022-02-28T18:44:31Z,Matthew Brett,matthew.brett@gmail.com,2022-02-28T18:44:31Z,Fix year of Murray talk.,pages/the_problem.md,False,False,False,False,1,1,2,"---FILE: pages/the_problem.md---
@@ -68,7 +68,7 @@ Even in good institutions, we have seen that the great difficulties in learning
   impossible to replicate an analysis by someone else.  This confusion is
   a poison to clear thought.  [Professor Robin
   Murray](https://www.kcl.ac.uk/people/professor-sir-robin-murray) gave a [talk
-  in 2003](http://www.mattababy.org/~belmonte/Talks/2001_London/speak.html)
+  in 2001](http://www.mattababy.org/~belmonte/Talks/2001_London/speak.html)
   called ""Should psychiatry take fMRI research seriously?"". His answer to the
   question in his title was more or less ""no"", or at least, ""not yet"".  Among
   many painful, wise and funny remarks, he wondered aloud whether the frontal"
nipraxis,textbook,d1ce0546882c6bbe459171c6f71765b5c1c9c680,Chris Markiewicz,effigies@gmail.com,2022-02-27T02:15:57Z,GitHub,noreply@github.com,2022-02-27T02:15:57Z,Fix minor typos,pages/the_problem.md,False,False,False,False,2,2,4,"---FILE: pages/the_problem.md---
@@ -40,7 +40,7 @@ That makes us ask — what is the typical practice of a neuroimaging researcher?
 
 ## Typical practice
 
-Even in good institutions, we have seen that the great difficulties in learning and doing imaging make it common to fall into this patter.
+Even in good institutions, we have seen that the great difficulties in learning and doing imaging make it common to fall into this pattern.
 
 * The practitioner **learns some of some of the areas** above, but at
   a superficial ""makes-sense"" level.  The ""makes-sense"" level is the level at
@@ -93,7 +93,7 @@ It will be hard to collaborate, because it is hard to explain your process to yo
 
 # Like science, but not science
 
-The problem then, is that the *process* of analysis can make it very difficult to do *science*.  That's a big claim, and one that begs the question, what is science?.
+The problem then, is that the *process* of analysis can make it very difficult to do *science*.  That's a big claim, and one that begs the question, ""What is science?""
 
 [Richard Feynman](https://en.wikipedia.org/wiki/Richard_Feynman) was a Nobel-laureate in theoretical physics, and a famous teacher and writer. He thought a lot about the nature of science.  In a famous talk, called ""What is Science""  {cite}`feynman1969science`, he wrote:
 "
nipraxis,textbook,ac774f8d08a337017c3f0339b2673bfb0cab2997,Matthew Brett,matthew.brett@gmail.com,2022-02-25T22:16:12Z,Matthew Brett,matthew.brett@gmail.com,2022-02-25T22:16:12Z,Fix repository etc links,_config.yml,False,False,False,False,2,2,4,"---FILE: _config.yml---
@@ -30,10 +30,10 @@ html:
   use_edit_page_button: true
   use_repository_button: true
   use_issues_button: true
-  baseurl: https://nipraxis.github.io
+  baseurl: https://textbook.nipraxis.org
 
 repository:
-  url: https://github.com/nipraxis/nipraxis.github.io
+  url: https://github.com/nipraxis/textbook
   branch: main
 
 launch_buttons:"
