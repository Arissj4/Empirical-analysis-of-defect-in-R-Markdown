repo_owner,repo_name,commit_hash,author_name,author_email,author_date,committer_name,committer_email,committer_date,message,filenames,touches_rmd,touches_r,touches_r_or_rmd,is_merge,added,deleted,changed,diff
NBISweden,workshop-mlbiostatistics,18942b9e2b9dc640b30bd73014c1b0f53cdb9df6,Olga Hrydziuszko,olga.dethlefsen@nbis.se,2025-06-12T08:39:20Z,Olga Hrydziuszko,olga.dethlefsen@nbis.se,2025-06-12T08:39:20Z,"Fix typos, add plot",session-survival-presentation/presentation.html;session-survival-presentation/presentation.qmd;session-survival-presentation/presentation_files/figure-revealjs/unnamed-chunk-3-1.png,True,False,True,False,48,5,53,"---FILE: session-survival-presentation/presentation.html---
@@ -253,7 +253,7 @@ <h2>Kaplan-Meier estimator</h2>
 <p>where:</p>
 <ul>
 <li><span class=""math inline"">\(d_j\)</span>: number of failures at time <span class=""math inline"">\(t_j\)</span></li>
-<li><span class=""math inline"">\(n_j\)</span>: number of patients at risk just before time <span class=""math inline"">\(t_ij\)</span></li>
+<li><span class=""math inline"">\(n_j\)</span>: number of patients at risk just before time <span class=""math inline"">\(t_j\)</span></li>
 <li>and the product is taken over all time intervals in which a death occurred, up to and including <span class=""math inline"">\(t\)</span></li>
 </ul>
 <p>Kapalan-Meier estimator is also known as <strong>product-limit</strong> estimator.</p>
@@ -274,7 +274,7 @@ <h2>Kaplan-Meier estimator</h2>
 </div>
 <p>To calculate the Kaplan-Meier Curve:</p>
 <ol type=""1"">
-<li>We <strong>sort</strong> the data by time to failure.2.</li>
+<li>We <strong>sort</strong> the data by time to failure.</li>
 <li>For each time, we calculate the <strong>survival probabilities</strong>, i.e.&nbsp;the probability the filling lasting beyond that time.</li>
 <li>We apply the Kaplan-Meier formula <span class=""math display"">\[\hat{S}(t) = \prod_{j=1}^{t} \left( 1 - \frac{d_j}{n_j} \right)\]</span> to calculate the probability of surviving up to time <span class=""math inline"">\(t\)</span>, as the product of all individual survival probabilities at each time up to <span class=""math inline"">\(t\)</span>:</li>
 </ol>
@@ -522,7 +522,8 @@ <h2>Kaplan-Meier estimator</h2>
 <li>A flatter slope indicates a lower event rate and therefore a better survival prognosis.</li>
 <li>The curve may have plateaus or flat areas, indicating periods of relatively stable survival.</li>
 </ul>
-</section>
+
+<img data-src=""presentation_files/figure-revealjs/unnamed-chunk-3-1.png"" width=""960"" class=""r-stretch""></section>
 <section id=""kaplan-meier-estimator-6"" class=""slide level2 smaller"">
 <h2>Kaplan-Meier estimator</h2>
 <p><em>Interpretations</em></p>

---FILE: session-survival-presentation/presentation.qmd---
@@ -229,7 +229,7 @@ The **Kaplan-Meier (KM) estimator** is a non-parametric statistic used to estima
 where: 
 
 - $d_j$: number of failures at time $t_j$
-- $n_j$: number of patients at risk just before time $t_ij$
+- $n_j$: number of patients at risk just before time $t_j$
 - and the product is taken over all time intervals in which a death occurred, up to and including $t$
 
 Kapalan-Meier estimator is also known as **product-limit** estimator.
@@ -258,7 +258,7 @@ str(data_tooth)
 
 To calculate the Kaplan-Meier Curve: 
 
-1. We **sort** the data by time to failure.2. 
+1. We **sort** the data by time to failure.
 2. For each time, we calculate the **survival probabilities**, i.e. the probability the filling lasting beyond that time.
 3. We apply the Kaplan-Meier formula $$\hat{S}(t) = \prod_{j=1}^{t} \left( 1 - \frac{d_j}{n_j} \right)$$ to calculate the probability of surviving up to time $t$, as the product of all individual survival probabilities at each time up to $t$: 
 
@@ -397,6 +397,48 @@ The Kaplan-Meier curve shows the cumulative survival probabilities.
 - The curve may have plateaus or flat areas, indicating periods of relatively stable survival.
 
 
+```{r}
+# Load required libraries
+library(survival)
+library(survminer)
+
+# Simulate survival data for two groups
+set.seed(123)
+
+# Group 1: Worse prognosis (shorter survival times)
+group1 <- data.frame(
+  time = rexp(50, rate = 0.2),   # higher event rate → shorter survival
+  status = rep(1, 50),
+  group = ""High Event Rate""
+)
+
+# Group 2: Better prognosis (longer survival times)
+group2 <- data.frame(
+  time = rexp(50, rate = 0.05),  # lower event rate → longer survival
+  status = rep(1, 50),
+  group = ""Low Event Rate""
+)
+
+# Combine the data
+data_km <- rbind(group1, group2)
+
+# Fit KM survival curves
+fit <- survfit(Surv(time, status) ~ group, data = data_km)
+
+# Plot the curves
+ggsurvplot(fit,
+           data = data_km,
+           xlab = ""Time"",
+           ylab = ""Survival Probability"",
+           title = ""Kaplan-Meier Curves for Two Groups"",
+           risk.table = FALSE,
+           palette = c(""red"", ""blue""),
+           legend.title = ""Group"",
+           legend.labs = c(""High Event Rate"", ""Low Event Rate""))
+
+```
+
+
 ## Kaplan-Meier estimator {.smaller}
 
 *Interpretations*"
NBISweden,workshop-mlbiostatistics,c33f946b77cb6e56e16276a89ae69a67fd1e8235,evaf,eva@freyhult.net,2025-06-11T12:30:08Z,evaf,eva@freyhult.net,2025-06-11T12:30:08Z,Fix typo.,session-mixedmodels/exercises/exMM2.html;session-mixedmodels/exercises/exMM2.qmd,True,False,True,False,4,4,8,"---FILE: session-mixedmodels/exercises/exMM2.html---
@@ -3482,8 +3482,8 @@ <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""lab-technician-and-sample
 <div class=""callout-body-container callout-body"">
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb20""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb20-1""><a href=""#cb20-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##These two models are equivalent, try it!</span></span>
-<span id=""cb20-2""><a href=""#cb20-2"" aria-hidden=""true"" tabindex=""-1""></a>mfat <span class=""ot"">&lt;-</span> <span class=""fu"">lmer</span>(Fat <span class=""sc"">~</span> Sample <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab<span class=""sc"">/</span>Technician<span class=""sc"">/</span>Sample), <span class=""at"">data =</span> eggs)</span>
-<span id=""cb20-3""><a href=""#cb20-3"" aria-hidden=""true"" tabindex=""-1""></a>mfat <span class=""ot"">&lt;-</span> <span class=""fu"">lmer</span>(Fat <span class=""sc"">~</span> Sample <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab) <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab<span class=""sc"">:</span>Technician) <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab<span class=""sc"">:</span>Technician<span class=""sc"">:</span>Sample), <span class=""at"">data =</span> eggs)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+<span id=""cb20-2""><a href=""#cb20-2"" aria-hidden=""true"" tabindex=""-1""></a>mfat <span class=""ot"">&lt;-</span> <span class=""fu"">lmer</span>(Fat <span class=""sc"">~</span> <span class=""dv"">1</span> <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab<span class=""sc"">/</span>Technician<span class=""sc"">/</span>Sample), <span class=""at"">data =</span> eggs)</span>
+<span id=""cb20-3""><a href=""#cb20-3"" aria-hidden=""true"" tabindex=""-1""></a>mfat <span class=""ot"">&lt;-</span> <span class=""fu"">lmer</span>(Fat <span class=""sc"">~</span> <span class=""dv"">1</span> <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab) <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab<span class=""sc"">:</span>Technician) <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab<span class=""sc"">:</span>Technician<span class=""sc"">:</span>Sample), <span class=""at"">data =</span> eggs)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 </div>
 </div>

---FILE: session-mixedmodels/exercises/exMM2.qmd---
@@ -176,8 +176,8 @@ There are multiple ways to specify the random effects structure, use the method
 
 ```{r}
 ##These two models are equivalent, try it!
-mfat <- lmer(Fat ~ Sample + (1|Lab/Technician/Sample), data = eggs)
-mfat <- lmer(Fat ~ Sample + (1|Lab) + (1|Lab:Technician) + (1|Lab:Technician:Sample), data = eggs)
+mfat <- lmer(Fat ~ 1 + (1|Lab/Technician/Sample), data = eggs)
+mfat <- lmer(Fat ~ 1 + (1|Lab) + (1|Lab:Technician) + (1|Lab:Technician:Sample), data = eggs)
 ```
 :::
 "
NBISweden,workshop-mlbiostatistics,950415a92d5bfe411f28d96a286cd7cd6919f074,evaf,eva@freyhult.net,2025-06-11T11:40:36Z,evaf,eva@freyhult.net,2025-06-11T11:40:36Z,Fix typo.,session-mixedmodels/lectures/MM2.html;session-mixedmodels/lectures/MM2.qmd,True,False,True,False,18,15,33,"---FILE: session-mixedmodels/lectures/MM2.html---
@@ -3920,7 +3920,7 @@ <h2>Fixed Effects Comparison</h2>
 </div>
 </div>
 </section>
-<section id=""p-value-calculations"" class=""slide level2"">
+<section id=""p-value-calculations"" class=""slide level2 smaller"">
 <h2>P-value calculations</h2>
 <p>As you have noticed <code>lme4</code> does not report p-values for the fixed effects.</p>
 <p>In linear regression, p-values are calculated based on the t-distribution of the estimated coefficients. The calculation of denomicator degrees of freedom is straightforward since all observations are independent.</p>
@@ -3964,36 +3964,39 @@ <h2><code>lmerTest</code></h2>
 age -0.422</code></pre>
 </div>
 </div>
+</section>
+<section id=""lmertest-1"" class=""slide level2"">
+<h2><code>lmerTest</code></h2>
 <div class=""cell"">
 <div class=""cell-output cell-output-stdout"">
 <pre><code>Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [
 lmerModLmerTest]
 Formula: Reaction ~ Days + (1 + Days | Subject)
-   Data: sleepstudy
+   Data: sleepall
 
-REML criterion at convergence: 1743.6
+REML criterion at convergence: 1404.1
 
 Scaled residuals: 
     Min      1Q  Median      3Q     Max 
--3.9536 -0.4634  0.0231  0.4634  5.1793 
+-4.0157 -0.3541  0.0069  0.4681  5.0732 
 
 Random effects:
  Groups   Name        Variance Std.Dev. Corr
- Subject  (Intercept) 612.10   24.741       
-          Days         35.07    5.922   0.07
- Residual             654.94   25.592       
-Number of obs: 180, groups:  Subject, 18
+ Subject  (Intercept) 958.35   30.957       
+          Days         45.78    6.766   0.18
+ Residual             651.60   25.526       
+Number of obs: 144, groups:  Subject, 18
 
 Fixed effects:
             Estimate Std. Error      df t value Pr(&gt;|t|)    
-(Intercept)  251.405      6.825  17.000  36.838  &lt; 2e-16 ***
-Days          10.467      1.546  17.000   6.771 3.26e-06 ***
+(Intercept)  267.967      8.266  17.000  32.418  &lt; 2e-16 ***
+Days          11.435      1.845  16.999   6.197 9.75e-06 ***
 ---
 Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
 
 Correlation of Fixed Effects:
      (Intr)
-Days -0.138</code></pre>
+Days -0.062</code></pre>
 </div>
 </div>
 

---FILE: session-mixedmodels/lectures/MM2.qmd---
@@ -378,7 +378,7 @@ m3 <- lmer(Reaction ~ Days + sex + (1 + Days | Subject), data = sleepall)
 anova(m2, m3, refit=TRUE)
 ```
 
-## P-value calculations
+## P-value calculations {.smaller}
 
 As you have noticed `lme4` does not report p-values for the fixed effects.
 
@@ -398,14 +398,14 @@ mm <- lmer(conc ~ age + (1 | group), data = df)
 summary(mm)
 ```
 
+## `lmerTest`
+
 ```{r lmerTest2, echo=FALSE}
 sleepall <- sleepstudy |> filter(Days>=2) |> mutate(Days=Days-2)
 ##Add sex
 set.seed(123)
 sleepall <- sleepall |> mutate(sex=sample(c(""M"", ""F""), nrow(sleepall), replace=TRUE))
-library(lmerTest)
-lmsleep <- lm(Reaction ~ Days, data = sleepstudy)
-mmsleep <- lmer(Reaction ~ Days + (1 + Days | Subject), data = sleepstudy)
+mmsleep <- lmer(Reaction ~ Days + (1 + Days | Subject), data = sleepall)
 summary(mmsleep)
 ```
 "
NBISweden,workshop-mlbiostatistics,a1a3ef6c2ee3470875c6c5d6b431b77ebf61d0f7,evaf,eva@freyhult.net,2025-06-11T09:00:42Z,evaf,eva@freyhult.net,2025-06-11T09:00:42Z,Fix typo.,session-mixedmodels/exercises/exMM1.html;session-mixedmodels/exercises/exMM1.qmd,True,False,True,False,6,6,12,"---FILE: session-mixedmodels/exercises/exMM1.qmd---
@@ -157,7 +157,7 @@ What would the average blood pressure be at hospital 3 for a patient treated wit
 ## Solution
 
 ```{r}
-fixef(model1)[] + ranef(model1)$hospital[3,1]
+fixef(model1)[1] + ranef(model1)$hospital[3,1]
 ```
 :::
 "
NBISweden,workshop-mlbiostatistics,3583b2b0e55e1029687c8e4b669e256774682f10,evaf,eva@freyhult.net,2025-06-11T05:23:02Z,evaf,eva@freyhult.net,2025-06-11T05:23:02Z,Fix typo.,session-mixedmodels/exercises/exMM1.html;session-mixedmodels/exercises/exMM1.qmd;session-mixedmodels/exercises/exMM2.html;session-mixedmodels/exercises/exMM2.qmd,True,False,True,False,68,39,107,"---FILE: session-mixedmodels/exercises/exMM1.qmd---
@@ -143,7 +143,7 @@ The random intercepts for each hospital can be computed using the `ranef` functi
 
 Compute the random effects using `ranef`. Note that `ranef` gives the deviations from the overall mean for each hospital.
 
-:::{.callout-tip collapse=""true""}}
+:::{.callout-tip collapse=""true""}
 ## Code
 
 ```{r bprandom}
@@ -306,6 +306,7 @@ Visualize the data.
 
 :::{.callout-tip collapse=""true""}
 ## Code 
+
 ```{r fig-orthodont, results=""hide""}
 Orthodont |> 
   ggplot(aes(x = age, y = distance, group = Subject, color = Sex)) +

---FILE: session-mixedmodels/exercises/exMM2.html---
@@ -3451,20 +3451,35 @@ <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""lab-technician-and-sample
 </div>
 </div>
 <p>What is the grouping structure and how is it nested. Using this information fit a linear mixed model.</p>
-<p>:::{.callout-tip collapse=“true”}} Sample is nested within Technician, which is nested within Lab. :::</p>
-<p>There are multiple ways to specify the random effects structure, use the method that is least confusing to you!</p>
 <div class=""callout callout-style-default callout-tip callout-titled"">
 <div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-6-contents"" aria-controls=""callout-6"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
 <i class=""callout-icon""></i>
 </div>
 <div class=""callout-title-container flex-fill"">
-Code
+Tip
 </div>
 <div class=""callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end""><i class=""callout-toggle""></i></div>
 </div>
 <div id=""callout-6"" class=""callout-6-contents callout-collapse collapse"">
 <div class=""callout-body-container callout-body"">
+<p>Sample is nested within Technician, which is nested within Lab.</p>
+</div>
+</div>
+</div>
+<p>There are multiple ways to specify the random effects structure, use the method that is least confusing to you!</p>
+<div class=""callout callout-style-default callout-tip callout-titled"">
+<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-7-contents"" aria-controls=""callout-7"" aria-expanded=""false"" aria-label=""Toggle callout"">
+<div class=""callout-icon-container"">
+<i class=""callout-icon""></i>
+</div>
+<div class=""callout-title-container flex-fill"">
+Code
+</div>
+<div class=""callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end""><i class=""callout-toggle""></i></div>
+</div>
+<div id=""callout-7"" class=""callout-7-contents callout-collapse collapse"">
+<div class=""callout-body-container callout-body"">
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb20""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb20-1""><a href=""#cb20-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##These two models are equivalent, try it!</span></span>
 <span id=""cb20-2""><a href=""#cb20-2"" aria-hidden=""true"" tabindex=""-1""></a>mfat <span class=""ot"">&lt;-</span> <span class=""fu"">lmer</span>(Fat <span class=""sc"">~</span> Sample <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Lab<span class=""sc"">/</span>Technician<span class=""sc"">/</span>Sample), <span class=""at"">data =</span> eggs)</span>
@@ -3529,7 +3544,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 </div>
 <p>There are both boys and girls in the dataset. Subjects are nested within sex. Take this nested grouping structure into account and fit a linear mixed model with random intercept.</p>
 <div class=""callout callout-style-default callout-tip callout-titled"">
-<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-7-contents"" aria-controls=""callout-7"" aria-expanded=""false"" aria-label=""Toggle callout"">
+<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-8-contents"" aria-controls=""callout-8"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
 <i class=""callout-icon""></i>
 </div>
@@ -3538,7 +3553,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 </div>
 <div class=""callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end""><i class=""callout-toggle""></i></div>
 </div>
-<div id=""callout-7"" class=""callout-7-contents callout-collapse collapse"">
+<div id=""callout-8"" class=""callout-8-contents callout-collapse collapse"">
 <div class=""callout-body-container callout-body"">
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb25""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb25-1""><a href=""#cb25-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##mo2 &lt;- lmer(distance ~ age + (1|Sex/Subject), data = Orthodont)</span></span>
@@ -3577,7 +3592,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 </div>
 <p>Add also random slope for age, first only per sex, then both in sex and subject (nested within sex).</p>
 <div class=""callout callout-style-default callout-tip callout-titled"">
-<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-8-contents"" aria-controls=""callout-8"" aria-expanded=""false"" aria-label=""Toggle callout"">
+<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-9-contents"" aria-controls=""callout-9"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
 <i class=""callout-icon""></i>
 </div>
@@ -3586,7 +3601,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 </div>
 <div class=""callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end""><i class=""callout-toggle""></i></div>
 </div>
-<div id=""callout-8"" class=""callout-8-contents callout-collapse collapse"">
+<div id=""callout-9"" class=""callout-9-contents callout-collapse collapse"">
 <div class=""callout-body-container callout-body"">
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb27""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb27-1""><a href=""#cb27-1"" aria-hidden=""true"" tabindex=""-1""></a>mo3 <span class=""ot"">&lt;-</span> <span class=""fu"">lmer</span>(distance <span class=""sc"">~</span> age <span class=""sc"">+</span> (<span class=""dv"">1</span> <span class=""sc"">+</span> age<span class=""sc"">|</span>Sex) <span class=""sc"">+</span> (<span class=""dv"">1</span><span class=""sc"">|</span>Sex<span class=""sc"">:</span>Subject), <span class=""at"">data =</span> Orthodont)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
@@ -3603,7 +3618,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 </div>
 <p>Use likelihood ratio test to compare the four models. Use the significance threshold of 0.05. What does the results tell you, which model would you choose?</p>
 <div class=""callout callout-style-default callout-tip callout-titled"">
-<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-9-contents"" aria-controls=""callout-9"" aria-expanded=""false"" aria-label=""Toggle callout"">
+<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-10-contents"" aria-controls=""callout-10"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
 <i class=""callout-icon""></i>
 </div>
@@ -3612,7 +3627,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 </div>
 <div class=""callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end""><i class=""callout-toggle""></i></div>
 </div>
-<div id=""callout-9"" class=""callout-9-contents callout-collapse collapse"">
+<div id=""callout-10"" class=""callout-10-contents callout-collapse collapse"">
 <div class=""callout-body-container callout-body"">
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb31""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb31-1""><a href=""#cb31-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## LRT</span></span>
@@ -3645,7 +3660,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 <p>Compare the models using likelihood ratio test. Use the <code>anova</code>function and remember to set <code>refit=TRUE</code> to use ML and not REML!</p>
 <p>What do the results tell you? Which model would you choose?</p>
 <div class=""callout callout-style-default callout-tip callout-titled"">
-<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-10-contents"" aria-controls=""callout-10"" aria-expanded=""false"" aria-label=""Toggle callout"">
+<div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-11-contents"" aria-controls=""callout-11"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
 <i class=""callout-icon""></i>
 </div>
@@ -3654,7 +3669,7 @@ <h2 data-number=""3.1"" class=""anchored"" data-anchor-id=""orthodont""><span class=""h
 </div>
 <div class=""callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end""><i class=""callout-toggle""></i></div>
 </div>
-<div id=""callout-10"" class=""callout-10-contents callout-collapse collapse"">
+<div id=""callout-11"" class=""callout-11-contents callout-collapse collapse"">
 <div class=""callout-body-container callout-body"">
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb34""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb34-1""><a href=""#cb34-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">anova</span>(mo1, mof2, mof3, <span class=""at"">refit =</span> <span class=""cn"">TRUE</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>

---FILE: session-mixedmodels/exercises/exMM2.qmd---
@@ -165,7 +165,7 @@ eggs |> ggplot(aes(x= Lab, y = Fat, color = Technician, shape=Sample)) +
 
 What is the grouping structure and how is it nested. Using this information fit a linear mixed model.
 
-:::{.callout-tip collapse=""true""}}
+:::{.callout-tip collapse=""true""}
 Sample is nested within Technician, which is nested within Lab.
 :::
 "
NBISweden,workshop-mlbiostatistics,a00f5b3312eda4956cfdb24f977a311fff8b19c7,evaf,eva@freyhult.net,2025-04-11T08:29:46Z,evaf,eva@freyhult.net,2025-04-11T08:29:46Z,Update wrap-up links and fix certificate.,admin/2025/certificates/diploma-adj_Andrea.pdf;admin/2025/certificates/diploma-adj_Andrea.tex;session-wrap-up/wrap-up.html;session-wrap-up/wrap-up.qmd,True,False,True,False,148,2,150,"---FILE: admin/2025/certificates/diploma-adj_Andrea.tex---
@@ -0,0 +1,141 @@
+\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
+\makeatletter
+\def\maxwidth{ %
+  \ifdim\Gin@nat@width>\linewidth
+    \linewidth
+  \else
+    \Gin@nat@width
+  \fi
+}
+\makeatother
+
+\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
+\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
+\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
+\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
+\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
+\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
+\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
+\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
+\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
+\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
+\let\hlipl\hlkwb
+
+\usepackage{framed}
+\makeatletter
+\newenvironment{kframe}{%
+ \def\at@end@of@kframe{}%
+ \ifinner\ifhmode%
+  \def\at@end@of@kframe{\end{minipage}}%
+  \begin{minipage}{\columnwidth}%
+ \fi\fi%
+ \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
+ \colorbox{shadecolor}{##1}\hskip-\fboxsep
+     % There is no \\@totalrightmargin, so:
+     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
+ \MakeFramed {\advance\hsize-\width
+   \@totalleftmargin\z@ \linewidth\hsize
+   \@setminipage}}%
+ {\par\unskip\endMakeFramed%
+ \at@end@of@kframe}
+\makeatother
+
+\definecolor{shadecolor}{rgb}{.97, .97, .97}
+\definecolor{messagecolor}{rgb}{0, 0, 0}
+\definecolor{warningcolor}{rgb}{1, 0, 1}
+\definecolor{errorcolor}{rgb}{1, 0, 0}
+\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX
+
+\usepackage{alltt}
+
+\usepackage{geometry}               
+\geometry{a4paper,
+ total={170mm,257mm},
+ left=20mm,
+ top=20mm,
+ bottom=40mm}                   
+
+\setlength{\parindent}{0cm}
+
+%% LOGOS
+\usepackage{fancyhdr}
+%\setlength{\headheight}{1.5cm}
+\addtolength{\headheight}{2cm} % make more space for the header
+\pagestyle{fancyplain} % use fancy for all pages except chapter start
+\lhead{\includegraphics[height=1.3cm, width=2cm]{logos/nbis.png}} % left logo
+\rhead{\includegraphics[height=1.3cm, width=5cm]{logos/scilifelab.pdf}} % right logo
+\cfoot{}
+\renewcommand{\headrulewidth}{0pt} % remove rule below header
+
+%% DEFINE TOOLS AND VARIABLES
+\newcommand{\courseName}{Introduction to biostatistics and machine learning}
+%\newcommand{\coursePoints}{1 hp}
+\newcommand{\courseLocation}{in Uppsala}
+\newcommand{\courseDate}{7th-11th April 2025}
+\newcommand{\courseLastDay}{2025-04-11}
+
+\newcommand{\courseExaminer}{Eva Freyhult, PhD}
+\newcommand{\affilations}{National Bioinformatics Infrastructure Sweden, Science for Life Laboratory, Uppsala University}
+
+
+
+\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
+\begin{document}
+
+
+\large
+This is to certify that 
+
+
+\LARGE
+Andrea Soler i Núñez
+
+\large
+has successfully completed the course \newline
+
+\LARGE
+\begin{center}{\courseName}  \end{center} 
+
+
+\large
+\begin{center} Held  {\courseLocation}, {\courseDate} \end{center} 
+
+\vspace{5mm}
+\normalsize
+The course consisted of lectures, exercises and group discussions. The following subjects were covered and graded  in the lectures and exercises:
+\begin{itemize}
+  \item Descriptive statistics (Absence)
+  \item Probability theory (Absence)
+  \item Statistical inference (Pass)
+  \item Linear regression methods (Pass)
+  \item Unsupervised learning: PCA and clustering (Pass)
+  \item Introduction to supervised learning (Pass)
+  \item Model selection and regularization (Pass)
+  \item Random Forest (Pass)
+\end{itemize}
+
+
+\vspace{5mm}
+Examination: \newline
+Active participation during lectures, group sessions and practical exercises. Grading system: Pass/Fail, Absence. 
+
+\vspace{8mm}
+The course was arranged by the National Bioinformatics Infrastructure Sweden (NBIS) at Science for Life Laboratory (SciLifeLab).
+
+%The course is accepted as {\coursePoints} university credits.
+
+\vspace{8mm}
+ {\courseLastDay}
+\vspace{22mm}
+
+
+{\courseExaminer} \newline
+\small
+National Bioinformatics Infrastructure Sweden \newline
+Science for Life Laboratory \newline
+Uppsala University \newline
+
+
+
+
+\end{document}

---FILE: session-wrap-up/wrap-up.html---
@@ -451,8 +451,9 @@ <h2>AI and IO Seminar series</h2>
 <h2>Training events</h2>
 <ul>
 <li><a href=""https://nbis.se/training"">https://nbis.se/training</a></li>
-<li><a href=""https://www.scilifelab.se/training/events"">https://www.scilifelab.se/training/events</a></li>
+<li><a href=""https://training.scilifelab.se/events"">https://training.scilifelab.se/events</a></li>
 </ul>
+<p><a href=""https://uppsala.instructure.com/courses/110506"">Machine Learning for Life Sciences</a></p>
 </section>
 <section id=""feedback"" class=""slide level2"">
 <h2>Feedback</h2>

---FILE: session-wrap-up/wrap-up.qmd---
@@ -149,7 +149,11 @@ Your supervisor knows you are back from the ""Introduction to Biostatistics & ML""
 ## Training events
 
 - [https://nbis.se/training](https://nbis.se/training)
-- [https://www.scilifelab.se/training/events](https://www.scilifelab.se/training/events)
+- [https://training.scilifelab.se/events](https://training.scilifelab.se/events)
+
+ [Machine Learning for Life Sciences](https://uppsala.instructure.com/courses/110506)
+ 
+ 
 
 ## Feedback
 "
NBISweden,workshop-mlbiostatistics,0cedf69a94587b87ae83f51c355378cf8855ea1f,evaf,eva@freyhult.net,2025-04-07T09:26:02Z,evaf,eva@freyhult.net,2025-04-07T09:26:02Z,Fix Poisson,session-probability/Rfigures/prob_CFUc-1.png;session-probability/Rfigures/prob_dicec-1.png;session-probability/Rfigures/prob_fig-leftskewed-1.png;session-probability/Rfigures/prob_fig-meanskew-1.png;session-probability/Rfigures/prob_fig-pdfnewborn-1.png;session-probability/Rfigures/prob_fig-wtbabiesdens3-1.png;session-probability/Rfigures/prob_histNheads-1.png;session-probability/Rfigures/prob_solcards-1.png;session-probability/Rfigures/prob_solcointossc-1.png;session-probability/Rfigures/prob_solrandome-1.png;session-probability/docs/Rfigures/prob_CFUc-1.png;session-probability/docs/Rfigures/prob_fig-leftskewed-1.png;session-probability/docs/Rfigures/prob_fig-meanskew-1.png;session-probability/docs/Rfigures/prob_fig-pdfnewborn-1.png;session-probability/docs/Rfigures/prob_fig-wtbabiesdens3-1.png;session-probability/docs/Rfigures/prob_histNheads-1.png;session-probability/docs/Rfigures/prob_solrandome-1.png;session-probability/docs/index.html;session-probability/docs/prob_01intro.html;session-probability/docs/prob_02discrv.html;session-probability/docs/prob_03contrv.html;session-probability/docs/prob_04sample.html;session-probability/docs/prob_exr1_discrv_solutions.html;session-probability/docs/prob_exr2_contrv_solutions.html;session-probability/docs/references.html;session-probability/docs/search.json;session-probability/docs/site_libs/bootstrap/bootstrap-icons.css;session-probability/docs/site_libs/bootstrap/bootstrap-icons.woff;session-probability/docs/site_libs/bootstrap/bootstrap.min.css;session-probability/docs/site_libs/bootstrap/bootstrap.min.js;session-probability/docs/site_libs/quarto-html/anchor.min.js;session-probability/docs/site_libs/quarto-html/popper.min.js;session-probability/docs/site_libs/quarto-html/quarto-syntax-highlighting.css;session-probability/docs/site_libs/quarto-html/quarto.js;session-probability/docs/site_libs/quarto-nav/quarto-nav.js;session-probability/docs/site_libs/quarto-search/autocomplete.umd.js;session-probability/docs/site_libs/quarto-search/quarto-search.js;session-probability/lectures/probabilityI.qmd;session-probability/prob_02discrv.qmd,True,False,True,False,2978,596,3574,"---FILE: session-probability/docs/index.html---
@@ -2,7 +2,7 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
@@ -47,7 +47,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -56,6 +62,7 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
@@ -71,13 +78,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./index.html"">Preface</a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./index.html"">Preface</a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -86,7 +93,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -150,7 +157,7 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
@@ -188,8 +195,10 @@ <h1 class=""title"">Probability Theory</h1>
   </div>
   
 
+
 </header>
 
+
 <section id=""preface"" class=""level1 unnumbered"">
 <h1 class=""unnumbered"">Preface</h1>
 <p>This repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course <a href=""https://uppsala.instructure.com/courses/93277"" class=""uri"">https://uppsala.instructure.com/courses/93277</a>.</p>
@@ -248,18 +257,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -291,11 +289,50 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -305,8 +342,17 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -318,7 +364,130 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -362,6 +531,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -387,6 +557,32 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -450,7 +646,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
   <div class=""nav-page nav-page-previous"">
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""./prob_01intro.html"" class=""pagination-link"">
+      <a href=""./prob_01intro.html"" class=""pagination-link"" aria-label=""Introduction to probability"">
         <span class=""nav-page-text""><span class=""chapter-number"">1</span>&nbsp; <span class=""chapter-title"">Introduction to probability</span></span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
@@ -459,4 +655,5 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""learning-outcomes"">Learning outc
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/prob_01intro.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Probability Theory - 1&nbsp; Introduction to probability</title>
+<title>1&nbsp; Introduction to probability – Probability Theory</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -47,7 +47,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -56,15 +62,43 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
   }
 }</script>
 
-  <script src=""https://polyfill.io/v3/polyfill.min.js?features=es6""></script>
+  <script src=""https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6""></script>
   <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
 
+<script type=""text/javascript"">
+const typesetMath = (el) => {
+  if (window.MathJax) {
+    // MathJax Typeset
+    window.MathJax.typeset([el]);
+  } else if (window.katex) {
+    // KaTeX Render
+    var mathElements = el.getElementsByClassName(""math"");
+    var macros = [];
+    for (var i = 0; i < mathElements.length; i++) {
+      var texText = mathElements[i].firstChild;
+      if (mathElements[i].tagName == ""SPAN"") {
+        window.katex.render(texText.data, mathElements[i], {
+          displayMode: mathElements[i].classList.contains('display'),
+          throwOnError: false,
+          macros: macros,
+          fleqn: false
+        });
+      }
+    }
+  }
+}
+window.Quarto = {
+  typesetMath
+};
+</script>
+
 </head>
 
 <body class=""nav-sidebar floating"">
@@ -73,13 +107,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_01intro.html""><span class=""chapter-number"">1</span>&nbsp; <span class=""chapter-title"">Introduction to probability</span></a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_01intro.html""><span class=""chapter-number"">1</span>&nbsp; <span class=""chapter-title"">Introduction to probability</span></a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -88,7 +122,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -152,7 +186,7 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
@@ -185,8 +219,10 @@ <h1 class=""title""><span id=""prob-01intro"" class=""quarto-section-identifier""><spa
   </div>
   
 
+
 </header>
 
+
 <p>Some things are more likely to occur than others. Compare:</p>
 <ul>
 <li>the chance of the sun rising tomorrow with the chance that no-one is infected with COVID-19 tomorrow</li>
@@ -227,35 +263,51 @@ <h2 data-number=""1.3"" class=""anchored"" data-anchor-id=""conditional-probability"">
 <section id=""the-urn-model"" class=""level2"" data-number=""1.4"">
 <h2 data-number=""1.4"" class=""anchored"" data-anchor-id=""the-urn-model""><span class=""header-section-number"">1.4</span> The urn model</h2>
 <p>The urn model is a simple mathematical model commonly used in statistics and probability. In the urn model, objects (such as people, mice, cells, genes, molecules, etc) are represented by balls of different colors or labels. A fair coin can be represented by an urn with two balls representing the coin’s two sides; H and T. A group of people can be modeled in an urn model, if age is the variable of interest, we write the age of each person on the balls. If we instead are interested in if the people are allergic to pollen or not, we color the balls according to allergy status.</p>
-<div id=""fig-urns"" class=""cell quarto-layout-panel"">
-<figure class=""figure"">
-<div class=""quarto-layout-row quarto-layout-valign-top"">
-<div class=""cell-output-display quarto-layout-cell quarto-layout-cell-subref"" style=""flex-basis: 33.3%;justify-content: center;"">
-<div id=""fig-urns-1"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""figures/coinurn.png"" class=""img-fluid figure-img"" data-ref-parent=""fig-urns""></p>
-<figcaption class=""figure-caption"">(a) Fair coin</figcaption>
+<div id=""fig-urns"" class=""quarto-layout-panel"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-urns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<div class=""quarto-layout-row"">
+<div class=""cell-output-display quarto-layout-cell-subref quarto-layout-cell"" data-ref-parent=""fig-urns"" style=""flex-basis: 33.3%;justify-content: center;"">
+<div id=""fig-urns-1"" class=""quarto-float quarto-figure quarto-figure-center anchored"" data-fig-align=""center"">
+<figure class=""quarto-float quarto-subfloat-fig figure"">
+<div aria-describedby=""fig-urns-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""figures/coinurn.png"" class=""img-fluid quarto-figure quarto-figure-center figure-img"" style=""width:20.0%"" data-ref-parent=""fig-urns"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig"" id=""fig-urns-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+(a) Fair coin
+</figcaption>
 </figure>
 </div>
 </div>
-<div class=""cell-output-display quarto-layout-cell quarto-layout-cell-subref"" style=""flex-basis: 33.3%;justify-content: center;"">
-<div id=""fig-urns-2"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""figures/ageurn.png"" class=""img-fluid figure-img"" data-ref-parent=""fig-urns""></p>
-<figcaption class=""figure-caption"">(b) Age</figcaption>
+<div class=""cell-output-display quarto-layout-cell-subref quarto-layout-cell"" data-ref-parent=""fig-urns"" style=""flex-basis: 33.3%;justify-content: center;"">
+<div id=""fig-urns-2"" class=""quarto-float quarto-figure quarto-figure-center anchored"" data-fig-align=""center"">
+<figure class=""quarto-float quarto-subfloat-fig figure"">
+<div aria-describedby=""fig-urns-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""figures/ageurn.png"" class=""img-fluid quarto-figure quarto-figure-center figure-img"" style=""width:20.0%"" data-ref-parent=""fig-urns"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig"" id=""fig-urns-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+(b) Age
+</figcaption>
 </figure>
 </div>
 </div>
-<div class=""cell-output-display quarto-layout-cell quarto-layout-cell-subref"" style=""flex-basis: 33.3%;justify-content: center;"">
-<div id=""fig-urns-3"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""figures/pollenageurn.png"" class=""img-fluid figure-img"" data-ref-parent=""fig-urns""></p>
-<figcaption class=""figure-caption"">(c) Pollen allergy status</figcaption>
+<div class=""cell-output-display quarto-layout-cell-subref quarto-layout-cell"" data-ref-parent=""fig-urns"" style=""flex-basis: 33.3%;justify-content: center;"">
+<div id=""fig-urns-3"" class=""quarto-float quarto-figure quarto-figure-center anchored"" data-fig-align=""center"">
+<figure class=""quarto-float quarto-subfloat-fig figure"">
+<div aria-describedby=""fig-urns-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""figures/pollenageurn.png"" class=""img-fluid quarto-figure quarto-figure-center figure-img"" style=""width:20.0%"" data-ref-parent=""fig-urns"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig"" id=""fig-urns-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+(c) Pollen allergy status
+</figcaption>
 </figure>
 </div>
 </div>
 </div>
-<p></p><figcaption class=""figure-caption"">Figure&nbsp;1.1: Urn models of a fair coin, age of a group of people, and pollen allergy status of a group of people.</figcaption><p></p>
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-urns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;1.1: Urn models of a fair coin, age of a group of people, and pollen allergy status of a group of people.
+</figcaption>
 </figure>
 </div>
 <p>In the urn model every unit (ball) is equally likely of being selected. This means that the urn model is well suited to represent flipping a fair coin. However, a biased coin can also be modelled using an urn model, by changing the number of balls that represent each side of the coin.</p>
@@ -270,7 +322,7 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
 <p>The <strong>population</strong> is the collection of all possible observations.</p>
 <p>A <strong>sample</strong> is a subset of the population.</p>
 <div id=""exm-dice"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 1.1 </strong></span>Paint a fair six sided dice black on one side and white on the other five sides. The outcome of throwing this dice is a random variable, <span class=""math inline"">\(X\)</span>. The possible outcomes, the sample space, is black and white. The population is all possible observations, i.e.&nbsp;the six sides.</p>
+<p><span class=""theorem-title""><strong>Example 1.1</strong></span> Paint a fair six sided dice black on one side and white on the other five sides. The outcome of throwing this dice is a random variable, <span class=""math inline"">\(X\)</span>. The possible outcomes, the sample space, is black and white. The population is all possible observations, i.e.&nbsp;the six sides.</p>
 </div>
 <p>Example random variables and probabilites:</p>
 <ul>
@@ -324,18 +376,7 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -367,11 +408,50 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -381,8 +461,17 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -394,7 +483,130 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -438,6 +650,7 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -463,6 +676,32 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -524,12 +763,12 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
 </script>
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
-      <a href=""./index.html"" class=""pagination-link"">
+      <a href=""./index.html"" class=""pagination-link"" aria-label=""Preface"">
         <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text"">Preface</span>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""./prob_02discrv.html"" class=""pagination-link"">
+      <a href=""./prob_02discrv.html"" class=""pagination-link"" aria-label=""Discrete random variables"">
         <span class=""nav-page-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Discrete random variables</span></span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
@@ -538,4 +777,5 @@ <h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""random-variables""><span c
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/prob_02discrv.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Probability Theory - 2&nbsp; Discrete random variables</title>
+<title>2&nbsp; Discrete random variables – Probability Theory</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -22,7 +22,7 @@
 }
 /* CSS for syntax highlighting */
 pre > code.sourceCode { white-space: pre; position: relative; }
-pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
+pre > code.sourceCode > span { line-height: 1.25; }
 pre > code.sourceCode > span:empty { height: 1.2em; }
 .sourceCode { overflow: visible; }
 code.sourceCode > span { color: inherit; text-decoration: inherit; }
@@ -33,7 +33,7 @@
 }
 @media print {
 pre > code.sourceCode { white-space: pre-wrap; }
-pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
+pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
 }
 pre.numberSource code
   { counter-reset: source-line 0; }
@@ -81,7 +81,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -90,18 +96,45 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
   }
 }</script>
-
 <script src=""site_libs/kePrint-0.0.1/kePrint.js""></script>
 <link href=""site_libs/lightable-0.0.1/lightable.css"" rel=""stylesheet"">
 
-  <script src=""https://polyfill.io/v3/polyfill.min.js?features=es6""></script>
+  <script src=""https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6""></script>
   <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
 
+<script type=""text/javascript"">
+const typesetMath = (el) => {
+  if (window.MathJax) {
+    // MathJax Typeset
+    window.MathJax.typeset([el]);
+  } else if (window.katex) {
+    // KaTeX Render
+    var mathElements = el.getElementsByClassName(""math"");
+    var macros = [];
+    for (var i = 0; i < mathElements.length; i++) {
+      var texText = mathElements[i].firstChild;
+      if (mathElements[i].tagName == ""SPAN"") {
+        window.katex.render(texText.data, mathElements[i], {
+          displayMode: mathElements[i].classList.contains('display'),
+          throwOnError: false,
+          macros: macros,
+          fleqn: false
+        });
+      }
+    }
+  }
+}
+window.Quarto = {
+  typesetMath
+};
+</script>
+
 </head>
 
 <body class=""nav-sidebar floating"">
@@ -110,13 +143,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_02discrv.html""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Discrete random variables</span></a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_02discrv.html""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Discrete random variables</span></a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -125,7 +158,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -189,7 +222,7 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
@@ -233,8 +266,10 @@ <h1 class=""title""><span id=""sec-prob02discrv"" class=""quarto-section-identifier"">
   </div>
   
 
+
 </header>
 
+
 <p>A categorical random variable has nominal or ordinal outcomes such as; {red, blue, green} or {tiny, small, average, large, huge}.</p>
 <p>A discrete random number is usually counts and has a countable number of outcome values, such as {1,2,3,4,5,6}; {0,2,4,6,8} or all integers.</p>
 <p>A discrete or categorical random variable can be described by its <strong>probability mass function (PMF)</strong>.</p>
@@ -244,13 +279,17 @@ <h1 class=""title""><span id=""sec-prob02discrv"" class=""quarto-section-identifier"">
 <li><span class=""math inline"">\(\sum p(x) = 1\)</span>, the sum over all possible outcomes is 1.</li>
 </ol>
 <div id=""exm-rolldice"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 2.1 (The number of dots on a dice) </strong></span>When rolling a dice the there are six possible outcomes; 1, 2, 3, 4, 5 and 6, each of which have the same probability, if the dice is fair. The outcome of one dice roll can be described by a random variable <span class=""math inline"">\(X\)</span>. The probability of a particular outcome <span class=""math inline"">\(x\)</span> is denoted <span class=""math inline"">\(P(X=x)\)</span> or <span class=""math inline"">\(p(x)\)</span>.</p>
+<p><span class=""theorem-title""><strong>Example 2.1 (The number of dots on a dice)</strong></span> When rolling a dice the there are six possible outcomes; 1, 2, 3, 4, 5 and 6, each of which have the same probability, if the dice is fair. The outcome of one dice roll can be described by a random variable <span class=""math inline"">\(X\)</span>. The probability of a particular outcome <span class=""math inline"">\(x\)</span> is denoted <span class=""math inline"">\(P(X=x)\)</span> or <span class=""math inline"">\(p(x)\)</span>.</p>
 <p>The probability mass function of a fair six-sided dice can be summarized in a table;</p>
 <div class=""cell"">
+<div id=""tbl-pmfdice"" class=""cell quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-tbl figure"">
+<figcaption class=""quarto-float-caption-top quarto-float-caption quarto-float-tbl"" id=""tbl-pmfdice-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Table&nbsp;2.1: Probability mass function of a fair six-sided dice.
+</figcaption>
+<div aria-describedby=""tbl-pmfdice-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
 <div class=""cell-output-display"">
-<div id=""tbl-pmfdice"" class=""anchored"">
-<table class=""table table-sm table-striped small"" data-quarto-postprocess=""true"">
-<caption>Table&nbsp;2.1: Probability mass function of a fair six-sided dice.</caption>
+<table class=""table do-not-create-environment cell caption-top table-sm table-striped small"" data-quarto-postprocess=""true"">
 <thead>
 <tr class=""header"">
 <th style=""text-align: left;"" data-quarto-table-cell-role=""th""></th>
@@ -283,30 +322,40 @@ <h1 class=""title""><span id=""sec-prob02discrv"" class=""quarto-section-identifier"">
 </tr>
 </tbody>
 </table>
-</div>
 
 
+</div>
+</div>
+</figure>
 </div>
 </div>
 <p>or in a barplot;</p>
 <div class=""cell"" data-layout-align=""center"">
 <div class=""cell-output-display"">
-<div id=""fig-dicepmf"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-dicepmf-1.png"" class=""img-fluid figure-img"" style=""width:45.0%""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;2.1: Probability mass function of a fair six-sided dice.</figcaption>
+<div id=""fig-dicepmf"" class=""quarto-float quarto-figure quarto-figure-center anchored"" data-fig-align=""center"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-dicepmf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-dicepmf-1.png"" class=""img-fluid quarto-figure quarto-figure-center figure-img"" style=""width:45.0%"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-dicepmf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;2.1: Probability mass function of a fair six-sided dice.
+</figcaption>
 </figure>
 </div>
 </div>
 </div>
 </div>
 <div id=""exm-nuclsite"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 2.2 (Nucleotide in a given site) </strong></span>The nucleotide at a given genomic site can be one of the four nucleotides; {A, C, T, G}. Unlike the sides on a dice the four nucleotides are usually not equally likely. The nucleotide at the site can be described by a random variable, <span class=""math inline"">\(X\)</span>. The probability mass function can be summarized in a table or a bar plot.</p>
+<p><span class=""theorem-title""><strong>Example 2.2 (Nucleotide in a given site)</strong></span> The nucleotide at a given genomic site can be one of the four nucleotides; {A, C, T, G}. Unlike the sides on a dice the four nucleotides are usually not equally likely. The nucleotide at the site can be described by a random variable, <span class=""math inline"">\(X\)</span>. The probability mass function can be summarized in a table or a bar plot.</p>
 <div class=""cell"">
+<div id=""tbl-nucl"" class=""cell quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-tbl figure"">
+<figcaption class=""quarto-float-caption-top quarto-float-caption quarto-float-tbl"" id=""tbl-nucl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Table&nbsp;2.2: Probability mass function of a nucleotide site.
+</figcaption>
+<div aria-describedby=""tbl-nucl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
 <div class=""cell-output-display"">
-<div id=""tbl-nucl"" class=""anchored"">
-<table class=""table table-sm table-striped small"" data-quarto-postprocess=""true"">
-<caption>Table&nbsp;2.2: Probability mass function of a nucleotide site.</caption>
+<table class=""table do-not-create-environment cell caption-top table-sm table-striped small"" data-quarto-postprocess=""true"">
 <thead>
 <tr class=""header"">
 <th style=""text-align: left;"" data-quarto-table-cell-role=""th""></th>
@@ -333,17 +382,23 @@ <h1 class=""title""><span id=""sec-prob02discrv"" class=""quarto-section-identifier"">
 </tr>
 </tbody>
 </table>
-</div>
 
 
+</div>
+</div>
+</figure>
 </div>
 </div>
 <div class=""cell"" data-layout-align=""center"">
 <div class=""cell-output-display"">
-<div id=""fig-nucl"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-nucl-1.png"" class=""img-fluid figure-img"" style=""width:45.0%""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;2.2: Probability mass function of a nucleotide site.</figcaption>
+<div id=""fig-nucl"" class=""quarto-float quarto-figure quarto-figure-center anchored"" data-fig-align=""center"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-nucl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-nucl-1.png"" class=""img-fluid quarto-figure quarto-figure-center figure-img"" style=""width:45.0%"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-nucl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;2.2: Probability mass function of a nucleotide site.
+</figcaption>
 </figure>
 </div>
 </div>
@@ -357,13 +412,17 @@ <h1 class=""title""><span id=""sec-prob02discrv"" class=""quarto-section-identifier"">
 <!-- ``` -->
 <!-- ::: -->
 <div id=""exm-bacteria"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 2.3 (CFU) </strong></span>The number of bacterial colony forming units (CFU) on a plate is a random number that can be described by a pmf such as in <a href=""#fig-CFU"">Figure&nbsp;<span>2.3</span></a>.</p>
+<p><span class=""theorem-title""><strong>Example 2.3 (CFU)</strong></span> The number of bacterial colony forming units (CFU) on a plate is a random number that can be described by a pmf such as in <a href=""#fig-CFU"" class=""quarto-xref"">Figure&nbsp;<span>2.3</span></a>.</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-CFU"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-CFU-1.png"" class=""img-fluid figure-img"" width=""4200""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;2.3: Probability mass distribution of the number of bacterial colonies on an agar plate.</figcaption>
+<div id=""fig-CFU"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-CFU-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-CFU-1.png"" class=""img-fluid figure-img"" width=""4200"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-CFU-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;2.3: Probability mass distribution of the number of bacterial colonies on an agar plate.
+</figcaption>
 </figure>
 </div>
 </div>
@@ -402,15 +461,19 @@ <h3 data-number=""2.1.1"" class=""anchored"" data-anchor-id=""linear-transformations-
 <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""simulate-distributions""><span class=""header-section-number"">2.2</span> Simulate distributions</h2>
 <p>Once a random variable’s probability distribution is known, probabilities, such as <span class=""math inline"">\(P(X=x), P(X&lt;x)\)</span> and <span class=""math inline"">\(P(X \geq x)\)</span>, and properties, such as expected value and variance, of the random variable can be computed. If the distribution is not known, simulation might be the solution.</p>
 <div id=""exm-cointoss"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 2.4 (Simulate coin toss) </strong></span>In a single coin toss the probabity of heads is 0.5. In 20 coin tosses, what is the probability of at least 15 heads?</p>
+<p><span class=""theorem-title""><strong>Example 2.4 (Simulate coin toss)</strong></span> In a single coin toss the probabity of heads is 0.5. In 20 coin tosses, what is the probability of at least 15 heads?</p>
 <!-- Biological example. Randomize patients into control and treatment group, 50 % In a bacterial sample, every single bacteria can either be antibiotic resistant or not. In a sample 50% of the bacteria are antibiotic resistant, grow them on an agar plat and pick 20 colonies  -->
 <p>The outcome of a single coin toss is a random variable, <span class=""math inline"">\(X\)</span>, with two possible outcomes <span class=""math inline"">\(\{H, T\}\)</span>. We know that <span class=""math inline"">\(P(X=H) = 0.5\)</span>. The random variable of interest is the number of heads in 20 coin tosses, <span class=""math inline"">\(Y\)</span>. The probability that we need to compute is <span class=""math inline"">\(P(Y \geq 15)\)</span>.</p>
 <div class=""cell"" data-layout-align=""center"">
 <div class=""cell-output-display"">
-<div id=""fig-coinurn"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""figures/coinurn.png"" class=""img-fluid figure-img"" style=""width:20.0%""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;2.4: A coin toss. Urn model with one black ball (heads) and one white ball (tails).</figcaption>
+<div id=""fig-coinurn"" class=""quarto-float quarto-figure quarto-figure-center anchored"" data-fig-align=""center"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-coinurn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""figures/coinurn.png"" class=""img-fluid quarto-figure quarto-figure-center figure-img"" style=""width:20.0%"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-coinurn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;2.4: A coin toss. Urn model with one black ball (heads) and one white ball (tails).
+</figcaption>
 </figure>
 </div>
 </div>
@@ -421,12 +484,12 @@ <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""simulate-distributions""><
 <div class=""sourceCode cell-code"" id=""cb1""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb1-1""><a href=""#cb1-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## A single coin toss</span></span>
 <span id=""cb1-2""><a href=""#cb1-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sample</span>(<span class=""fu"">c</span>(<span class=""st"">""H""</span>, <span class=""st"">""T""</span>), <span class=""at"">size=</span><span class=""dv"">1</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] ""T""</code></pre>
+<pre><code>[1] ""H""</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb3""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb3-1""><a href=""#cb3-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Another coin toss</span></span>
 <span id=""cb3-2""><a href=""#cb3-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sample</span>(<span class=""fu"">c</span>(<span class=""st"">""H""</span>, <span class=""st"">""T""</span>), <span class=""at"">size=</span><span class=""dv"">1</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] ""H""</code></pre>
+<pre><code>[1] ""T""</code></pre>
 </div>
 </div>
 <p>Every time you run <code>sample</code> a new coin toss is simulated.</p>
@@ -436,7 +499,7 @@ <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""simulate-distributions""><
 <div class=""sourceCode cell-code"" id=""cb5""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb5-1""><a href=""#cb5-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 20 independent coin tosses</span></span>
 <span id=""cb5-2""><a href=""#cb5-2"" aria-hidden=""true"" tabindex=""-1""></a>(coins <span class=""ot"">&lt;-</span> <span class=""fu"">sample</span>(<span class=""fu"">c</span>(<span class=""st"">""H""</span>, <span class=""st"">""T""</span>), <span class=""at"">size=</span><span class=""dv"">20</span>, <span class=""at"">replace=</span><span class=""cn"">TRUE</span>))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code> [1] ""T"" ""H"" ""T"" ""H"" ""H"" ""T"" ""H"" ""T"" ""H"" ""H"" ""H"" ""T"" ""T"" ""T"" ""T"" ""T"" ""T"" ""T"" ""T""
+<pre><code> [1] ""T"" ""H"" ""T"" ""T"" ""T"" ""T"" ""H"" ""T"" ""T"" ""H"" ""T"" ""H"" ""T"" ""T"" ""T"" ""T"" ""H"" ""T"" ""H""
 [20] ""H""</code></pre>
 </div>
 </div>
@@ -445,7 +508,7 @@ <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""simulate-distributions""><
 <div class=""sourceCode cell-code"" id=""cb7""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb7-1""><a href=""#cb7-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## How many heads?</span></span>
 <span id=""cb7-2""><a href=""#cb7-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sum</span>(coins <span class=""sc"">==</span> <span class=""st"">""H""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 8</code></pre>
+<pre><code>[1] 7</code></pre>
 </div>
 </div>
 <p>We can repeat this experiment (toss 20 coins and count the number of heads) several times to estimate the distribution of number of heads in 20 coin tosses.</p>
@@ -461,7 +524,11 @@ <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""simulate-distributions""><
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb10""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb10-1""><a href=""#cb10-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">hist</span>(Nheads, <span class=""at"">breaks=</span><span class=""dv"">0</span><span class=""sc"">:</span><span class=""dv"">20</span><span class=""fl"">-0.5</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output-display"">
-<p><img src=""Rfigures/prob_histNheads-1.png"" class=""img-fluid"" style=""width:70.0%""></p>
+<div>
+<figure class=""figure"">
+<p><img src=""Rfigures/prob_histNheads-1.png"" class=""img-fluid figure-img"" style=""width:70.0%""></p>
+</figure>
+</div>
 </div>
 </div>
 <p>Now, let’s get back to the question; when tossing 20 coins, what is the probability of at least 15 heads?</p>
@@ -470,11 +537,11 @@ <h2 data-number=""2.2"" class=""anchored"" data-anchor-id=""simulate-distributions""><
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb11""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb11-1""><a href=""#cb11-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sum</span>(Nheads <span class=""sc"">&gt;=</span> <span class=""dv"">15</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 203</code></pre>
+<pre><code>[1] 190</code></pre>
 </div>
 </div>
 <p>From this we conclude that</p>
-<p><span class=""math inline"">\(P(Y \geq 15) =\)</span> 203/10000 = 0.0203</p>
+<p><span class=""math inline"">\(P(Y \geq 15) =\)</span> 190/10000 = 0.0190</p>
 </div>
 <p>Resampling can also be used to compute other properties of a random variable, such as the expected value.</p>
 <p>The <strong>law of large numbers</strong> states that if the same experiment is performed many times the average of the result will be close to the expected value.</p>
@@ -533,7 +600,7 @@ <h3 data-number=""2.3.4"" class=""anchored"" data-anchor-id=""poisson""><span class=""h
 <p>The Poisson distribution describe the number of times a rare event occurs in a large number of trials. Commonly used to describe the number of events during a given time period.</p>
 <p>A rare disease has a very low probability for a single individual. The number of individuals in a large population that catch the disease in a certain time period can be modelled using the Poisson distribution.</p>
 <p>The probability mass function has a single parameter, <span class=""math inline"">\(\lambda\)</span>, the expected value, and can be described as;</p>
-<p><span class=""math display"">\[P(X=k) = \frac{\lambda}{k!}e^{-\lambda}\]</span></p>
+<p><span class=""math display"">\[P(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}\]</span></p>
 <p>The expected value <span class=""math inline"">\(\lambda = n \pi\)</span>, where <span class=""math inline"">\(n\)</span> is the number of objects sampled from the population and <span class=""math inline"">\(\pi\)</span> is the probability of a single object.</p>
 <p>The variance is equal to the expected value;</p>
 <p><span class=""math display"">\[var(X) = E[X] = \lambda = n \pi\]</span></p>
@@ -571,10 +638,14 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
 <p>The binomial, hypoergeometric, negative binomial and poisson distributions have similarities. For large N (large population) the hypergeometric and binomial distributions are very similar. The Poisson distribution has equal variance and mean, whereas the binomial has a variance less than the mean and the negative binomial has a variance greater than the mean.</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-distr"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-distr-1.png"" class=""img-fluid figure-img"" width=""4200""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;2.5: Probability mass functions for the binomial distribution (n=20, p=0.1, 0.3 or 0.5), hypergeometric distribution (N=100, n=20, p=0.1, 0.3 or 0.5), negative binomial distribution (n=20, r=n*p, p=0.1, 0.3 or 0.5) and Poisson distribution (n=20, p=0.1, 0.3 or 0.5).</figcaption>
+<div id=""fig-distr"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-distr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-distr-1.png"" class=""img-fluid figure-img"" width=""4200"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-distr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;2.5: Probability mass functions for the binomial distribution (n=20, p=0.1, 0.3 or 0.5), hypergeometric distribution (N=100, n=20, p=0.1, 0.3 or 0.5), negative binomial distribution (n=20, r=n*p, p=0.1, 0.3 or 0.5) and Poisson distribution (n=20, p=0.1, 0.3 or 0.5).
+</figcaption>
 </figure>
 </div>
 </div>
@@ -623,18 +694,7 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -666,11 +726,50 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -680,8 +779,17 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -693,7 +801,130 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -737,6 +968,7 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -762,6 +994,32 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -823,12 +1081,12 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
 </script>
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
-      <a href=""./prob_01intro.html"" class=""pagination-link"">
+      <a href=""./prob_01intro.html"" class=""pagination-link"" aria-label=""Introduction to probability"">
         <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text""><span class=""chapter-number"">1</span>&nbsp; <span class=""chapter-title"">Introduction to probability</span></span>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""./prob_exr1_discrv_solutions.html"" class=""pagination-link"">
+      <a href=""./prob_exr1_discrv_solutions.html"" class=""pagination-link"" aria-label=""Exercises: Discrete random variables"">
         <span class=""nav-page-text"">Exercises: Discrete random variables</span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
@@ -837,4 +1095,5 @@ <h3 data-number=""2.3.8"" class=""anchored"" data-anchor-id=""summary-of-discrete-dis
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/prob_03contrv.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Probability Theory - 3&nbsp; Continuous random variable</title>
+<title>3&nbsp; Continuous random variable – Probability Theory</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -47,7 +47,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -56,18 +62,45 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
   }
 }</script>
-
 <script src=""site_libs/kePrint-0.0.1/kePrint.js""></script>
 <link href=""site_libs/lightable-0.0.1/lightable.css"" rel=""stylesheet"">
 
-  <script src=""https://polyfill.io/v3/polyfill.min.js?features=es6""></script>
+  <script src=""https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6""></script>
   <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
 
+<script type=""text/javascript"">
+const typesetMath = (el) => {
+  if (window.MathJax) {
+    // MathJax Typeset
+    window.MathJax.typeset([el]);
+  } else if (window.katex) {
+    // KaTeX Render
+    var mathElements = el.getElementsByClassName(""math"");
+    var macros = [];
+    for (var i = 0; i < mathElements.length; i++) {
+      var texText = mathElements[i].firstChild;
+      if (mathElements[i].tagName == ""SPAN"") {
+        window.katex.render(texText.data, mathElements[i], {
+          displayMode: mathElements[i].classList.contains('display'),
+          throwOnError: false,
+          macros: macros,
+          fleqn: false
+        });
+      }
+    }
+  }
+}
+window.Quarto = {
+  typesetMath
+};
+</script>
+
 </head>
 
 <body class=""nav-sidebar floating"">
@@ -76,13 +109,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_03contrv.html""><span class=""chapter-number"">3</span>&nbsp; <span class=""chapter-title"">Continuous random variable</span></a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_03contrv.html""><span class=""chapter-number"">3</span>&nbsp; <span class=""chapter-title"">Continuous random variable</span></a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -91,7 +124,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -155,7 +188,7 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
@@ -193,17 +226,23 @@ <h1 class=""title""><span class=""chapter-number"">3</span>&nbsp; <span class=""chapt
   </div>
   
 
+
 </header>
 
+
 <p>A continuous random number is not limited to discrete values, but any continuous number within one or several ranges is possible.</p>
 <p>Examples: weight, height, speed, intensity, …</p>
 <p>A continuous random variable can be described by its <strong>probability density function</strong>, pdf.</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-pdfnewborn"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-pdfnewborn-1.png"" class=""img-fluid figure-img"" width=""4200""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.1: Probability density function of the weight of a newborn baby.</figcaption>
+<div id=""fig-pdfnewborn"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-pdfnewborn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-pdfnewborn-1.png"" class=""img-fluid figure-img"" width=""4200"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-pdfnewborn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.1: Probability density function of the weight of a newborn baby.
+</figcaption>
 </figure>
 </div>
 </div>
@@ -214,10 +253,14 @@ <h1 class=""title""><span class=""chapter-number"">3</span>&nbsp; <span class=""chapt
 \]</span></p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-wtbabiesdens3"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-wtbabiesdens3-1.png"" class=""img-fluid figure-img"" width=""2100""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.2: The probability <span class=""math inline"">\(P(a \leq X \leq b)\)</span> can be computed by computing the area under the probability density function between <span class=""math inline"">\(a\)</span> and <span class=""math inline"">\(b\)</span>.</figcaption>
+<div id=""fig-wtbabiesdens3"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-wtbabiesdens3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-wtbabiesdens3-1.png"" class=""img-fluid figure-img"" width=""2100"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-wtbabiesdens3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.2: The probability <span class=""math inline"">\(P(a \leq X \leq b)\)</span> can be computed by computing the area under the probability density function between <span class=""math inline"">\(a\)</span> and <span class=""math inline"">\(b\)</span>.
+</figcaption>
 </figure>
 </div>
 </div>
@@ -226,34 +269,43 @@ <h1 class=""title""><span class=""chapter-number"">3</span>&nbsp; <span class=""chapt
 <p><span class=""math display"">\[P(a \leq X \leq b) = \int_a^b f(x) dx\]</span></p>
 <p>The <strong>cumulative distribution function</strong>, cdf, sometimes called just the distribution function, <span class=""math inline"">\(F(x)\)</span>, is defined as:</p>
 <p><span class=""math display"">\[F(x) = P(X \leq x) = \int_{-\infty}^x f(t) dt\]</span></p>
-<div>
-<div class=""cell-output cell-output-stderr"">
 <pre><code>Warning in geom_point(aes(x = 4, y = pnorm(4, 3.5, 0.5))): All aesthetics have length 1, but the data has 401 rows.
-ℹ Did you mean to use `annotate()`?</code></pre>
+ℹ Please consider using `annotate()` or provide this layer with data containing
+  a single row.</code></pre>
+<div id=""fig-wtpdfcdf"" class=""quarto-layout-panel"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-wtpdfcdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<div class=""quarto-layout-row"">
+<div class=""cell-output-display quarto-layout-cell-subref quarto-layout-cell"" data-ref-parent=""fig-wtpdfcdf"" style=""flex-basis: 50.0%;justify-content: flex-start;"">
+<div id=""fig-wtpdfcdf-1"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-subfloat-fig figure"">
+<div aria-describedby=""fig-wtpdfcdf-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-wtpdfcdf-1.png"" class=""img-fluid figure-img"" data-ref-parent=""fig-wtpdfcdf"" width=""2100"">
 </div>
-<div id=""fig-wtpdfcdf"" class=""cell quarto-layout-panel"">
-<figure class=""figure"">
-<div class=""quarto-layout-row quarto-layout-valign-top"">
-<div class=""cell-output-display quarto-layout-cell quarto-layout-cell-subref"" style=""flex-basis: 50.0%;justify-content: center;"">
-<div id=""fig-wtpdfcdf-1"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-wtpdfcdf-1.png"" class=""img-fluid figure-img"" data-ref-parent=""fig-wtpdfcdf"" width=""2100""></p>
-<figcaption class=""figure-caption"">(a) PDF</figcaption>
+<figcaption class=""quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig"" id=""fig-wtpdfcdf-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+(a) PDF
+</figcaption>
 </figure>
 </div>
 </div>
-<div class=""cell-output-display quarto-layout-cell quarto-layout-cell-subref"" style=""flex-basis: 50.0%;justify-content: center;"">
-<div id=""fig-wtpdfcdf-2"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-wtpdfcdf-2.png"" class=""img-fluid figure-img"" data-ref-parent=""fig-wtpdfcdf"" width=""2100""></p>
-<figcaption class=""figure-caption"">(b) CDF</figcaption>
+<div class=""cell-output-display quarto-layout-cell-subref quarto-layout-cell"" data-ref-parent=""fig-wtpdfcdf"" style=""flex-basis: 50.0%;justify-content: flex-start;"">
+<div id=""fig-wtpdfcdf-2"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-subfloat-fig figure"">
+<div aria-describedby=""fig-wtpdfcdf-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-wtpdfcdf-2.png"" class=""img-fluid figure-img"" data-ref-parent=""fig-wtpdfcdf"" width=""2100"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig"" id=""fig-wtpdfcdf-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+(b) CDF
+</figcaption>
 </figure>
 </div>
 </div>
 </div>
-<p></p><figcaption class=""figure-caption"">Figure&nbsp;3.3: Probability density function (pdf) and cumulative distribution function (cdf).</figcaption><p></p>
-</figure>
 </div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-wtpdfcdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.3: Probability density function (pdf) and cumulative distribution function (cdf).
+</figcaption>
+</figure>
 </div>
 <p><span class=""math display"">\[P(X \leq x) = F(x)\]</span></p>
 <p>As we know that the total probability (over all x) is 1, we can conclude that</p>
@@ -277,21 +329,32 @@ <h2 data-number=""3.2"" class=""anchored"" data-anchor-id=""normal-distribution""><spa
 <!-- In short we write $X \sim N(\mu, \sigma)$. -->
 <p>The bell-shaped normal distributions is symmetric around <span class=""math inline"">\(\mu\)</span> and <span class=""math inline"">\(f(x) \rightarrow 0\)</span> as <span class=""math inline"">\(x \rightarrow \infty\)</span> and as <span class=""math inline"">\(x \rightarrow -\infty\)</span>.</p>
 <p>As <span class=""math inline"">\(f(x)\)</span> is well defined, values for the cumulative distribution function <span class=""math inline"">\(F(x) = \int_{- \infty}^x f(x) dx\)</span> can be computed.</p>
-<div class=""cell quarto-layout-panel"">
-<div class=""quarto-layout-row quarto-layout-valign-top"">
-<div class=""cell-output-display quarto-layout-cell"" style=""flex-basis: 50.0%;justify-content: center;"">
-<div id=""fig-pdfcdfnorm-1"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-pdfcdfnorm-1.png"" class=""img-fluid figure-img"" width=""2100""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.4: Normal probability density function and cumulative distribution functions.</figcaption>
+<div>
+
+</div>
+<div class=""cell quarto-layout-panel"" data-layout-ncol=""2"">
+<div class=""quarto-layout-row"">
+<div class=""cell-output-display quarto-layout-cell"" style=""flex-basis: 50.0%;justify-content: flex-start;"">
+<div id=""fig-pdfcdfnorm-1"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-pdfcdfnorm-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-pdfcdfnorm-1.png"" class=""img-fluid figure-img"" width=""2100"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-pdfcdfnorm-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.4: Normal probability density function and cumulative distribution functions.
+</figcaption>
 </figure>
 </div>
 </div>
-<div class=""cell-output-display quarto-layout-cell"" style=""flex-basis: 50.0%;justify-content: center;"">
-<div id=""fig-pdfcdfnorm-2"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-pdfcdfnorm-2.png"" class=""img-fluid figure-img"" width=""2100""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.5: Normal probability density function and cumulative distribution functions.</figcaption>
+<div class=""cell-output-display quarto-layout-cell"" style=""flex-basis: 50.0%;justify-content: flex-start;"">
+<div id=""fig-pdfcdfnorm-2"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-pdfcdfnorm-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-pdfcdfnorm-2.png"" class=""img-fluid figure-img"" width=""2100"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-pdfcdfnorm-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.5: Normal probability density function and cumulative distribution functions.
+</figcaption>
 </figure>
 </div>
 </div>
@@ -304,19 +367,27 @@ <h2 data-number=""3.2"" class=""anchored"" data-anchor-id=""normal-distribution""><spa
 <p>Values for the cumulative standard normal distribution, <span class=""math inline"">\(F(z)\)</span>, are tabulated and easy to compute in R using the function <code>pnorm</code>.</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-FZ"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-FZ-1.png"" class=""img-fluid figure-img"" style=""width:50.0%""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.6: The shaded area under the curve is the tabulated value <span class=""math inline"">\(P(Z \leq z) = F(z)\)</span>.</figcaption>
+<div id=""fig-FZ"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-FZ-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-FZ-1.png"" class=""img-fluid figure-img"" style=""width:50.0%"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-FZ-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.6: The shaded area under the curve is the tabulated value <span class=""math inline"">\(P(Z \leq z) = F(z)\)</span>.
+</figcaption>
 </figure>
 </div>
 </div>
 </div>
 <div class=""cell"">
+<div id=""tbl-FZtab"" class=""cell quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-tbl figure"">
+<figcaption class=""quarto-float-caption-top quarto-float-caption quarto-float-tbl"" id=""tbl-FZtab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Table&nbsp;3.1: Cumulative distribution function for the standard normal distribution. The table gives F(z) = P(Z &lt; z) for standard normal Z.
+</figcaption>
+<div aria-describedby=""tbl-FZtab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
 <div class=""cell-output-display"">
-<div id=""tbl-FZtab"" class=""anchored"">
-<table class=""table table-sm table-striped small"" data-quarto-postprocess=""true"">
-<caption>Table&nbsp;3.1: Cumulative distribution function for the standard normal distribution. The table gives F(z) = P(Z &lt; z) for standard normal Z.</caption>
+<table class=""table do-not-create-environment cell caption-top table-sm table-striped small"" data-quarto-postprocess=""true"">
 <thead>
 <tr class=""header"">
 <th style=""text-align: left;"" data-quarto-table-cell-role=""th""></th>
@@ -790,16 +861,17 @@ <h2 data-number=""3.2"" class=""anchored"" data-anchor-id=""normal-distribution""><spa
 </tr>
 </tbody>
 </table>
-</div>
 
 
+</div>
+</div>
+</figure>
 </div>
 </div>
 <p>Some values of particular interest:</p>
 <p><span class=""math display"">\[F(1.64) = 0.95\]</span> <span class=""math display"">\[F(1.96) = 0.975\]</span></p>
 <p>As the normal distribution is symmetric F(-z) = 1 - F(z)</p>
 <p><span class=""math display"">\[F(-1.64) = 0.05\]</span> <span class=""math display"">\[F(-1.96) = 0.025\]</span> <span class=""math display"">\[P(-1.96 &lt; Z &lt; 1.96) = 0.95\]</span></p>
-
 <!-- Show table? -->
 <!-- dnorm -->
 <!-- pnorm -->
@@ -814,21 +886,25 @@ <h3 data-number=""3.2.1"" class=""anchored"" data-anchor-id=""sum-of-two-normal-rando
 <section id=""central-limit-theorem"" class=""level2"" data-number=""3.3"">
 <h2 data-number=""3.3"" class=""anchored"" data-anchor-id=""central-limit-theorem""><span class=""header-section-number"">3.3</span> Central limit theorem</h2>
 <div id=""thm-CLT"" class=""theorem"">
-<p><span class=""theorem-title""><strong>Theorem 3.1 (CLT) </strong></span>The sum of <span class=""math inline"">\(n\)</span> independent and equally distributed random variables is normally distributed, if <span class=""math inline"">\(n\)</span> is large enough.</p>
+<p><span class=""theorem-title""><strong>Theorem 3.1 (CLT)</strong></span> The sum of <span class=""math inline"">\(n\)</span> independent and equally distributed random variables is normally distributed, if <span class=""math inline"">\(n\)</span> is large enough.</p>
 </div>
 <p>As a result of the central limit theorem, the distribution of fractions or mean values of a sample follow the normal distribution, at least if the sample is large enough (a rule of thumb is that the sample size <span class=""math inline"">\(n&gt;30\)</span>).</p>
 <!-- ```{example, ""Mean BMI"", eval=FALSE} -->
 <!-- Percentage of body fat, age, weight, height, BMI and ten body circumference -->
 <!-- measurements are recorded for 252 men. Consider these 252 as a population and compute the population mean ans standard deviation. -->
 <!-- ``` -->
 <div id=""exm-skewed"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 3.1 (A skewed distribution) </strong></span>A left skewed distribution has a heavier left tail than right tail. An example might be age at death of natural causes.</p>
+<p><span class=""theorem-title""><strong>Example 3.1 (A skewed distribution)</strong></span> A left skewed distribution has a heavier left tail than right tail. An example might be age at death of natural causes.</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-leftskewed"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-leftskewed-1.png"" class=""img-fluid figure-img"" width=""3000""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.7: A left skewed distribution. Can for example show the distribution of age of a mouse who died of natural causes.</figcaption>
+<div id=""fig-leftskewed"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-leftskewed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-leftskewed-1.png"" class=""img-fluid figure-img"" width=""3000"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-leftskewed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.7: A left skewed distribution. Can for example show the distribution of age of a mouse who died of natural causes.
+</figcaption>
 </figure>
 </div>
 </div>
@@ -843,10 +919,14 @@ <h2 data-number=""3.3"" class=""anchored"" data-anchor-id=""central-limit-theorem""><s
   always returns an ungrouped data frame and adjust accordingly.</code></pre>
 </div>
 <div class=""cell-output-display"">
-<div id=""fig-meanskew"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-meanskew-1.png"" class=""img-fluid figure-img"" width=""3000""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.8: Distribution of sample means, where the means are computed based on random samples of sizes 3, 5, 10, 15, 20 and 30, respectively.</figcaption>
+<div id=""fig-meanskew"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-meanskew-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-meanskew-1.png"" class=""img-fluid figure-img"" width=""3000"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-meanskew-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.8: Distribution of sample means, where the means are computed based on random samples of sizes 3, 5, 10, 15, 20 and 30, respectively.
+</figcaption>
 </figure>
 </div>
 </div>
@@ -857,7 +937,6 @@ <h2 data-number=""3.3"" class=""anchored"" data-anchor-id=""central-limit-theorem""><s
 <!-- # Mean BMI -->
 <!-- In a population of 252 men we can study the distribution of BMI. -->
 <!-- ```{r fatdata} -->
-
 <!-- fat <- read.table(""http://jse.amstat.org/datasets/fat.dat.txt"") -->
 <!-- colnames(fat) <- c(""case"",""body.fat"",""body.fat.siri"",""density"",""age"",""weight"",""height"",""BMI"",""ffweight"",""neck"",""chest"",""abdomen"",""hip"",""thigh"",""knee"",""ankle"" ,""bicep"",""forearm"",""wrist"" ) -->
 <!-- ``` -->
@@ -891,16 +970,20 @@ <h2 data-number=""3.4"" class=""anchored"" data-anchor-id=""chi2-distribution""><span
 <p>In short <span class=""math inline"">\(Y \in \chi^2(n-1)\)</span>.</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-Xdistr"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-Xdistr-1.png"" class=""img-fluid figure-img"" width=""4200""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.9: The <span class=""math inline"">\(\chi^2\)</span>-distribution.</figcaption>
+<div id=""fig-Xdistr"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-Xdistr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-Xdistr-1.png"" class=""img-fluid figure-img"" width=""4200"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-Xdistr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.9: The <span class=""math inline"">\(\chi^2\)</span>-distribution.
+</figcaption>
 </figure>
 </div>
 </div>
 </div>
 <div id=""exm-chisq"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 3.2 </strong></span>The sample variance <span class=""math inline"">\(S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2\)</span> is such that <span class=""math inline"">\(\frac{(n-1)S^2}{\sigma^2}\)</span> is <span class=""math inline"">\(\chi^2\)</span> distributed with <span class=""math inline"">\(n-1\)</span> degrees of freedom.</p>
+<p><span class=""theorem-title""><strong>Example 3.2</strong></span> The sample variance <span class=""math inline"">\(S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2\)</span> is such that <span class=""math inline"">\(\frac{(n-1)S^2}{\sigma^2}\)</span> is <span class=""math inline"">\(\chi^2\)</span> distributed with <span class=""math inline"">\(n-1\)</span> degrees of freedom.</p>
 </div>
 <!-- Example. $\chi^2$-test for variance -->
 </section>
@@ -909,16 +992,20 @@ <h2 data-number=""3.5"" class=""anchored"" data-anchor-id=""f-distribution""><span cla
 <p>The ratio of two <span class=""math inline"">\(\chi^2\)</span>-distributed variables divided by their degrees of freedom is F-distributed</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-Fdistr"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-Fdistr-1.png"" class=""img-fluid figure-img"" width=""4200""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.10: The F-distribution</figcaption>
+<div id=""fig-Fdistr"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-Fdistr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-Fdistr-1.png"" class=""img-fluid figure-img"" width=""4200"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-Fdistr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.10: The F-distribution
+</figcaption>
 </figure>
 </div>
 </div>
 </div>
 <div id=""exm-Fdistr"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 3.3 </strong></span>The ratio of two sample variances is F-distributed</p>
+<p><span class=""theorem-title""><strong>Example 3.3</strong></span> The ratio of two sample variances is F-distributed</p>
 </div>
 <!-- Example. F-test of equality of variances -->
 </section>
@@ -927,16 +1014,20 @@ <h2 data-number=""3.6"" class=""anchored"" data-anchor-id=""t-distribution""><span cla
 <p>The ratio of a normally distributed variable and a <span class=""math inline"">\(\chi^2\)</span>-distributed variable is t-distributed.</p>
 <div class=""cell"">
 <div class=""cell-output-display"">
-<div id=""fig-exampletdistr"" class=""quarto-figure quarto-figure-center anchored"">
-<figure class=""figure"">
-<p><img src=""Rfigures/prob_fig-exampletdistr-1.png"" class=""img-fluid figure-img"" width=""4200""></p>
-<figcaption class=""figure-caption"">Figure&nbsp;3.11: The t-distribution.</figcaption>
+<div id=""fig-exampletdistr"" class=""quarto-float quarto-figure quarto-figure-center anchored"">
+<figure class=""quarto-float quarto-float-fig figure"">
+<div aria-describedby=""fig-exampletdistr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+<img src=""Rfigures/prob_fig-exampletdistr-1.png"" class=""img-fluid figure-img"" width=""4200"">
+</div>
+<figcaption class=""quarto-float-caption-bottom quarto-float-caption quarto-float-fig"" id=""fig-exampletdistr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca"">
+Figure&nbsp;3.11: The t-distribution.
+</figcaption>
 </figure>
 </div>
 </div>
 </div>
 <div id=""exm-tdistr"" class=""theorem example"">
-<p><span class=""theorem-title""><strong>Example 3.4 </strong></span>The ratio between sample mean and sample variance is t-distributed.</p>
+<p><span class=""theorem-title""><strong>Example 3.4</strong></span> The ratio between sample mean and sample variance is t-distributed.</p>
 </div>
 </section>
 <section id=""distributions-in-r"" class=""level2"" data-number=""3.7"">
@@ -984,18 +1075,7 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -1027,11 +1107,50 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -1041,8 +1160,17 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -1054,7 +1182,130 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -1098,6 +1349,7 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -1123,6 +1375,32 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -1184,12 +1462,12 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
 </script>
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
-      <a href=""./prob_exr1_discrv_solutions.html"" class=""pagination-link"">
+      <a href=""./prob_exr1_discrv_solutions.html"" class=""pagination-link"" aria-label=""Exercises: Discrete random variables"">
         <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text"">Exercises: Discrete random variables</span>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""./prob_04sample.html"" class=""pagination-link"">
+      <a href=""./prob_04sample.html"" class=""pagination-link"" aria-label=""Sampling and experimental design"">
         <span class=""nav-page-text""><span class=""chapter-number"">4</span>&nbsp; <span class=""chapter-title"">Sampling and experimental design</span></span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
@@ -1198,4 +1476,5 @@ <h2 data-number=""3.7"" class=""anchored"" data-anchor-id=""distributions-in-r""><span
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/prob_04sample.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Probability Theory - 4&nbsp; Sampling and experimental design</title>
+<title>4&nbsp; Sampling and experimental design – Probability Theory</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -47,7 +47,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -56,15 +62,43 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
   }
 }</script>
 
-  <script src=""https://polyfill.io/v3/polyfill.min.js?features=es6""></script>
+  <script src=""https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6""></script>
   <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
 
+<script type=""text/javascript"">
+const typesetMath = (el) => {
+  if (window.MathJax) {
+    // MathJax Typeset
+    window.MathJax.typeset([el]);
+  } else if (window.katex) {
+    // KaTeX Render
+    var mathElements = el.getElementsByClassName(""math"");
+    var macros = [];
+    for (var i = 0; i < mathElements.length; i++) {
+      var texText = mathElements[i].firstChild;
+      if (mathElements[i].tagName == ""SPAN"") {
+        window.katex.render(texText.data, mathElements[i], {
+          displayMode: mathElements[i].classList.contains('display'),
+          throwOnError: false,
+          macros: macros,
+          fleqn: false
+        });
+      }
+    }
+  }
+}
+window.Quarto = {
+  typesetMath
+};
+</script>
+
 </head>
 
 <body class=""nav-sidebar floating"">
@@ -73,13 +107,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_04sample.html""><span class=""chapter-number"">4</span>&nbsp; <span class=""chapter-title"">Sampling and experimental design</span></a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_04sample.html""><span class=""chapter-number"">4</span>&nbsp; <span class=""chapter-title"">Sampling and experimental design</span></a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -88,7 +122,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -152,7 +186,7 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
@@ -190,8 +224,10 @@ <h1 class=""title""><span id=""prob-04sample"" class=""quarto-section-identifier""><sp
   </div>
   
 
+
 </header>
 
+
 <section id=""random-sampling"" class=""level2"" data-number=""4.1"">
 <h2 data-number=""4.1"" class=""anchored"" data-anchor-id=""random-sampling""><span class=""header-section-number"">4.1</span> Random sampling</h2>
 <p>In many (most) experiments it is not feasible (or even possible) to examine the entire population. Instead a random sample is studied.</p>
@@ -299,18 +335,7 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -342,11 +367,50 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -356,8 +420,17 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -369,7 +442,130 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -413,6 +609,7 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -438,6 +635,32 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -499,12 +722,12 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
 </script>
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
-      <a href=""./prob_03contrv.html"" class=""pagination-link"">
+      <a href=""./prob_03contrv.html"" class=""pagination-link"" aria-label=""Continuous random variable"">
         <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text""><span class=""chapter-number"">3</span>&nbsp; <span class=""chapter-title"">Continuous random variable</span></span>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""./prob_exr2_contrv_solutions.html"" class=""pagination-link"">
+      <a href=""./prob_exr2_contrv_solutions.html"" class=""pagination-link"" aria-label=""Exercises: Continuous random variables"">
         <span class=""nav-page-text"">Exercises: Continuous random variables</span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
@@ -513,4 +736,5 @@ <h3 data-number=""4.3.5"" class=""anchored"" data-anchor-id=""standard-error""><span c
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/prob_exr1_discrv_solutions.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Probability Theory - Exercises: Discrete random variables</title>
+<title>Exercises: Discrete random variables – Probability Theory</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -22,7 +22,7 @@
 }
 /* CSS for syntax highlighting */
 pre > code.sourceCode { white-space: pre; position: relative; }
-pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
+pre > code.sourceCode > span { line-height: 1.25; }
 pre > code.sourceCode > span:empty { height: 1.2em; }
 .sourceCode { overflow: visible; }
 code.sourceCode > span { color: inherit; text-decoration: inherit; }
@@ -33,7 +33,7 @@
 }
 @media print {
 pre > code.sourceCode { white-space: pre-wrap; }
-pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
+pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
 }
 pre.numberSource code
   { counter-reset: source-line 0; }
@@ -81,7 +81,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -90,15 +96,43 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
   }
 }</script>
 
-  <script src=""https://polyfill.io/v3/polyfill.min.js?features=es6""></script>
+  <script src=""https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6""></script>
   <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
 
+<script type=""text/javascript"">
+const typesetMath = (el) => {
+  if (window.MathJax) {
+    // MathJax Typeset
+    window.MathJax.typeset([el]);
+  } else if (window.katex) {
+    // KaTeX Render
+    var mathElements = el.getElementsByClassName(""math"");
+    var macros = [];
+    for (var i = 0; i < mathElements.length; i++) {
+      var texText = mathElements[i].firstChild;
+      if (mathElements[i].tagName == ""SPAN"") {
+        window.katex.render(texText.data, mathElements[i], {
+          displayMode: mathElements[i].classList.contains('display'),
+          throwOnError: false,
+          macros: macros,
+          fleqn: false
+        });
+      }
+    }
+  }
+}
+window.Quarto = {
+  typesetMath
+};
+</script>
+
 </head>
 
 <body class=""nav-sidebar floating"">
@@ -107,13 +141,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_exr1_discrv_solutions.html"">Exercises: Discrete random variables</a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_exr1_discrv_solutions.html"">Exercises: Discrete random variables</a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -122,7 +156,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -186,7 +220,7 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
@@ -217,12 +251,14 @@ <h1 class=""title"">Exercises: Discrete random variables</h1>
   </div>
   
 
+
 </header>
 
+
 <section id=""introduction-to-probability"" class=""level2"">
 <h2 class=""anchored"" data-anchor-id=""introduction-to-probability"">Introduction to probability</h2>
 <div id=""exr-mutation"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 1 (BRCA) </strong></span>The probability of carrying mutations (one or more) in the breast cancer gene BRCA1 is 0.01. What is the probability of not carrying any mutations in BRCA1?</p>
+<p><span class=""theorem-title""><strong>Exercise 1 (BRCA)</strong></span> The probability of carrying mutations (one or more) in the breast cancer gene BRCA1 is 0.01. What is the probability of not carrying any mutations in BRCA1?</p>
 <div class=""callout callout-style-default callout-tip callout-titled"">
 <div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-1-contents"" aria-controls=""callout-1"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
@@ -257,7 +293,7 @@ <h2 class=""anchored"" data-anchor-id=""introduction-to-probability"">Introduction t
 </div>
 </div>
 <div id=""exr-probcoin"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 2 (A coin toss) </strong></span>When tossing a fair coin</p>
+<p><span class=""theorem-title""><strong>Exercise 2 (A coin toss)</strong></span> When tossing a fair coin</p>
 <ol type=""a"">
 <li>what is the probability of heads?</li>
 <li>what is the probability of tails?</li>
@@ -302,8 +338,8 @@ <h2 class=""anchored"" data-anchor-id=""introduction-to-probability"">Introduction t
 <!-- End solution -->
 </div>
 <div id=""exr-children"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 3 (Number of children) </strong></span>In a region in Sweden with many children the number of children per household is between 0 and 6. The probability mass function is as follows;</p>
-<table class=""table"">
+<p><span class=""theorem-title""><strong>Exercise 3 (Number of children)</strong></span> In a region in Sweden with many children the number of children per household is between 0 and 6. The probability mass function is as follows;</p>
+<table class=""caption-top table"">
 <thead>
 <tr class=""header"">
 <th style=""text-align: left;"">x</th>
@@ -361,7 +397,7 @@ <h2 class=""anchored"" data-anchor-id=""introduction-to-probability"">Introduction t
 </div>
 </div>
 <div id=""exr-probdie"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 4 (Rolling dice) </strong></span>When tossing a fair six-sided dice</p>
+<p><span class=""theorem-title""><strong>Exercise 4 (Rolling dice)</strong></span> When tossing a fair six-sided dice</p>
 <ol type=""a"">
 <li>what is the probability of getting 6?</li>
 <li>what is the probability of an even number?</li>
@@ -410,7 +446,6 @@ <h2 class=""anchored"" data-anchor-id=""introduction-to-probability"">Introduction t
 </section>
 <section id=""simulation"" class=""level2 unnumbered"">
 <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
-
 <!-- ::: {#exr-cointoss}
 In a single coin toss the probability of heads is 0.5. 
 
@@ -440,6 +475,10 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 Simulate as in the lecture;
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
@@ -448,9 +487,12 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 [1] ""T""
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -459,9 +501,12 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 [1] ""T""
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -470,10 +515,13 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
- [1] ""T"" ""H"" ""H"" ""H"" ""T"" ""T"" ""H"" ""T"" ""H"" ""T"" ""T"" ""H"" ""T"" ""T"" ""H"" ""H"" ""H"" ""H"" ""T""
+ [1] ""T"" ""H"" ""H"" ""H"" ""T"" ""T"" ""T"" ""H"" ""T"" ""T"" ""T"" ""H"" ""T"" ""T"" ""T"" ""H"" ""T"" ""T"" ""T""
 [20] ""T""
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -482,9 +530,12 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 10
+[1] 6
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -497,9 +548,17 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 :::
 
 
+
+
+
+
 a.  Probability of exactly 15 heads
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
@@ -508,9 +567,12 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 144
+[1] 152
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -519,9 +581,12 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 0.014
+[1] 0.015
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -530,33 +595,55 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 0.014
+[1] 0.015
 ```
+
+
 :::
 :::
 
 
+
+
+
+
 b.  Probability of less than 7 heads
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
 mean(Nheads<7)
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 0.06
+[1] 0.055
 ```
+
+
 :::
 :::
 
 
+
+
+
+
 c.  What is the most probable number of heads?
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
@@ -574,22 +661,31 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 Nheads
-   2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17 
-   1   11   48  156  384  692 1247 1599 1746 1562 1210  790  356  144   44    9 
-  18 
-   1 
+   3    4    5    6    7    8    9   10   11   12   13   14   15   16   17 
+  13   49  150  342  723 1226 1577 1779 1632 1184  761  359  152   44    9 
 ```
+
+
 :::
 :::
 
 
+
+
+
+
 d.  What is the probability of 5 tails or less?
 
 To get five or less tails out of 20 throws is equal to getting 15 or more heads out of 20.
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
@@ -598,36 +694,53 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 0.02
+[1] 0.021
 ```
+
+
 :::
 :::
 
 
+
+
+
+
 e.  what is the probability of 2 heads or less?
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
 mean(Nheads<=2)
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 1e-04
+[1] 0
 ```
+
+
 :::
 
 ```{.r .cell-code}
 sum(Nheads<=2)
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 1
+[1] 0
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -641,26 +754,36 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 182
+[1] 210
 ```
+
+
 :::
 
 ```{.r .cell-code}
 mean(Nheads<=2)
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 0.00018
+[1] 0.00021
 ```
+
+
 :::
 :::
 
+
+
+
+
 :::
 ::: -->
 <div id=""exr-rand"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 5 (Randomization) </strong></span>In a clinical trial, enrolled patients are randomly assigned to treatment or control group with equal probability.</p>
+<p><span class=""theorem-title""><strong>Exercise 5 (Randomization)</strong></span> In a clinical trial, enrolled patients are randomly assigned to treatment or control group with equal probability.</p>
 <p>For a single patient, what is the probability of being assigned to</p>
 <ol type=""a"">
 <li>the treatment group?</li>
@@ -715,18 +838,18 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""sourceCode cell-code"" id=""cb1""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb1-1""><a href=""#cb1-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Randomization for a single patient</span></span>
 <span id=""cb1-2""><a href=""#cb1-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sample</span>(<span class=""fu"">c</span>(<span class=""st"">""T""</span>, <span class=""st"">""C""</span>), <span class=""at"">size=</span><span class=""dv"">1</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] ""C""</code></pre>
+<pre><code>[1] ""T""</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb3""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb3-1""><a href=""#cb3-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Randomize 20 independent patients</span></span>
 <span id=""cb3-2""><a href=""#cb3-2"" aria-hidden=""true"" tabindex=""-1""></a>(patients <span class=""ot"">&lt;-</span> <span class=""fu"">sample</span>(<span class=""fu"">c</span>(<span class=""st"">""T""</span>, <span class=""st"">""c""</span>), <span class=""at"">size=</span><span class=""dv"">20</span>, <span class=""at"">replace=</span><span class=""cn"">TRUE</span>))</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code> [1] ""c"" ""T"" ""T"" ""c"" ""c"" ""c"" ""c"" ""T"" ""c"" ""T"" ""T"" ""c"" ""c"" ""T"" ""T"" ""c"" ""T"" ""T"" ""c""
-[20] ""T""</code></pre>
+<pre><code> [1] ""T"" ""T"" ""T"" ""T"" ""c"" ""T"" ""T"" ""c"" ""T"" ""c"" ""T"" ""T"" ""T"" ""c"" ""c"" ""c"" ""c"" ""c"" ""T""
+[20] ""c""</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb5""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb5-1""><a href=""#cb5-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## How many patients are assigned to treatment group?</span></span>
 <span id=""cb5-2""><a href=""#cb5-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sum</span>(patients <span class=""sc"">==</span> <span class=""st"">""T""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 10</code></pre>
+<pre><code>[1] 11</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb7""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb7-1""><a href=""#cb7-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Simulate by repeating 10000 times</span></span>
 <span id=""cb7-2""><a href=""#cb7-2"" aria-hidden=""true"" tabindex=""-1""></a>Ntreat <span class=""ot"">&lt;-</span> <span class=""fu"">replicate</span>(<span class=""dv"">10000</span>, {</span>
@@ -741,7 +864,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""sourceCode cell-code"" id=""cb8""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb8-1""><a href=""#cb8-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Proportion of the 10000 repeats with exactly 15 T</span></span>
 <span id=""cb8-2""><a href=""#cb8-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(Ntreat<span class=""sc"">==</span><span class=""dv"">15</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 0.016</code></pre>
+<pre><code>[1] 0.015</code></pre>
 </div>
 </div>
 <ol start=""4"" type=""a"">
@@ -760,16 +883,18 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""sourceCode cell-code"" id=""cb12""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb12-1""><a href=""#cb12-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## plot the distribution and read the graph</span></span>
 <span id=""cb12-2""><a href=""#cb12-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">hist</span>(Ntreat, <span class=""at"">breaks=</span><span class=""dv"">0</span><span class=""sc"">:</span><span class=""dv"">21</span><span class=""fl"">-0.5</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output-display"">
-<p><img src=""Rfigures/prob_solrandome-1.png"" class=""img-fluid"" width=""2100""></p>
+<div>
+<figure class=""figure"">
+<p><img src=""Rfigures/prob_solrandome-1.png"" class=""img-fluid figure-img"" width=""2100""></p>
+</figure>
+</div>
 </div>
 <div class=""sourceCode cell-code"" id=""cb13""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb13-1""><a href=""#cb13-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## or tabulate</span></span>
 <span id=""cb13-2""><a href=""#cb13-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">table</span>(Ntreat)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>Ntreat
-   2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17 
-   6   11   55  133  391  699 1203 1598 1753 1617 1214  713  376  164   50   16 
-  18 
-   1 </code></pre>
+   3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18 
+  12   58  154  375  761 1195 1555 1778 1599 1194  751  348  147   52   18    3 </code></pre>
 </div>
 </div>
 <ol start=""6"" type=""a"">
@@ -780,7 +905,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""sourceCode cell-code"" id=""cb15""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb15-1""><a href=""#cb15-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## probability of 15 T or more</span></span>
 <span id=""cb15-2""><a href=""#cb15-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(Ntreat<span class=""sc"">&gt;=</span><span class=""dv"">15</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 0.023</code></pre>
+<pre><code>[1] 0.022</code></pre>
 </div>
 </div>
 <ol start=""7"" type=""a"">
@@ -789,11 +914,11 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb17""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb17-1""><a href=""#cb17-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(Ntreat<span class=""sc"">&lt;=</span><span class=""dv"">2</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 6e-04</code></pre>
+<pre><code>[1] 0</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb19""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb19-1""><a href=""#cb19-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">sum</span>(Ntreat<span class=""sc"">&lt;=</span><span class=""dv"">2</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 6</code></pre>
+<pre><code>[1] 0</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb21""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb21-1""><a href=""#cb21-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## with this low number of observations, more repeats is required to get a more accurate answer</span></span>
 <span id=""cb21-2""><a href=""#cb21-2"" aria-hidden=""true"" tabindex=""-1""></a>  </span>
@@ -814,7 +939,6 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 </div>
 </div>
 </div>
-
 <!-- ::: {#exr-dice}
 ## Ten dice
 
@@ -843,6 +967,10 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 c.  
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
@@ -852,11 +980,14 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 N
     0     1     2     3     4     5     6     7     8 
-16027 32267 29322 15365  5428  1349   215    24     3 
+16308 32309 28885 15510  5460  1290   209    28     1 
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -865,11 +996,14 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 N
       0       1       2       3       4       5       6       7       8 
-0.16027 0.32267 0.29322 0.15365 0.05428 0.01349 0.00215 0.00024 0.00003 
+0.16308 0.32309 0.28885 0.15510 0.05460 0.01290 0.00209 0.00028 0.00001 
 ```
+
+
 :::
 
 ```{.r .cell-code}
@@ -882,79 +1016,125 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 :::
 
 
+
+
+
+
 d)  0.015
 
 
+
+
+
+
 ::: {.cell}
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 1591
+[1] 1528
 ```
+
+
 :::
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 0.016
+[1] 0.015
 ```
+
+
 :::
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 0.016
+[1] 0.015
 ```
+
+
 :::
 :::
 
 
+
+
+
+
 e)  1 (Use the PMF to answer this question)
 
 f)  0.29
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
 mean(N==2)
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 [1] 0.29
 ```
+
+
 :::
 :::
 
 
+
+
+
+
 g)  1.7
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
 mean(N)
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 [1] 1.7
 ```
+
+
 :::
 
 ```{.r .cell-code}
 10*1/6
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
 [1] 1.7
 ```
+
+
 :::
 :::
 
+
+
+
+
 :::
 ::: -->
 <div id=""exr-bacteria"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 6 (Bacterial colonies) </strong></span>In a bacterial sample, 1/6 are antibiotic resistant. From bacterial colonies on an agar plate, you randomly pick 10 colonies and investigate how many that are antibiotic resistant.</p>
+<p><span class=""theorem-title""><strong>Exercise 6 (Bacterial colonies)</strong></span> In a bacterial sample, 1/6 are antibiotic resistant. From bacterial colonies on an agar plate, you randomly pick 10 colonies and investigate how many that are antibiotic resistant.</p>
 <ol type=""a"">
 <li>Define the random variable of interest</li>
 <li>What are the possible outcomes?</li>
@@ -1004,26 +1184,30 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>N
     0     1     2     3     4     5     6     7     8 
-16390 32371 29043 15314  5364  1268   224    24     2 </code></pre>
+16204 32333 28995 15570  5363  1285   222    25     3 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb27""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb27-1""><a href=""#cb27-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##The probability mass function</span></span>
 <span id=""cb27-2""><a href=""#cb27-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">table</span>(N)<span class=""sc"">/</span><span class=""fu"">length</span>(N)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>N
       0       1       2       3       4       5       6       7       8 
-0.16390 0.32371 0.29043 0.15314 0.05364 0.01268 0.00224 0.00024 0.00002 </code></pre>
+0.16204 0.32333 0.28995 0.15570 0.05363 0.01285 0.00222 0.00025 0.00003 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb29""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb29-1""><a href=""#cb29-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">hist</span>(N, <span class=""at"">breaks=</span>(<span class=""dv"">0</span><span class=""sc"">:</span><span class=""dv"">11</span>)<span class=""sc"">-</span><span class=""fl"">0.5</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output-display"">
-<p><img src=""Rfigures/prob_CFUc-1.png"" class=""img-fluid"" width=""2100""></p>
+<div>
+<figure class=""figure"">
+<p><img src=""Rfigures/prob_CFUc-1.png"" class=""img-fluid figure-img"" width=""2100""></p>
+</figure>
+</div>
 </div>
 </div>
 <ol start=""4"" type=""a"">
 <li>0.015</li>
 </ol>
 <div class=""cell"">
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 1518</code></pre>
+<pre><code>[1] 1535</code></pre>
 </div>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>[1] 0.015</code></pre>
@@ -1076,6 +1260,10 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ### Solution
 
 
+
+
+
+
 ::: {.cell}
 
 ```{.r .cell-code}
@@ -1093,16 +1281,23 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 ```
 
 ::: {.cell-output .cell-output-stdout}
+
 ```
-[1] 5e-04
+[1] 0.00051
 ```
+
+
 :::
 :::
 
+
+
+
+
 :::
 ::: -->
 <div id=""exr-pollen"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 7 (Pollen allergy) </strong></span>&nbsp;</p>
+<p><span class=""theorem-title""><strong>Exercise 7 (Pollen allergy)</strong></span> &nbsp;</p>
 <ol type=""a"">
 <li>30% of a large population is allergic to pollen. If you randomly select 3 people to participate in your study, what is the probability than none of them will be allergic to pollen?</li>
 </ol>
@@ -1125,35 +1320,35 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>x
  0  1  2  3 
-38 42 18  2 </code></pre>
+35 43 20  2 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb41""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb41-1""><a href=""#cb41-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(x<span class=""sc"">==</span><span class=""dv"">0</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 0.38</code></pre>
+<pre><code>[1] 0.35</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb43""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb43-1""><a href=""#cb43-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Solution using 1000 replicates</span></span>
 <span id=""cb43-2""><a href=""#cb43-2"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> <span class=""fu"">replicate</span>(<span class=""dv"">1000</span>, <span class=""fu"">sum</span>(<span class=""fu"">sample</span>(<span class=""fu"">c</span>(<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">1</span>,<span class=""dv"">1</span>,<span class=""dv"">1</span>), <span class=""at"">size=</span><span class=""dv"">3</span>, <span class=""at"">replace=</span><span class=""cn"">TRUE</span>)))</span>
 <span id=""cb43-3""><a href=""#cb43-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">table</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>x
   0   1   2   3 
-330 467 184  19 </code></pre>
+353 440 188  19 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb45""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb45-1""><a href=""#cb45-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(x<span class=""sc"">==</span><span class=""dv"">0</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 0.33</code></pre>
+<pre><code>[1] 0.35</code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb47""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb47-1""><a href=""#cb47-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Solution using 100000 replicates</span></span>
 <span id=""cb47-2""><a href=""#cb47-2"" aria-hidden=""true"" tabindex=""-1""></a>x <span class=""ot"">&lt;-</span> <span class=""fu"">replicate</span>(<span class=""dv"">100000</span>, <span class=""fu"">sum</span>(<span class=""fu"">sample</span>(<span class=""fu"">c</span>(<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">0</span>,<span class=""dv"">1</span>,<span class=""dv"">1</span>,<span class=""dv"">1</span>), <span class=""at"">size=</span><span class=""dv"">3</span>, <span class=""at"">replace=</span><span class=""cn"">TRUE</span>)))</span>
 <span id=""cb47-3""><a href=""#cb47-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">table</span>(x)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>x
     0     1     2     3 
-34549 43850 19021  2580 </code></pre>
+34439 43866 19001  2694 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb49""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb49-1""><a href=""#cb49-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(x<span class=""sc"">==</span><span class=""dv"">0</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
-<pre><code>[1] 0.35</code></pre>
+<pre><code>[1] 0.34</code></pre>
 </div>
 </div>
 </div>
@@ -1181,7 +1376,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>x
     0     1     2     3 
-32006 47761 18462  1771 </code></pre>
+31824 48007 18366  1803 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb53""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb53-1""><a href=""#cb53-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(x<span class=""sc"">==</span><span class=""dv"">0</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
@@ -1213,7 +1408,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <div class=""cell-output cell-output-stdout"">
 <pre><code>x
     0     1     2     3 
-33896 44637 18820  2647 </code></pre>
+34101 44388 18976  2535 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb57""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb57-1""><a href=""#cb57-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">mean</span>(x<span class=""sc"">==</span><span class=""dv"">0</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
@@ -1247,7 +1442,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""simulation"">Simulation</h2>
 <section id=""parametric-discrete-distribution"" class=""level2 unnumbered"">
 <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution"">Parametric discrete distribution</h2>
 <div id=""exr-parampollen"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 8 (Pollen) </strong></span>Do <a href=""#exr-pollen"">Exercise&nbsp;<span>7</span></a> again, but using parametric distributions. Compare your results.</p>
+<p><span class=""theorem-title""><strong>Exercise 8 (Pollen)</strong></span> Do <a href=""#exr-pollen"" class=""quarto-xref"">Exercise&nbsp;<span>7</span></a> again, but using parametric distributions. Compare your results.</p>
 <div class=""callout callout-style-default callout-note callout-titled"">
 <div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-16-contents"" aria-controls=""callout-16"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
@@ -1282,7 +1477,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
 </div>
 </div>
 <div id=""exr-gsea"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 9 (Gene set enrichment analysis) </strong></span>You have analyzed 20000 genes and a bioinformatician you are collaborating with has sent you a list of 1000 genes that she says are important. You are interested in a particular pathway A. 200 genes in pathway A are represented among the 20000 genes, 20 of these are in the bioinformaticians important list.</p>
+<p><span class=""theorem-title""><strong>Exercise 9 (Gene set enrichment analysis)</strong></span> You have analyzed 20000 genes and a bioinformatician you are collaborating with has sent you a list of 1000 genes that she says are important. You are interested in a particular pathway A. 200 genes in pathway A are represented among the 20000 genes, 20 of these are in the bioinformaticians important list.</p>
 <p>If the bioinformatician selected the 1000 genes at random, what is the probability to see 20 or more genes from pathway A in this list?</p>
 <div class=""callout callout-style-default callout-note callout-titled"">
 <div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-17-contents"" aria-controls=""callout-17"" aria-expanded=""false"" aria-label=""Toggle callout"">
@@ -1307,7 +1502,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
 </div>
 </div>
 <div id=""exr-boss"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 10 (Chance of meeting boss) </strong></span>Your boss comes in to the office three days per week. You do also come in to work three days per week. If you both choose which days to come in to work at random, what is the probability that a particular week you are in the office at the same time 0, 1, 2 or 3 days, respectively?</p>
+<p><span class=""theorem-title""><strong>Exercise 10 (Chance of meeting boss)</strong></span> Your boss comes in to the office three days per week. You do also come in to work three days per week. If you both choose which days to come in to work at random, what is the probability that a particular week you are in the office at the same time 0, 1, 2 or 3 days, respectively?</p>
 <div class=""callout callout-style-default callout-note callout-titled"">
 <div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-18-contents"" aria-controls=""callout-18"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
@@ -1326,7 +1521,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
 <div class=""cell-output cell-output-stdout"">
 <pre><code>x
     1     2     3 
-29948 60038 10014 </code></pre>
+30028 59894 10078 </code></pre>
 </div>
 <div class=""sourceCode cell-code"" id=""cb69""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb69-1""><a href=""#cb69-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">dhyper</span>(<span class=""dv"">0</span><span class=""sc"">:</span><span class=""dv"">3</span>, <span class=""dv"">3</span>, <span class=""dv"">2</span>, <span class=""dv"">3</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 <div class=""cell-output cell-output-stdout"">
@@ -1338,7 +1533,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
 </div>
 </div>
 <div id=""exr-poisson"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 11 (Rare disease) </strong></span>A rare disease affects 3 in 100000 in a large population. If 10000 people are randomly selected from the population, what is the probability</p>
+<p><span class=""theorem-title""><strong>Exercise 11 (Rare disease)</strong></span> A rare disease affects 3 in 100000 in a large population. If 10000 people are randomly selected from the population, what is the probability</p>
 <ol type=""a"">
 <li>that no one in the sample is affected?</li>
 <li>that at least two in the sample are affected?</li>
@@ -1379,7 +1574,6 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
 </div>
 </div>
 </div>
-
 <!-- ## Conditional probability {.unnumbered}
 
 ::: {#exr-diagnostic}
@@ -1455,18 +1649,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -1498,11 +1681,50 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -1512,8 +1734,17 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -1525,7 +1756,130 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -1569,6 +1923,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -1594,6 +1949,32 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -1655,12 +2036,12 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
 </script>
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
-      <a href=""./prob_02discrv.html"" class=""pagination-link"">
+      <a href=""./prob_02discrv.html"" class=""pagination-link"" aria-label=""Discrete random variables"">
         <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Discrete random variables</span></span>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""./prob_03contrv.html"" class=""pagination-link"">
+      <a href=""./prob_03contrv.html"" class=""pagination-link"" aria-label=""Continuous random variable"">
         <span class=""nav-page-text""><span class=""chapter-number"">3</span>&nbsp; <span class=""chapter-title"">Continuous random variable</span></span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
@@ -1669,4 +2050,5 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""parametric-discrete-distribution
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/prob_exr2_contrv_solutions.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Probability Theory - Exercises: Continuous random variables</title>
+<title>Exercises: Continuous random variables – Probability Theory</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -22,7 +22,7 @@
 }
 /* CSS for syntax highlighting */
 pre > code.sourceCode { white-space: pre; position: relative; }
-pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
+pre > code.sourceCode > span { line-height: 1.25; }
 pre > code.sourceCode > span:empty { height: 1.2em; }
 .sourceCode { overflow: visible; }
 code.sourceCode > span { color: inherit; text-decoration: inherit; }
@@ -33,7 +33,7 @@
 }
 @media print {
 pre > code.sourceCode { white-space: pre-wrap; }
-pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
+pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
 }
 pre.numberSource code
   { counter-reset: source-line 0; }
@@ -81,7 +81,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -90,15 +96,43 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
   }
 }</script>
 
-  <script src=""https://polyfill.io/v3/polyfill.min.js?features=es6""></script>
+  <script src=""https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6""></script>
   <script src=""https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"" type=""text/javascript""></script>
 
+<script type=""text/javascript"">
+const typesetMath = (el) => {
+  if (window.MathJax) {
+    // MathJax Typeset
+    window.MathJax.typeset([el]);
+  } else if (window.katex) {
+    // KaTeX Render
+    var mathElements = el.getElementsByClassName(""math"");
+    var macros = [];
+    for (var i = 0; i < mathElements.length; i++) {
+      var texText = mathElements[i].firstChild;
+      if (mathElements[i].tagName == ""SPAN"") {
+        window.katex.render(texText.data, mathElements[i], {
+          displayMode: mathElements[i].classList.contains('display'),
+          throwOnError: false,
+          macros: macros,
+          fleqn: false
+        });
+      }
+    }
+  }
+}
+window.Quarto = {
+  typesetMath
+};
+</script>
+
 </head>
 
 <body class=""nav-sidebar floating"">
@@ -107,13 +141,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_exr2_contrv_solutions.html"">Exercises: Continuous random variables</a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./prob_exr2_contrv_solutions.html"">Exercises: Continuous random variables</a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -122,7 +156,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -186,7 +220,7 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
     <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
         <nav id=""TOC"" role=""doc-toc"" class=""toc-active"">
@@ -218,12 +252,14 @@ <h1 class=""title"">Exercises: Continuous random variables</h1>
   </div>
   
 
+
 </header>
 
+
 <section id=""parametric-continuous-distribution"" class=""level2"">
 <h2 class=""anchored"" data-anchor-id=""parametric-continuous-distribution"">Parametric continuous distribution</h2>
 <div id=""exr-normtable"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 1 (The normal table) </strong></span>Let <span class=""math inline"">\(Z \sim N(0,1)\)</span> be a standard normal random variable, and compute;</p>
+<p><span class=""theorem-title""><strong>Exercise 1 (The normal table)</strong></span> Let <span class=""math inline"">\(Z \sim N(0,1)\)</span> be a standard normal random variable, and compute;</p>
 <ol type=""a"">
 <li><span class=""math inline"">\(P(Z&lt;1.64)\)</span></li>
 <li><span class=""math inline"">\(P(Z&gt;-1.64)\)</span></li>
@@ -258,7 +294,7 @@ <h2 class=""anchored"" data-anchor-id=""parametric-continuous-distribution"">Paramet
 </div>
 </div>
 <div id=""exr-ztransform"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 2 (Exercise in standardization/transformation) </strong></span>If <span class=""math inline"">\(X \sim N(3,2)\)</span>, compute the probabilities</p>
+<p><span class=""theorem-title""><strong>Exercise 2 (Exercise in standardization/transformation)</strong></span> If <span class=""math inline"">\(X \sim N(3,2)\)</span>, compute the probabilities</p>
 <ol type=""a"">
 <li><span class=""math inline"">\(P(X&lt;5)\)</span></li>
 <li><span class=""math inline"">\(P(3&lt;X&lt;5)\)</span></li>
@@ -286,7 +322,7 @@ <h2 class=""anchored"" data-anchor-id=""parametric-continuous-distribution"">Paramet
 </div>
 </div>
 <div id=""exr-sumdistr"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 3 (Hemoglobin) </strong></span>The hemoglobin (Hb) value in a male population is normally distributed with mean 188 g/L and standard deviation 14 g/L.</p>
+<p><span class=""theorem-title""><strong>Exercise 3 (Hemoglobin)</strong></span> The hemoglobin (Hb) value in a male population is normally distributed with mean 188 g/L and standard deviation 14 g/L.</p>
 <ol type=""a"">
 <li>Men with Hb below 158 g/L are considered anemic. What is the probability of a random man being anemic?</li>
 <li>When randomly selecting 10 men from the population, what is the probability that none of them are anemic?</li>
@@ -320,7 +356,7 @@ <h2 class=""anchored"" data-anchor-id=""parametric-continuous-distribution"">Paramet
 </div>
 </div>
 <div id=""exr-pill"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 4 (Pill) </strong></span>A drug company is producing a pill, with on average 12 mg of active substance. The amount of active substance is normally distributed with mean 12 mg and standard deviation 0.5 mg, if the production is without problems. Sometimes there is a problem with the production and the amount of active substance will be too high or too low, in which case the pill has to be discarded. What should the upper and lower critical values (limits for when a pill is acceptable) be in order not to discard more than 1/20 pills from a problem free production?</p>
+<p><span class=""theorem-title""><strong>Exercise 4 (Pill)</strong></span> A drug company is producing a pill, with on average 12 mg of active substance. The amount of active substance is normally distributed with mean 12 mg and standard deviation 0.5 mg, if the production is without problems. Sometimes there is a problem with the production and the amount of active substance will be too high or too low, in which case the pill has to be discarded. What should the upper and lower critical values (limits for when a pill is acceptable) be in order not to discard more than 1/20 pills from a problem free production?</p>
 <div class=""callout callout-style-default callout-note callout-titled"">
 <div class=""callout-header d-flex align-content-center"" data-bs-toggle=""collapse"" data-bs-target="".callout-4-contents"" aria-controls=""callout-4"" aria-expanded=""false"" aria-label=""Toggle callout"">
 <div class=""callout-icon-container"">
@@ -346,7 +382,7 @@ <h2 class=""anchored"" data-anchor-id=""parametric-continuous-distribution"">Paramet
 <section id=""random-sample"" class=""level3 unnumbered"">
 <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3>
 <div id=""exr-cholesterol"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 5 (Exercise in distribution of sample mean) </strong></span>The total cholesterol in population (mg/dL) is normally distributed with <span class=""math inline"">\(\mu = 202\)</span> and <span class=""math inline"">\(\sigma = 40\)</span>.</p>
+<p><span class=""theorem-title""><strong>Exercise 5 (Exercise in distribution of sample mean)</strong></span> The total cholesterol in population (mg/dL) is normally distributed with <span class=""math inline"">\(\mu = 202\)</span> and <span class=""math inline"">\(\sigma = 40\)</span>.</p>
 <ol type=""a"">
 <li>How is the sample mean of a sample of 4 persons distributed?</li>
 <li>What is the probability to see a sample mean of 260 mg/dL or higher?</li>
@@ -377,7 +413,7 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
 </div>
 </div>
 <div id=""exr-pill2cont"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 6 (Amount of active substance) </strong></span>The amount of active substance in a pill is stated by the manufacturer to be normally distributed with mean 12 mg and standard deviation 0.5 mg. You take a sample of five pills and measure the amount of active substance to; 13.0, 12.3, 12.6, 12.5, 12.7 mg.</p>
+<p><span class=""theorem-title""><strong>Exercise 6 (Amount of active substance)</strong></span> The amount of active substance in a pill is stated by the manufacturer to be normally distributed with mean 12 mg and standard deviation 0.5 mg. You take a sample of five pills and measure the amount of active substance to; 13.0, 12.3, 12.6, 12.5, 12.7 mg.</p>
 <p>[Note: a-c were already computed in the descriptive statistics session.]</p>
 <ol type=""a"">
 <li>Compute the sample mean</li>
@@ -499,18 +535,7 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -542,11 +567,50 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -556,8 +620,17 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -569,7 +642,130 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -613,6 +809,7 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -638,6 +835,32 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -699,12 +922,12 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
 </script>
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
-      <a href=""./prob_04sample.html"" class=""pagination-link"">
+      <a href=""./prob_04sample.html"" class=""pagination-link"" aria-label=""Sampling and experimental design"">
         <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text""><span class=""chapter-number"">4</span>&nbsp; <span class=""chapter-title"">Sampling and experimental design</span></span>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">
-      <a href=""./references.html"" class=""pagination-link"">
+      <a href=""./references.html"" class=""pagination-link"" aria-label=""References"">
         <span class=""nav-page-text"">References</span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
@@ -713,4 +936,5 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""random-sample"">Random sample</h3
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/references.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.450"">
+<meta name=""generator"" content=""quarto-1.5.57"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Probability Theory - References</title>
+<title>References – Probability Theory</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -65,7 +65,13 @@
   ""collapse-after"": 3,
   ""panel-placement"": ""start"",
   ""type"": ""textbox"",
-  ""limit"": 20,
+  ""limit"": 50,
+  ""keyboard-shortcut"": [
+    ""f"",
+    ""/"",
+    ""s""
+  ],
+  ""show-item-context"": false,
   ""language"": {
     ""search-no-results-text"": ""No results"",
     ""search-matching-documents-text"": ""matching documents"",
@@ -74,6 +80,7 @@
     ""search-more-match-text"": ""more match in this document"",
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
+    ""search-text-placeholder"": """",
     ""search-detached-cancel-button-title"": ""Cancel"",
     ""search-submit-button-title"": ""Submit"",
     ""search-label"": ""Search""
@@ -89,13 +96,13 @@
   <header id=""quarto-header"" class=""headroom fixed-top"">
   <nav class=""quarto-secondary-nav"">
     <div class=""container-fluid d-flex"">
-      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
+      <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" role=""button"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./references.html"">References</a></li></ol></nav>
-      <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
-      </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
+        <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./references.html"">References</a></li></ol></nav>
+        <a class=""flex-grow-1"" role=""navigation"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
+        </a>
+      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -104,7 +111,7 @@
 <!-- content -->
 <div id=""quarto-content"" class=""quarto-container page-columns page-rows-contents page-layout-article"">
 <!-- sidebar -->
-  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"">
+  <nav id=""quarto-sidebar"" class=""sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"">
     <div class=""pt-lg-2 mt-2 text-left sidebar-header"">
     <div class=""sidebar-title mb-0 py-0"">
       <a href=""./"">Probability Theory</a> 
@@ -168,9 +175,9 @@
     </ul>
     </div>
 </nav>
-<div id=""quarto-sidebar-glass"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass""></div>
+<div id=""quarto-sidebar-glass"" class=""quarto-sidebar-collapse-item"" data-bs-toggle=""collapse"" data-bs-target="".quarto-sidebar-collapse-item""></div>
 <!-- margin-sidebar -->
-    <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar"">
+    <div id=""quarto-margin-sidebar"" class=""sidebar margin-sidebar zindex-bottom"">
         
     </div>
 <!-- main -->
@@ -191,8 +198,10 @@ <h1 class=""title"">References</h1>
   </div>
   
 
+
 </header>
 
+
 <div id=""refs"" role=""list"" style=""display: none"">
 
 </div>
@@ -235,18 +244,7 @@ <h1 class=""title"">References</h1>
     }
     return false;
   }
-  const clipboard = new window.ClipboardJS('.code-copy-button', {
-    text: function(trigger) {
-      const codeEl = trigger.previousElementSibling.cloneNode(true);
-      for (const childEl of codeEl.children) {
-        if (isCodeAnnotation(childEl)) {
-          childEl.remove();
-        }
-      }
-      return codeEl.innerText;
-    }
-  });
-  clipboard.on('success', function(e) {
+  const onCopySuccess = function(e) {
     // button target
     const button = e.trigger;
     // don't keep focus
@@ -278,11 +276,50 @@ <h1 class=""title"">References</h1>
     }, 1000);
     // clear code selection
     e.clearSelection();
+  }
+  const getTextToCopy = function(trigger) {
+      const codeEl = trigger.previousElementSibling.cloneNode(true);
+      for (const childEl of codeEl.children) {
+        if (isCodeAnnotation(childEl)) {
+          childEl.remove();
+        }
+      }
+      return codeEl.innerText;
+  }
+  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
+    text: getTextToCopy
   });
-  function tippyHover(el, contentFn) {
+  clipboard.on('success', onCopySuccess);
+  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
+    // For code content inside modals, clipBoardJS needs to be initialized with a container option
+    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
+    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
+      text: getTextToCopy,
+      container: window.document.getElementById('quarto-embedded-source-code-modal')
+    });
+    clipboardModal.on('success', onCopySuccess);
+  }
+    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
+    var mailtoRegex = new RegExp(/^mailto:/);
+      var filterRegex = new RegExp('/' + window.location.host + '/');
+    var isInternal = (href) => {
+        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
+    }
+    // Inspect non-navigation links and adorn them if external
+ 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
+    for (var i=0; i<links.length; i++) {
+      const link = links[i];
+      if (!isInternal(link.href)) {
+        // undo the damage that might have been done by quarto-nav.js in the case of
+        // links that we want to consider external
+        if (link.dataset.originalHref !== undefined) {
+          link.href = link.dataset.originalHref;
+        }
+      }
+    }
+  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
     const config = {
       allowHTML: true,
-      content: contentFn,
       maxWidth: 500,
       delay: 100,
       arrow: false,
@@ -292,8 +329,17 @@ <h1 class=""title"">References</h1>
       interactive: true,
       interactiveBorder: 10,
       theme: 'quarto',
-      placement: 'bottom-start'
+      placement: 'bottom-start',
     };
+    if (contentFn) {
+      config.content = contentFn;
+    }
+    if (onTriggerFn) {
+      config.onTrigger = onTriggerFn;
+    }
+    if (onUntriggerFn) {
+      config.onUntrigger = onUntriggerFn;
+    }
     window.tippy(el, config); 
   }
   const noterefs = window.document.querySelectorAll('a[role=""doc-noteref""]');
@@ -305,7 +351,130 @@ <h1 class=""title"">References</h1>
       try { href = new URL(href).hash; } catch {}
       const id = href.replace(/^#\/?/, """");
       const note = window.document.getElementById(id);
-      return note.innerHTML;
+      if (note) {
+        return note.innerHTML;
+      } else {
+        return """";
+      }
+    });
+  }
+  const xrefs = window.document.querySelectorAll('a.quarto-xref');
+  const processXRef = (id, note) => {
+    // Strip column container classes
+    const stripColumnClz = (el) => {
+      el.classList.remove(""page-full"", ""page-columns"");
+      if (el.children) {
+        for (const child of el.children) {
+          stripColumnClz(child);
+        }
+      }
+    }
+    stripColumnClz(note)
+    if (id === null || id.startsWith('sec-')) {
+      // Special case sections, only their first couple elements
+      const container = document.createElement(""div"");
+      if (note.children && note.children.length > 2) {
+        container.appendChild(note.children[0].cloneNode(true));
+        for (let i = 1; i < note.children.length; i++) {
+          const child = note.children[i];
+          if (child.tagName === ""P"" && child.innerText === """") {
+            continue;
+          } else {
+            container.appendChild(child.cloneNode(true));
+            break;
+          }
+        }
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(container);
+        }
+        return container.innerHTML
+      } else {
+        if (window.Quarto?.typesetMath) {
+          window.Quarto.typesetMath(note);
+        }
+        return note.innerHTML;
+      }
+    } else {
+      // Remove any anchor links if they are present
+      const anchorLink = note.querySelector('a.anchorjs-link');
+      if (anchorLink) {
+        anchorLink.remove();
+      }
+      if (window.Quarto?.typesetMath) {
+        window.Quarto.typesetMath(note);
+      }
+      // TODO in 1.5, we should make sure this works without a callout special case
+      if (note.classList.contains(""callout"")) {
+        return note.outerHTML;
+      } else {
+        return note.innerHTML;
+      }
+    }
+  }
+  for (var i=0; i<xrefs.length; i++) {
+    const xref = xrefs[i];
+    tippyHover(xref, undefined, function(instance) {
+      instance.disable();
+      let url = xref.getAttribute('href');
+      let hash = undefined; 
+      if (url.startsWith('#')) {
+        hash = url;
+      } else {
+        try { hash = new URL(url).hash; } catch {}
+      }
+      if (hash) {
+        const id = hash.replace(/^#\/?/, """");
+        const note = window.document.getElementById(id);
+        if (note !== null) {
+          try {
+            const html = processXRef(id, note.cloneNode(true));
+            instance.setContent(html);
+          } finally {
+            instance.enable();
+            instance.show();
+          }
+        } else {
+          // See if we can fetch this
+          fetch(url.split('#')[0])
+          .then(res => res.text())
+          .then(html => {
+            const parser = new DOMParser();
+            const htmlDoc = parser.parseFromString(html, ""text/html"");
+            const note = htmlDoc.getElementById(id);
+            if (note !== null) {
+              const html = processXRef(id, note);
+              instance.setContent(html);
+            } 
+          }).finally(() => {
+            instance.enable();
+            instance.show();
+          });
+        }
+      } else {
+        // See if we can fetch a full url (with no hash to target)
+        // This is a special case and we should probably do some content thinning / targeting
+        fetch(url)
+        .then(res => res.text())
+        .then(html => {
+          const parser = new DOMParser();
+          const htmlDoc = parser.parseFromString(html, ""text/html"");
+          const note = htmlDoc.querySelector('main.content');
+          if (note !== null) {
+            // This should only happen for chapter cross references
+            // (since there is no id in the URL)
+            // remove the first header
+            if (note.children.length > 0 && note.children[0].tagName === ""HEADER"") {
+              note.children[0].remove();
+            }
+            const html = processXRef(null, note);
+            instance.setContent(html);
+          } 
+        }).finally(() => {
+          instance.enable();
+          instance.show();
+        });
+      }
+    }, function(instance) {
     });
   }
       let selectedAnnoteEl;
@@ -349,6 +518,7 @@ <h1 class=""title"">References</h1>
             }
             div.style.top = top - 2 + ""px"";
             div.style.height = height + 4 + ""px"";
+            div.style.left = 0;
             let gutterDiv = window.document.getElementById(""code-annotation-line-highlight-gutter"");
             if (gutterDiv === null) {
               gutterDiv = window.document.createElement(""div"");
@@ -374,6 +544,32 @@ <h1 class=""title"">References</h1>
         });
         selectedAnnoteEl = undefined;
       };
+        // Handle positioning of the toggle
+    window.addEventListener(
+      ""resize"",
+      throttle(() => {
+        elRect = undefined;
+        if (selectedAnnoteEl) {
+          selectCodeLines(selectedAnnoteEl);
+        }
+      }, 10)
+    );
+    function throttle(fn, ms) {
+    let throttle = false;
+    let timer;
+      return (...args) => {
+        if(!throttle) { // first call gets through
+            fn.apply(this, args);
+            throttle = true;
+        } else { // all the others get throttled
+            if(timer) clearTimeout(timer); // cancel #2
+            timer = setTimeout(() => {
+              fn.apply(this, args);
+              timer = throttle = false;
+            }, ms);
+        }
+      };
+    }
       // Attach click handler to the DT
       const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
       for (const annoteDlNode of annoteDls) {
@@ -435,7 +631,7 @@ <h1 class=""title"">References</h1>
 </script>
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
-      <a href=""./prob_exr2_contrv_solutions.html"" class=""pagination-link"">
+      <a href=""./prob_exr2_contrv_solutions.html"" class=""pagination-link"" aria-label=""Exercises: Continuous random variables"">
         <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text"">Exercises: Continuous random variables</span>
       </a>          
   </div>
@@ -446,4 +642,5 @@ <h1 class=""title"">References</h1>
 
 
 
+
 </body></html>
\ No newline at end of file

---FILE: session-probability/docs/site_libs/bootstrap/bootstrap-icons.css---
@@ -1,8 +1,14 @@
+/*!
+ * Bootstrap Icons v1.11.1 (https://icons.getbootstrap.com/)
+ * Copyright 2019-2023 The Bootstrap Authors
+ * Licensed under MIT (https://github.com/twbs/icons/blob/main/LICENSE)
+ */
+
 @font-face {
   font-display: block;
   font-family: ""bootstrap-icons"";
   src: 
-url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
+url(""./bootstrap-icons.woff?2820a3852bdb9a5832199cc61cec4e65"") format(""woff"");
 }
 
 .bi::before,
@@ -441,7 +447,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-cloud-fog2::before { content: ""\f2a2""; }
 .bi-cloud-hail-fill::before { content: ""\f2a3""; }
 .bi-cloud-hail::before { content: ""\f2a4""; }
-.bi-cloud-haze-1::before { content: ""\f2a5""; }
 .bi-cloud-haze-fill::before { content: ""\f2a6""; }
 .bi-cloud-haze::before { content: ""\f2a7""; }
 .bi-cloud-haze2-fill::before { content: ""\f2a8""; }
@@ -1437,21 +1442,16 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-dpad::before { content: ""\f687""; }
 .bi-ear-fill::before { content: ""\f688""; }
 .bi-ear::before { content: ""\f689""; }
-.bi-envelope-check-1::before { content: ""\f68a""; }
 .bi-envelope-check-fill::before { content: ""\f68b""; }
 .bi-envelope-check::before { content: ""\f68c""; }
-.bi-envelope-dash-1::before { content: ""\f68d""; }
 .bi-envelope-dash-fill::before { content: ""\f68e""; }
 .bi-envelope-dash::before { content: ""\f68f""; }
-.bi-envelope-exclamation-1::before { content: ""\f690""; }
 .bi-envelope-exclamation-fill::before { content: ""\f691""; }
 .bi-envelope-exclamation::before { content: ""\f692""; }
 .bi-envelope-plus-fill::before { content: ""\f693""; }
 .bi-envelope-plus::before { content: ""\f694""; }
-.bi-envelope-slash-1::before { content: ""\f695""; }
 .bi-envelope-slash-fill::before { content: ""\f696""; }
 .bi-envelope-slash::before { content: ""\f697""; }
-.bi-envelope-x-1::before { content: ""\f698""; }
 .bi-envelope-x-fill::before { content: ""\f699""; }
 .bi-envelope-x::before { content: ""\f69a""; }
 .bi-explicit-fill::before { content: ""\f69b""; }
@@ -1461,8 +1461,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-list-columns-reverse::before { content: ""\f69f""; }
 .bi-list-columns::before { content: ""\f6a0""; }
 .bi-meta::before { content: ""\f6a1""; }
-.bi-mortorboard-fill::before { content: ""\f6a2""; }
-.bi-mortorboard::before { content: ""\f6a3""; }
 .bi-nintendo-switch::before { content: ""\f6a4""; }
 .bi-pc-display-horizontal::before { content: ""\f6a5""; }
 .bi-pc-display::before { content: ""\f6a6""; }
@@ -1481,7 +1479,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-send-check::before { content: ""\f6b3""; }
 .bi-send-dash-fill::before { content: ""\f6b4""; }
 .bi-send-dash::before { content: ""\f6b5""; }
-.bi-send-exclamation-1::before { content: ""\f6b6""; }
 .bi-send-exclamation-fill::before { content: ""\f6b7""; }
 .bi-send-exclamation::before { content: ""\f6b8""; }
 .bi-send-fill::before { content: ""\f6b9""; }
@@ -1493,7 +1490,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-send-x::before { content: ""\f6bf""; }
 .bi-send::before { content: ""\f6c0""; }
 .bi-steam::before { content: ""\f6c1""; }
-.bi-terminal-dash-1::before { content: ""\f6c2""; }
 .bi-terminal-dash::before { content: ""\f6c3""; }
 .bi-terminal-plus::before { content: ""\f6c4""; }
 .bi-terminal-split::before { content: ""\f6c5""; }
@@ -1523,7 +1519,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-usb-symbol::before { content: ""\f6dd""; }
 .bi-usb::before { content: ""\f6de""; }
 .bi-boombox-fill::before { content: ""\f6df""; }
-.bi-displayport-1::before { content: ""\f6e0""; }
 .bi-displayport::before { content: ""\f6e1""; }
 .bi-gpu-card::before { content: ""\f6e2""; }
 .bi-memory::before { content: ""\f6e3""; }
@@ -1536,8 +1531,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-pci-card::before { content: ""\f6ea""; }
 .bi-router-fill::before { content: ""\f6eb""; }
 .bi-router::before { content: ""\f6ec""; }
-.bi-ssd-fill::before { content: ""\f6ed""; }
-.bi-ssd::before { content: ""\f6ee""; }
 .bi-thunderbolt-fill::before { content: ""\f6ef""; }
 .bi-thunderbolt::before { content: ""\f6f0""; }
 .bi-usb-drive-fill::before { content: ""\f6f1""; }
@@ -1644,7 +1637,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-filetype-pdf::before { content: ""\f756""; }
 .bi-filetype-php::before { content: ""\f757""; }
 .bi-filetype-png::before { content: ""\f758""; }
-.bi-filetype-ppt-1::before { content: ""\f759""; }
 .bi-filetype-ppt::before { content: ""\f75a""; }
 .bi-filetype-psd::before { content: ""\f75b""; }
 .bi-filetype-py::before { content: ""\f75c""; }
@@ -1660,7 +1652,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-filetype-txt::before { content: ""\f766""; }
 .bi-filetype-wav::before { content: ""\f767""; }
 .bi-filetype-woff::before { content: ""\f768""; }
-.bi-filetype-xls-1::before { content: ""\f769""; }
 .bi-filetype-xls::before { content: ""\f76a""; }
 .bi-filetype-xml::before { content: ""\f76b""; }
 .bi-filetype-yml::before { content: ""\f76c""; }
@@ -1703,56 +1694,38 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-filetype-json::before { content: ""\f791""; }
 .bi-filetype-pptx::before { content: ""\f792""; }
 .bi-filetype-xlsx::before { content: ""\f793""; }
-.bi-1-circle-1::before { content: ""\f794""; }
-.bi-1-circle-fill-1::before { content: ""\f795""; }
 .bi-1-circle-fill::before { content: ""\f796""; }
 .bi-1-circle::before { content: ""\f797""; }
 .bi-1-square-fill::before { content: ""\f798""; }
 .bi-1-square::before { content: ""\f799""; }
-.bi-2-circle-1::before { content: ""\f79a""; }
-.bi-2-circle-fill-1::before { content: ""\f79b""; }
 .bi-2-circle-fill::before { content: ""\f79c""; }
 .bi-2-circle::before { content: ""\f79d""; }
 .bi-2-square-fill::before { content: ""\f79e""; }
 .bi-2-square::before { content: ""\f79f""; }
-.bi-3-circle-1::before { content: ""\f7a0""; }
-.bi-3-circle-fill-1::before { content: ""\f7a1""; }
 .bi-3-circle-fill::before { content: ""\f7a2""; }
 .bi-3-circle::before { content: ""\f7a3""; }
 .bi-3-square-fill::before { content: ""\f7a4""; }
 .bi-3-square::before { content: ""\f7a5""; }
-.bi-4-circle-1::before { content: ""\f7a6""; }
-.bi-4-circle-fill-1::before { content: ""\f7a7""; }
 .bi-4-circle-fill::before { content: ""\f7a8""; }
 .bi-4-circle::before { content: ""\f7a9""; }
 .bi-4-square-fill::before { content: ""\f7aa""; }
 .bi-4-square::before { content: ""\f7ab""; }
-.bi-5-circle-1::before { content: ""\f7ac""; }
-.bi-5-circle-fill-1::before { content: ""\f7ad""; }
 .bi-5-circle-fill::before { content: ""\f7ae""; }
 .bi-5-circle::before { content: ""\f7af""; }
 .bi-5-square-fill::before { content: ""\f7b0""; }
 .bi-5-square::before { content: ""\f7b1""; }
-.bi-6-circle-1::before { content: ""\f7b2""; }
-.bi-6-circle-fill-1::before { content: ""\f7b3""; }
 .bi-6-circle-fill::before { content: ""\f7b4""; }
 .bi-6-circle::before { content: ""\f7b5""; }
 .bi-6-square-fill::before { content: ""\f7b6""; }
 .bi-6-square::before { content: ""\f7b7""; }
-.bi-7-circle-1::before { content: ""\f7b8""; }
-.bi-7-circle-fill-1::before { content: ""\f7b9""; }
 .bi-7-circle-fill::before { content: ""\f7ba""; }
 .bi-7-circle::before { content: ""\f7bb""; }
 .bi-7-square-fill::before { content: ""\f7bc""; }
 .bi-7-square::before { content: ""\f7bd""; }
-.bi-8-circle-1::before { content: ""\f7be""; }
-.bi-8-circle-fill-1::before { content: ""\f7bf""; }
 .bi-8-circle-fill::before { content: ""\f7c0""; }
 .bi-8-circle::before { content: ""\f7c1""; }
 .bi-8-square-fill::before { content: ""\f7c2""; }
 .bi-8-square::before { content: ""\f7c3""; }
-.bi-9-circle-1::before { content: ""\f7c4""; }
-.bi-9-circle-fill-1::before { content: ""\f7c5""; }
 .bi-9-circle-fill::before { content: ""\f7c6""; }
 .bi-9-circle::before { content: ""\f7c7""; }
 .bi-9-square-fill::before { content: ""\f7c8""; }
@@ -1771,8 +1744,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-browser-edge::before { content: ""\f7d5""; }
 .bi-browser-firefox::before { content: ""\f7d6""; }
 .bi-browser-safari::before { content: ""\f7d7""; }
-.bi-c-circle-1::before { content: ""\f7d8""; }
-.bi-c-circle-fill-1::before { content: ""\f7d9""; }
 .bi-c-circle-fill::before { content: ""\f7da""; }
 .bi-c-circle::before { content: ""\f7db""; }
 .bi-c-square-fill::before { content: ""\f7dc""; }
@@ -1783,8 +1754,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-car-front::before { content: ""\f7e1""; }
 .bi-cassette-fill::before { content: ""\f7e2""; }
 .bi-cassette::before { content: ""\f7e3""; }
-.bi-cc-circle-1::before { content: ""\f7e4""; }
-.bi-cc-circle-fill-1::before { content: ""\f7e5""; }
 .bi-cc-circle-fill::before { content: ""\f7e6""; }
 .bi-cc-circle::before { content: ""\f7e7""; }
 .bi-cc-square-fill::before { content: ""\f7e8""; }
@@ -1803,8 +1772,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-filetype-sql::before { content: ""\f7f5""; }
 .bi-fire::before { content: ""\f7f6""; }
 .bi-google-play::before { content: ""\f7f7""; }
-.bi-h-circle-1::before { content: ""\f7f8""; }
-.bi-h-circle-fill-1::before { content: ""\f7f9""; }
 .bi-h-circle-fill::before { content: ""\f7fa""; }
 .bi-h-circle::before { content: ""\f7fb""; }
 .bi-h-square-fill::before { content: ""\f7fc""; }
@@ -1813,8 +1780,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-lungs-fill::before { content: ""\f7ff""; }
 .bi-lungs::before { content: ""\f800""; }
 .bi-microsoft-teams::before { content: ""\f801""; }
-.bi-p-circle-1::before { content: ""\f802""; }
-.bi-p-circle-fill-1::before { content: ""\f803""; }
 .bi-p-circle-fill::before { content: ""\f804""; }
 .bi-p-circle::before { content: ""\f805""; }
 .bi-p-square-fill::before { content: ""\f806""; }
@@ -1823,8 +1788,6 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-pass::before { content: ""\f809""; }
 .bi-prescription::before { content: ""\f80a""; }
 .bi-prescription2::before { content: ""\f80b""; }
-.bi-r-circle-1::before { content: ""\f80c""; }
-.bi-r-circle-fill-1::before { content: ""\f80d""; }
 .bi-r-circle-fill::before { content: ""\f80e""; }
 .bi-r-circle::before { content: ""\f80f""; }
 .bi-r-square-fill::before { content: ""\f810""; }
@@ -2016,3 +1979,100 @@ url(""./bootstrap-icons.woff?2ab2cbbe07fcebb53bdaa7313bb290f2"") format(""woff"");
 .bi-sina-weibo::before { content: ""\f8ca""; }
 .bi-tencent-qq::before { content: ""\f8cb""; }
 .bi-wikipedia::before { content: ""\f8cc""; }
+.bi-alphabet-uppercase::before { content: ""\f2a5""; }
+.bi-alphabet::before { content: ""\f68a""; }
+.bi-amazon::before { content: ""\f68d""; }
+.bi-arrows-collapse-vertical::before { content: ""\f690""; }
+.bi-arrows-expand-vertical::before { content: ""\f695""; }
+.bi-arrows-vertical::before { content: ""\f698""; }
+.bi-arrows::before { content: ""\f6a2""; }
+.bi-ban-fill::before { content: ""\f6a3""; }
+.bi-ban::before { content: ""\f6b6""; }
+.bi-bing::before { content: ""\f6c2""; }
+.bi-cake::before { content: ""\f6e0""; }
+.bi-cake2::before { content: ""\f6ed""; }
+.bi-cookie::before { content: ""\f6ee""; }
+.bi-copy::before { content: ""\f759""; }
+.bi-crosshair::before { content: ""\f769""; }
+.bi-crosshair2::before { content: ""\f794""; }
+.bi-emoji-astonished-fill::before { content: ""\f795""; }
+.bi-emoji-astonished::before { content: ""\f79a""; }
+.bi-emoji-grimace-fill::before { content: ""\f79b""; }
+.bi-emoji-grimace::before { content: ""\f7a0""; }
+.bi-emoji-grin-fill::before { content: ""\f7a1""; }
+.bi-emoji-grin::before { content: ""\f7a6""; }
+.bi-emoji-surprise-fill::before { content: ""\f7a7""; }
+.bi-emoji-surprise::before { content: ""\f7ac""; }
+.bi-emoji-tear-fill::before { content: ""\f7ad""; }
+.bi-emoji-tear::before { content: ""\f7b2""; }
+.bi-envelope-arrow-down-fill::before { content: ""\f7b3""; }
+.bi-envelope-arrow-down::before { content: ""\f7b8""; }
+.bi-envelope-arrow-up-fill::before { content: ""\f7b9""; }
+.bi-envelope-arrow-up::before { content: ""\f7be""; }
+.bi-feather::before { content: ""\f7bf""; }
+.bi-feather2::before { content: ""\f7c4""; }
+.bi-floppy-fill::before { content: ""\f7c5""; }
+.bi-floppy::before { content: ""\f7d8""; }
+.bi-floppy2-fill::before { content: ""\f7d9""; }
+.bi-floppy2::before { content: ""\f7e4""; }
+.bi-gitlab::before { content: ""\f7e5""; }
+.bi-highlighter::before { content: ""\f7f8""; }
+.bi-marker-tip::before { content: ""\f802""; }
+.bi-nvme-fill::before { content: ""\f803""; }
+.bi-nvme::before { content: ""\f80c""; }
+.bi-opencollective::before { content: ""\f80d""; }
+.bi-pci-card-network::before { content: ""\f8cd""; }
+.bi-pci-card-sound::before { content: ""\f8ce""; }
+.bi-radar::before { content: ""\f8cf""; }
+.bi-send-arrow-down-fill::before { content: ""\f8d0""; }
+.bi-send-arrow-down::before { content: ""\f8d1""; }
+.bi-send-arrow-up-fill::before { content: ""\f8d2""; }
+.bi-send-arrow-up::before { content: ""\f8d3""; }
+.bi-sim-slash-fill::before { content: ""\f8d4""; }
+.bi-sim-slash::before { content: ""\f8d5""; }
+.bi-sourceforge::before { content: ""\f8d6""; }
+.bi-substack::before { content: ""\f8d7""; }
+.bi-threads-fill::before { content: ""\f8d8""; }
+.bi-threads::before { content: ""\f8d9""; }
+.bi-transparency::before { content: ""\f8da""; }
+.bi-twitter-x::before { content: ""\f8db""; }
+.bi-type-h4::before { content: ""\f8dc""; }
+.bi-type-h5::before { content: ""\f8dd""; }
+.bi-type-h6::before { content: ""\f8de""; }
+.bi-backpack-fill::before { content: ""\f8df""; }
+.bi-backpack::before { content: ""\f8e0""; }
+.bi-backpack2-fill::before { content: ""\f8e1""; }
+.bi-backpack2::before { content: ""\f8e2""; }
+.bi-backpack3-fill::before { content: ""\f8e3""; }
+.bi-backpack3::before { content: ""\f8e4""; }
+.bi-backpack4-fill::before { content: ""\f8e5""; }
+.bi-backpack4::before { content: ""\f8e6""; }
+.bi-brilliance::before { content: ""\f8e7""; }
+.bi-cake-fill::before { content: ""\f8e8""; }
+.bi-cake2-fill::before { content: ""\f8e9""; }
+.bi-duffle-fill::before { content: ""\f8ea""; }
+.bi-duffle::before { content: ""\f8eb""; }
+.bi-exposure::before { content: ""\f8ec""; }
+.bi-gender-neuter::before { content: ""\f8ed""; }
+.bi-highlights::before { content: ""\f8ee""; }
+.bi-luggage-fill::before { content: ""\f8ef""; }
+.bi-luggage::before { content: ""\f8f0""; }
+.bi-mailbox-flag::before { content: ""\f8f1""; }
+.bi-mailbox2-flag::before { content: ""\f8f2""; }
+.bi-noise-reduction::before { content: ""\f8f3""; }
+.bi-passport-fill::before { content: ""\f8f4""; }
+.bi-passport::before { content: ""\f8f5""; }
+.bi-person-arms-up::before { content: ""\f8f6""; }
+.bi-person-raised-hand::before { content: ""\f8f7""; }
+.bi-person-standing-dress::before { content: ""\f8f8""; }
+.bi-person-standing::before { content: ""\f8f9""; }
+.bi-person-walking::before { content: ""\f8fa""; }
+.bi-person-wheelchair::before { content: ""\f8fb""; }
+.bi-shadows::before { content: ""\f8fc""; }
+.bi-suitcase-fill::before { content: ""\f8fd""; }
+.bi-suitcase-lg-fill::before { content: ""\f8fe""; }
+.bi-suitcase-lg::before { content: ""\f8ff""; }
+.bi-suitcase::before { content: ""\f900""; }
+.bi-suitcase2-fill::before { content: ""\f901""; }
+.bi-suitcase2::before { content: ""\f902""; }
+.bi-vignette::before { content: ""\f903""; }

---FILE: session-probability/docs/site_libs/quarto-html/anchor.min.js---
@@ -1,9 +1,9 @@
 // @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&dn=expat.txt Expat
 //
-// AnchorJS - v4.3.1 - 2021-04-17
+// AnchorJS - v5.0.0 - 2023-01-18
 // https://www.bryanbraun.com/anchorjs/
-// Copyright (c) 2021 Bryan Braun; Licensed MIT
+// Copyright (c) 2023 Bryan Braun; Licensed MIT
 //
 // @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&dn=expat.txt Expat
-!function(A,e){""use strict"";""function""==typeof define&&define.amd?define([],e):""object""==typeof module&&module.exports?module.exports=e():(A.AnchorJS=e(),A.anchors=new A.AnchorJS)}(this,function(){""use strict"";return function(A){function d(A){A.icon=Object.prototype.hasOwnProperty.call(A,""icon"")?A.icon:"""",A.visible=Object.prototype.hasOwnProperty.call(A,""visible"")?A.visible:""hover"",A.placement=Object.prototype.hasOwnProperty.call(A,""placement"")?A.placement:""right"",A.ariaLabel=Object.prototype.hasOwnProperty.call(A,""ariaLabel"")?A.ariaLabel:""Anchor"",A.class=Object.prototype.hasOwnProperty.call(A,""class"")?A.class:"""",A.base=Object.prototype.hasOwnProperty.call(A,""base"")?A.base:"""",A.truncate=Object.prototype.hasOwnProperty.call(A,""truncate"")?Math.floor(A.truncate):64,A.titleText=Object.prototype.hasOwnProperty.call(A,""titleText"")?A.titleText:""""}function w(A){var e;if(""string""==typeof A||A instanceof String)e=[].slice.call(document.querySelectorAll(A));else{if(!(Array.isArray(A)||A instanceof NodeList))throw new TypeError(""The selector provided to AnchorJS was invalid."");e=[].slice.call(A)}return e}this.options=A||{},this.elements=[],d(this.options),this.isTouchDevice=function(){return Boolean(""ontouchstart""in window||window.TouchEvent||window.DocumentTouch&&document instanceof DocumentTouch)},this.add=function(A){var e,t,o,i,n,s,a,c,r,l,h,u,p=[];if(d(this.options),""touch""===(l=this.options.visible)&&(l=this.isTouchDevice()?""always"":""hover""),0===(e=w(A=A||""h2, h3, h4, h5, h6"")).length)return this;for(null===document.head.querySelector(""style.anchorjs"")&&((u=document.createElement(""style"")).className=""anchorjs"",u.appendChild(document.createTextNode("""")),void 0===(A=document.head.querySelector('[rel=""stylesheet""],style'))?document.head.appendChild(u):document.head.insertBefore(u,A),u.sheet.insertRule("".anchorjs-link{opacity:0;text-decoration:none;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}"",u.sheet.cssRules.length),u.sheet.insertRule("":hover>.anchorjs-link,.anchorjs-link:focus{opacity:1}"",u.sheet.cssRules.length),u.sheet.insertRule(""[data-anchorjs-icon]::after{content:attr(data-anchorjs-icon)}"",u.sheet.cssRules.length),u.sheet.insertRule('@font-face{font-family:anchorjs-icons;src:url(data:n/a;base64,AAEAAAALAIAAAwAwT1MvMg8yG2cAAAE4AAAAYGNtYXDp3gC3AAABpAAAAExnYXNwAAAAEAAAA9wAAAAIZ2x5ZlQCcfwAAAH4AAABCGhlYWQHFvHyAAAAvAAAADZoaGVhBnACFwAAAPQAAAAkaG10eASAADEAAAGYAAAADGxvY2EACACEAAAB8AAAAAhtYXhwAAYAVwAAARgAAAAgbmFtZQGOH9cAAAMAAAAAunBvc3QAAwAAAAADvAAAACAAAQAAAAEAAHzE2p9fDzz1AAkEAAAAAADRecUWAAAAANQA6R8AAAAAAoACwAAAAAgAAgAAAAAAAAABAAADwP/AAAACgAAA/9MCrQABAAAAAAAAAAAAAAAAAAAAAwABAAAAAwBVAAIAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAMCQAGQAAUAAAKZAswAAACPApkCzAAAAesAMwEJAAAAAAAAAAAAAAAAAAAAARAAAAAAAAAAAAAAAAAAAAAAQAAg//0DwP/AAEADwABAAAAAAQAAAAAAAAAAAAAAIAAAAAAAAAIAAAACgAAxAAAAAwAAAAMAAAAcAAEAAwAAABwAAwABAAAAHAAEADAAAAAIAAgAAgAAACDpy//9//8AAAAg6cv//f///+EWNwADAAEAAAAAAAAAAAAAAAAACACEAAEAAAAAAAAAAAAAAAAxAAACAAQARAKAAsAAKwBUAAABIiYnJjQ3NzY2MzIWFxYUBwcGIicmNDc3NjQnJiYjIgYHBwYUFxYUBwYGIwciJicmNDc3NjIXFhQHBwYUFxYWMzI2Nzc2NCcmNDc2MhcWFAcHBgYjARQGDAUtLXoWOR8fORYtLTgKGwoKCjgaGg0gEhIgDXoaGgkJBQwHdR85Fi0tOAobCgoKOBoaDSASEiANehoaCQkKGwotLXoWOR8BMwUFLYEuehYXFxYugC44CQkKGwo4GkoaDQ0NDXoaShoKGwoFBe8XFi6ALjgJCQobCjgaShoNDQ0NehpKGgobCgoKLYEuehYXAAAADACWAAEAAAAAAAEACAAAAAEAAAAAAAIAAwAIAAEAAAAAAAMACAAAAAEAAAAAAAQACAAAAAEAAAAAAAUAAQALAAEAAAAAAAYACAAAAAMAAQQJAAEAEAAMAAMAAQQJAAIABgAcAAMAAQQJAAMAEAAMAAMAAQQJAAQAEAAMAAMAAQQJAAUAAgAiAAMAAQQJAAYAEAAMYW5jaG9yanM0MDBAAGEAbgBjAGgAbwByAGoAcwA0ADAAMABAAAAAAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAH//wAP) format(""truetype"")}',u.sheet.cssRules.length)),u=document.querySelectorAll(""[id]""),t=[].map.call(u,function(A){return A.id}),i=0;i<e.length;i++)if(this.hasAnchorJSLink(e[i]))p.push(i);else{if(e[i].hasAttribute(""id""))o=e[i].getAttribute(""id"");else if(e[i].hasAttribute(""data-anchor-id""))o=e[i].getAttribute(""data-anchor-id"");else{for(c=a=this.urlify(e[i].textContent),s=0;n=t.indexOf(c=void 0!==n?a+""-""+s:c),s+=1,-1!==n;);n=void 0,t.push(c),e[i].setAttribute(""id"",c),o=c}(r=document.createElement(""a"")).className=""anchorjs-link ""+this.options.class,r.setAttribute(""aria-label"",this.options.ariaLabel),r.setAttribute(""data-anchorjs-icon"",this.options.icon),this.options.titleText&&(r.title=this.options.titleText),h=document.querySelector(""base"")?window.location.pathname+window.location.search:"""",h=this.options.base||h,r.href=h+""#""+o,""always""===l&&(r.style.opacity=""1""),""""===this.options.icon&&(r.style.font=""1em/1 anchorjs-icons"",""left""===this.options.placement&&(r.style.lineHeight=""inherit"")),""left""===this.options.placement?(r.style.position=""absolute"",r.style.marginLeft=""-1em"",r.style.paddingRight="".5em"",e[i].insertBefore(r,e[i].firstChild)):(r.style.paddingLeft="".375em"",e[i].appendChild(r))}for(i=0;i<p.length;i++)e.splice(p[i]-i,1);return this.elements=this.elements.concat(e),this},this.remove=function(A){for(var e,t,o=w(A),i=0;i<o.length;i++)(t=o[i].querySelector("".anchorjs-link""))&&(-1!==(e=this.elements.indexOf(o[i]))&&this.elements.splice(e,1),o[i].removeChild(t));return this},this.removeAll=function(){this.remove(this.elements)},this.urlify=function(A){var e=document.createElement(""textarea"");return e.innerHTML=A,A=e.value,this.options.truncate||d(this.options),A.trim().replace(/'/gi,"""").replace(/[& +$,:;=?@""#{}|^~[`%!'<>\]./()*\\\n\t\b\v\u00A0]/g,""-"").replace(/-{2,}/g,""-"").substring(0,this.options.truncate).replace(/^-+|-+$/gm,"""").toLowerCase()},this.hasAnchorJSLink=function(A){var e=A.firstChild&&-1<("" ""+A.firstChild.className+"" "").indexOf("" anchorjs-link ""),A=A.lastChild&&-1<("" ""+A.lastChild.className+"" "").indexOf("" anchorjs-link "");return e||A||!1}}});
+!function(A,e){""use strict"";""function""==typeof define&&define.amd?define([],e):""object""==typeof module&&module.exports?module.exports=e():(A.AnchorJS=e(),A.anchors=new A.AnchorJS)}(globalThis,function(){""use strict"";return function(A){function u(A){A.icon=Object.prototype.hasOwnProperty.call(A,""icon"")?A.icon:"""",A.visible=Object.prototype.hasOwnProperty.call(A,""visible"")?A.visible:""hover"",A.placement=Object.prototype.hasOwnProperty.call(A,""placement"")?A.placement:""right"",A.ariaLabel=Object.prototype.hasOwnProperty.call(A,""ariaLabel"")?A.ariaLabel:""Anchor"",A.class=Object.prototype.hasOwnProperty.call(A,""class"")?A.class:"""",A.base=Object.prototype.hasOwnProperty.call(A,""base"")?A.base:"""",A.truncate=Object.prototype.hasOwnProperty.call(A,""truncate"")?Math.floor(A.truncate):64,A.titleText=Object.prototype.hasOwnProperty.call(A,""titleText"")?A.titleText:""""}function d(A){var e;if(""string""==typeof A||A instanceof String)e=[].slice.call(document.querySelectorAll(A));else{if(!(Array.isArray(A)||A instanceof NodeList))throw new TypeError(""The selector provided to AnchorJS was invalid."");e=[].slice.call(A)}return e}this.options=A||{},this.elements=[],u(this.options),this.add=function(A){var e,t,o,i,n,s,a,r,l,c,h,p=[];if(u(this.options),0!==(e=d(A=A||""h2, h3, h4, h5, h6"")).length){for(null===document.head.querySelector(""style.anchorjs"")&&((A=document.createElement(""style"")).className=""anchorjs"",A.appendChild(document.createTextNode("""")),void 0===(h=document.head.querySelector('[rel=""stylesheet""],style'))?document.head.appendChild(A):document.head.insertBefore(A,h),A.sheet.insertRule("".anchorjs-link{opacity:0;text-decoration:none;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}"",A.sheet.cssRules.length),A.sheet.insertRule("":hover>.anchorjs-link,.anchorjs-link:focus{opacity:1}"",A.sheet.cssRules.length),A.sheet.insertRule(""[data-anchorjs-icon]::after{content:attr(data-anchorjs-icon)}"",A.sheet.cssRules.length),A.sheet.insertRule('@font-face{font-family:anchorjs-icons;src:url(data:n/a;base64,AAEAAAALAIAAAwAwT1MvMg8yG2cAAAE4AAAAYGNtYXDp3gC3AAABpAAAAExnYXNwAAAAEAAAA9wAAAAIZ2x5ZlQCcfwAAAH4AAABCGhlYWQHFvHyAAAAvAAAADZoaGVhBnACFwAAAPQAAAAkaG10eASAADEAAAGYAAAADGxvY2EACACEAAAB8AAAAAhtYXhwAAYAVwAAARgAAAAgbmFtZQGOH9cAAAMAAAAAunBvc3QAAwAAAAADvAAAACAAAQAAAAEAAHzE2p9fDzz1AAkEAAAAAADRecUWAAAAANQA6R8AAAAAAoACwAAAAAgAAgAAAAAAAAABAAADwP/AAAACgAAA/9MCrQABAAAAAAAAAAAAAAAAAAAAAwABAAAAAwBVAAIAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAMCQAGQAAUAAAKZAswAAACPApkCzAAAAesAMwEJAAAAAAAAAAAAAAAAAAAAARAAAAAAAAAAAAAAAAAAAAAAQAAg//0DwP/AAEADwABAAAAAAQAAAAAAAAAAAAAAIAAAAAAAAAIAAAACgAAxAAAAAwAAAAMAAAAcAAEAAwAAABwAAwABAAAAHAAEADAAAAAIAAgAAgAAACDpy//9//8AAAAg6cv//f///+EWNwADAAEAAAAAAAAAAAAAAAAACACEAAEAAAAAAAAAAAAAAAAxAAACAAQARAKAAsAAKwBUAAABIiYnJjQ3NzY2MzIWFxYUBwcGIicmNDc3NjQnJiYjIgYHBwYUFxYUBwYGIwciJicmNDc3NjIXFhQHBwYUFxYWMzI2Nzc2NCcmNDc2MhcWFAcHBgYjARQGDAUtLXoWOR8fORYtLTgKGwoKCjgaGg0gEhIgDXoaGgkJBQwHdR85Fi0tOAobCgoKOBoaDSASEiANehoaCQkKGwotLXoWOR8BMwUFLYEuehYXFxYugC44CQkKGwo4GkoaDQ0NDXoaShoKGwoFBe8XFi6ALjgJCQobCjgaShoNDQ0NehpKGgobCgoKLYEuehYXAAAADACWAAEAAAAAAAEACAAAAAEAAAAAAAIAAwAIAAEAAAAAAAMACAAAAAEAAAAAAAQACAAAAAEAAAAAAAUAAQALAAEAAAAAAAYACAAAAAMAAQQJAAEAEAAMAAMAAQQJAAIABgAcAAMAAQQJAAMAEAAMAAMAAQQJAAQAEAAMAAMAAQQJAAUAAgAiAAMAAQQJAAYAEAAMYW5jaG9yanM0MDBAAGEAbgBjAGgAbwByAGoAcwA0ADAAMABAAAAAAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAH//wAP) format(""truetype"")}',A.sheet.cssRules.length)),h=document.querySelectorAll(""[id]""),t=[].map.call(h,function(A){return A.id}),i=0;i<e.length;i++)if(this.hasAnchorJSLink(e[i]))p.push(i);else{if(e[i].hasAttribute(""id""))o=e[i].getAttribute(""id"");else if(e[i].hasAttribute(""data-anchor-id""))o=e[i].getAttribute(""data-anchor-id"");else{for(r=a=this.urlify(e[i].textContent),s=0;n=t.indexOf(r=void 0!==n?a+""-""+s:r),s+=1,-1!==n;);n=void 0,t.push(r),e[i].setAttribute(""id"",r),o=r}(l=document.createElement(""a"")).className=""anchorjs-link ""+this.options.class,l.setAttribute(""aria-label"",this.options.ariaLabel),l.setAttribute(""data-anchorjs-icon"",this.options.icon),this.options.titleText&&(l.title=this.options.titleText),c=document.querySelector(""base"")?window.location.pathname+window.location.search:"""",c=this.options.base||c,l.href=c+""#""+o,""always""===this.options.visible&&(l.style.opacity=""1""),""""===this.options.icon&&(l.style.font=""1em/1 anchorjs-icons"",""left""===this.options.placement)&&(l.style.lineHeight=""inherit""),""left""===this.options.placement?(l.style.position=""absolute"",l.style.marginLeft=""-1.25em"",l.style.paddingRight="".25em"",l.style.paddingLeft="".25em"",e[i].insertBefore(l,e[i].firstChild)):(l.style.marginLeft="".1875em"",l.style.paddingRight="".1875em"",l.style.paddingLeft="".1875em"",e[i].appendChild(l))}for(i=0;i<p.length;i++)e.splice(p[i]-i,1);this.elements=this.elements.concat(e)}return this},this.remove=function(A){for(var e,t,o=d(A),i=0;i<o.length;i++)(t=o[i].querySelector("".anchorjs-link""))&&(-1!==(e=this.elements.indexOf(o[i]))&&this.elements.splice(e,1),o[i].removeChild(t));return this},this.removeAll=function(){this.remove(this.elements)},this.urlify=function(A){var e=document.createElement(""textarea"");return e.innerHTML=A,A=e.value,this.options.truncate||u(this.options),A.trim().replace(/'/gi,"""").replace(/[& +$,:;=?@""#{}|^~[`%!'<>\]./()*\\\n\t\b\v\u00A0]/g,""-"").replace(/-{2,}/g,""-"").substring(0,this.options.truncate).replace(/^-+|-+$/gm,"""").toLowerCase()},this.hasAnchorJSLink=function(A){var e=A.firstChild&&-1<("" ""+A.firstChild.className+"" "").indexOf("" anchorjs-link ""),A=A.lastChild&&-1<("" ""+A.lastChild.className+"" "").indexOf("" anchorjs-link "");return e||A||!1}}});
 // @license-end
\ No newline at end of file

---FILE: session-probability/docs/site_libs/quarto-html/popper.min.js---
@@ -1,6 +1,6 @@
 /**
- * @popperjs/core v2.11.4 - MIT License
+ * @popperjs/core v2.11.7 - MIT License
  */
 
-!function(e,t){""object""==typeof exports&&""undefined""!=typeof module?t(exports):""function""==typeof define&&define.amd?define([""exports""],t):t((e=""undefined""!=typeof globalThis?globalThis:e||self).Popper={})}(this,(function(e){""use strict"";function t(e){if(null==e)return window;if(""[object Window]""!==e.toString()){var t=e.ownerDocument;return t&&t.defaultView||window}return e}function n(e){return e instanceof t(e).Element||e instanceof Element}function r(e){return e instanceof t(e).HTMLElement||e instanceof HTMLElement}function o(e){return""undefined""!=typeof ShadowRoot&&(e instanceof t(e).ShadowRoot||e instanceof ShadowRoot)}var i=Math.max,a=Math.min,s=Math.round;function f(e,t){void 0===t&&(t=!1);var n=e.getBoundingClientRect(),o=1,i=1;if(r(e)&&t){var a=e.offsetHeight,f=e.offsetWidth;f>0&&(o=s(n.width)/f||1),a>0&&(i=s(n.height)/a||1)}return{width:n.width/o,height:n.height/i,top:n.top/i,right:n.right/o,bottom:n.bottom/i,left:n.left/o,x:n.left/o,y:n.top/i}}function c(e){var n=t(e);return{scrollLeft:n.pageXOffset,scrollTop:n.pageYOffset}}function p(e){return e?(e.nodeName||"""").toLowerCase():null}function u(e){return((n(e)?e.ownerDocument:e.document)||window.document).documentElement}function l(e){return f(u(e)).left+c(e).scrollLeft}function d(e){return t(e).getComputedStyle(e)}function h(e){var t=d(e),n=t.overflow,r=t.overflowX,o=t.overflowY;return/auto|scroll|overlay|hidden/.test(n+o+r)}function m(e,n,o){void 0===o&&(o=!1);var i,a,d=r(n),m=r(n)&&function(e){var t=e.getBoundingClientRect(),n=s(t.width)/e.offsetWidth||1,r=s(t.height)/e.offsetHeight||1;return 1!==n||1!==r}(n),v=u(n),g=f(e,m),y={scrollLeft:0,scrollTop:0},b={x:0,y:0};return(d||!d&&!o)&&((""body""!==p(n)||h(v))&&(y=(i=n)!==t(i)&&r(i)?{scrollLeft:(a=i).scrollLeft,scrollTop:a.scrollTop}:c(i)),r(n)?((b=f(n,!0)).x+=n.clientLeft,b.y+=n.clientTop):v&&(b.x=l(v))),{x:g.left+y.scrollLeft-b.x,y:g.top+y.scrollTop-b.y,width:g.width,height:g.height}}function v(e){var t=f(e),n=e.offsetWidth,r=e.offsetHeight;return Math.abs(t.width-n)<=1&&(n=t.width),Math.abs(t.height-r)<=1&&(r=t.height),{x:e.offsetLeft,y:e.offsetTop,width:n,height:r}}function g(e){return""html""===p(e)?e:e.assignedSlot||e.parentNode||(o(e)?e.host:null)||u(e)}function y(e){return[""html"",""body"",""#document""].indexOf(p(e))>=0?e.ownerDocument.body:r(e)&&h(e)?e:y(g(e))}function b(e,n){var r;void 0===n&&(n=[]);var o=y(e),i=o===(null==(r=e.ownerDocument)?void 0:r.body),a=t(o),s=i?[a].concat(a.visualViewport||[],h(o)?o:[]):o,f=n.concat(s);return i?f:f.concat(b(g(s)))}function x(e){return[""table"",""td"",""th""].indexOf(p(e))>=0}function w(e){return r(e)&&""fixed""!==d(e).position?e.offsetParent:null}function O(e){for(var n=t(e),i=w(e);i&&x(i)&&""static""===d(i).position;)i=w(i);return i&&(""html""===p(i)||""body""===p(i)&&""static""===d(i).position)?n:i||function(e){var t=-1!==navigator.userAgent.toLowerCase().indexOf(""firefox"");if(-1!==navigator.userAgent.indexOf(""Trident"")&&r(e)&&""fixed""===d(e).position)return null;var n=g(e);for(o(n)&&(n=n.host);r(n)&&[""html"",""body""].indexOf(p(n))<0;){var i=d(n);if(""none""!==i.transform||""none""!==i.perspective||""paint""===i.contain||-1!==[""transform"",""perspective""].indexOf(i.willChange)||t&&""filter""===i.willChange||t&&i.filter&&""none""!==i.filter)return n;n=n.parentNode}return null}(e)||n}var j=""top"",E=""bottom"",D=""right"",A=""left"",L=""auto"",P=[j,E,D,A],M=""start"",k=""end"",W=""viewport"",B=""popper"",H=P.reduce((function(e,t){return e.concat([t+""-""+M,t+""-""+k])}),[]),T=[].concat(P,[L]).reduce((function(e,t){return e.concat([t,t+""-""+M,t+""-""+k])}),[]),R=[""beforeRead"",""read"",""afterRead"",""beforeMain"",""main"",""afterMain"",""beforeWrite"",""write"",""afterWrite""];function S(e){var t=new Map,n=new Set,r=[];function o(e){n.add(e.name),[].concat(e.requires||[],e.requiresIfExists||[]).forEach((function(e){if(!n.has(e)){var r=t.get(e);r&&o(r)}})),r.push(e)}return e.forEach((function(e){t.set(e.name,e)})),e.forEach((function(e){n.has(e.name)||o(e)})),r}function C(e){return e.split(""-"")[0]}function q(e,t){var n=t.getRootNode&&t.getRootNode();if(e.contains(t))return!0;if(n&&o(n)){var r=t;do{if(r&&e.isSameNode(r))return!0;r=r.parentNode||r.host}while(r)}return!1}function V(e){return Object.assign({},e,{left:e.x,top:e.y,right:e.x+e.width,bottom:e.y+e.height})}function N(e,r){return r===W?V(function(e){var n=t(e),r=u(e),o=n.visualViewport,i=r.clientWidth,a=r.clientHeight,s=0,f=0;return o&&(i=o.width,a=o.height,/^((?!chrome|android).)*safari/i.test(navigator.userAgent)||(s=o.offsetLeft,f=o.offsetTop)),{width:i,height:a,x:s+l(e),y:f}}(e)):n(r)?function(e){var t=f(e);return t.top=t.top+e.clientTop,t.left=t.left+e.clientLeft,t.bottom=t.top+e.clientHeight,t.right=t.left+e.clientWidth,t.width=e.clientWidth,t.height=e.clientHeight,t.x=t.left,t.y=t.top,t}(r):V(function(e){var t,n=u(e),r=c(e),o=null==(t=e.ownerDocument)?void 0:t.body,a=i(n.scrollWidth,n.clientWidth,o?o.scrollWidth:0,o?o.clientWidth:0),s=i(n.scrollHeight,n.clientHeight,o?o.scrollHeight:0,o?o.clientHeight:0),f=-r.scrollLeft+l(e),p=-r.scrollTop;return""rtl""===d(o||n).direction&&(f+=i(n.clientWidth,o?o.clientWidth:0)-a),{width:a,height:s,x:f,y:p}}(u(e)))}function I(e,t,o){var s=""clippingParents""===t?function(e){var t=b(g(e)),o=[""absolute"",""fixed""].indexOf(d(e).position)>=0&&r(e)?O(e):e;return n(o)?t.filter((function(e){return n(e)&&q(e,o)&&""body""!==p(e)})):[]}(e):[].concat(t),f=[].concat(s,[o]),c=f[0],u=f.reduce((function(t,n){var r=N(e,n);return t.top=i(r.top,t.top),t.right=a(r.right,t.right),t.bottom=a(r.bottom,t.bottom),t.left=i(r.left,t.left),t}),N(e,c));return u.width=u.right-u.left,u.height=u.bottom-u.top,u.x=u.left,u.y=u.top,u}function _(e){return e.split(""-"")[1]}function F(e){return[""top"",""bottom""].indexOf(e)>=0?""x"":""y""}function U(e){var t,n=e.reference,r=e.element,o=e.placement,i=o?C(o):null,a=o?_(o):null,s=n.x+n.width/2-r.width/2,f=n.y+n.height/2-r.height/2;switch(i){case j:t={x:s,y:n.y-r.height};break;case E:t={x:s,y:n.y+n.height};break;case D:t={x:n.x+n.width,y:f};break;case A:t={x:n.x-r.width,y:f};break;default:t={x:n.x,y:n.y}}var c=i?F(i):null;if(null!=c){var p=""y""===c?""height"":""width"";switch(a){case M:t[c]=t[c]-(n[p]/2-r[p]/2);break;case k:t[c]=t[c]+(n[p]/2-r[p]/2)}}return t}function z(e){return Object.assign({},{top:0,right:0,bottom:0,left:0},e)}function X(e,t){return t.reduce((function(t,n){return t[n]=e,t}),{})}function Y(e,t){void 0===t&&(t={});var r=t,o=r.placement,i=void 0===o?e.placement:o,a=r.boundary,s=void 0===a?""clippingParents"":a,c=r.rootBoundary,p=void 0===c?W:c,l=r.elementContext,d=void 0===l?B:l,h=r.altBoundary,m=void 0!==h&&h,v=r.padding,g=void 0===v?0:v,y=z(""number""!=typeof g?g:X(g,P)),b=d===B?""reference"":B,x=e.rects.popper,w=e.elements[m?b:d],O=I(n(w)?w:w.contextElement||u(e.elements.popper),s,p),A=f(e.elements.reference),L=U({reference:A,element:x,strategy:""absolute"",placement:i}),M=V(Object.assign({},x,L)),k=d===B?M:A,H={top:O.top-k.top+y.top,bottom:k.bottom-O.bottom+y.bottom,left:O.left-k.left+y.left,right:k.right-O.right+y.right},T=e.modifiersData.offset;if(d===B&&T){var R=T[i];Object.keys(H).forEach((function(e){var t=[D,E].indexOf(e)>=0?1:-1,n=[j,E].indexOf(e)>=0?""y"":""x"";H[e]+=R[n]*t}))}return H}var G={placement:""bottom"",modifiers:[],strategy:""absolute""};function J(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return!t.some((function(e){return!(e&&""function""==typeof e.getBoundingClientRect)}))}function K(e){void 0===e&&(e={});var t=e,r=t.defaultModifiers,o=void 0===r?[]:r,i=t.defaultOptions,a=void 0===i?G:i;return function(e,t,r){void 0===r&&(r=a);var i,s,f={placement:""bottom"",orderedModifiers:[],options:Object.assign({},G,a),modifiersData:{},elements:{reference:e,popper:t},attributes:{},styles:{}},c=[],p=!1,u={state:f,setOptions:function(r){var i=""function""==typeof r?r(f.options):r;l(),f.options=Object.assign({},a,f.options,i),f.scrollParents={reference:n(e)?b(e):e.contextElement?b(e.contextElement):[],popper:b(t)};var s,p,d=function(e){var t=S(e);return R.reduce((function(e,n){return e.concat(t.filter((function(e){return e.phase===n})))}),[])}((s=[].concat(o,f.options.modifiers),p=s.reduce((function(e,t){var n=e[t.name];return e[t.name]=n?Object.assign({},n,t,{options:Object.assign({},n.options,t.options),data:Object.assign({},n.data,t.data)}):t,e}),{}),Object.keys(p).map((function(e){return p[e]}))));return f.orderedModifiers=d.filter((function(e){return e.enabled})),f.orderedModifiers.forEach((function(e){var t=e.name,n=e.options,r=void 0===n?{}:n,o=e.effect;if(""function""==typeof o){var i=o({state:f,name:t,instance:u,options:r}),a=function(){};c.push(i||a)}})),u.update()},forceUpdate:function(){if(!p){var e=f.elements,t=e.reference,n=e.popper;if(J(t,n)){f.rects={reference:m(t,O(n),""fixed""===f.options.strategy),popper:v(n)},f.reset=!1,f.placement=f.options.placement,f.orderedModifiers.forEach((function(e){return f.modifiersData[e.name]=Object.assign({},e.data)}));for(var r=0;r<f.orderedModifiers.length;r++)if(!0!==f.reset){var o=f.orderedModifiers[r],i=o.fn,a=o.options,s=void 0===a?{}:a,c=o.name;""function""==typeof i&&(f=i({state:f,options:s,name:c,instance:u})||f)}else f.reset=!1,r=-1}}},update:(i=function(){return new Promise((function(e){u.forceUpdate(),e(f)}))},function(){return s||(s=new Promise((function(e){Promise.resolve().then((function(){s=void 0,e(i())}))}))),s}),destroy:function(){l(),p=!0}};if(!J(e,t))return u;function l(){c.forEach((function(e){return e()})),c=[]}return u.setOptions(r).then((function(e){!p&&r.onFirstUpdate&&r.onFirstUpdate(e)})),u}}var Q={passive:!0};var Z={name:""eventListeners"",enabled:!0,phase:""write"",fn:function(){},effect:function(e){var n=e.state,r=e.instance,o=e.options,i=o.scroll,a=void 0===i||i,s=o.resize,f=void 0===s||s,c=t(n.elements.popper),p=[].concat(n.scrollParents.reference,n.scrollParents.popper);return a&&p.forEach((function(e){e.addEventListener(""scroll"",r.update,Q)})),f&&c.addEventListener(""resize"",r.update,Q),function(){a&&p.forEach((function(e){e.removeEventListener(""scroll"",r.update,Q)})),f&&c.removeEventListener(""resize"",r.update,Q)}},data:{}};var $={name:""popperOffsets"",enabled:!0,phase:""read"",fn:function(e){var t=e.state,n=e.name;t.modifiersData[n]=U({reference:t.rects.reference,element:t.rects.popper,strategy:""absolute"",placement:t.placement})},data:{}},ee={top:""auto"",right:""auto"",bottom:""auto"",left:""auto""};function te(e){var n,r=e.popper,o=e.popperRect,i=e.placement,a=e.variation,f=e.offsets,c=e.position,p=e.gpuAcceleration,l=e.adaptive,h=e.roundOffsets,m=e.isFixed,v=f.x,g=void 0===v?0:v,y=f.y,b=void 0===y?0:y,x=""function""==typeof h?h({x:g,y:b}):{x:g,y:b};g=x.x,b=x.y;var w=f.hasOwnProperty(""x""),L=f.hasOwnProperty(""y""),P=A,M=j,W=window;if(l){var B=O(r),H=""clientHeight"",T=""clientWidth"";if(B===t(r)&&""static""!==d(B=u(r)).position&&""absolute""===c&&(H=""scrollHeight"",T=""scrollWidth""),B=B,i===j||(i===A||i===D)&&a===k)M=E,b-=(m&&B===W&&W.visualViewport?W.visualViewport.height:B[H])-o.height,b*=p?1:-1;if(i===A||(i===j||i===E)&&a===k)P=D,g-=(m&&B===W&&W.visualViewport?W.visualViewport.width:B[T])-o.width,g*=p?1:-1}var R,S=Object.assign({position:c},l&&ee),C=!0===h?function(e){var t=e.x,n=e.y,r=window.devicePixelRatio||1;return{x:s(t*r)/r||0,y:s(n*r)/r||0}}({x:g,y:b}):{x:g,y:b};return g=C.x,b=C.y,p?Object.assign({},S,((R={})[M]=L?""0"":"""",R[P]=w?""0"":"""",R.transform=(W.devicePixelRatio||1)<=1?""translate(""+g+""px, ""+b+""px)"":""translate3d(""+g+""px, ""+b+""px, 0)"",R)):Object.assign({},S,((n={})[M]=L?b+""px"":"""",n[P]=w?g+""px"":"""",n.transform="""",n))}var ne={name:""computeStyles"",enabled:!0,phase:""beforeWrite"",fn:function(e){var t=e.state,n=e.options,r=n.gpuAcceleration,o=void 0===r||r,i=n.adaptive,a=void 0===i||i,s=n.roundOffsets,f=void 0===s||s,c={placement:C(t.placement),variation:_(t.placement),popper:t.elements.popper,popperRect:t.rects.popper,gpuAcceleration:o,isFixed:""fixed""===t.options.strategy};null!=t.modifiersData.popperOffsets&&(t.styles.popper=Object.assign({},t.styles.popper,te(Object.assign({},c,{offsets:t.modifiersData.popperOffsets,position:t.options.strategy,adaptive:a,roundOffsets:f})))),null!=t.modifiersData.arrow&&(t.styles.arrow=Object.assign({},t.styles.arrow,te(Object.assign({},c,{offsets:t.modifiersData.arrow,position:""absolute"",adaptive:!1,roundOffsets:f})))),t.attributes.popper=Object.assign({},t.attributes.popper,{""data-popper-placement"":t.placement})},data:{}};var re={name:""applyStyles"",enabled:!0,phase:""write"",fn:function(e){var t=e.state;Object.keys(t.elements).forEach((function(e){var n=t.styles[e]||{},o=t.attributes[e]||{},i=t.elements[e];r(i)&&p(i)&&(Object.assign(i.style,n),Object.keys(o).forEach((function(e){var t=o[e];!1===t?i.removeAttribute(e):i.setAttribute(e,!0===t?"""":t)})))}))},effect:function(e){var t=e.state,n={popper:{position:t.options.strategy,left:""0"",top:""0"",margin:""0""},arrow:{position:""absolute""},reference:{}};return Object.assign(t.elements.popper.style,n.popper),t.styles=n,t.elements.arrow&&Object.assign(t.elements.arrow.style,n.arrow),function(){Object.keys(t.elements).forEach((function(e){var o=t.elements[e],i=t.attributes[e]||{},a=Object.keys(t.styles.hasOwnProperty(e)?t.styles[e]:n[e]).reduce((function(e,t){return e[t]="""",e}),{});r(o)&&p(o)&&(Object.assign(o.style,a),Object.keys(i).forEach((function(e){o.removeAttribute(e)})))}))}},requires:[""computeStyles""]};var oe={name:""offset"",enabled:!0,phase:""main"",requires:[""popperOffsets""],fn:function(e){var t=e.state,n=e.options,r=e.name,o=n.offset,i=void 0===o?[0,0]:o,a=T.reduce((function(e,n){return e[n]=function(e,t,n){var r=C(e),o=[A,j].indexOf(r)>=0?-1:1,i=""function""==typeof n?n(Object.assign({},t,{placement:e})):n,a=i[0],s=i[1];return a=a||0,s=(s||0)*o,[A,D].indexOf(r)>=0?{x:s,y:a}:{x:a,y:s}}(n,t.rects,i),e}),{}),s=a[t.placement],f=s.x,c=s.y;null!=t.modifiersData.popperOffsets&&(t.modifiersData.popperOffsets.x+=f,t.modifiersData.popperOffsets.y+=c),t.modifiersData[r]=a}},ie={left:""right"",right:""left"",bottom:""top"",top:""bottom""};function ae(e){return e.replace(/left|right|bottom|top/g,(function(e){return ie[e]}))}var se={start:""end"",end:""start""};function fe(e){return e.replace(/start|end/g,(function(e){return se[e]}))}function ce(e,t){void 0===t&&(t={});var n=t,r=n.placement,o=n.boundary,i=n.rootBoundary,a=n.padding,s=n.flipVariations,f=n.allowedAutoPlacements,c=void 0===f?T:f,p=_(r),u=p?s?H:H.filter((function(e){return _(e)===p})):P,l=u.filter((function(e){return c.indexOf(e)>=0}));0===l.length&&(l=u);var d=l.reduce((function(t,n){return t[n]=Y(e,{placement:n,boundary:o,rootBoundary:i,padding:a})[C(n)],t}),{});return Object.keys(d).sort((function(e,t){return d[e]-d[t]}))}var pe={name:""flip"",enabled:!0,phase:""main"",fn:function(e){var t=e.state,n=e.options,r=e.name;if(!t.modifiersData[r]._skip){for(var o=n.mainAxis,i=void 0===o||o,a=n.altAxis,s=void 0===a||a,f=n.fallbackPlacements,c=n.padding,p=n.boundary,u=n.rootBoundary,l=n.altBoundary,d=n.flipVariations,h=void 0===d||d,m=n.allowedAutoPlacements,v=t.options.placement,g=C(v),y=f||(g===v||!h?[ae(v)]:function(e){if(C(e)===L)return[];var t=ae(e);return[fe(e),t,fe(t)]}(v)),b=[v].concat(y).reduce((function(e,n){return e.concat(C(n)===L?ce(t,{placement:n,boundary:p,rootBoundary:u,padding:c,flipVariations:h,allowedAutoPlacements:m}):n)}),[]),x=t.rects.reference,w=t.rects.popper,O=new Map,P=!0,k=b[0],W=0;W<b.length;W++){var B=b[W],H=C(B),T=_(B)===M,R=[j,E].indexOf(H)>=0,S=R?""width"":""height"",q=Y(t,{placement:B,boundary:p,rootBoundary:u,altBoundary:l,padding:c}),V=R?T?D:A:T?E:j;x[S]>w[S]&&(V=ae(V));var N=ae(V),I=[];if(i&&I.push(q[H]<=0),s&&I.push(q[V]<=0,q[N]<=0),I.every((function(e){return e}))){k=B,P=!1;break}O.set(B,I)}if(P)for(var F=function(e){var t=b.find((function(t){var n=O.get(t);if(n)return n.slice(0,e).every((function(e){return e}))}));if(t)return k=t,""break""},U=h?3:1;U>0;U--){if(""break""===F(U))break}t.placement!==k&&(t.modifiersData[r]._skip=!0,t.placement=k,t.reset=!0)}},requiresIfExists:[""offset""],data:{_skip:!1}};function ue(e,t,n){return i(e,a(t,n))}var le={name:""preventOverflow"",enabled:!0,phase:""main"",fn:function(e){var t=e.state,n=e.options,r=e.name,o=n.mainAxis,s=void 0===o||o,f=n.altAxis,c=void 0!==f&&f,p=n.boundary,u=n.rootBoundary,l=n.altBoundary,d=n.padding,h=n.tether,m=void 0===h||h,g=n.tetherOffset,y=void 0===g?0:g,b=Y(t,{boundary:p,rootBoundary:u,padding:d,altBoundary:l}),x=C(t.placement),w=_(t.placement),L=!w,P=F(x),k=""x""===P?""y"":""x"",W=t.modifiersData.popperOffsets,B=t.rects.reference,H=t.rects.popper,T=""function""==typeof y?y(Object.assign({},t.rects,{placement:t.placement})):y,R=""number""==typeof T?{mainAxis:T,altAxis:T}:Object.assign({mainAxis:0,altAxis:0},T),S=t.modifiersData.offset?t.modifiersData.offset[t.placement]:null,q={x:0,y:0};if(W){if(s){var V,N=""y""===P?j:A,I=""y""===P?E:D,U=""y""===P?""height"":""width"",z=W[P],X=z+b[N],G=z-b[I],J=m?-H[U]/2:0,K=w===M?B[U]:H[U],Q=w===M?-H[U]:-B[U],Z=t.elements.arrow,$=m&&Z?v(Z):{width:0,height:0},ee=t.modifiersData[""arrow#persistent""]?t.modifiersData[""arrow#persistent""].padding:{top:0,right:0,bottom:0,left:0},te=ee[N],ne=ee[I],re=ue(0,B[U],$[U]),oe=L?B[U]/2-J-re-te-R.mainAxis:K-re-te-R.mainAxis,ie=L?-B[U]/2+J+re+ne+R.mainAxis:Q+re+ne+R.mainAxis,ae=t.elements.arrow&&O(t.elements.arrow),se=ae?""y""===P?ae.clientTop||0:ae.clientLeft||0:0,fe=null!=(V=null==S?void 0:S[P])?V:0,ce=z+ie-fe,pe=ue(m?a(X,z+oe-fe-se):X,z,m?i(G,ce):G);W[P]=pe,q[P]=pe-z}if(c){var le,de=""x""===P?j:A,he=""x""===P?E:D,me=W[k],ve=""y""===k?""height"":""width"",ge=me+b[de],ye=me-b[he],be=-1!==[j,A].indexOf(x),xe=null!=(le=null==S?void 0:S[k])?le:0,we=be?ge:me-B[ve]-H[ve]-xe+R.altAxis,Oe=be?me+B[ve]+H[ve]-xe-R.altAxis:ye,je=m&&be?function(e,t,n){var r=ue(e,t,n);return r>n?n:r}(we,me,Oe):ue(m?we:ge,me,m?Oe:ye);W[k]=je,q[k]=je-me}t.modifiersData[r]=q}},requiresIfExists:[""offset""]};var de={name:""arrow"",enabled:!0,phase:""main"",fn:function(e){var t,n=e.state,r=e.name,o=e.options,i=n.elements.arrow,a=n.modifiersData.popperOffsets,s=C(n.placement),f=F(s),c=[A,D].indexOf(s)>=0?""height"":""width"";if(i&&a){var p=function(e,t){return z(""number""!=typeof(e=""function""==typeof e?e(Object.assign({},t.rects,{placement:t.placement})):e)?e:X(e,P))}(o.padding,n),u=v(i),l=""y""===f?j:A,d=""y""===f?E:D,h=n.rects.reference[c]+n.rects.reference[f]-a[f]-n.rects.popper[c],m=a[f]-n.rects.reference[f],g=O(i),y=g?""y""===f?g.clientHeight||0:g.clientWidth||0:0,b=h/2-m/2,x=p[l],w=y-u[c]-p[d],L=y/2-u[c]/2+b,M=ue(x,L,w),k=f;n.modifiersData[r]=((t={})[k]=M,t.centerOffset=M-L,t)}},effect:function(e){var t=e.state,n=e.options.element,r=void 0===n?""[data-popper-arrow]"":n;null!=r&&(""string""!=typeof r||(r=t.elements.popper.querySelector(r)))&&q(t.elements.popper,r)&&(t.elements.arrow=r)},requires:[""popperOffsets""],requiresIfExists:[""preventOverflow""]};function he(e,t,n){return void 0===n&&(n={x:0,y:0}),{top:e.top-t.height-n.y,right:e.right-t.width+n.x,bottom:e.bottom-t.height+n.y,left:e.left-t.width-n.x}}function me(e){return[j,D,E,A].some((function(t){return e[t]>=0}))}var ve={name:""hide"",enabled:!0,phase:""main"",requiresIfExists:[""preventOverflow""],fn:function(e){var t=e.state,n=e.name,r=t.rects.reference,o=t.rects.popper,i=t.modifiersData.preventOverflow,a=Y(t,{elementContext:""reference""}),s=Y(t,{altBoundary:!0}),f=he(a,r),c=he(s,o,i),p=me(f),u=me(c);t.modifiersData[n]={referenceClippingOffsets:f,popperEscapeOffsets:c,isReferenceHidden:p,hasPopperEscaped:u},t.attributes.popper=Object.assign({},t.attributes.popper,{""data-popper-reference-hidden"":p,""data-popper-escaped"":u})}},ge=K({defaultModifiers:[Z,$,ne,re]}),ye=[Z,$,ne,re,oe,pe,le,de,ve],be=K({defaultModifiers:ye});e.applyStyles=re,e.arrow=de,e.computeStyles=ne,e.createPopper=be,e.createPopperLite=ge,e.defaultModifiers=ye,e.detectOverflow=Y,e.eventListeners=Z,e.flip=pe,e.hide=ve,e.offset=oe,e.popperGenerator=K,e.popperOffsets=$,e.preventOverflow=le,Object.defineProperty(e,""__esModule"",{value:!0})}));
+!function(e,t){""object""==typeof exports&&""undefined""!=typeof module?t(exports):""function""==typeof define&&define.amd?define([""exports""],t):t((e=""undefined""!=typeof globalThis?globalThis:e||self).Popper={})}(this,(function(e){""use strict"";function t(e){if(null==e)return window;if(""[object Window]""!==e.toString()){var t=e.ownerDocument;return t&&t.defaultView||window}return e}function n(e){return e instanceof t(e).Element||e instanceof Element}function r(e){return e instanceof t(e).HTMLElement||e instanceof HTMLElement}function o(e){return""undefined""!=typeof ShadowRoot&&(e instanceof t(e).ShadowRoot||e instanceof ShadowRoot)}var i=Math.max,a=Math.min,s=Math.round;function f(){var e=navigator.userAgentData;return null!=e&&e.brands&&Array.isArray(e.brands)?e.brands.map((function(e){return e.brand+""/""+e.version})).join("" ""):navigator.userAgent}function c(){return!/^((?!chrome|android).)*safari/i.test(f())}function p(e,o,i){void 0===o&&(o=!1),void 0===i&&(i=!1);var a=e.getBoundingClientRect(),f=1,p=1;o&&r(e)&&(f=e.offsetWidth>0&&s(a.width)/e.offsetWidth||1,p=e.offsetHeight>0&&s(a.height)/e.offsetHeight||1);var u=(n(e)?t(e):window).visualViewport,l=!c()&&i,d=(a.left+(l&&u?u.offsetLeft:0))/f,h=(a.top+(l&&u?u.offsetTop:0))/p,m=a.width/f,v=a.height/p;return{width:m,height:v,top:h,right:d+m,bottom:h+v,left:d,x:d,y:h}}function u(e){var n=t(e);return{scrollLeft:n.pageXOffset,scrollTop:n.pageYOffset}}function l(e){return e?(e.nodeName||"""").toLowerCase():null}function d(e){return((n(e)?e.ownerDocument:e.document)||window.document).documentElement}function h(e){return p(d(e)).left+u(e).scrollLeft}function m(e){return t(e).getComputedStyle(e)}function v(e){var t=m(e),n=t.overflow,r=t.overflowX,o=t.overflowY;return/auto|scroll|overlay|hidden/.test(n+o+r)}function y(e,n,o){void 0===o&&(o=!1);var i,a,f=r(n),c=r(n)&&function(e){var t=e.getBoundingClientRect(),n=s(t.width)/e.offsetWidth||1,r=s(t.height)/e.offsetHeight||1;return 1!==n||1!==r}(n),m=d(n),y=p(e,c,o),g={scrollLeft:0,scrollTop:0},b={x:0,y:0};return(f||!f&&!o)&&((""body""!==l(n)||v(m))&&(g=(i=n)!==t(i)&&r(i)?{scrollLeft:(a=i).scrollLeft,scrollTop:a.scrollTop}:u(i)),r(n)?((b=p(n,!0)).x+=n.clientLeft,b.y+=n.clientTop):m&&(b.x=h(m))),{x:y.left+g.scrollLeft-b.x,y:y.top+g.scrollTop-b.y,width:y.width,height:y.height}}function g(e){var t=p(e),n=e.offsetWidth,r=e.offsetHeight;return Math.abs(t.width-n)<=1&&(n=t.width),Math.abs(t.height-r)<=1&&(r=t.height),{x:e.offsetLeft,y:e.offsetTop,width:n,height:r}}function b(e){return""html""===l(e)?e:e.assignedSlot||e.parentNode||(o(e)?e.host:null)||d(e)}function x(e){return[""html"",""body"",""#document""].indexOf(l(e))>=0?e.ownerDocument.body:r(e)&&v(e)?e:x(b(e))}function w(e,n){var r;void 0===n&&(n=[]);var o=x(e),i=o===(null==(r=e.ownerDocument)?void 0:r.body),a=t(o),s=i?[a].concat(a.visualViewport||[],v(o)?o:[]):o,f=n.concat(s);return i?f:f.concat(w(b(s)))}function O(e){return[""table"",""td"",""th""].indexOf(l(e))>=0}function j(e){return r(e)&&""fixed""!==m(e).position?e.offsetParent:null}function E(e){for(var n=t(e),i=j(e);i&&O(i)&&""static""===m(i).position;)i=j(i);return i&&(""html""===l(i)||""body""===l(i)&&""static""===m(i).position)?n:i||function(e){var t=/firefox/i.test(f());if(/Trident/i.test(f())&&r(e)&&""fixed""===m(e).position)return null;var n=b(e);for(o(n)&&(n=n.host);r(n)&&[""html"",""body""].indexOf(l(n))<0;){var i=m(n);if(""none""!==i.transform||""none""!==i.perspective||""paint""===i.contain||-1!==[""transform"",""perspective""].indexOf(i.willChange)||t&&""filter""===i.willChange||t&&i.filter&&""none""!==i.filter)return n;n=n.parentNode}return null}(e)||n}var D=""top"",A=""bottom"",L=""right"",P=""left"",M=""auto"",k=[D,A,L,P],W=""start"",B=""end"",H=""viewport"",T=""popper"",R=k.reduce((function(e,t){return e.concat([t+""-""+W,t+""-""+B])}),[]),S=[].concat(k,[M]).reduce((function(e,t){return e.concat([t,t+""-""+W,t+""-""+B])}),[]),V=[""beforeRead"",""read"",""afterRead"",""beforeMain"",""main"",""afterMain"",""beforeWrite"",""write"",""afterWrite""];function q(e){var t=new Map,n=new Set,r=[];function o(e){n.add(e.name),[].concat(e.requires||[],e.requiresIfExists||[]).forEach((function(e){if(!n.has(e)){var r=t.get(e);r&&o(r)}})),r.push(e)}return e.forEach((function(e){t.set(e.name,e)})),e.forEach((function(e){n.has(e.name)||o(e)})),r}function C(e){return e.split(""-"")[0]}function N(e,t){var n=t.getRootNode&&t.getRootNode();if(e.contains(t))return!0;if(n&&o(n)){var r=t;do{if(r&&e.isSameNode(r))return!0;r=r.parentNode||r.host}while(r)}return!1}function I(e){return Object.assign({},e,{left:e.x,top:e.y,right:e.x+e.width,bottom:e.y+e.height})}function _(e,r,o){return r===H?I(function(e,n){var r=t(e),o=d(e),i=r.visualViewport,a=o.clientWidth,s=o.clientHeight,f=0,p=0;if(i){a=i.width,s=i.height;var u=c();(u||!u&&""fixed""===n)&&(f=i.offsetLeft,p=i.offsetTop)}return{width:a,height:s,x:f+h(e),y:p}}(e,o)):n(r)?function(e,t){var n=p(e,!1,""fixed""===t);return n.top=n.top+e.clientTop,n.left=n.left+e.clientLeft,n.bottom=n.top+e.clientHeight,n.right=n.left+e.clientWidth,n.width=e.clientWidth,n.height=e.clientHeight,n.x=n.left,n.y=n.top,n}(r,o):I(function(e){var t,n=d(e),r=u(e),o=null==(t=e.ownerDocument)?void 0:t.body,a=i(n.scrollWidth,n.clientWidth,o?o.scrollWidth:0,o?o.clientWidth:0),s=i(n.scrollHeight,n.clientHeight,o?o.scrollHeight:0,o?o.clientHeight:0),f=-r.scrollLeft+h(e),c=-r.scrollTop;return""rtl""===m(o||n).direction&&(f+=i(n.clientWidth,o?o.clientWidth:0)-a),{width:a,height:s,x:f,y:c}}(d(e)))}function F(e,t,o,s){var f=""clippingParents""===t?function(e){var t=w(b(e)),o=[""absolute"",""fixed""].indexOf(m(e).position)>=0&&r(e)?E(e):e;return n(o)?t.filter((function(e){return n(e)&&N(e,o)&&""body""!==l(e)})):[]}(e):[].concat(t),c=[].concat(f,[o]),p=c[0],u=c.reduce((function(t,n){var r=_(e,n,s);return t.top=i(r.top,t.top),t.right=a(r.right,t.right),t.bottom=a(r.bottom,t.bottom),t.left=i(r.left,t.left),t}),_(e,p,s));return u.width=u.right-u.left,u.height=u.bottom-u.top,u.x=u.left,u.y=u.top,u}function U(e){return e.split(""-"")[1]}function z(e){return[""top"",""bottom""].indexOf(e)>=0?""x"":""y""}function X(e){var t,n=e.reference,r=e.element,o=e.placement,i=o?C(o):null,a=o?U(o):null,s=n.x+n.width/2-r.width/2,f=n.y+n.height/2-r.height/2;switch(i){case D:t={x:s,y:n.y-r.height};break;case A:t={x:s,y:n.y+n.height};break;case L:t={x:n.x+n.width,y:f};break;case P:t={x:n.x-r.width,y:f};break;default:t={x:n.x,y:n.y}}var c=i?z(i):null;if(null!=c){var p=""y""===c?""height"":""width"";switch(a){case W:t[c]=t[c]-(n[p]/2-r[p]/2);break;case B:t[c]=t[c]+(n[p]/2-r[p]/2)}}return t}function Y(e){return Object.assign({},{top:0,right:0,bottom:0,left:0},e)}function G(e,t){return t.reduce((function(t,n){return t[n]=e,t}),{})}function J(e,t){void 0===t&&(t={});var r=t,o=r.placement,i=void 0===o?e.placement:o,a=r.strategy,s=void 0===a?e.strategy:a,f=r.boundary,c=void 0===f?""clippingParents"":f,u=r.rootBoundary,l=void 0===u?H:u,h=r.elementContext,m=void 0===h?T:h,v=r.altBoundary,y=void 0!==v&&v,g=r.padding,b=void 0===g?0:g,x=Y(""number""!=typeof b?b:G(b,k)),w=m===T?""reference"":T,O=e.rects.popper,j=e.elements[y?w:m],E=F(n(j)?j:j.contextElement||d(e.elements.popper),c,l,s),P=p(e.elements.reference),M=X({reference:P,element:O,strategy:""absolute"",placement:i}),W=I(Object.assign({},O,M)),B=m===T?W:P,R={top:E.top-B.top+x.top,bottom:B.bottom-E.bottom+x.bottom,left:E.left-B.left+x.left,right:B.right-E.right+x.right},S=e.modifiersData.offset;if(m===T&&S){var V=S[i];Object.keys(R).forEach((function(e){var t=[L,A].indexOf(e)>=0?1:-1,n=[D,A].indexOf(e)>=0?""y"":""x"";R[e]+=V[n]*t}))}return R}var K={placement:""bottom"",modifiers:[],strategy:""absolute""};function Q(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return!t.some((function(e){return!(e&&""function""==typeof e.getBoundingClientRect)}))}function Z(e){void 0===e&&(e={});var t=e,r=t.defaultModifiers,o=void 0===r?[]:r,i=t.defaultOptions,a=void 0===i?K:i;return function(e,t,r){void 0===r&&(r=a);var i,s,f={placement:""bottom"",orderedModifiers:[],options:Object.assign({},K,a),modifiersData:{},elements:{reference:e,popper:t},attributes:{},styles:{}},c=[],p=!1,u={state:f,setOptions:function(r){var i=""function""==typeof r?r(f.options):r;l(),f.options=Object.assign({},a,f.options,i),f.scrollParents={reference:n(e)?w(e):e.contextElement?w(e.contextElement):[],popper:w(t)};var s,p,d=function(e){var t=q(e);return V.reduce((function(e,n){return e.concat(t.filter((function(e){return e.phase===n})))}),[])}((s=[].concat(o,f.options.modifiers),p=s.reduce((function(e,t){var n=e[t.name];return e[t.name]=n?Object.assign({},n,t,{options:Object.assign({},n.options,t.options),data:Object.assign({},n.data,t.data)}):t,e}),{}),Object.keys(p).map((function(e){return p[e]}))));return f.orderedModifiers=d.filter((function(e){return e.enabled})),f.orderedModifiers.forEach((function(e){var t=e.name,n=e.options,r=void 0===n?{}:n,o=e.effect;if(""function""==typeof o){var i=o({state:f,name:t,instance:u,options:r}),a=function(){};c.push(i||a)}})),u.update()},forceUpdate:function(){if(!p){var e=f.elements,t=e.reference,n=e.popper;if(Q(t,n)){f.rects={reference:y(t,E(n),""fixed""===f.options.strategy),popper:g(n)},f.reset=!1,f.placement=f.options.placement,f.orderedModifiers.forEach((function(e){return f.modifiersData[e.name]=Object.assign({},e.data)}));for(var r=0;r<f.orderedModifiers.length;r++)if(!0!==f.reset){var o=f.orderedModifiers[r],i=o.fn,a=o.options,s=void 0===a?{}:a,c=o.name;""function""==typeof i&&(f=i({state:f,options:s,name:c,instance:u})||f)}else f.reset=!1,r=-1}}},update:(i=function(){return new Promise((function(e){u.forceUpdate(),e(f)}))},function(){return s||(s=new Promise((function(e){Promise.resolve().then((function(){s=void 0,e(i())}))}))),s}),destroy:function(){l(),p=!0}};if(!Q(e,t))return u;function l(){c.forEach((function(e){return e()})),c=[]}return u.setOptions(r).then((function(e){!p&&r.onFirstUpdate&&r.onFirstUpdate(e)})),u}}var $={passive:!0};var ee={name:""eventListeners"",enabled:!0,phase:""write"",fn:function(){},effect:function(e){var n=e.state,r=e.instance,o=e.options,i=o.scroll,a=void 0===i||i,s=o.resize,f=void 0===s||s,c=t(n.elements.popper),p=[].concat(n.scrollParents.reference,n.scrollParents.popper);return a&&p.forEach((function(e){e.addEventListener(""scroll"",r.update,$)})),f&&c.addEventListener(""resize"",r.update,$),function(){a&&p.forEach((function(e){e.removeEventListener(""scroll"",r.update,$)})),f&&c.removeEventListener(""resize"",r.update,$)}},data:{}};var te={name:""popperOffsets"",enabled:!0,phase:""read"",fn:function(e){var t=e.state,n=e.name;t.modifiersData[n]=X({reference:t.rects.reference,element:t.rects.popper,strategy:""absolute"",placement:t.placement})},data:{}},ne={top:""auto"",right:""auto"",bottom:""auto"",left:""auto""};function re(e){var n,r=e.popper,o=e.popperRect,i=e.placement,a=e.variation,f=e.offsets,c=e.position,p=e.gpuAcceleration,u=e.adaptive,l=e.roundOffsets,h=e.isFixed,v=f.x,y=void 0===v?0:v,g=f.y,b=void 0===g?0:g,x=""function""==typeof l?l({x:y,y:b}):{x:y,y:b};y=x.x,b=x.y;var w=f.hasOwnProperty(""x""),O=f.hasOwnProperty(""y""),j=P,M=D,k=window;if(u){var W=E(r),H=""clientHeight"",T=""clientWidth"";if(W===t(r)&&""static""!==m(W=d(r)).position&&""absolute""===c&&(H=""scrollHeight"",T=""scrollWidth""),W=W,i===D||(i===P||i===L)&&a===B)M=A,b-=(h&&W===k&&k.visualViewport?k.visualViewport.height:W[H])-o.height,b*=p?1:-1;if(i===P||(i===D||i===A)&&a===B)j=L,y-=(h&&W===k&&k.visualViewport?k.visualViewport.width:W[T])-o.width,y*=p?1:-1}var R,S=Object.assign({position:c},u&&ne),V=!0===l?function(e,t){var n=e.x,r=e.y,o=t.devicePixelRatio||1;return{x:s(n*o)/o||0,y:s(r*o)/o||0}}({x:y,y:b},t(r)):{x:y,y:b};return y=V.x,b=V.y,p?Object.assign({},S,((R={})[M]=O?""0"":"""",R[j]=w?""0"":"""",R.transform=(k.devicePixelRatio||1)<=1?""translate(""+y+""px, ""+b+""px)"":""translate3d(""+y+""px, ""+b+""px, 0)"",R)):Object.assign({},S,((n={})[M]=O?b+""px"":"""",n[j]=w?y+""px"":"""",n.transform="""",n))}var oe={name:""computeStyles"",enabled:!0,phase:""beforeWrite"",fn:function(e){var t=e.state,n=e.options,r=n.gpuAcceleration,o=void 0===r||r,i=n.adaptive,a=void 0===i||i,s=n.roundOffsets,f=void 0===s||s,c={placement:C(t.placement),variation:U(t.placement),popper:t.elements.popper,popperRect:t.rects.popper,gpuAcceleration:o,isFixed:""fixed""===t.options.strategy};null!=t.modifiersData.popperOffsets&&(t.styles.popper=Object.assign({},t.styles.popper,re(Object.assign({},c,{offsets:t.modifiersData.popperOffsets,position:t.options.strategy,adaptive:a,roundOffsets:f})))),null!=t.modifiersData.arrow&&(t.styles.arrow=Object.assign({},t.styles.arrow,re(Object.assign({},c,{offsets:t.modifiersData.arrow,position:""absolute"",adaptive:!1,roundOffsets:f})))),t.attributes.popper=Object.assign({},t.attributes.popper,{""data-popper-placement"":t.placement})},data:{}};var ie={name:""applyStyles"",enabled:!0,phase:""write"",fn:function(e){var t=e.state;Object.keys(t.elements).forEach((function(e){var n=t.styles[e]||{},o=t.attributes[e]||{},i=t.elements[e];r(i)&&l(i)&&(Object.assign(i.style,n),Object.keys(o).forEach((function(e){var t=o[e];!1===t?i.removeAttribute(e):i.setAttribute(e,!0===t?"""":t)})))}))},effect:function(e){var t=e.state,n={popper:{position:t.options.strategy,left:""0"",top:""0"",margin:""0""},arrow:{position:""absolute""},reference:{}};return Object.assign(t.elements.popper.style,n.popper),t.styles=n,t.elements.arrow&&Object.assign(t.elements.arrow.style,n.arrow),function(){Object.keys(t.elements).forEach((function(e){var o=t.elements[e],i=t.attributes[e]||{},a=Object.keys(t.styles.hasOwnProperty(e)?t.styles[e]:n[e]).reduce((function(e,t){return e[t]="""",e}),{});r(o)&&l(o)&&(Object.assign(o.style,a),Object.keys(i).forEach((function(e){o.removeAttribute(e)})))}))}},requires:[""computeStyles""]};var ae={name:""offset"",enabled:!0,phase:""main"",requires:[""popperOffsets""],fn:function(e){var t=e.state,n=e.options,r=e.name,o=n.offset,i=void 0===o?[0,0]:o,a=S.reduce((function(e,n){return e[n]=function(e,t,n){var r=C(e),o=[P,D].indexOf(r)>=0?-1:1,i=""function""==typeof n?n(Object.assign({},t,{placement:e})):n,a=i[0],s=i[1];return a=a||0,s=(s||0)*o,[P,L].indexOf(r)>=0?{x:s,y:a}:{x:a,y:s}}(n,t.rects,i),e}),{}),s=a[t.placement],f=s.x,c=s.y;null!=t.modifiersData.popperOffsets&&(t.modifiersData.popperOffsets.x+=f,t.modifiersData.popperOffsets.y+=c),t.modifiersData[r]=a}},se={left:""right"",right:""left"",bottom:""top"",top:""bottom""};function fe(e){return e.replace(/left|right|bottom|top/g,(function(e){return se[e]}))}var ce={start:""end"",end:""start""};function pe(e){return e.replace(/start|end/g,(function(e){return ce[e]}))}function ue(e,t){void 0===t&&(t={});var n=t,r=n.placement,o=n.boundary,i=n.rootBoundary,a=n.padding,s=n.flipVariations,f=n.allowedAutoPlacements,c=void 0===f?S:f,p=U(r),u=p?s?R:R.filter((function(e){return U(e)===p})):k,l=u.filter((function(e){return c.indexOf(e)>=0}));0===l.length&&(l=u);var d=l.reduce((function(t,n){return t[n]=J(e,{placement:n,boundary:o,rootBoundary:i,padding:a})[C(n)],t}),{});return Object.keys(d).sort((function(e,t){return d[e]-d[t]}))}var le={name:""flip"",enabled:!0,phase:""main"",fn:function(e){var t=e.state,n=e.options,r=e.name;if(!t.modifiersData[r]._skip){for(var o=n.mainAxis,i=void 0===o||o,a=n.altAxis,s=void 0===a||a,f=n.fallbackPlacements,c=n.padding,p=n.boundary,u=n.rootBoundary,l=n.altBoundary,d=n.flipVariations,h=void 0===d||d,m=n.allowedAutoPlacements,v=t.options.placement,y=C(v),g=f||(y===v||!h?[fe(v)]:function(e){if(C(e)===M)return[];var t=fe(e);return[pe(e),t,pe(t)]}(v)),b=[v].concat(g).reduce((function(e,n){return e.concat(C(n)===M?ue(t,{placement:n,boundary:p,rootBoundary:u,padding:c,flipVariations:h,allowedAutoPlacements:m}):n)}),[]),x=t.rects.reference,w=t.rects.popper,O=new Map,j=!0,E=b[0],k=0;k<b.length;k++){var B=b[k],H=C(B),T=U(B)===W,R=[D,A].indexOf(H)>=0,S=R?""width"":""height"",V=J(t,{placement:B,boundary:p,rootBoundary:u,altBoundary:l,padding:c}),q=R?T?L:P:T?A:D;x[S]>w[S]&&(q=fe(q));var N=fe(q),I=[];if(i&&I.push(V[H]<=0),s&&I.push(V[q]<=0,V[N]<=0),I.every((function(e){return e}))){E=B,j=!1;break}O.set(B,I)}if(j)for(var _=function(e){var t=b.find((function(t){var n=O.get(t);if(n)return n.slice(0,e).every((function(e){return e}))}));if(t)return E=t,""break""},F=h?3:1;F>0;F--){if(""break""===_(F))break}t.placement!==E&&(t.modifiersData[r]._skip=!0,t.placement=E,t.reset=!0)}},requiresIfExists:[""offset""],data:{_skip:!1}};function de(e,t,n){return i(e,a(t,n))}var he={name:""preventOverflow"",enabled:!0,phase:""main"",fn:function(e){var t=e.state,n=e.options,r=e.name,o=n.mainAxis,s=void 0===o||o,f=n.altAxis,c=void 0!==f&&f,p=n.boundary,u=n.rootBoundary,l=n.altBoundary,d=n.padding,h=n.tether,m=void 0===h||h,v=n.tetherOffset,y=void 0===v?0:v,b=J(t,{boundary:p,rootBoundary:u,padding:d,altBoundary:l}),x=C(t.placement),w=U(t.placement),O=!w,j=z(x),M=""x""===j?""y"":""x"",k=t.modifiersData.popperOffsets,B=t.rects.reference,H=t.rects.popper,T=""function""==typeof y?y(Object.assign({},t.rects,{placement:t.placement})):y,R=""number""==typeof T?{mainAxis:T,altAxis:T}:Object.assign({mainAxis:0,altAxis:0},T),S=t.modifiersData.offset?t.modifiersData.offset[t.placement]:null,V={x:0,y:0};if(k){if(s){var q,N=""y""===j?D:P,I=""y""===j?A:L,_=""y""===j?""height"":""width"",F=k[j],X=F+b[N],Y=F-b[I],G=m?-H[_]/2:0,K=w===W?B[_]:H[_],Q=w===W?-H[_]:-B[_],Z=t.elements.arrow,$=m&&Z?g(Z):{width:0,height:0},ee=t.modifiersData[""arrow#persistent""]?t.modifiersData[""arrow#persistent""].padding:{top:0,right:0,bottom:0,left:0},te=ee[N],ne=ee[I],re=de(0,B[_],$[_]),oe=O?B[_]/2-G-re-te-R.mainAxis:K-re-te-R.mainAxis,ie=O?-B[_]/2+G+re+ne+R.mainAxis:Q+re+ne+R.mainAxis,ae=t.elements.arrow&&E(t.elements.arrow),se=ae?""y""===j?ae.clientTop||0:ae.clientLeft||0:0,fe=null!=(q=null==S?void 0:S[j])?q:0,ce=F+ie-fe,pe=de(m?a(X,F+oe-fe-se):X,F,m?i(Y,ce):Y);k[j]=pe,V[j]=pe-F}if(c){var ue,le=""x""===j?D:P,he=""x""===j?A:L,me=k[M],ve=""y""===M?""height"":""width"",ye=me+b[le],ge=me-b[he],be=-1!==[D,P].indexOf(x),xe=null!=(ue=null==S?void 0:S[M])?ue:0,we=be?ye:me-B[ve]-H[ve]-xe+R.altAxis,Oe=be?me+B[ve]+H[ve]-xe-R.altAxis:ge,je=m&&be?function(e,t,n){var r=de(e,t,n);return r>n?n:r}(we,me,Oe):de(m?we:ye,me,m?Oe:ge);k[M]=je,V[M]=je-me}t.modifiersData[r]=V}},requiresIfExists:[""offset""]};var me={name:""arrow"",enabled:!0,phase:""main"",fn:function(e){var t,n=e.state,r=e.name,o=e.options,i=n.elements.arrow,a=n.modifiersData.popperOffsets,s=C(n.placement),f=z(s),c=[P,L].indexOf(s)>=0?""height"":""width"";if(i&&a){var p=function(e,t){return Y(""number""!=typeof(e=""function""==typeof e?e(Object.assign({},t.rects,{placement:t.placement})):e)?e:G(e,k))}(o.padding,n),u=g(i),l=""y""===f?D:P,d=""y""===f?A:L,h=n.rects.reference[c]+n.rects.reference[f]-a[f]-n.rects.popper[c],m=a[f]-n.rects.reference[f],v=E(i),y=v?""y""===f?v.clientHeight||0:v.clientWidth||0:0,b=h/2-m/2,x=p[l],w=y-u[c]-p[d],O=y/2-u[c]/2+b,j=de(x,O,w),M=f;n.modifiersData[r]=((t={})[M]=j,t.centerOffset=j-O,t)}},effect:function(e){var t=e.state,n=e.options.element,r=void 0===n?""[data-popper-arrow]"":n;null!=r&&(""string""!=typeof r||(r=t.elements.popper.querySelector(r)))&&N(t.elements.popper,r)&&(t.elements.arrow=r)},requires:[""popperOffsets""],requiresIfExists:[""preventOverflow""]};function ve(e,t,n){return void 0===n&&(n={x:0,y:0}),{top:e.top-t.height-n.y,right:e.right-t.width+n.x,bottom:e.bottom-t.height+n.y,left:e.left-t.width-n.x}}function ye(e){return[D,L,A,P].some((function(t){return e[t]>=0}))}var ge={name:""hide"",enabled:!0,phase:""main"",requiresIfExists:[""preventOverflow""],fn:function(e){var t=e.state,n=e.name,r=t.rects.reference,o=t.rects.popper,i=t.modifiersData.preventOverflow,a=J(t,{elementContext:""reference""}),s=J(t,{altBoundary:!0}),f=ve(a,r),c=ve(s,o,i),p=ye(f),u=ye(c);t.modifiersData[n]={referenceClippingOffsets:f,popperEscapeOffsets:c,isReferenceHidden:p,hasPopperEscaped:u},t.attributes.popper=Object.assign({},t.attributes.popper,{""data-popper-reference-hidden"":p,""data-popper-escaped"":u})}},be=Z({defaultModifiers:[ee,te,oe,ie]}),xe=[ee,te,oe,ie,ae,le,he,me,ge],we=Z({defaultModifiers:xe});e.applyStyles=ie,e.arrow=me,e.computeStyles=oe,e.createPopper=we,e.createPopperLite=be,e.defaultModifiers=xe,e.detectOverflow=J,e.eventListeners=ee,e.flip=le,e.hide=ge,e.offset=ae,e.popperGenerator=Z,e.popperOffsets=te,e.preventOverflow=he,Object.defineProperty(e,""__esModule"",{value:!0})}));
 

---FILE: session-probability/docs/site_libs/quarto-html/quarto-syntax-highlighting.css---
@@ -85,6 +85,7 @@ code span.st {
 
 code span.cf {
   color: #003B4F;
+  font-weight: bold;
   font-style: inherit;
 }
 
@@ -193,6 +194,7 @@ code span.dv {
 
 code span.kw {
   color: #003B4F;
+  font-weight: bold;
   font-style: inherit;
 }
 

---FILE: session-probability/docs/site_libs/quarto-html/quarto.js---
@@ -9,7 +9,7 @@ const layoutMarginEls = () => {
   // Find any conflicting margin elements and add margins to the
   // top to prevent overlap
   const marginChildren = window.document.querySelectorAll(
-    "".column-margin.column-container > * ""
+    "".column-margin.column-container > *, .margin-caption, .aside""
   );
 
   let lastBottom = 0;
@@ -18,25 +18,14 @@ const layoutMarginEls = () => {
       // clear the top margin so we recompute it
       marginChild.style.marginTop = null;
       const top = marginChild.getBoundingClientRect().top + window.scrollY;
-      console.log({
-        childtop: marginChild.getBoundingClientRect().top,
-        scroll: window.scrollY,
-        top,
-        lastBottom,
-      });
       if (top < lastBottom) {
-        const margin = lastBottom - top;
+        const marginChildStyle = window.getComputedStyle(marginChild);
+        const marginBottom = parseFloat(marginChildStyle[""marginBottom""]);
+        const margin = lastBottom - top + marginBottom;
         marginChild.style.marginTop = `${margin}px`;
       }
       const styles = window.getComputedStyle(marginChild);
       const marginTop = parseFloat(styles[""marginTop""]);
-
-      console.log({
-        top,
-        height: marginChild.getBoundingClientRect().height,
-        marginTop,
-        total: top + marginChild.getBoundingClientRect().height + marginTop,
-      });
       lastBottom = top + marginChild.getBoundingClientRect().height + marginTop;
     }
   }
@@ -46,7 +35,15 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
   // Recompute the position of margin elements anytime the body size changes
   if (window.ResizeObserver) {
     const resizeObserver = new window.ResizeObserver(
-      throttle(layoutMarginEls, 50)
+      throttle(() => {
+        layoutMarginEls();
+        if (
+          window.document.body.getBoundingClientRect().width < 990 &&
+          isReaderMode()
+        ) {
+          quartoToggleReader();
+        }
+      }, 50)
     );
     resizeObserver.observe(window.document.body);
   }
@@ -97,7 +94,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
       if (link.href.indexOf(""#"") !== -1) {
         const anchor = link.href.split(""#"")[1];
         const heading = window.document.querySelector(
-          `[data-anchor-id=${anchor}]`
+          `[data-anchor-id=""${anchor}""]`
         );
         if (heading) {
           // Add the class
@@ -137,8 +134,10 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
       window.innerHeight + window.pageYOffset >=
       window.document.body.offsetHeight
     ) {
+      // This is the no-scroll case where last section should be the active one
       sectionIndex = 0;
     } else {
+      // This finds the last section visible on screen that should be made active
       sectionIndex = [...sections].reverse().findIndex((section) => {
         if (section) {
           return window.pageYOffset >= section.offsetTop - sectionMargin;
@@ -320,6 +319,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
           for (const child of el.children) {
             child.style.opacity = 0;
             child.style.overflow = ""hidden"";
+            child.style.pointerEvents = ""none"";
           }
 
           nexttick(() => {
@@ -361,6 +361,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
 
               const clone = child.cloneNode(true);
               clone.style.opacity = 1;
+              clone.style.pointerEvents = null;
               clone.style.display = null;
               toggleContents.append(clone);
             }
@@ -435,6 +436,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
           for (const child of el.children) {
             child.style.opacity = 1;
             child.style.overflow = null;
+            child.style.pointerEvents = null;
           }
 
           const placeholderEl = window.document.getElementById(
@@ -742,6 +744,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
     // Process the collapse state if this is an UL
     if (el.tagName === ""UL"") {
       if (tocOpenDepth === -1 && depth > 1) {
+        // toc-expand: false
         el.classList.add(""collapse"");
       } else if (
         depth <= tocOpenDepth ||
@@ -760,10 +763,9 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
   };
 
   // walk the TOC and expand / collapse any items that should be shown
-
   if (tocEl) {
-    walk(tocEl, 0);
     updateActiveLink();
+    walk(tocEl, 0);
   }
 
   // Throttle the scroll event and walk peridiocally
@@ -782,6 +784,10 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
   window.addEventListener(
     ""resize"",
     throttle(() => {
+      if (tocEl) {
+        updateActiveLink();
+        walk(tocEl, 0);
+      }
       if (!isReaderMode()) {
         hideOverlappedSidebars();
       }

---FILE: session-probability/docs/site_libs/quarto-nav/quarto-nav.js---
@@ -5,9 +5,45 @@ const headroomChanged = new CustomEvent(""quarto-hrChanged"", {
   composed: false,
 });
 
+const announceDismiss = () => {
+  const annEl = window.document.getElementById(""quarto-announcement"");
+  if (annEl) {
+    annEl.remove();
+
+    const annId = annEl.getAttribute(""data-announcement-id"");
+    window.localStorage.setItem(`quarto-announce-${annId}`, ""true"");
+  }
+};
+
+const announceRegister = () => {
+  const annEl = window.document.getElementById(""quarto-announcement"");
+  if (annEl) {
+    const annId = annEl.getAttribute(""data-announcement-id"");
+    const isDismissed =
+      window.localStorage.getItem(`quarto-announce-${annId}`) || false;
+    if (isDismissed) {
+      announceDismiss();
+      return;
+    } else {
+      annEl.classList.remove(""hidden"");
+    }
+
+    const actionEl = annEl.querySelector("".quarto-announcement-action"");
+    if (actionEl) {
+      actionEl.addEventListener(""click"", function (e) {
+        e.preventDefault();
+        // Hide the bar immediately
+        announceDismiss();
+      });
+    }
+  }
+};
+
 window.document.addEventListener(""DOMContentLoaded"", function () {
   let init = false;
 
+  announceRegister();
+
   // Manage the back to top button, if one is present.
   let lastScrollTop = window.pageYOffset || document.documentElement.scrollTop;
   const scrollDownBuffer = 5;
@@ -85,14 +121,25 @@ window.document.addEventListener(""DOMContentLoaded"", function () {
     }
   }
 
+  function dashboardOffset() {
+    const dashboardNavEl = window.document.getElementById(
+      ""quarto-dashboard-header""
+    );
+    if (dashboardNavEl !== null) {
+      return dashboardNavEl.clientHeight;
+    } else {
+      return 0;
+    }
+  }
+
   function updateDocumentOffsetWithoutAnimation() {
     updateDocumentOffset(false);
   }
 
   function updateDocumentOffset(animated) {
     // set body offset
     const topOffset = headerOffset();
-    const bodyOffset = topOffset + footerOffset();
+    const bodyOffset = topOffset + footerOffset() + dashboardOffset();
     const bodyEl = window.document.body;
     bodyEl.setAttribute(""data-bs-offset"", topOffset);
     bodyEl.style.paddingTop = topOffset + ""px"";
@@ -205,9 +252,9 @@ window.document.addEventListener(""DOMContentLoaded"", function () {
   // Observe size changed for the header
   const headerEl = window.document.querySelector(""header.fixed-top"");
   if (headerEl && window.ResizeObserver) {
-    const observer = new window.ResizeObserver(
-      updateDocumentOffsetWithoutAnimation
-    );
+    const observer = new window.ResizeObserver(() => {
+      setTimeout(updateDocumentOffsetWithoutAnimation, 0);
+    });
     observer.observe(headerEl, {
       attributes: true,
       childList: true,
@@ -226,14 +273,15 @@ window.document.addEventListener(""DOMContentLoaded"", function () {
     const links = window.document.querySelectorAll(""a"");
     for (let i = 0; i < links.length; i++) {
       if (links[i].href) {
+        links[i].dataset.originalHref = links[i].href;
         links[i].href = links[i].href.replace(/\/index\.html/, ""/"");
       }
     }
 
     // Fixup any sharing links that require urls
     // Append url to any sharing urls
     const sharingLinks = window.document.querySelectorAll(
-      ""a.sidebar-tools-main-item""
+      ""a.sidebar-tools-main-item, a.quarto-navigation-tool, a.quarto-navbar-tools, a.quarto-navbar-tools-item""
     );
     for (let i = 0; i < sharingLinks.length; i++) {
       const sharingLink = sharingLinks[i];

---FILE: session-probability/docs/site_libs/quarto-search/quarto-search.js---
@@ -43,7 +43,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
   const mainEl = window.document.querySelector(""main"");
 
   // highlight matches on the page
-  if (query !== null && mainEl) {
+  if (query && mainEl) {
     // perform any highlighting
     highlight(escapeRegExp(query), mainEl);
 
@@ -57,7 +57,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
   // (e.g. if the user edits the query or clears it)
   let highlighting = true;
   const resetHighlighting = (searchTerm) => {
-    if (mainEl && highlighting && query !== null && searchTerm !== query) {
+    if (mainEl && highlighting && query && searchTerm !== query) {
       clearHighlight(query, mainEl);
       highlighting = false;
     }
@@ -98,6 +98,7 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
     classNames: {
       form: ""d-flex"",
     },
+    placeholder: language[""search-text-placeholder""],
     translations: {
       clearButtonTitle: language[""search-clear-button-title""],
       detachedCancelButtonText: language[""search-detached-cancel-button-title""],
@@ -110,6 +111,8 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
       return item.href;
     },
     onStateChange({ state }) {
+      // If this is a file URL, note that
+
       // Perhaps reset highlighting
       resetHighlighting(state.query);
 
@@ -359,7 +362,8 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
                 state,
                 setActiveItemId,
                 setContext,
-                refresh
+                refresh,
+                quartoSearchOptions
               );
             },
           },
@@ -374,6 +378,32 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
     focusSearchInput();
   };
 
+  document.addEventListener(""keyup"", (event) => {
+    const { key } = event;
+    const kbds = quartoSearchOptions[""keyboard-shortcut""];
+    const focusedEl = document.activeElement;
+
+    const isFormElFocused = [
+      ""input"",
+      ""select"",
+      ""textarea"",
+      ""button"",
+      ""option"",
+    ].find((tag) => {
+      return focusedEl.tagName.toLowerCase() === tag;
+    });
+
+    if (
+      kbds &&
+      kbds.includes(key) &&
+      !isFormElFocused &&
+      !document.activeElement.isContentEditable
+    ) {
+      event.preventDefault();
+      window.quartoOpenSearch();
+    }
+  });
+
   // Remove the labeleledby attribute since it is pointing
   // to a non-existent label
   if (quartoSearchOptions.type === ""overlay"") {
@@ -385,11 +415,30 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
     }
   }
 
+  function throttle(func, wait) {
+    let waiting = false;
+    return function () {
+      if (!waiting) {
+        func.apply(this, arguments);
+        waiting = true;
+        setTimeout(function () {
+          waiting = false;
+        }, wait);
+      }
+    };
+  }
+
   // If the main document scrolls dismiss the search results
   // (otherwise, since they're floating in the document they can scroll with the document)
-  window.document.body.onscroll = () => {
-    setIsOpen(false);
-  };
+  window.document.body.onscroll = throttle(() => {
+    // Only do this if we're not detached
+    // Bug #7117
+    // This will happen when the keyboard is shown on ios (resulting in a scroll)
+    // which then closed the search UI
+    if (!window.matchMedia(detachedMediaQuery).matches) {
+      setIsOpen(false);
+    }
+  }, 50);
 
   if (showSearchResults) {
     setIsOpen(true);
@@ -429,15 +478,27 @@ function configurePlugins(quartoSearchOptions) {
         const algoliaInsightsPlugin = createAlgoliaInsightsPlugin({
           insightsClient: window.aa,
           onItemsChange({ insights, insightsEvents }) {
-            const events = insightsEvents.map((event) => {
-              const maxEvents = event.objectIDs.slice(0, 20);
-              return {
-                ...event,
-                objectIDs: maxEvents,
-              };
+            const events = insightsEvents.flatMap((event) => {
+              // This API limits the number of items per event to 20
+              const chunkSize = 20;
+              const itemChunks = [];
+              const eventItems = event.items;
+              for (let i = 0; i < eventItems.length; i += chunkSize) {
+                itemChunks.push(eventItems.slice(i, i + chunkSize));
+              }
+              // Split the items into multiple events that can be sent
+              const events = itemChunks.map((items) => {
+                return {
+                  ...event,
+                  items,
+                };
+              });
+              return events;
             });
 
-            insights.viewedObjectIDs(...events);
+            for (const event of events) {
+              insights.viewedObjectIDs(event);
+            }
           },
         });
         return algoliaInsightsPlugin;
@@ -613,20 +674,30 @@ function showCopyLink(query, options) {
 /* Search Index Handling */
 // create the index
 var fuseIndex = undefined;
+var shownWarning = false;
+
+// fuse index options
+const kFuseIndexOptions = {
+  keys: [
+    { name: ""title"", weight: 20 },
+    { name: ""section"", weight: 20 },
+    { name: ""text"", weight: 10 },
+  ],
+  ignoreLocation: true,
+  threshold: 0.1,
+};
+
 async function readSearchData() {
   // Initialize the search index on demand
   if (fuseIndex === undefined) {
-    // create fuse index
-    const options = {
-      keys: [
-        { name: ""title"", weight: 20 },
-        { name: ""section"", weight: 20 },
-        { name: ""text"", weight: 10 },
-      ],
-      ignoreLocation: true,
-      threshold: 0.1,
-    };
-    const fuse = new window.Fuse([], options);
+    if (window.location.protocol === ""file:"" && !shownWarning) {
+      window.alert(
+        ""Search requires JavaScript features disabled when running in file://... URLs. In order to use search, please run this document in a web server.""
+      );
+      shownWarning = true;
+      return;
+    }
+    const fuse = new window.Fuse([], kFuseIndexOptions);
 
     // fetch the main search.json
     const response = await fetch(offsetURL(""search.json""));
@@ -646,6 +717,7 @@ async function readSearchData() {
       );
     }
   }
+
   return fuseIndex;
 }
 
@@ -674,7 +746,8 @@ function renderItem(
   state,
   setActiveItemId,
   setContext,
-  refresh
+  refresh,
+  quartoSearchOptions
 ) {
   switch (item.type) {
     case kItemTypeDoc:
@@ -684,7 +757,9 @@ function renderItem(
         item.title,
         item.section,
         item.text,
-        item.href
+        item.href,
+        item.crumbs,
+        quartoSearchOptions
       );
     case kItemTypeMore:
       return createMoreCard(
@@ -709,15 +784,46 @@ function renderItem(
   }
 }
 
-function createDocumentCard(createElement, icon, title, section, text, href) {
+function createDocumentCard(
+  createElement,
+  icon,
+  title,
+  section,
+  text,
+  href,
+  crumbs,
+  quartoSearchOptions
+) {
   const iconEl = createElement(""i"", {
     class: `bi bi-${icon} search-result-icon`,
   });
   const titleEl = createElement(""p"", { class: ""search-result-title"" }, title);
+  const titleContents = [iconEl, titleEl];
+  const showParent = quartoSearchOptions[""show-item-context""];
+  if (crumbs && showParent) {
+    let crumbsOut = undefined;
+    const crumbClz = [""search-result-crumbs""];
+    if (showParent === ""root"") {
+      crumbsOut = crumbs.length > 1 ? crumbs[0] : undefined;
+    } else if (showParent === ""parent"") {
+      crumbsOut = crumbs.length > 1 ? crumbs[crumbs.length - 2] : undefined;
+    } else {
+      crumbsOut = crumbs.length > 1 ? crumbs.join("" > "") : undefined;
+      crumbClz.push(""search-result-crumbs-wrap"");
+    }
+
+    const crumbEl = createElement(
+      ""p"",
+      { class: crumbClz.join("" "") },
+      crumbsOut
+    );
+    titleContents.push(crumbEl);
+  }
+
   const titleContainerEl = createElement(
     ""div"",
     { class: ""search-result-title-container"" },
-    [iconEl, titleEl]
+    titleContents
   );
 
   const textEls = [];
@@ -1099,17 +1205,19 @@ function algoliaSearch(query, limit, algoliaOptions) {
         const remappedHits = response.hits.map((hit) => {
           return hit.map((item) => {
             const newItem = { ...item };
-            [""href"", ""section"", ""title"", ""text""].forEach((keyName) => {
-              const mappedName = indexFields[keyName];
-              if (
-                mappedName &&
-                item[mappedName] !== undefined &&
-                mappedName !== keyName
-              ) {
-                newItem[keyName] = item[mappedName];
-                delete newItem[mappedName];
+            [""href"", ""section"", ""title"", ""text"", ""crumbs""].forEach(
+              (keyName) => {
+                const mappedName = indexFields[keyName];
+                if (
+                  mappedName &&
+                  item[mappedName] !== undefined &&
+                  mappedName !== keyName
+                ) {
+                  newItem[keyName] = item[mappedName];
+                  delete newItem[mappedName];
+                }
               }
-            });
+            );
             newItem.text = highlightMatch(query, newItem.text);
             return newItem;
           });
@@ -1120,8 +1228,34 @@ function algoliaSearch(query, limit, algoliaOptions) {
   });
 }
 
-function fuseSearch(query, fuse, fuseOptions) {
-  return fuse.search(query, fuseOptions).map((result) => {
+let subSearchTerm = undefined;
+let subSearchFuse = undefined;
+const kFuseMaxWait = 125;
+
+async function fuseSearch(query, fuse, fuseOptions) {
+  let index = fuse;
+  // Fuse.js using the Bitap algorithm for text matching which runs in
+  // O(nm) time (no matter the structure of the text). In our case this
+  // means that long search terms mixed with large index gets very slow
+  //
+  // This injects a subIndex that will be used once the terms get long enough
+  // Usually making this subindex is cheap since there will typically be
+  // a subset of results matching the existing query
+  if (subSearchFuse !== undefined && query.startsWith(subSearchTerm)) {
+    // Use the existing subSearchFuse
+    index = subSearchFuse;
+  } else if (subSearchFuse !== undefined) {
+    // The term changed, discard the existing fuse
+    subSearchFuse = undefined;
+    subSearchTerm = undefined;
+  }
+
+  // Search using the active fuse
+  const then = performance.now();
+  const resultsRaw = await index.search(query, fuseOptions);
+  const now = performance.now();
+
+  const results = resultsRaw.map((result) => {
     const addParam = (url, name, value) => {
       const anchorParts = url.split(""#"");
       const baseUrl = anchorParts[0];
@@ -1135,6 +1269,22 @@ function fuseSearch(query, fuse, fuseOptions) {
       section: result.item.section,
       href: addParam(result.item.href, kQueryArg, query),
       text: highlightMatch(query, result.item.text),
+      crumbs: result.item.crumbs,
     };
   });
+
+  // If we don't have a subfuse and the query is long enough, go ahead
+  // and create a subfuse to use for subsequent queries
+  if (
+    now - then > kFuseMaxWait &&
+    subSearchFuse === undefined &&
+    resultsRaw.length < fuseOptions.limit
+  ) {
+    subSearchTerm = query;
+    subSearchFuse = new window.Fuse([], kFuseIndexOptions);
+    resultsRaw.forEach((rr) => {
+      subSearchFuse.add(rr.item);
+    });
+  }
+  return results;
 }

---FILE: session-probability/lectures/probabilityI.qmd---
@@ -580,7 +580,7 @@ occurs in a large number ($n$) of trials.
 ::: {.column width=""55%""}
 The probability mass function;
 
-$$P(X=k) = \frac{\lambda}{k!}e^{-\lambda},$$
+$$P(X=k) = \frac{\lambda^k}{k!}e^{-\lambda},$$
 
 $$E[X] = var(X) = \lambda = n p$$
 :::

---FILE: session-probability/prob_02discrv.qmd---
@@ -307,7 +307,7 @@ A rare disease has a very low probability for a single individual. The number of
 
 The probability mass function has a single parameter, $\lambda$, the expected value, and can be described as;
 
-$$P(X=k) = \frac{\lambda}{k!}e^{-\lambda}$$
+$$P(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}$$
 
 The expected value $\lambda = n \pi$, where $n$ is the number of objects sampled from the population and $\pi$ is the probability of a single object.
 "
NBISweden,workshop-mlbiostatistics,e1ebe46ae568fc5a9c65101a8e993ffbf1bdd7d6,olgadet,,2024-04-29T15:22:34Z,olgadet,,2024-04-29T15:22:34Z,Fix typos and notations,session-lm-presentation/.quarto/_freeze/session-lm-presentation/execute-results/html.json;session-lm-presentation/.quarto/_freeze/session-lm-presentation/figure-revealjs/fig-obesity-1.png;session-lm-presentation/.quarto/_freeze/session-lm-presentation/figure-revealjs/log-example-1.png;session-lm-presentation/.quarto/_freeze/session-lm-presentation/figure-revealjs/unnamed-chunk-4-1.png;session-lm-presentation/.quarto/_freeze/session-lm-presentation/figure-revealjs/unnamed-chunk-5-1.png;session-lm-presentation/.quarto/_freeze/session-lm-presentation/figure-revealjs/unnamed-chunk-7-1.png;session-lm-presentation/.quarto/xref/1eca1403;session-lm-presentation/renv.lock;session-lm-presentation/session-lm-presentation.html;session-lm-presentation/session-lm-presentation.qmd;session-lm-presentation/session-lm-presentation_files/figure-revealjs/fig-obesity-1.png;session-lm-presentation/session-lm-presentation_files/figure-revealjs/log-example-1.png;session-lm-presentation/session-lm-presentation_files/figure-revealjs/unnamed-chunk-4-1.png;session-lm-presentation/session-lm-presentation_files/figure-revealjs/unnamed-chunk-47-1.png;session-lm-presentation/session-lm-presentation_files/figure-revealjs/unnamed-chunk-5-1.png;session-lm-presentation/session-lm-presentation_files/figure-revealjs/unnamed-chunk-7-1.png;session-lm/.quarto/_freeze/lm-coeff/execute-results/html.json;session-lm/.quarto/_freeze/lm-coeff/figure-html/unnamed-chunk-17-1.png;session-lm/.quarto/_freeze/lm-intro/execute-results/html.json;session-lm/.quarto/_freeze/lm-intro/figure-html/fig-linear-adv-1.png;session-lm/.quarto/_freeze/lm-intro/figure-html/fig-relationship-1.png;session-lm/.quarto/cites/index.json;session-lm/.quarto/idx/lm-coeff.qmd.json;session-lm/.quarto/idx/lm-intro.qmd.json;session-lm/.quarto/xref/28d59191;session-lm/.quarto/xref/295884aa;session-lm/.quarto/xref/4d506fc9;session-lm/.quarto/xref/5b28360f;session-lm/.quarto/xref/691de0dc;session-lm/.quarto/xref/9e3bf041;session-lm/docs/lm-coeff.html;session-lm/docs/lm-coeff_files/figure-html/unnamed-chunk-17-1.png;session-lm/docs/lm-intro.html;session-lm/docs/lm-intro_files/figure-html/fig-linear-adv-1.png;session-lm/docs/lm-intro_files/figure-html/fig-relationship-1.png;session-lm/docs/lm-reg-cls_files/figure-html/fig-obesity-1.png;session-lm/docs/lm-reg-cls_files/figure-html/unnamed-chunk-12-1.png;session-lm/docs/lm-reg-cls_files/figure-html/unnamed-chunk-9-1.png;session-lm/docs/search.json;session-lm/lm-coeff.qmd;session-lm/lm-intro.qmd,True,False,True,False,235,88,323,"---FILE: session-lm-presentation/.quarto/xref/1eca1403---
@@ -1 +1 @@
-{""headings"":[""we-will-learn"",""why-linear-models"",""statistical-vs.-deterministic-relationship"",""statistical-vs.-deterministic-relationship-1"",""statistical-vs.-deterministic-relationship-2"",""statistical-vs.-deterministic-relationship-3"",""what-linear-models-are"",""what-linear-models-are-1"",""simple-linear-regression"",""simple-linear-regression-1"",""weight-and-plasma-volume"",""simple-linear-regression-2"",""simple-linear-regression-3"",""simple-linear-regression-4"",""simple-linear-regression-5"",""least-squares"",""least-squares-1"",""slope"",""intercept"",""hypothesis-testing"",""hypothesis-testing-1"",""hypothesis-testing-2"",""hypothesis-testing-3"",""vector-matrix-notations"",""vector-matrix-notations-1"",""vector-matrix-form-of-the-linear-model"",""vector-matrix-notations-2"",""least-squares-in-vector-matrix-notation"",""vector-matrix-notations-3"",""vector-matrix-notation"",""vector-matrix-notations-least-squares"",""vector-matrix-notations-least-squares-1"",""assessing-model-fit"",""r2-summary-of-the-fitted-model"",""r2-summary-of-the-fitted-model-1"",""r2-summary-of-the-fitted-model-2"",""r2-summary-of-the-fitted-model-3"",""r2"",""r2-and-correlation-coefficient"",""r2-1"",""r2adj"",""r2adj-1"",""checking-model-assumptions"",""the-assumptions-of-a-linear-model"",""the-assumptions-of-a-linear-model-1"",""checking-assumptions"",""checking-assumptions-1"",""exercises"",""linear-models-regression-and-classification-tasks"",""linear-models-in-ml-context"",""evaluating-linear-models"",""feature-selection"",""feature-selection-1"",""there-are-generally-three-main-groups-of-feature-selection-methods"",""regularized-regression"",""regularized-regression-1"",""regularized-regression-2"",""regularized-regression-3"",""bias-variance-trade-off"",""bias-variance-trade-off-1"",""bias-variance-trade-off-2"",""ridge-vs.-lasso"",""ridge-vs.-lasso-1"",""elastic-net"",""elastic-net-1"",""generalized-linear-models"",""why-generalized-linear-models"",""logistic-regression"",""logistic-regression-1"",""logistic-regression-2"",""logistic-regression-3"",""logistic-regression-4"",""logistic-regression-5"",""logistic-regression-6"",""common-glm-models"",""logistic-lasso"",""logistic-lasso-1"",""common-cases""],""entries"":[{""key"":""exm-vector-matrix-notation"",""order"":{""section"":[1,24,0,0,0,0,0],""number"":3},""caption"":""vector-matrix-notation""},{""key"":""thm-lss-vector-matrix"",""order"":{""section"":[1,23,0,0,0,0,0],""number"":1},""caption"":""Least squares in vector-matrix notation""},{""key"":""fig-obesity"",""order"":{""section"":[6,2,0,0,0,0,0],""number"":4},""caption"":""?(caption)""},{""key"":""eq-ridge2"",""order"":{""section"":[5,12,0,0,0,0,0],""number"":6},""caption"":""""},{""key"":""exm-hypothesis-testing"",""order"":{""section"":[1,20,0,0,0,0,0],""number"":2},""caption"":""Hypothesis testing""},{""key"":""eq-elastic-net"",""order"":{""section"":[5,14,0,0,0,0,0],""number"":8},""caption"":""""},{""key"":""eq-lasso"",""order"":{""section"":[5,12,0,0,0,0,0],""number"":7},""caption"":""""},{""key"":""eq-lm-no-error"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":1},""caption"":""""},{""key"":""fig-reg-errors"",""order"":{""section"":[1,15,0,0,0,0,0],""number"":2},""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice versa. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line""},{""key"":""def-r2"",""order"":{""section"":[2,4,0,0,0,0,0],""number"":2},""caption"":""R^2""},{""key"":""def-vector-matrix-lm"",""order"":{""section"":[1,22,0,0,0,0,0],""number"":1},""caption"":""vector matrix form of the linear model""},{""key"":""eq-lm"",""order"":{""section"":[5,6,0,0,0,0,0],""number"":3},""caption"":""""},{""key"":""fig-bias-variance"",""order"":{""section"":[5,11,0,0,0,0,0],""number"":3},""caption"":""Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning""},{""key"":""eq-ridge"",""order"":{""section"":[5,7,0,0,0,0,0],""number"":5},""caption"":""""},{""key"":""exm-simple-lm"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":1},""caption"":""Weight and plasma volume""},{""key"":""fig-lm-example-reg"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":1},""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable""},{""key"":""thm-r2adj"",""order"":{""section"":[2,6,0,0,0,0,0],""number"":3},""caption"":""R^2(adj)""},{""key"":""thm-r2"",""order"":{""section"":[2,5,0,0,0,0,0],""number"":2},""caption"":""R^2""}]}
\ No newline at end of file
+{""headings"":[""we-will-learn"",""why-linear-models"",""statistical-vs.-deterministic-relationship"",""statistical-vs.-deterministic-relationship-1"",""statistical-vs.-deterministic-relationship-2"",""statistical-vs.-deterministic-relationship-3"",""what-linear-models-are"",""what-linear-models-are-1"",""simple-linear-regression"",""simple-linear-regression-1"",""weight-and-plasma-volume"",""simple-linear-regression-2"",""simple-linear-regression-3"",""simple-linear-regression-4"",""simple-linear-regression-5"",""least-squares"",""least-squares-1"",""slope"",""intercept"",""hypothesis-testing"",""hypothesis-testing-1"",""hypothesis-testing-2"",""hypothesis-testing-3"",""vector-matrix-notations"",""vector-matrix-notations-1"",""vector-matrix-form-of-the-linear-model"",""vector-matrix-notations-2"",""least-squares-in-vector-matrix-notation"",""vector-matrix-notations-3"",""vector-matrix-notation"",""vector-matrix-notations-least-squares"",""vector-matrix-notations-least-squares-1"",""assessing-model-fit"",""r2-summary-of-the-fitted-model"",""r2-summary-of-the-fitted-model-1"",""r2-summary-of-the-fitted-model-2"",""r2-summary-of-the-fitted-model-3"",""r2"",""r2-and-correlation-coefficient"",""r2-1"",""r2adj"",""r2adj-1"",""checking-model-assumptions"",""the-assumptions-of-a-linear-model"",""the-assumptions-of-a-linear-model-1"",""checking-assumptions"",""checking-assumptions-1"",""exercises"",""linear-models-regression-and-classification-tasks"",""linear-models-in-ml-context"",""evaluating-linear-models"",""feature-selection"",""feature-selection-1"",""there-are-generally-three-main-groups-of-feature-selection-methods"",""regularized-regression"",""regularized-regression-1"",""regularized-regression-2"",""regularized-regression-3"",""bias-variance-trade-off"",""bias-variance-trade-off-1"",""bias-variance-trade-off-2"",""ridge-vs.-lasso"",""ridge-vs.-lasso-1"",""elastic-net"",""elastic-net-1"",""generalized-linear-models"",""why-generalized-linear-models"",""logistic-regression"",""logistic-regression-1"",""logistic-regression-2"",""logistic-regression-3"",""logistic-regression-4"",""logistic-regression-5"",""logistic-regression-6"",""common-glm-models"",""logistic-lasso"",""logistic-lasso-1"",""common-cases""],""entries"":[{""order"":{""section"":[2,6,0,0,0,0,0],""number"":3},""caption"":""R^2(adj)"",""key"":""thm-r2adj""},{""order"":{""section"":[1,15,0,0,0,0,0],""number"":2},""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice versa. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line"",""key"":""fig-reg-errors""},{""order"":{""section"":[5,12,0,0,0,0,0],""number"":5},""caption"":"""",""key"":""eq-lasso""},{""order"":{""section"":[1,20,0,0,0,0,0],""number"":2},""caption"":""Hypothesis testing"",""key"":""exm-hypothesis-testing""},{""order"":{""section"":[1,11,0,0,0,0,0],""number"":1},""caption"":"""",""key"":""eq-lm-no-error""},{""order"":{""section"":[5,6,0,0,0,0,0],""number"":3},""caption"":"""",""key"":""eq-ridge""},{""order"":{""section"":[5,14,0,0,0,0,0],""number"":6},""caption"":"""",""key"":""eq-elastic-net""},{""order"":{""section"":[1,11,0,0,0,0,0],""number"":1},""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable"",""key"":""fig-lm-example-reg""},{""order"":{""section"":[5,11,0,0,0,0,0],""number"":3},""caption"":""Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning"",""key"":""fig-bias-variance""},{""order"":{""section"":[1,13,0,0,0,0,0],""number"":2},""caption"":"""",""key"":""eq-lm""},{""order"":{""section"":[5,12,0,0,0,0,0],""number"":4},""caption"":"""",""key"":""eq-ridge2""},{""order"":{""section"":[6,2,0,0,0,0,0],""number"":4},""caption"":""?(caption)"",""key"":""fig-obesity""},{""order"":{""section"":[2,5,0,0,0,0,0],""number"":2},""caption"":""R^2"",""key"":""thm-r2""},{""order"":{""section"":[1,22,0,0,0,0,0],""number"":1},""caption"":""vector matrix form of the linear model"",""key"":""def-vector-matrix-lm""},{""order"":{""section"":[2,4,0,0,0,0,0],""number"":2},""caption"":""R^2"",""key"":""def-r2""},{""order"":{""section"":[1,23,0,0,0,0,0],""number"":1},""caption"":""Least squares in vector-matrix notation"",""key"":""thm-lss-vector-matrix""},{""order"":{""section"":[1,9,0,0,0,0,0],""number"":1},""caption"":""Weight and plasma volume"",""key"":""exm-simple-lm""},{""order"":{""section"":[1,24,0,0,0,0,0],""number"":3},""caption"":""vector-matrix-notation"",""key"":""exm-vector-matrix-notation""}]}
\ No newline at end of file

---FILE: session-lm-presentation/renv.lock---
@@ -1,6 +1,6 @@
 {
   ""R"": {
-    ""Version"": ""4.2.1"",
+    ""Version"": ""4.3.1"",
     ""Repositories"": [
       {
         ""Name"": ""CRAN"",

---FILE: session-lm-presentation/session-lm-presentation.html---
@@ -732,7 +732,7 @@ <h2>Simple linear regression</h2>
 <li class=""fragment""><span class=""math inline"">\(2.638\)</span> is not exactly the as same as <span class=""math inline"">\(2.75\)</span>, the first measurement we have in our dataset, i.e.&nbsp;<span class=""math inline"">\(2.75 - 2.638 = 0.112 \neq 0\)</span>.</li>
 <li class=""fragment"">We thus add to the previous equation (<a href=""#/simple-linear-regression-3"">Equation&nbsp;1</a>) an <strong>error term</strong> to account for this and now we can write our <strong>simple regression model</strong> more formally as:</li>
 <li class=""fragment""><span id=""eq-lm""><span class=""math display"">\[Y_i = \alpha + \beta \cdot x_i + \epsilon_i \qquad(2)\]</span></span> where:</li>
-<li class=""fragment""><span class=""math inline"">\(x\)</span>: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate</li>
+<li class=""fragment""><span class=""math inline"">\(x\)</span>: is called: exposure variable, explanatory variable, independent variable, predictor, covariate</li>
 <li class=""fragment""><span class=""math inline"">\(y\)</span>: is called: response, outcome, dependent variable</li>
 <li class=""fragment""><span class=""math inline"">\(\alpha\)</span> and <span class=""math inline"">\(\beta\)</span> are <strong>model coefficients</strong></li>
 <li class=""fragment"">and <span class=""math inline"">\(\epsilon_i\)</span> is an <strong>error terms</strong></li>
@@ -1402,11 +1402,11 @@ <h2>Regularized regression</h2>
 <h2>Regularized regression</h2>
 <p><em>Ridge regression</em> <br></p>
 <ul>
-<li>Previously we saw that the least squares fitting procedure estimates model coefficients <span class=""math inline"">\(\beta_0, \beta_1, \cdots, \beta_p\)</span> using the values that minimize the residual sum of squares: <span id=""eq-lm""><span class=""math display"">\[RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 \qquad(3)\]</span></span></li>
+<li>Previously we saw that the least squares fitting procedure estimates model coefficients <span class=""math inline"">\(\beta_0, \beta_1, \cdots, \beta_p\)</span> using the values that minimize the residual sum of squares: <span class=""math display"">\[RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2\]</span></li>
 </ul>
 <div class=""fragment"">
 <ul>
-<li>In <strong>regularized regression</strong> the coefficients are estimated by minimizing slightly different quantity. Specifically, in <strong>Ridge regression</strong> we estimate <span class=""math inline"">\(\hat\beta^{L}\)</span> that minimizes <span id=""eq-ridge""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(4)\]</span></span></li>
+<li>In <strong>regularized regression</strong> the coefficients are estimated by minimizing slightly different quantity. Specifically, in <strong>Ridge regression</strong> we estimate <span class=""math inline"">\(\hat\beta^{L}\)</span> that minimizes <span id=""eq-ridge""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(3)\]</span></span></li>
 </ul>
 <p>where:</p>
 <p><span class=""math inline"">\(\lambda \ge 0\)</span> is a <strong>tuning parameter</strong> to be determined separately e.g.&nbsp;via cross-validation</p>
@@ -1415,8 +1415,8 @@ <h2>Regularized regression</h2>
 <section id=""regularized-regression-2"" class=""slide level2"">
 <h2>Regularized regression</h2>
 <p><em>Ridge regression</em> <br></p>
-<p><span id=""eq-ridge""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(5)\]</span></span></p>
-<p><a href=""#/regularized-regression-1"">Equation&nbsp;5</a> trades two different criteria:</p>
+<p><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2\]</span></p>
+<p><a href=""#/regularized-regression-1"">Equation&nbsp;3</a> trades two different criteria:</p>
 <ul>
 <li>lasso regression seeks coefficient estimates that fit the data well, by making RSS small</li>
 <li>however, the second term <span class=""math inline"">\(\lambda \sum_{j=1}^{p}\beta_j^2\)</span>, called <strong>shrinkage penalty</strong> is small when <span class=""math inline"">\(\beta_1, \cdots, \beta_p\)</span> are close to zero, so it has the effect of <strong>shrinking</strong> the estimates of <span class=""math inline"">\(\beta_j\)</span> towards zero.</li>
@@ -1473,8 +1473,8 @@ <h2>Bias-variance trade-off</h2>
 <img data-src=""images/bias-variance.png"" style=""width:100.0%"" class=""r-stretch quarto-figure-center""><p class=""caption"">Figure&nbsp;3: Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning</p></section>
 <section id=""ridge-vs.-lasso"" class=""slide level2"">
 <h2>Ridge vs.&nbsp;Lasso</h2>
-<p>In <strong>Ridge</strong> regression we minimize: <span id=""eq-ridge2""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(6)\]</span></span> where <span class=""math inline"">\(\lambda \sum_{j=1}^{p}\beta_j^2\)</span> is also known as <strong>L2</strong> regularization element or <span class=""math inline"">\(l_2\)</span> penalty</p>
-<p>In <strong>Lasso</strong> regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: <span id=""eq-lasso""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| = RSS + \lambda \sum_{j=1}^{p}|\beta_j| \qquad(7)\]</span></span> where <span class=""math inline"">\(\lambda \sum_{j=1}^{p}|\beta_j|\)</span> is also known as <strong>L1</strong> regularization element or <span class=""math inline"">\(l_1\)</span> penalty</p>
+<p>In <strong>Ridge</strong> regression we minimize: <span id=""eq-ridge2""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2 \qquad(4)\]</span></span> where <span class=""math inline"">\(\lambda \sum_{j=1}^{p}\beta_j^2\)</span> is also known as <strong>L2</strong> regularization element or <span class=""math inline"">\(l_2\)</span> penalty</p>
+<p>In <strong>Lasso</strong> regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: <span id=""eq-lasso""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| = RSS + \lambda \sum_{j=1}^{p}|\beta_j| \qquad(5)\]</span></span> where <span class=""math inline"">\(\lambda \sum_{j=1}^{p}|\beta_j|\)</span> is also known as <strong>L1</strong> regularization element or <span class=""math inline"">\(l_1\)</span> penalty</p>
 <p>Lasso regression was introduced to help model interpretation. With Ridge regression we improve model performance but unless <span class=""math inline"">\(\lambda = \infty\)</span> all beta coefficients are non-zero, hence all variables remain in the model. By using <span class=""math inline"">\(l_1\)</span> penalty we can force some of the coefficients estimates to be exactly equal to 0, hence perform <strong>variable selection</strong></p>
 </section>
 <section id=""ridge-vs.-lasso-1"" class=""slide level2"">
@@ -1508,7 +1508,7 @@ <h2>Ridge vs.&nbsp;Lasso</h2>
 <section id=""elastic-net"" class=""slide level2"">
 <h2>Elastic Net</h2>
 <p><br></p>
-<p><strong>Elastic Net</strong> use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. <span id=""eq-elastic-net""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2  \qquad(8)\]</span></span></p>
+<p><strong>Elastic Net</strong> use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. <span id=""eq-elastic-net""><span class=""math display"">\[\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2  \qquad(6)\]</span></span></p>
 
 <img data-src=""session-lm-presentation_files/figure-revealjs/elastic-net-1.png"" width=""960"" class=""r-stretch quarto-figure-center""><p class=""caption"">Example of Elastic Net regression to model BMI using age, chol, hdl and glucose variables: model coefficients are plotted over a range of lambda values and alpha value 0.1, showing the changes of model coefficients as a function of lambda being somewhere between those for Ridge and Lasso regression.</p></section>
 <section id=""elastic-net-1"" class=""slide level2"">

---FILE: session-lm-presentation/session-lm-presentation.qmd---
@@ -112,12 +112,10 @@ data_diabetes %>%
 
 Relationships in probability and statistics can generally be one of three things:
 
-
 -   deterministic
 -   random
 -   statistical
 
-
 ## Statistical vs. deterministic relationship {.smaller}
 
 *deterministic*
@@ -243,76 +241,129 @@ data_diabetes %>%
 <!-- ## What linear models are and are not -->
 
 <!-- ```{r} -->
+
 <!-- #| warning: false -->
+
 <!-- #| echo: false -->
+
 <!-- #| fig-align: center -->
+
 <!-- #| fig-width: 8 -->
+
 <!-- #| fig-height: 6 -->
 
 <!-- my.ggtheme <- theme_bw() +  -->
+
 <!--   theme(axis.title = element_text(size = font.size),  -->
+
 <!--         axis.text = element_text(size = font.size),  -->
+
 <!--         legend.text = element_text(size = font.size),  -->
+
 <!--         legend.title = element_blank(),  -->
+
 <!--         legend.position = ""none"",  -->
+
 <!--         axis.title.y=element_text(angle=0)) -->
 
 <!-- # simple linear regression -->
+
 <!-- x <- seq(-10, 10, 1) -->
+
 <!-- set.seed(123) -->
+
 <!-- y <- x + rnorm(length(x), mean(x), 2) -->
+
 <!-- data.xy <- data.frame(x=x, y = y, ymodel = x) -->
+
 <!-- p.simple <-  data.xy %>% ggplot(aes(x = x, y = y)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
+
 <!--   my.ggtheme +  -->
+
 <!--   ggtitle(""A"") -->
 
 <!-- # simple linear regression with group -->
+
 <!-- x <- seq(0, 10, length.out = 20) -->
+
 <!-- set.seed(123) -->
+
 <!-- y1 <- 0 + x + rnorm(length(x), 0, 2) -->
+
 <!-- set.seed(123) -->
+
 <!-- y2 <- 0 + 4*x + rnorm(length(x), 0, 2) -->
 
 <!-- x.all <- c(x, x) -->
+
 <!-- y.all <- c(y1, y2) -->
+
 <!-- group <- c(rep(""CTRL"", length(x)), rep(""TX"", length(x))) -->
+
 <!-- ymodel <- c(0+x, 0+4*x) -->
 
 <!-- data.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel) -->
 
 <!-- p.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel)) +  -->
+
 <!--   theme_classic() + -->
+
 <!--   scale_color_brewer(palette = ""Set2"") +  -->
+
 <!--   my.ggtheme +  -->
+
 <!--   ggtitle(""B"")  -->
 
 <!-- # advanced 1 -->
+
 <!-- x <- seq(-10, 10, 1) -->
+
 <!-- set.seed(123) -->
+
 <!-- y <- x^2 + rnorm(length(x), mean(x), 10) -->
+
 <!-- data.xy <- data.frame(x=x, y = y, ymodel = x^2) -->
+
 <!-- p.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
+
 <!--   theme_classic() + -->
+
 <!--   my.ggtheme +  -->
-<!--   ggtitle(""C"") -->
 
+<!--   ggtitle(""C"") -->
 
 <!-- # advanced 2 -->
+
 <!-- x <- seq(-10, 10, 1) -->
+
 <!-- set.seed(123) -->
+
 <!-- y <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05) -->
+
 <!-- data.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000) -->
+
 <!-- p.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
+
 <!--   theme_classic() + -->
+
 <!--   my.ggtheme +  -->
+
 <!--   ggtitle(""D"") -->
 
 <!-- grid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2) -->
@@ -322,76 +373,129 @@ data_diabetes %>%
 <!-- ## What linear models are and are not -->
 
 <!-- ```{r} -->
+
 <!-- #| warning: false -->
+
 <!-- #| echo: false -->
+
 <!-- #| fig-align: center -->
+
 <!-- #| fig-width: 8 -->
+
 <!-- #| fig-height: 6 -->
 
 <!-- my.ggtheme <- theme_bw() +  -->
+
 <!--   theme(axis.title = element_text(size = font.size),  -->
+
 <!--         axis.text = element_text(size = font.size),  -->
+
 <!--         legend.text = element_text(size = font.size),  -->
+
 <!--         legend.title = element_blank(),  -->
+
 <!--         legend.position = ""none"",  -->
+
 <!--         axis.title.y=element_text(angle=0)) -->
 
 <!-- # simple linear regression -->
+
 <!-- x <- seq(-10, 10, 1) -->
+
 <!-- set.seed(123) -->
+
 <!-- y <- x + rnorm(length(x), mean(x), 2) -->
+
 <!-- data.xy <- data.frame(x=x, y = y, ymodel = x) -->
+
 <!-- p.simple <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
+
 <!--   my.ggtheme +  -->
+
 <!--   ggtitle(TeX(r'(A: $y_i = x_1 + e_i$)')) -->
 
 <!-- # simple linear regression with group -->
+
 <!-- x <- seq(0, 10, length.out = 20) -->
+
 <!-- set.seed(123) -->
+
 <!-- y1 <- 0 + x + rnorm(length(x), 0, 2) -->
+
 <!-- set.seed(123) -->
+
 <!-- y2 <- 0 + 4*x + rnorm(length(x), 0, 2) -->
 
 <!-- x.all <- c(x, x) -->
+
 <!-- y.all <- c(y1, y2) -->
+
 <!-- group <- c(rep(""CTRL"", length(x)), rep(""TX"", length(x))) -->
+
 <!-- ymodel <- c(0+x, 0+4*x) -->
 
 <!-- data.xy <- data.frame(x=x.all, y = y.all, ymodel = ymodel) -->
 
 <!-- p.group <- data.xy %>% ggplot(aes(x = x, y = y, colour = group)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel)) +  -->
+
 <!--   theme_classic() + -->
+
 <!--   scale_color_brewer(palette = ""Set2"") +  -->
+
 <!--   my.ggtheme +  -->
+
 <!--   ggtitle(TeX(r'(B: $x_1 + I_{x_i} + e_i$)')) -->
 
 <!-- # advanced 1 -->
+
 <!-- x <- seq(-10, 10, 1) -->
+
 <!-- set.seed(123) -->
+
 <!-- y <- x^2 + rnorm(length(x), mean(x), 10) -->
+
 <!-- data.xy <- data.frame(x=x, y = y, ymodel = x^2) -->
+
 <!-- p.adv1 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
+
 <!--   theme_classic() + -->
+
 <!--   my.ggtheme +  -->
-<!--   ggtitle(TeX(r'(C: $y_i = x_i^2 + e_i$)')) -->
 
+<!--   ggtitle(TeX(r'(C: $y_i = x_i^2 + e_i$)')) -->
 
 <!-- # advanced 2 -->
+
 <!-- x <- seq(-10, 10, 1) -->
+
 <!-- set.seed(123) -->
+
 <!-- y <- (x + (x^3))/1000 + rnorm(length(x), mean(x), 0.05) -->
+
 <!-- data.xy <- data.frame(x=x, x2=x^2, x3=x^3, y = y, ymodel = (x + x^3)/1000) -->
+
 <!-- p.adv2 <- data.xy %>% ggplot(aes(x = x, y = y)) + -->
+
 <!--   geom_point() + -->
+
 <!--   geom_line(aes(x = x, y=ymodel), color = col.blue.dark) +  -->
+
 <!--   theme_classic() + -->
+
 <!--   my.ggtheme +  -->
+
 <!--   ggtitle(TeX(r'(D: $y_i = x + x_i^3 + e_i$)')) -->
 
 <!-- grid.arrange(p.simple, p.group, p.adv1, p.adv2, ncol = 2) -->
@@ -469,7 +573,6 @@ plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (lit
 ```
 :::
 
-
 ```{r}
 #| label: fig-lm-intro-example
 #| fig-cap: ""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.""
@@ -721,7 +824,7 @@ plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (lit
 -   $2.638$ is not exactly the as same as $2.75$, the first measurement we have in our dataset, i.e. $2.75 - 2.638 = 0.112 \neq 0$.
 -   We thus add to the previous equation (@eq-lm-no-error) an **error term** to account for this and now we can write our **simple regression model** more formally as:
 -   $$Y_i = \alpha + \beta \cdot x_i + \epsilon_i$$ {#eq-lm} where:
--   $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate
+-   $x$: is called: exposure variable, explanatory variable, independent variable, predictor, covariate
 -   $y$: is called: response, outcome, dependent variable
 -   $\alpha$ and $\beta$ are **model coefficients**
 -   and $\epsilon_i$ is an **error terms**
@@ -1289,6 +1392,7 @@ lm(plasma ~ weight)
 # Assessing model fit
 
 <!-- -   In addition to knowing how to estimate model parameters we need to learn to assess the goodness of fit of a model. -->
+
 <!-- -   We do that by calculating the amount of variability in the response that is explained by the model. -->
 
 ## $R^2$: summary of the fitted model
@@ -1813,7 +1917,7 @@ Feature selection is the process of selecting the most relevant and informative
 
 *Ridge regression* <br>
 
--   Previously we saw that the least squares fitting procedure estimates model coefficients $\beta_0, \beta_1, \cdots, \beta_p$ using the values that minimize the residual sum of squares: $$RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2$$ {#eq-lm}
+-   Previously we saw that the least squares fitting procedure estimates model coefficients $\beta_0, \beta_1, \cdots, \beta_p$ using the values that minimize the residual sum of squares: $$RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2$$
 
 . . .
 
@@ -1827,7 +1931,7 @@ $\lambda \ge 0$ is a **tuning parameter** to be determined separately e.g. via c
 
 *Ridge regression* <br>
 
-$$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2$$ {#eq-ridge}
+$$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2$$
 
 @eq-ridge trades two different criteria:
 

---FILE: session-lm/.quarto/_freeze/lm-coeff/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""cb19d3d8313ccf769cff4456f54fc54f"",
+  ""hash"": ""b0c88f6c530b45c9d2374a0419c4800c"",
   ""result"": {
-    ""markdown"": ""---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n```\n:::\n\n\n## Example: simple linear regression\n\n::: {.cell .column-margin fig-cap-location='margin' fig-heigth='4'}\n\n```{.r .cell-code  code-fold=\""false\""}\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n## \n## Call:\n## lm(formula = BMI ~ waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.2374  -2.7689  -0.4532   2.4065  19.3549 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -5.12445    2.73538  -1.873   0.0633 .  \n## waist        0.35298    0.02723  12.965   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.426 on 128 degrees of freedom\n## Multiple R-squared:  0.5677,\tAdjusted R-squared:  0.5643 \n## F-statistic: 168.1 on 1 and 128 DF,  p-value: < 2.2e-16\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n```\n\n::: {.cell-output-display}\n![Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.](lm-coeff_files/figure-html/fig-simple-1-1.png){#fig-simple-1 width=384}\n:::\n:::\n\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n2*pt(12.96291, df=128, lower=F)\n## [1] 4.605102e-25\n```\n:::\n\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n##        1 \n## 30.17348\n```\n:::\n\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""true\""}\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 35.456968   3.149661  11.257  < 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,\tAdjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n```\n:::\n\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864,\tAdjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code  code-fold=\""true\""}\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n## [1] \""factor\""\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n## \n## Call:\n## lm(formula = BMI ~ gender, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -14.167  -4.117  -0.327   3.160  19.273 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   27.7674     0.8527  32.566  < 2e-16 ***\n## genderfemale   3.9396     1.1379   3.462 0.000729 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.437 on 128 degrees of freedom\n## Multiple R-squared:  0.08563,\tAdjusted R-squared:  0.07849 \n## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286\n```\n:::\n\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m3) + \n  my.ggtheme \n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,\tAdjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n```\n:::\n\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n## \n## Call:\n## lm(formula = BMI ~ gender * height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.5564  -4.1137  -0.3072   3.1057  19.2005 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)\n## (Intercept)           31.222     20.318   1.537    0.127\n## genderfemale          14.219     26.032   0.546    0.586\n## height                -1.981     11.638  -0.170    0.865\n## genderfemale:height   -6.558     15.414  -0.425    0.671\n## \n## Residual standard error: 6.469 on 126 degrees of freedom\n## Multiple R-squared:  0.09099,\tAdjusted R-squared:  0.06935 \n## F-statistic: 4.204 on 3 and 126 DF,  p-value: 0.007155\n```\n:::\n\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n\n## Example: logistic regression with categorical variable\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n## \n## Call:\n## glm(formula = obese ~ hdl + gender, family = binomial(link = \""logit\""), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)   0.55047    0.58718   0.937   0.3485   \n## hdl          -0.02997    0.01197  -2.504   0.0123 * \n## genderfemale  1.26586    0.40120   3.155   0.0016 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 164.39  on 127  degrees of freedom\n## AIC: 170.39\n## \n## Number of Fisher Scoring iterations: 4\n```\n:::\n\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-17-1.png){width=768}\n:::\n:::\n\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n##         1 \n## 0.2792396\n```\n:::\n"",
+    ""markdown"": ""---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n```\n:::\n\n\n## Example: simple linear regression\n\n::: {.cell .column-margin fig-cap-location='margin' fig-heigth='4'}\n\n```{.r .cell-code  code-fold=\""false\""}\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n## \n## Call:\n## lm(formula = BMI ~ waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.2374  -2.7689  -0.4532   2.4065  19.3549 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -5.12445    2.73538  -1.873   0.0633 .  \n## waist        0.35298    0.02723  12.965   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.426 on 128 degrees of freedom\n## Multiple R-squared:  0.5677,\tAdjusted R-squared:  0.5643 \n## F-statistic: 168.1 on 1 and 128 DF,  p-value: < 2.2e-16\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n```\n\n::: {.cell-output-display}\n![Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.](lm-coeff_files/figure-html/fig-simple-1-1.png){#fig-simple-1 width=384}\n:::\n:::\n\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n2*pt(12.96291, df=128, lower=F)\n## [1] 4.605102e-25\n```\n:::\n\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n##        1 \n## 30.17348\n```\n:::\n\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""true\""}\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 35.456968   3.149661  11.257  < 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,\tAdjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n```\n:::\n\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864,\tAdjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?^[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code  code-fold=\""true\""}\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n**Model**\n\n$$Y_i = \\beta_{o} + \\beta_{1} I_{x_1,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n## [1] \""factor\""\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n## \n## Call:\n## lm(formula = BMI ~ gender, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -14.167  -4.117  -0.327   3.160  19.273 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   27.7674     0.8527  32.566  < 2e-16 ***\n## genderfemale   3.9396     1.1379   3.462 0.000729 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.437 on 128 degrees of freedom\n## Multiple R-squared:  0.08563,\tAdjusted R-squared:  0.07849 \n## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286\n```\n:::\n\n\n**Estimates**\n$$\\hat{\\beta_{0}} = 27.7674$$\n$$\\hat{\\beta_{1}} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m3) + \n  my.ggtheme \n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a significant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,\tAdjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n```\n:::\n\n\n**Model together with estimates**\n\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the height of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\beta_{0}} = 37.743 $$\n$$\\hat{\\beta_{1}} = 3.163$$\n$$\\hat{\\beta_{2}} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n<br />\n\n## Example: interactions\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n<!-- $$Y_{i,j} = \\alpha_i + \\beta_i\\cdot x_{ij} + \\epsilon_{i,j}$$ -->\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i$$\nwhere:\n\n- $Y_{i}$ is the BMI of person $i$\n\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\n- $x_{1,i}$ is the height of person $i$\n- and $\\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}$ is the interaction term\n\n\nIn `R` we define the interaction term with `*`:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n## \n## Call:\n## lm(formula = BMI ~ gender * height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.5564  -4.1137  -0.3072   3.1057  19.2005 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)\n## (Intercept)           31.222     20.318   1.537    0.127\n## genderfemale          14.219     26.032   0.546    0.586\n## height                -1.981     11.638  -0.170    0.865\n## genderfemale:height   -6.558     15.414  -0.425    0.671\n## \n## Residual standard error: 6.469 on 126 degrees of freedom\n## Multiple R-squared:  0.09099,\tAdjusted R-squared:  0.06935 \n## F-statistic: 4.204 on 3 and 126 DF,  p-value: 0.007155\n```\n:::\n\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n\\begin{align*}\nE(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = \\\\ 45.441 -8.539 \\cdot x\n\\end{align*}\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\beta_0} = 31.22$$\n$$\\hat{\\beta_1} = 14.22$$\n\n$$\\hat{\\beta_2} = -1.98$$\n$$\\hat{\\beta_3} = -6.558$$\n\nTo model relationship between BMI and height in males, the model reduces to: \n$$Y_i = \\beta_{0} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i = \\\\ 31.22 - 1.98 \\cdot height_i$$\nTo model relationship between BMI and height in females, the models sums up to: \n\\begin{align*}\nY_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i = \\\\ (\\beta_{0} + \\beta_{1}) + (\\beta_{2} + \\beta_{3})\\cdot height_i = \\\\  (31.22 + 14.219)  + (-1.98 -6.558)*height_i = \\\\ 45.44 - 4.58 \\cdot height_i\n\\end{align*}\n\nThis lets us model different relationships of BMI and height in both groups, with **individual intercept and slope values**.\n\nIn addition: \n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n\n## Example: logistic regression with categorical variable\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n## \n## Call:\n## glm(formula = obese ~ hdl + gender, family = binomial(link = \""logit\""), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)   0.55047    0.58718   0.937   0.3485   \n## hdl          -0.02997    0.01197  -2.504   0.0123 * \n## genderfemale  1.26586    0.40120   3.155   0.0016 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 164.39  on 127  degrees of freedom\n## AIC: 170.39\n## \n## Number of Fisher Scoring iterations: 4\n```\n:::\n\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-17-1.png){width=768}\n:::\n:::\n\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n##         1 \n## 0.2792396\n```\n:::\n"",
     ""supporting"": [
       ""lm-coeff_files""
     ],

---FILE: session-lm/.quarto/_freeze/lm-intro/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""3a5ab7f8c41eadc0a5adbd7394af3e27"",
+  ""hash"": ""d6f50aa74ee29b0a8f89116037a9de23"",
   ""result"": {
-    ""markdown"": ""---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Introduction to linear models\n\n\n::: {.cell}\n\n:::\n\n\n## Why linear models? \n\nWith linear models we can answer questions such as: \n\n  - is there a relationship between exposure and outcome, e.g. height and weight?\n  - how strong is the relationship between the two variables?\n  - what will be a predicted value of the outcome given a new set of exposure values?\n  - how accurately can we predict outcome?\n  - which variables are associated with the response, e.g. is it only height that explains weight or could it be height and age that are both associated with the response?\n  \n\n::: {.cell layout-align=\""center\"" fig-cap-location='bottom'}\n\n```{.r .cell-code}\ndata_diabetes %>%\n  ggplot(aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \""lm\"", se = FALSE) + \n  my.ggtheme + \n  xlab(\""height [m]\"") + \n  ylab(\""weight [kg]\"")\n```\n\n::: {.cell-output-display}\n![Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA.](lm-intro_files/figure-html/fig-scatter-1.png){#fig-scatter fig-align='center' width=672}\n:::\n:::\n\n\n\n## Statistical vs. deterministic relationship\n\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\n- a **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n- there is **no relationship** between variables in the **random relationship**, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\n- **a statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between $x$ and $y$ is not perfect (increasing relationship), c)  statistical relationship between $x$ and $y$ is not perfect (decreasing relationship), d) random signal](lm-intro_files/figure-html/fig-relationship-1.png){#fig-relationship width=672}\n:::\n:::\n\n\n## What linear models are and are not\n\n- In an linear model we model (explain) the relationship between a single continuous variable $Y$ and one or more variables $X$. The $X$ variables can be numerical, categorical or a mixture of both.\n- One very general form for the model would be: \n$$Y = f(X_1, X_2, \\dots X_p) + \\epsilon$$ where $f$ is some unknown function and $\\epsilon$ is the error in this representation.\n\n- For instance a **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta \\cdot x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled.\n- Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n- Formally, linear models are a way of describing a response variable in terms of **linear combination** of predictor variables, i.e. expression constructed from a a set of terms by multiplying each term by a constant and/or adding the results. \n- For instance these are all models that can be constructed using linear combinations of predictors:\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Examples of a linear models: A) $y_i = x_1 + e_i$, B) $x_1 + I_{x_i} + e_i$ C) $y_i = x_i^2 + e_i$, D) $y_i = x + x_i^3 + e_i$ showing that linear models can get more complex and/or capture more than a straight line relationship.](lm-intro_files/figure-html/fig-linear-adv-1.png){#fig-linear-adv width=672}\n:::\n:::\n\n\n\n- $Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i$\n- $Y_i = \\alpha + \\beta x_i^2 \\epsilon$\n- $Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon$\n- $Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon$\n\nvs. an example of a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$\n\n- $Y_i = \\alpha + x_i^\\beta +  \\epsilon$\n\n\n## Terminology\nThere are many terms and notations used interchangeably:\n\n- $y$ is being called:\n  - response\n  - outcome\n  - dependent variable\n\n- $x$ is being called:\n  - exposure\n  - explanatory variable\n  - dependent variable\n  - predictor\n  - covariate\n\n\n\n## Simple linear regression\n- It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n- In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\n:::{#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\nExample data:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n```\n:::\n\n:::\n\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.](lm-intro_files/figure-html/fig-lm-intro-example-1.png){#fig-lm-intro-example width=384}\n:::\n:::\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable](lm-intro_files/figure-html/fig-lm-example-reg-1.png){#fig-lm-example-reg width=384}\n:::\n:::\n\n\nThe equation for the red line is:\n$$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\nand in general:\n$$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$\n\n- In other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n- If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$), we would notice that we would get $Y=0.086 +  0.044 \\cdot 58 = 2.638$, not exactly $2.75$ as we have for our first man in our dataset that we started with, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n- We thus add to the above equation an **error term** to account for this and now we can write our **simple regression model** more formally as:\n\n$$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm}\nwhere:\n\n- $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n- $y$: is called: response, outcome, dependent variable\n- $\\alpha$ and $\\beta$ are **model coefficients**\n- and $\\epsilon_i$ is an **error terms**\n\n\n## Least squares\n- in the above **\""body weight - plasma volume\""** example, the values of $\\alpha$ and $\\beta$ have just appeared\n- in practice, $\\alpha$ and $\\beta$ values are unknown and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n- **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line](lm-intro_files/figure-html/fig-reg-errors-1.png){#fig-reg-errors width=384}\n:::\n:::\n\n\n\n<br>\n\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n- Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n- RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or\nequivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n- the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**. With some calculus, a good video explanation for the interested ones is [here](https://www.youtube.com/watch?v=ewnc1cXJmGA), we get @thm-lss\n\n::: {#thm-lss}\n## Least squares estimates for a simple linear regression\n\n$$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$\n$$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$\n\nwhere:\n\n- $\\bar{x}$: mean value of $x$\n- $\\bar{y}$: mean value of $y$\n- $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$\n- $S_{yy}$: sum of squares of $Y$ defined as  $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$\n- $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n\n:::\n\n\n<!-- We can further re-write the above sum of squares to obtain -->\n\n<!-- - sum of squares of $X$, $$S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})$$ -->\n<!-- - sum of products of $X$ and $Y$ -->\n\n<!-- $$S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}$$ -->\n\n\n::: {#exm-lss}\n\n## Least squares\n\nLet's try least squares method to find coefficient estimates in the **\""body weight and plasma volume example\""**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# initial data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx <- weight\ny <- plasma\n\n# mean values of x and y\nx.bar <- mean(x)\ny.bar <- mean(y)\n\n# Sum of squares\nSxx <-  sum((x - x.bar)^2)\nSxy <- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat <- Sxy / Sxx\nalpha.hat <- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08572428\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\nprint(beta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04361534\n```\n:::\n:::\n\n\n:::\n\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nlm(plasma ~ weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nCoefficients:\n(Intercept)       weight  \n    0.08572      0.04362  \n```\n:::\n:::\n\n\n## Intercept and Slope\n- Linear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n- $\\alpha$ is known as the **intercept**\n- $\\beta$ is known as the **slope**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)](lm-intro_files/figure-html/fig-lm-parameters-1.png){#fig-lm-parameters width=672}\n:::\n:::\n\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n- the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n- their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n- these estimated standard errors are used in **hypothesis testing** and in constructing **confidence and prediction intervals**\n\n**The most common hypothesis test** involves testing the ``null hypothesis`` of:\n\n- $H_0:$ There is no relationship between $X$ and $Y$\n- versus the ``alternative hypothesis`` $H_a:$ there is some relationship between $X$ and $Y$\n\n**Mathematically**, this corresponds to testing:\n\n- $H_0: \\beta=0$\n- versus $H_a: \\beta\\neq0$\n- since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n**Under the null hypothesis** $H_0: \\beta = 0$\n<!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  -->\n![](figures/linear-models/lm-tstatistics.png)\n\n- $n$ is number of observations\n- $p$ is number of model parameters\n- $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n- the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n::: {#exm-hypothesis-testing}\n\n## Hypothesis testing\n\nLet's look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the `R summary` of the model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n:::\n\n\n:::\n\n- Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n- If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n- and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n- we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n2*pt(2.856582, df=6, lower=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02893095\n```\n:::\n:::\n\n\n- here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n\n## Vector-matrix notations\n\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of **multiple regression**, with more than one explanatory variable in the model, it is **more efficient to use vectors and matrices to define the regression model**.\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n1. First we rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$ as it is easier to keep tracking the number of model parameters this way\n\n2. Then we notice that we actually have $n$ equations such as:\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$\n$$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$\n$$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$\n$$\\dots$$\n$$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n3. We group all $Y_i$ and $\\epsilon_i$ into column vectors:\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$ and\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n\n4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n6. We write our linear model in a vector-matrix notations as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n::: {#def-vector-matrix-lm}\n\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nwhere:\n\n- $\\mathbf{Y}$ is $n \\times1$ vector of observations\n- $\\mathbf{X}$ is $n \\times p$ **design matrix**\n- $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n- $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1    \\\\\n  \\vdots \\\\\n  \\beta_{p}\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\\n  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & x_{n,1} & \\dots & x_{n,p-1}\n\\end{bmatrix}$\n\n:::\n\n\n::: {#thm-lss-vector-matrix}\n\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nis given by:\n$$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n\n:::\n\n\n::: {#exm-vector-matrix-notation}\n\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\""weight - plasma volume model\""** as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\nwhere:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n 2.75  \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12\n\\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{8}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & 58.0 \\\\\n  1 & 70.0 \\\\\n  1 & 74.0 \\\\\n  1 & 63.5 \\\\\n  1 & 62.0 \\\\\n  1 & 70.5 \\\\\n  1 & 71.0 \\\\\n  1 & 66.0 \\\\\n\\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\nWe can do it by hand or in `R` as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 2.75\n[2,] 2.86\n[3,] 3.37\n[4,] 2.76\n[5,] 2.62\n[6,] 3.49\n[7,] 3.05\n[8,] 3.12\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       weight\n[1,] 1   58.0\n[2,] 1   70.0\n[3,] 1   74.0\n[4,] 1   63.5\n[5,] 1   62.0\n[6,] 1   70.5\n[7,] 1   71.0\n[8,] 1   66.0\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n       0.08572428\nweight 0.04361534\n```\n:::\n:::\n\n\n:::\n\n## Confidence intervals and prediction intervals\n\n- when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\n- we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval\n\n::: {#exm-prediction-and-intervals}\n\n## Prediction and intervals\n\nLet's:\n\n- find confidence intervals for our coefficient estimates\n- predict plasma volume for a men weighting 60 kg\n- find prediction interval\n- plot original data, fitted regression model, predicted observation and prediction interval\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit regression model\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# find confidence intervals for the model coefficients\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   2.5 %     97.5 %\n(Intercept) -2.419908594 2.59135716\nweight       0.006255005 0.08097567\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs <- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n2.702645 \n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# find prediction intervals\nprediction.interval <- predict(model, newdata = new.obs,  interval = \""prediction\"")\nprint(prediction.interval)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 2.702645 2.079373 3.325916\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\""weight [kg]\"", ylab=\""plasma [l]\"", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\""red\"") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\""blue\"") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval\n```\n\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Assessing model fit\n\n- Earlier we learned how to estimate parameters in a liner model using least squares estimation.\n- Now we will consider how to assess the goodness of fit of a model, i.e. how well does the model explain our data. \n- We do that by calculating the amount of variability in the response that is explained by the model.\n\n### $R^2$: summary of the fitted model {-}\n- considering a simple linear regression, the simplest model, **Model 0**, we could consider fitting is $$Y_i = \\beta_0+ \\epsilon_i$$ that corresponds to a line that run through the data but lies parallel to the horizontal axis\n- in our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple)\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n- TSS, denoted **Total corrected sum-of-squares** is the residual sum-of-squares for Model 0\n$$S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}$$ corresponding the to the sum of squared distances to the purple line\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n- Fitting **Model 1** of the form $$Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$$ we have earlier defined\n- **RSS**, the residual sum-of-squares as:\n$$RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n- that corresponds to the squared distances between the observed values $y_i, \\dots,y_n$ to fitted values $\\hat{y_1}, \\dots \\hat{y_n}$, i.e. distances to the red fitted line\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n::: {#def-r2}\n\n## $R^2$ {-}\n\nA simple but useful measure of model fit is given by $$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n\n- RSS is the residual sum-of-squares for Model 1, the fitted model of interest\n- TSS is the sum of squares of the **null model**\n\n:::\n\n- $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model\n- $R^2$ is also referred as **coefficient of determination**\n- It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data\n- Values of $R^2$ approaching 1 indicate the model to be a good fit\n- Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data\n\n### $R^2$ and correlation coefficient {-}\n\n::: {#thm-r2}\n## $R^2$\n\nIn the case of simple linear regression:\n\nModel 1: $Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$\n$$R^2 = r^2$$\nwhere:\n\n- $R^2$ is the coefficient of determination\n- $r^2$ is the sample correlation coefficient\n\n:::\n\n\n### $R^2(adj)$ {-}\n- in the case of multiple linear regression, where there is more than one explanatory variable in the model\n- we are using the adjusted version of $R^2$ to assess the model fit\n- as the number of explanatory variables increase, $R^2$ also increases\n- $R^2(adj)$ takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model\n\n\n::: {#thm-r2adj}\n\n## $R^2(adj)$\n\nFor any multiple linear regression\n$$Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i$$ $R^2(adj)$ is defined as\n$$R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$$ where\n\n- $p$ is the number of independent predictors, i.e. the number of variables in the model, excluding the constant\n\n$R^2(adj)$ can also be calculated from $R^2$:\n$$R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}$$\n\n:::\n\n<!-- We can calculate the values in R and compare the results to the output of linear regression -->\n\n\n\n\n\n## The assumptions of a linear model\n\n\n::: {.cell}\n\n:::\n\n\nUp until now we were fitting models and discussed how to assess the model fit. Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data. Informally we have been using box plots and scatter plots to look at the data. There are however formal definitions of the assumptions.\n\n**Assumption A: The deterministic part of the model captures all the non-random structure in the data**\n\n- This implies that the **mean of the errors $\\epsilon_i$** is zero.\n- Tt applies only over the range of explanatory variables.\n\n**Assumption B: the scale of variability of the errors is constant at all values of the explanatory variables**\n\n- Practically we are looking at whether the observations are equally spread on both side of the regression line.\n\n**Assumption C: the errors are independent**\n\n- Broadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another.\n\n**Assumptions D: the errors are normally distributed**\n\n- This will allow us to describe the variation in the model's parameters estimates and therefore make inferences about the population from which our sample was taken.\n\n**Assumption E: the values of the explanatory variables are recorded without error**\n\n- This one is not possible to check via examining the data, instead we have to consider the nature of the experiment.\n\n\n### Checking assumptions\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**. We use plots such as:\n\n1. Histograms or normal probability plots of $\\hat{\\epsilon_i}$\n- useful to check the assumption of normality\n\n2. Plots of $\\hat{\\epsilon_i}$ versus the fitted values $\\hat{y_i}$\n- used to detect changes in error variance\n- used to check if the mean of the errors is zero\n\n3. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{ij}$\n- this helps to check that the variable $x_j$ has a linear relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{kj}$ that is **not** in the model\n- this helps to check whether the additional variable $x_k$ might have a relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ in the order of the observations were collected\n- this is useful to check whether errors might be correlated over time\n\nLet's fit a simple model to predict `BMI` given `waist` for the diabetes study and see if the model meets the assumptions of linear models.\n\n\n\n::: {.cell fig-cap-location='margin'}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit simple linear regression model\nmodel <- lm(BMI ~ waist, data = data_diabetes)\n\n# plot diagnostic plots of the linear model\n# by default plot(model) calls four diagnostics plots\n# par() divides plot window in 2 x 2 grid\npar(mfrow=c(2,2))\nplot(model)\n```\n\n::: {.cell-output-display}\n![Default diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Simple regression to model BMI with waist explanatory variable.](lm-intro_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n- The residual plots provides examples of a situation where the assumptions appear to be met.\n- The linear regression appears to describe data quite well.\n- There is no obvious trend of any kind in the residuals vs. fitted values (the shape is scattered) with potential few outliers that we may want to decided to exclude later.\n- Points lie reasonably well along the line in the normal probability plot, hence normality appears to be met.\n\n**Examples of assumptions not being met**\n\n\n::: {.cell layout-align=\""center\"" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case](figures/linear-models/lm-assumptions-01.png){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\""center\"" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of non-constant variance](figures/linear-models/lm-assumptions-02.png){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\""center\"" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected](figures/linear-models/lm-assumptions-03.png){fig-align='center'}\n:::\n:::\n\n\n\n### Influential observations\n- Sometimes individual observations can exert a great deal of influence on the fitted model.\n- One routine way of checking for this is to fit the model $n$ times, missing out each observation in turn.\n- If we removed i-th observation and compared the fitted value from the full model, say $\\hat{y_j}$ to those obtained by removing this point, denoted $\\hat{y_{j(i)}}$ then\n- observations with a high Cook's distance (measuring the effect of deleting a given observation) could be influential.\n\nLet's remove some observation with higher Cook's distance from protein data set, re-fit our model and compare the diagnostics plots\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# observations to be removed (based on Residuals vs. Leverage plot)\nobs <- c(13, 78, 83, 84)\n\n# fit models removing observations\ndata_diabetes_flr <- data_diabetes[-obs, ]\n\nmodel_flr <- lm(BMI ~ waist, data = data_diabetes_flr)\n\n# plot diagnostics plot\npar(mfrow=c(2,2))\nplot(model_flr)\n```\n\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-25-1.png){width=768}\n:::\n:::\n"",
+    ""markdown"": ""---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Introduction to linear models\n\n\n::: {.cell}\n\n:::\n\n\n## Why linear models? \n\nWith linear models we can answer questions such as: \n\n  - is there a relationship between exposure and outcome, e.g. height and weight?\n  - how strong is the relationship between the two variables?\n  - what will be a predicted value of the outcome given a new set of exposure values?\n  - how accurately can we predict outcome?\n  - which variables are associated with the response, e.g. is it only height that explains weight or could it be height and age that are both associated with the response?\n  \n\n::: {.cell layout-align=\""center\"" fig-cap-location='bottom'}\n\n```{.r .cell-code}\ndata_diabetes %>%\n  ggplot(aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \""lm\"", se = FALSE) + \n  my.ggtheme + \n  xlab(\""height [m]\"") + \n  ylab(\""weight [kg]\"")\n```\n\n::: {.cell-output-display}\n![Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA.](lm-intro_files/figure-html/fig-scatter-1.png){#fig-scatter fig-align='center' width=672}\n:::\n:::\n\n\n\n## Statistical vs. deterministic relationship\n\nRelationships in probability and statistics can generally be one of three things: deterministic, random, or statistical:\n\n- a **deterministic** relationship involves **an exact relationship** between two variables, for instance Fahrenheit and Celsius degrees is defined by an equation $Fahrenheit=\\frac{9}{5}\\cdot Celcius+32$\n- there is **no relationship** between variables in the **random relationship**, for instance number of succulents Olga buys and time of the year as Olga keeps buying succulents whenever she feels like it throughout the entire year\n- **a statistical relationship** is a **mixture of deterministic and random relationship**, e.g. the savings that Olga has left in the bank account depend on Olga's monthly salary income (deterministic part) and the money spent on buying succulents (random part)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between $x$ and $y$ is not perfect (increasing relationship), c)  statistical relationship between $x$ and $y$ is not perfect (decreasing relationship), d) random signal](lm-intro_files/figure-html/fig-relationship-1.png){#fig-relationship width=672}\n:::\n:::\n\n\n## What linear models are and are not\n\n- In an linear model we model (explain) the relationship between a single continuous variable $Y$ and one or more variables $X$. The $X$ variables can be numerical, categorical or a mixture of both.\n- One very general form for the model would be: \n$$Y = f(X_1, X_2, \\dots X_p) + \\epsilon$$ where $f$ is some unknown function and $\\epsilon$ is the error in this representation.\n\n- For instance a **simple linear regression** through the origin is a simple linear model of the form $$Y_i = \\beta \\cdot x + \\epsilon$$ often used to express a relationship of **one numerical variable to another**, e.g. the calories burnt and the kilometers cycled.\n- Linear models can become quite advanced by including **many variables**, e.g. the calories burnt could be a function of the kilometers cycled, road incline and status of a bike, or the **transformation of the variables**, e.g. a function of kilometers cycled squared\n- Formally, linear models are a way of describing a response variable in terms of **linear combination** of predictor variables, i.e. expression constructed from a a set of terms by multiplying each term by a constant and/or adding the results. \n- For instance these are all models that can be constructed using linear combinations of predictors:\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Examples of a linear models: A) $y_i = x_1 + e_i$, B) $x_1 + I_{x_i} + e_i$ C) $y_i = x_i^2 + e_i$, D) $y_i = x + x_i^3 + e_i$ showing that linear models can get more complex and/or capture more than a straight line relationship.](lm-intro_files/figure-html/fig-linear-adv-1.png){#fig-linear-adv width=672}\n:::\n:::\n\n\n\n- $Y_i = \\alpha + \\beta x_i + \\gamma y_i + \\epsilon_i$\n- $Y_i = \\alpha + \\beta x_i^2 \\epsilon$\n- $Y_i = \\alpha + \\beta x_i^2 + \\gamma x_i^3 + \\epsilon$\n- $Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 y_i + \\beta_4 \\sqrt {y_i} + \\beta_5 x_i y_i + \\epsilon$\n\nvs. an example of a non-linear model where parameter $\\beta$ appears in the exponent of $x_i$\n\n- $Y_i = \\alpha + x_i^\\beta +  \\epsilon$\n\n\n## Terminology\nThere are many terms and notations used interchangeably:\n\n- $y$ is being called:\n  - response\n  - outcome\n  - dependent variable\n\n- $x$ is being called:\n  - exposure\n  - explanatory variable\n  - independent variable\n  - predictor\n  - covariate\n\n\n\n## Simple linear regression\n- It is used to check the association between **the numerical outcome and one numerical explanatory variable**\n- In practice, we are finding the best-fitting straight line to describe the relationship between the outcome and exposure\n\n\n:::{#exm-simple-lm}\n## Weight and plasma volume\n\nLet's look at the example data containing body weight (kg) and plasma volume (liters) for eight healthy men to see what the best-fitting straight line is.\n\nExample data:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n```\n:::\n\n:::\n\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*.](lm-intro_files/figure-html/fig-lm-intro-example-1.png){#fig-lm-intro-example width=384}\n:::\n:::\n\n::: {.cell fig-cap-location='margin' fig-heigth='4'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable](lm-intro_files/figure-html/fig-lm-example-reg-1.png){#fig-lm-example-reg width=384}\n:::\n:::\n\n\nThe equation for the red line is:\n$$Y_i=0.086 +  0.044 \\cdot x_i \\quad for \\;i = 1 \\dots 8$$\nand in general:\n$$Y_i=\\alpha + \\beta \\cdot x_i \\quad for \\; i = 1 \\dots n$$\n\n- In other words, by finding the best-fitting straight line we are **building a statistical model** to represent the relationship between plasma volume ($Y$) and explanatory body weight variable ($x$)\n- If we were to use our model $Y_i=0.086 + 0.044 \\cdot x_i$ to find plasma volume given a weight of 58 kg (our first observation, $i=1$), we would notice that we would get $Y=0.086 +  0.044 \\cdot 58 = 2.638$, not exactly $2.75$ as we have for our first man in our dataset that we started with, i.e. $2.75 - 2.638 = 0.112 \\neq 0$.\n- We thus add to the above equation an **error term** to account for this and now we can write our **simple regression model** more formally as:\n\n$$Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$$ {#eq-lm}\nwhere:\n\n- $x$: is called: exposure variable, explanatory variable, dependent variable, predictor, covariate\n- $y$: is called: response, outcome, dependent variable\n- $\\alpha$ and $\\beta$ are **model coefficients**\n- and $\\epsilon_i$ is an **error terms**\n\n\n## Least squares\n- in the above **\""body weight - plasma volume\""** example, the values of $\\alpha$ and $\\beta$ have just appeared\n- in practice, $\\alpha$ and $\\beta$ values are unknown and we use data to **estimate these coefficients**, noting the estimates with a **hat**, $\\hat{\\alpha}$ and $\\hat{\\beta}$\n- **least squares** is one of the methods of parameters estimation, i.e. finding $\\hat{\\alpha}$ and $\\hat{\\beta}$\n\n\n::: {.cell fig-cap-location='margin'}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line](lm-intro_files/figure-html/fig-reg-errors-1.png){#fig-reg-errors width=384}\n:::\n:::\n\n\n\n<br>\n\nLet $\\hat{y_i}=\\hat{\\alpha} + \\hat{\\beta}x_i$ be the prediction $y_i$ based on the $i$-th value of $x$:\n\n- Then $\\epsilon_i = y_i - \\hat{y_i}$ represents the $i$-th **residual**, i.e. the difference between the $i$-th observed response value and the $i$-th response value that is predicted by the linear model\n- RSS, the **residual sum of squares** is defined as: $$RSS = \\epsilon_1^2 + \\epsilon_2^2 + \\dots + \\epsilon_n^2$$ or\nequivalently as: $$RSS=(y_1-\\hat{\\alpha}-\\hat{\\beta}x_1)^2+(y_2-\\hat{\\alpha}-\\hat{\\beta}x_2)^2+...+(y_n-\\hat{\\alpha}-\\hat{\\beta}x_n)^2$$\n- the least squares approach chooses $\\hat{\\alpha}$ and $\\hat{\\beta}$ **to minimize the RSS**. With some calculus, a good video explanation for the interested ones is [here](https://www.youtube.com/watch?v=ewnc1cXJmGA), we get @thm-lss\n\n::: {#thm-lss}\n## Least squares estimates for a simple linear regression\n\n$$\\hat{\\beta} = \\frac{S_{xy}}{S_{xx}}$$\n$$\\hat{\\alpha} = \\bar{y}-\\frac{S_{xy}}{S_{xx}}\\cdot \\bar{x}$$\n\nwhere:\n\n- $\\bar{x}$: mean value of $x$\n- $\\bar{y}$: mean value of $y$\n- $S_{xx}$: sum of squares of $X$ defined as $S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2$\n- $S_{yy}$: sum of squares of $Y$ defined as  $S_{yy} = \\displaystyle \\sum_{i=1}^{n}(y_i-\\bar{y})^2$\n- $S_{xy}$: sum of products of $X$ and $Y$ defined as $S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})$\n\n:::\n\n\n<!-- We can further re-write the above sum of squares to obtain -->\n\n<!-- - sum of squares of $X$, $$S_{xx} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2-\\frac{(\\sum_{i=1}^{n}x_i)^2}{n})$$ -->\n<!-- - sum of products of $X$ and $Y$ -->\n\n<!-- $$S_{xy} = \\displaystyle \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})=\\sum_{i=1}^nx_iy_i-\\frac{\\sum_{i=1}^{n}x_i\\sum_{i=1}^{n}y_i}{n}$$ -->\n\n\n::: {#exm-lss}\n\n## Least squares\n\nLet's try least squares method to find coefficient estimates in the **\""body weight and plasma volume example\""**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# initial data\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\n# rename variables for convenience\nx <- weight\ny <- plasma\n\n# mean values of x and y\nx.bar <- mean(x)\ny.bar <- mean(y)\n\n# Sum of squares\nSxx <-  sum((x - x.bar)^2)\nSxy <- sum((x-x.bar)*(y-y.bar))\n\n# Coefficient estimates\nbeta.hat <- Sxy / Sxx\nalpha.hat <- y.bar - Sxy/Sxx*x.bar\n\n# Print estimated coefficients alpha and beta\nprint(alpha.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08572428\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\nprint(beta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04361534\n```\n:::\n:::\n\n\n:::\n\n\nIn R we can use `lm()`, the built-in function, to fit a linear regression model and we can replace the above code with one line\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nlm(plasma ~ weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nCoefficients:\n(Intercept)       weight  \n    0.08572      0.04362  \n```\n:::\n:::\n\n\n## Intercept and Slope\n- Linear regression gives us estimates of model coefficient $Y_i = \\alpha + \\beta x_i + \\epsilon_i$\n- $\\alpha$ is known as the **intercept**\n- $\\beta$ is known as the **slope**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)](lm-intro_files/figure-html/fig-lm-parameters-1.png){#fig-lm-parameters width=672}\n:::\n:::\n\n\n## Hypothesis testing\n\n**Is there a relationship between the response and the predictor?**\n\n- the calculated $\\hat{\\alpha}$ and $\\hat{\\beta}$ are **estimates of the population values** of the intercept and slope and are therefore subject to **sampling variation**\n- their precision is measured by their **estimated standard errors**, `e.s.e`($\\hat{\\alpha}$) and `e.s.e`($\\hat{\\beta}$)\n- these estimated standard errors are used in **hypothesis testing** and in constructing **confidence and prediction intervals**\n\n**The most common hypothesis test** involves testing the ``null hypothesis`` of:\n\n- $H_0:$ There is no relationship between $X$ and $Y$\n- versus the ``alternative hypothesis`` $H_a:$ there is some relationship between $X$ and $Y$\n\n**Mathematically**, this corresponds to testing:\n\n- $H_0: \\beta=0$\n- versus $H_a: \\beta\\neq0$\n- since if $\\beta=0$ then the model $Y_i=\\alpha+\\beta x_i + \\epsilon_i$ reduces to $Y=\\alpha + \\epsilon_i$\n\n**Under the null hypothesis** $H_0: \\beta = 0$\n<!-- we have: $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} \\sim t(n-p)$$  -->\n![](figures/linear-models/lm-tstatistics.png)\n\n- $n$ is number of observations\n- $p$ is number of model parameters\n- $\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})}$ is the ratio of the departure of the estimated value of a parameter, $\\hat\\beta$, from its hypothesized value, $\\beta$, to its standard error, called `t-statistics`\n- the `t-statistics` follows Student's t distribution with $n-p$ degrees of freedom\n\n::: {#exm-hypothesis-testing}\n\n## Hypothesis testing\n\nLet's look again at our example data. This time we will not only fit the linear regression model but also look a bit more closely at the `R summary` of the model\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nweight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)\nplasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)\n\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n:::\n\n\n:::\n\n- Under `Estimate` we see estimates of our model coefficients, $\\hat{\\alpha}$ (intercept) and $\\hat{\\beta}$ (slope, here weight), followed by their estimated standard errors, `Std. Errors`\n- If we were to test if there is an **association between weight and plasma volume** we would write under the null hypothesis $H_0: \\beta = 0$ $$\\frac{\\hat{\\beta}-\\beta}{e.s.e(\\hat{\\beta})} = \\frac{0.04362-0}{0.01527} = 2.856582$$\n- and we would **compare** `t-statistics` to `Student's t distribution` with $n-p = 8 - 2 = 6$ degrees of freedom (as we have 8 observations and two model parameters, $\\alpha$ and $\\beta$)\n- we can use **Student's t distribution table** or **R code** to obtain the associated *P*-value\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n2*pt(2.856582, df=6, lower=F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02893095\n```\n:::\n:::\n\n\n- here the observed t-statistics is large and therefore yields a small *P*-value, meaning that **there is sufficient evidence to reject null hypothesis in favor of the alternative** and conclude that there is a significant association between weight and plasma volume\n\n\n## Vector-matrix notations\n\nWhile in simple linear regression it is feasible to arrive at the parameters estimates using calculus in more realistic settings of **multiple regression**, with more than one explanatory variable in the model, it is **more efficient to use vectors and matrices to define the regression model**.\n\nLet's **rewrite** our simple linear regression model $Y_i = \\alpha + \\beta_i + \\epsilon_i \\quad i=1,\\dots n$ **into vector-matrix notation** in **6 steps**.\n\n1. First we rename our $\\alpha$ to $\\beta_0$ and $\\beta$ to $\\beta_1$ as it is easier to keep tracking the number of model parameters this way\n\n2. Then we notice that we actually have $n$ equations such as:\n$$y_1 = \\beta_0 + \\beta_1 x_1 + \\epsilon_1$$\n$$y_2 = \\beta_0 + \\beta_1 x_2 + \\epsilon_2$$\n$$y_3 = \\beta_0 + \\beta_1 x_3 + \\epsilon_3$$\n$$\\dots$$\n$$y_n = \\beta_0 + \\beta_1 x_n + \\epsilon_n$$\n\n3. We group all $Y_i$ and $\\epsilon_i$ into column vectors:\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$ and\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n\n4. We stack two parameters $\\beta_0$ and $\\beta_1$ into another column vector:$$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$$\n\n5. We append a vector of ones with the single predictor for each $i$ and create a matrix with two columns called **design matrix** $$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_1  \\\\\n  1 & x_2  \\\\\n  \\vdots & \\vdots \\\\\n  1 & x_{n}\n\\end{bmatrix}$$\n\n6. We write our linear model in a vector-matrix notations as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\n::: {#def-vector-matrix-lm}\n\n## vector matrix form of the linear model\n\nThe vector-matrix representation of a linear model with $p-1$ predictors can be written as\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nwhere:\n\n- $\\mathbf{Y}$ is $n \\times1$ vector of observations\n- $\\mathbf{X}$ is $n \\times p$ **design matrix**\n- $\\boldsymbol\\beta$ is $p \\times1$ vector of parameters\n- $\\boldsymbol\\epsilon$ is $n \\times1$ vector of vector of random errors, indepedent and identically distributed (i.i.d) N(0, $\\sigma^2$)\n\nIn full, the above vectors and matrix have the form:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n  y_1  \\\\\n  y_2    \\\\\n  \\vdots \\\\\n  y_{n}\n\\end{bmatrix}$\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1    \\\\\n  \\vdots \\\\\n  \\beta_{p}\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{n}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & x_{1,1} & \\dots & x_{1,p-1} \\\\\n  1 & x_{2,1} & \\dots & x_{2,p-1} \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots \\\\\n  1 & x_{n,1} & \\dots & x_{n,p-1}\n\\end{bmatrix}$\n\n:::\n\n\n::: {#thm-lss-vector-matrix}\n\n## Least squares in vector-matrix notation\n\nThe least squares estimates for a linear regression of the form:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\n\nis given by:\n$$\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n\n:::\n\n\n::: {#exm-vector-matrix-notation}\n\n## vector-matrix-notation\n\nFollowing the above definition we can write the **\""weight - plasma volume model\""** as:\n$$\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$$\nwhere:\n\n$\\mathbf{Y}=\\begin{bmatrix}\n 2.75  \\\\ 2.86 \\\\ 3.37 \\\\ 2.76 \\\\ 2.62 \\\\ 3.49 \\\\ 3.05 \\\\ 3.12\n\\end{bmatrix}$\n\n$\\boldsymbol\\beta=\\begin{bmatrix}\n  \\beta_0  \\\\\n  \\beta_1\n\\end{bmatrix}$\n$\\boldsymbol\\epsilon=\\begin{bmatrix}\n  \\epsilon_1  \\\\\n  \\epsilon_2    \\\\\n  \\vdots \\\\\n  \\epsilon_{8}\n\\end{bmatrix}$\n$\\mathbf{X}=\\begin{bmatrix}\n  1 & 58.0 \\\\\n  1 & 70.0 \\\\\n  1 & 74.0 \\\\\n  1 & 63.5 \\\\\n  1 & 62.0 \\\\\n  1 & 70.5 \\\\\n  1 & 71.0 \\\\\n  1 & 66.0 \\\\\n\\end{bmatrix}$\n\nand we can estimate model parameters using $\\hat{\\mathbf{\\beta}}= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$.\n\nWe can do it by hand or in `R` as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nn <- length(plasma) # no. of observation\nY <- as.matrix(plasma, ncol=1)\nX <- cbind(rep(1, length=n), weight)\nX <- as.matrix(X)\n\n# print Y and X to double-check that the format is according to the definition\nprint(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 2.75\n[2,] 2.86\n[3,] 3.37\n[4,] 2.76\n[5,] 2.62\n[6,] 3.49\n[7,] 3.05\n[8,] 3.12\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\nprint(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       weight\n[1,] 1   58.0\n[2,] 1   70.0\n[3,] 1   74.0\n[4,] 1   63.5\n[5,] 1   62.0\n[6,] 1   70.5\n[7,] 1   71.0\n[8,] 1   66.0\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# least squares estimate\n# solve() finds inverse of matrix\nbeta.hat <- solve(t(X)%*%X)%*%t(X)%*%Y\nprint(beta.hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]\n       0.08572428\nweight 0.04361534\n```\n:::\n:::\n\n\n:::\n\n## Confidence intervals and prediction intervals\n\n- when we estimate coefficients we can also find their **confidence intervals**, typically 95\\% confidence intervals, i.e. a range of values that contain the true unknown value of the parameter\n- we can also use linear regression models to predict the response value given a new observation and find **prediction intervals**. Here, we look at any specific value of $x_i$, and find an interval around the predicted value $y_i'$ for $x_i$ such that there is a 95\\% probability that the real value of y (in the population) corresponding to $x_i$ is within this interval\n\n::: {#exm-prediction-and-intervals}\n\n## Prediction and intervals\n\nLet's:\n\n- find confidence intervals for our coefficient estimates\n- predict plasma volume for a men weighting 60 kg\n- find prediction interval\n- plot original data, fitted regression model, predicted observation and prediction interval\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit regression model\nmodel <- lm(plasma ~ weight)\nprint(summary(model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = plasma ~ weight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27880 -0.14178 -0.01928  0.13986  0.32939 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.08572    1.02400   0.084   0.9360  \nweight       0.04362    0.01527   2.857   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2188 on 6 degrees of freedom\nMultiple R-squared:  0.5763,\tAdjusted R-squared:  0.5057 \nF-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# find confidence intervals for the model coefficients\nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   2.5 %     97.5 %\n(Intercept) -2.419908594 2.59135716\nweight       0.006255005 0.08097567\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# predict plasma volume for a new observation of 60 kg\n# we have to create data frame with a variable name matching the one used to build the model\nnew.obs <- data.frame(weight = 60)\npredict(model, newdata = new.obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n2.702645 \n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# find prediction intervals\nprediction.interval <- predict(model, newdata = new.obs,  interval = \""prediction\"")\nprint(prediction.interval)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 2.702645 2.079373 3.325916\n```\n:::\n\n```{.r .cell-code  code-fold=\""false\""}\n# plot the original data, fitted regression and predicted value\nplot(weight, plasma, pch=19, xlab=\""weight [kg]\"", ylab=\""plasma [l]\"", ylim=c(2,4))\nlines(weight, model$fitted.values, col=\""red\"") # fitted model in red\npoints(new.obs, predict(model, newdata = new.obs), pch=19, col=\""blue\"") # predicted value at 60kg\nsegments(60, prediction.interval[2], 60, prediction.interval[3], lty = 3) # add prediction interval\n```\n\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Assessing model fit\n\n- Earlier we learned how to estimate parameters in a liner model using least squares estimation.\n- Now we will consider how to assess the goodness of fit of a model, i.e. how well does the model explain our data. \n- We do that by calculating the amount of variability in the response that is explained by the model.\n\n### $R^2$: summary of the fitted model {-}\n- considering a simple linear regression, the simplest model, **Model 0**, we could consider fitting is $$Y_i = \\beta_0+ \\epsilon_i$$ that corresponds to a line that run through the data but lies parallel to the horizontal axis\n- in our plasma volume example that would correspond the mean value of plasma volume being predicted for any value of weight (in purple)\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n- TSS, denoted **Total corrected sum-of-squares** is the residual sum-of-squares for Model 0\n$$S(\\hat{\\beta_0}) = TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 = S_{yy}$$ corresponding the to the sum of squared distances to the purple line\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n- Fitting **Model 1** of the form $$Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$$ we have earlier defined\n- **RSS**, the residual sum-of-squares as:\n$$RSS = \\displaystyle \\sum_{i=1}^{n}(y_i - \\{\\hat{\\beta_0} + \\hat{\\beta}_1x_{1i} + \\dots + \\hat{\\beta}_px_{pi}\\}) = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n- that corresponds to the squared distances between the observed values $y_i, \\dots,y_n$ to fitted values $\\hat{y_1}, \\dots \\hat{y_n}$, i.e. distances to the red fitted line\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n::: {#def-r2}\n\n## $R^2$ {-}\n\nA simple but useful measure of model fit is given by $$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n\n- RSS is the residual sum-of-squares for Model 1, the fitted model of interest\n- TSS is the sum of squares of the **null model**\n\n:::\n\n- $R^2$ quantifies how much of a drop in the residual sum-of-squares is accounted for by fitting the proposed model\n- $R^2$ is also referred as **coefficient of determination**\n- It is expressed on a scale, as a proportion (between 0 and 1) of the total variation in the data\n- Values of $R^2$ approaching 1 indicate the model to be a good fit\n- Values of $R^2$ less than 0.5 suggest that the model gives rather a poor fit to the data\n\n### $R^2$ and correlation coefficient {-}\n\n::: {#thm-r2}\n## $R^2$\n\nIn the case of simple linear regression:\n\nModel 1: $Y_i = \\beta_0 + \\beta_1x + \\epsilon_i$\n$$R^2 = r^2$$\nwhere:\n\n- $R^2$ is the coefficient of determination\n- $r^2$ is the sample correlation coefficient\n\n:::\n\n\n### $R^2(adj)$ {-}\n- in the case of multiple linear regression, where there is more than one explanatory variable in the model\n- we are using the adjusted version of $R^2$ to assess the model fit\n- as the number of explanatory variables increase, $R^2$ also increases\n- $R^2(adj)$ takes this into account, i.e. adjusts for the fact that there is more than one explanatory variable in the model\n\n\n::: {#thm-r2adj}\n\n## $R^2(adj)$\n\nFor any multiple linear regression\n$$Y_i = \\beta_0 + \\beta_1x_{1i} + \\dots + \\beta_{p-1}x_{(p-1)i} +  \\epsilon_i$$ $R^2(adj)$ is defined as\n$$R^2(adj) = 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$$ where\n\n- $p$ is the number of independent predictors, i.e. the number of variables in the model, excluding the constant\n\n$R^2(adj)$ can also be calculated from $R^2$:\n$$R^2(adj) = 1 - (1-R^2)\\frac{n-1}{n-p-1}$$\n\n:::\n\n<!-- We can calculate the values in R and compare the results to the output of linear regression -->\n\n\n\n\n\n## The assumptions of a linear model\n\n\n::: {.cell}\n\n:::\n\n\nUp until now we were fitting models and discussed how to assess the model fit. Before making use of a fitted model for explanation or prediction, it is wise to check that the model provides an adequate description of the data. Informally we have been using box plots and scatter plots to look at the data. There are however formal definitions of the assumptions.\n\n**Assumption A: The deterministic part of the model captures all the non-random structure in the data**\n\n- This implies that the **mean of the errors $\\epsilon_i$** is zero.\n- Tt applies only over the range of explanatory variables.\n\n**Assumption B: the scale of variability of the errors is constant at all values of the explanatory variables**\n\n- Practically we are looking at whether the observations are equally spread on both side of the regression line.\n\n**Assumption C: the errors are independent**\n\n- Broadly speaking this means that knowledge of errors attached to one observation does not give us any information about the error attached to another.\n\n**Assumptions D: the errors are normally distributed**\n\n- This will allow us to describe the variation in the model's parameters estimates and therefore make inferences about the population from which our sample was taken.\n\n**Assumption E: the values of the explanatory variables are recorded without error**\n\n- This one is not possible to check via examining the data, instead we have to consider the nature of the experiment.\n\n\n### Checking assumptions\n**Residuals**, $\\hat{\\epsilon_i} = y_i - \\hat{y_i}$ are the **main ingredient to check model assumptions**. We use plots such as:\n\n1. Histograms or normal probability plots of $\\hat{\\epsilon_i}$\n- useful to check the assumption of normality\n\n2. Plots of $\\hat{\\epsilon_i}$ versus the fitted values $\\hat{y_i}$\n- used to detect changes in error variance\n- used to check if the mean of the errors is zero\n\n3. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{ij}$\n- this helps to check that the variable $x_j$ has a linear relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ vs. an explanatory variable $x_{kj}$ that is **not** in the model\n- this helps to check whether the additional variable $x_k$ might have a relationship with the response variable\n\n4. Plots of $\\hat{\\epsilon_i}$ in the order of the observations were collected\n- this is useful to check whether errors might be correlated over time\n\nLet's fit a simple model to predict `BMI` given `waist` for the diabetes study and see if the model meets the assumptions of linear models.\n\n\n\n::: {.cell fig-cap-location='margin'}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit simple linear regression model\nmodel <- lm(BMI ~ waist, data = data_diabetes)\n\n# plot diagnostic plots of the linear model\n# by default plot(model) calls four diagnostics plots\n# par() divides plot window in 2 x 2 grid\npar(mfrow=c(2,2))\nplot(model)\n```\n\n::: {.cell-output-display}\n![Default diagnostic residual plots based on the lm() model used to assess whether the assumptions of linear models are met. Simple regression to model BMI with waist explanatory variable.](lm-intro_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n- The residual plots provides examples of a situation where the assumptions appear to be met.\n- The linear regression appears to describe data quite well.\n- There is no obvious trend of any kind in the residuals vs. fitted values (the shape is scattered) with potential few outliers that we may want to decided to exclude later.\n- Points lie reasonably well along the line in the normal probability plot, hence normality appears to be met.\n\n**Examples of assumptions not being met**\n\n\n::: {.cell layout-align=\""center\"" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of data with a typical seasonal variation (up and down) coupled wtih a linear trend. The blue line gives the linear regression fit to the data, which clearly is not adequate. In comparison, if we used a non-parametric fit, we will get the red line as the fitted relationship. The residual plot retains pattern, given by orange line, indicating that the linear model is not appropriate in this case](figures/linear-models/lm-assumptions-01.png){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\""center\"" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of non-constant variance](figures/linear-models/lm-assumptions-02.png){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\""center\"" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of residulas deviating from QQ plot, i.e. not following normal distribution. The residuals can deviate in both upper and lower tail. On the left tails are lighter meaning that they have smaller values that what would be expected, on the right there are heavier tails with values larger than expected](figures/linear-models/lm-assumptions-03.png){fig-align='center'}\n:::\n:::\n\n\n\n### Influential observations\n- Sometimes individual observations can exert a great deal of influence on the fitted model.\n- One routine way of checking for this is to fit the model $n$ times, missing out each observation in turn.\n- If we removed i-th observation and compared the fitted value from the full model, say $\\hat{y_j}$ to those obtained by removing this point, denoted $\\hat{y_{j(i)}}$ then\n- observations with a high Cook's distance (measuring the effect of deleting a given observation) could be influential.\n\nLet's remove some observation with higher Cook's distance from protein data set, re-fit our model and compare the diagnostics plots\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# observations to be removed (based on Residuals vs. Leverage plot)\nobs <- c(13, 78, 83, 84)\n\n# fit models removing observations\ndata_diabetes_flr <- data_diabetes[-obs, ]\n\nmodel_flr <- lm(BMI ~ waist, data = data_diabetes_flr)\n\n# plot diagnostics plot\npar(mfrow=c(2,2))\nplot(model_flr)\n```\n\n::: {.cell-output-display}\n![](lm-intro_files/figure-html/unnamed-chunk-25-1.png){width=768}\n:::\n:::\n"",
     ""supporting"": [
       ""lm-intro_files/figure-html""
     ],

---FILE: session-lm/.quarto/cites/index.json---
@@ -1 +1 @@
-{""lm-ml.qmd"":[],""lm-lasso-exercises.qmd"":[],""lm-diagn.qmd"":[],""lm-coeff.qmd"":[],""lm-intro.qmd"":[],""lm-diagn-exercises.qmd"":[],""lm-glm.qmd"":[],""lm-intro-exercises.qmd"":[],""lm-reg-cls.qmd"":[],""index.qmd"":[],""lm-coeff-exercises.qmd"":[]}
+{""lm-coeff-exercises.qmd"":[],""lm-ml.qmd"":[],""lm-glm.qmd"":[],""lm-intro-exercises.qmd"":[],""lm-reg-cls.qmd"":[],""lm-coeff.qmd"":[],""lm-diagn-exercises.qmd"":[],""lm-lasso-exercises.qmd"":[],""index.qmd"":[],""lm-intro.qmd"":[],""lm-diagn.qmd"":[]}

---FILE: session-lm/.quarto/idx/lm-coeff.qmd.json---
@@ -1 +1 @@
-{""title"":""Common cases"",""markdown"":{""yaml"":{""output"":""html_document"",""editor_options"":{""chunk_output_type"":""console""}},""headingText"":""Common cases"",""containsRefs"":false,""markdown"":""\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n"",""srcMarkdownNoYaml"":""\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n""},""formats"":{""html"":{""identifier"":{""display-name"":""HTML"",""target-format"":""html"",""base-format"":""html""},""execute"":{""fig-width"":7,""fig-height"":5,""fig-format"":""retina"",""fig-dpi"":96,""df-print"":""default"",""error"":false,""eval"":true,""cache"":null,""freeze"":false,""echo"":true,""output"":""html_document"",""warning"":true,""include"":true,""keep-md"":false,""keep-ipynb"":false,""ipynb"":null,""enabled"":null,""daemon"":null,""daemon-restart"":false,""debug"":false,""ipynb-filters"":[],""engine"":""knitr""},""render"":{""keep-tex"":false,""keep-source"":false,""keep-hidden"":false,""prefer-html"":false,""output-divs"":true,""output-ext"":""html"",""fig-align"":""default"",""fig-pos"":null,""fig-env"":null,""code-fold"":true,""code-overflow"":""scroll"",""code-link"":false,""code-line-numbers"":false,""code-tools"":false,""tbl-colwidths"":""auto"",""merge-includes"":true,""inline-includes"":false,""preserve-yaml"":false,""latex-auto-mk"":true,""latex-auto-install"":true,""latex-clean"":true,""latex-max-runs"":10,""latex-makeindex"":""makeindex"",""latex-makeindex-opts"":[],""latex-tlmgr-opts"":[],""latex-input-paths"":[],""latex-output-dir"":null,""link-external-icon"":false,""link-external-newwindow"":false,""self-contained-math"":false,""format-resources"":[],""notebook-links"":true,""format-links"":true},""pandoc"":{""standalone"":true,""wrap"":""none"",""default-image-extension"":""png"",""to"":""html"",""output-file"":""lm-coeff.html""},""language"":{""toc-title-document"":""Table of contents"",""toc-title-website"":""On this page"",""related-formats-title"":""Other Formats"",""related-notebooks-title"":""Notebooks"",""source-notebooks-prefix"":""Source"",""section-title-abstract"":""Abstract"",""section-title-appendices"":""Appendices"",""section-title-footnotes"":""Footnotes"",""section-title-references"":""References"",""section-title-reuse"":""Reuse"",""section-title-copyright"":""Copyright"",""section-title-citation"":""Citation"",""appendix-attribution-cite-as"":""For attribution, please cite this work as:"",""appendix-attribution-bibtex"":""BibTeX citation:"",""title-block-author-single"":""Author"",""title-block-author-plural"":""Authors"",""title-block-affiliation-single"":""Affiliation"",""title-block-affiliation-plural"":""Affiliations"",""title-block-published"":""Published"",""title-block-modified"":""Modified"",""callout-tip-title"":""Tip"",""callout-note-title"":""Note"",""callout-warning-title"":""Warning"",""callout-important-title"":""Important"",""callout-caution-title"":""Caution"",""code-summary"":""Code"",""code-tools-menu-caption"":""Code"",""code-tools-show-all-code"":""Show All Code"",""code-tools-hide-all-code"":""Hide All Code"",""code-tools-view-source"":""View Source"",""code-tools-source-code"":""Source Code"",""code-line"":""Line"",""code-lines"":""Lines"",""copy-button-tooltip"":""Copy to Clipboard"",""copy-button-tooltip-success"":""Copied!"",""repo-action-links-edit"":""Edit this page"",""repo-action-links-source"":""View source"",""repo-action-links-issue"":""Report an issue"",""back-to-top"":""Back to top"",""search-no-results-text"":""No results"",""search-matching-documents-text"":""matching documents"",""search-copy-link-title"":""Copy link to search"",""search-hide-matches-text"":""Hide additional matches"",""search-more-match-text"":""more match in this document"",""search-more-matches-text"":""more matches in this document"",""search-clear-button-title"":""Clear"",""search-detached-cancel-button-title"":""Cancel"",""search-submit-button-title"":""Submit"",""search-label"":""Search"",""toggle-section"":""Toggle section"",""toggle-sidebar"":""Toggle sidebar navigation"",""toggle-dark-mode"":""Toggle dark mode"",""toggle-reader-mode"":""Toggle reader mode"",""toggle-navigation"":""Toggle navigation"",""crossref-fig-title"":""Figure"",""crossref-tbl-title"":""Table"",""crossref-lst-title"":""Listing"",""crossref-thm-title"":""Theorem"",""crossref-lem-title"":""Lemma"",""crossref-cor-title"":""Corollary"",""crossref-prp-title"":""Proposition"",""crossref-cnj-title"":""Conjecture"",""crossref-def-title"":""Definition"",""crossref-exm-title"":""Example"",""crossref-exr-title"":""Exercise"",""crossref-ch-prefix"":""Chapter"",""crossref-apx-prefix"":""Appendix"",""crossref-sec-prefix"":""Section"",""crossref-eq-prefix"":""Equation"",""crossref-lof-title"":""List of Figures"",""crossref-lot-title"":""List of Tables"",""crossref-lol-title"":""List of Listings"",""environment-proof-title"":""Proof"",""environment-remark-title"":""Remark"",""environment-solution-title"":""Solution"",""listing-page-order-by"":""Order By"",""listing-page-order-by-default"":""Default"",""listing-page-order-by-date-asc"":""Oldest"",""listing-page-order-by-date-desc"":""Newest"",""listing-page-order-by-number-desc"":""High to Low"",""listing-page-order-by-number-asc"":""Low to High"",""listing-page-field-date"":""Date"",""listing-page-field-title"":""Title"",""listing-page-field-description"":""Description"",""listing-page-field-author"":""Author"",""listing-page-field-filename"":""File Name"",""listing-page-field-filemodified"":""Modified"",""listing-page-field-subtitle"":""Subtitle"",""listing-page-field-readingtime"":""Reading Time"",""listing-page-field-categories"":""Categories"",""listing-page-minutes-compact"":""{0} min"",""listing-page-category-all"":""All"",""listing-page-no-matches"":""No matching items""},""metadata"":{""lang"":""en"",""fig-responsive"":true,""quarto-version"":""1.3.450"",""bibliography"":[""references.bib""],""theme"":""cosmo"",""editor_options"":{""chunk_output_type"":""console""}},""extensions"":{""book"":{""multiFile"":true}}}},""projectFormats"":[""html""]}
\ No newline at end of file
+{""title"":""Common cases"",""markdown"":{""yaml"":{""output"":""html_document"",""editor_options"":{""chunk_output_type"":""console""}},""headingText"":""Common cases"",""containsRefs"":false,""markdown"":""\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?^[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\beta_{o} + \\beta_{1} I_{x_1,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\beta_{0}} = 27.7674$$\n$$\\hat{\\beta_{1}} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a significant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the height of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\beta_{0}} = 37.743 $$\n$$\\hat{\\beta_{1}} = 3.163$$\n$$\\hat{\\beta_{2}} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n<!-- $$Y_{i,j} = \\alpha_i + \\beta_i\\cdot x_{ij} + \\epsilon_{i,j}$$ -->\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i$$\nwhere:\n\n- $Y_{i}$ is the BMI of person $i$\n\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\n- $x_{1,i}$ is the height of person $i$\n- and $\\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}$ is the interaction term\n\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n\\begin{align*}\nE(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = \\\\ 45.441 -8.539 \\cdot x\n\\end{align*}\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\beta_0} = 31.22$$\n$$\\hat{\\beta_1} = 14.22$$\n\n$$\\hat{\\beta_2} = -1.98$$\n$$\\hat{\\beta_3} = -6.558$$\n\nTo model relationship between BMI and height in males, the model reduces to: \n$$Y_i = \\beta_{0} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i = \\\\ 31.22 - 1.98 \\cdot height_i$$\nTo model relationship between BMI and height in females, the models sums up to: \n\\begin{align*}\nY_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i = \\\\ (\\beta_{0} + \\beta_{1}) + (\\beta_{2} + \\beta_{3})\\cdot height_i = \\\\  (31.22 + 14.219)  + (-1.98 -6.558)*height_i = \\\\ 45.44 - 4.58 \\cdot height_i\n\\end{align*}\n\nThis lets us model different relationships of BMI and height in both groups, with **individual intercept and slope values**.\n\nIn addition: \n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n"",""srcMarkdownNoYaml"":""\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?^[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\beta_{o} + \\beta_{1} I_{x_1,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\beta_{0}} = 27.7674$$\n$$\\hat{\\beta_{1}} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a significant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the height of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\beta_{0}} = 37.743 $$\n$$\\hat{\\beta_{1}} = 3.163$$\n$$\\hat{\\beta_{2}} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n<!-- $$Y_{i,j} = \\alpha_i + \\beta_i\\cdot x_{ij} + \\epsilon_{i,j}$$ -->\n$$Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i$$\nwhere:\n\n- $Y_{i}$ is the BMI of person $i$\n\n\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\n- $x_{1,i}$ is the height of person $i$\n- and $\\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}$ is the interaction term\n\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n\\begin{align*}\nE(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = \\\\ 45.441 -8.539 \\cdot x\n\\end{align*}\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\beta_0} = 31.22$$\n$$\\hat{\\beta_1} = 14.22$$\n\n$$\\hat{\\beta_2} = -1.98$$\n$$\\hat{\\beta_3} = -6.558$$\n\nTo model relationship between BMI and height in males, the model reduces to: \n$$Y_i = \\beta_{0} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i = \\\\ 31.22 - 1.98 \\cdot height_i$$\nTo model relationship between BMI and height in females, the models sums up to: \n\\begin{align*}\nY_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i = \\\\ (\\beta_{0} + \\beta_{1}) + (\\beta_{2} + \\beta_{3})\\cdot height_i = \\\\  (31.22 + 14.219)  + (-1.98 -6.558)*height_i = \\\\ 45.44 - 4.58 \\cdot height_i\n\\end{align*}\n\nThis lets us model different relationships of BMI and height in both groups, with **individual intercept and slope values**.\n\nIn addition: \n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n""},""formats"":{""html"":{""identifier"":{""display-name"":""HTML"",""target-format"":""html"",""base-format"":""html""},""execute"":{""fig-width"":7,""fig-height"":5,""fig-format"":""retina"",""fig-dpi"":96,""df-print"":""default"",""error"":false,""eval"":true,""cache"":null,""freeze"":false,""echo"":true,""output"":""html_document"",""warning"":true,""include"":true,""keep-md"":false,""keep-ipynb"":false,""ipynb"":null,""enabled"":null,""daemon"":null,""daemon-restart"":false,""debug"":false,""ipynb-filters"":[],""engine"":""knitr""},""render"":{""keep-tex"":false,""keep-source"":false,""keep-hidden"":false,""prefer-html"":false,""output-divs"":true,""output-ext"":""html"",""fig-align"":""default"",""fig-pos"":null,""fig-env"":null,""code-fold"":true,""code-overflow"":""scroll"",""code-link"":false,""code-line-numbers"":false,""code-tools"":false,""tbl-colwidths"":""auto"",""merge-includes"":true,""inline-includes"":false,""preserve-yaml"":false,""latex-auto-mk"":true,""latex-auto-install"":true,""latex-clean"":true,""latex-max-runs"":10,""latex-makeindex"":""makeindex"",""latex-makeindex-opts"":[],""latex-tlmgr-opts"":[],""latex-input-paths"":[],""latex-output-dir"":null,""link-external-icon"":false,""link-external-newwindow"":false,""self-contained-math"":false,""format-resources"":[],""notebook-links"":true,""format-links"":true},""pandoc"":{""standalone"":true,""wrap"":""none"",""default-image-extension"":""png"",""to"":""html"",""output-file"":""lm-coeff.html""},""language"":{""toc-title-document"":""Table of contents"",""toc-title-website"":""On this page"",""related-formats-title"":""Other Formats"",""related-notebooks-title"":""Notebooks"",""source-notebooks-prefix"":""Source"",""section-title-abstract"":""Abstract"",""section-title-appendices"":""Appendices"",""section-title-footnotes"":""Footnotes"",""section-title-references"":""References"",""section-title-reuse"":""Reuse"",""section-title-copyright"":""Copyright"",""section-title-citation"":""Citation"",""appendix-attribution-cite-as"":""For attribution, please cite this work as:"",""appendix-attribution-bibtex"":""BibTeX citation:"",""title-block-author-single"":""Author"",""title-block-author-plural"":""Authors"",""title-block-affiliation-single"":""Affiliation"",""title-block-affiliation-plural"":""Affiliations"",""title-block-published"":""Published"",""title-block-modified"":""Modified"",""callout-tip-title"":""Tip"",""callout-note-title"":""Note"",""callout-warning-title"":""Warning"",""callout-important-title"":""Important"",""callout-caution-title"":""Caution"",""code-summary"":""Code"",""code-tools-menu-caption"":""Code"",""code-tools-show-all-code"":""Show All Code"",""code-tools-hide-all-code"":""Hide All Code"",""code-tools-view-source"":""View Source"",""code-tools-source-code"":""Source Code"",""code-line"":""Line"",""code-lines"":""Lines"",""copy-button-tooltip"":""Copy to Clipboard"",""copy-button-tooltip-success"":""Copied!"",""repo-action-links-edit"":""Edit this page"",""repo-action-links-source"":""View source"",""repo-action-links-issue"":""Report an issue"",""back-to-top"":""Back to top"",""search-no-results-text"":""No results"",""search-matching-documents-text"":""matching documents"",""search-copy-link-title"":""Copy link to search"",""search-hide-matches-text"":""Hide additional matches"",""search-more-match-text"":""more match in this document"",""search-more-matches-text"":""more matches in this document"",""search-clear-button-title"":""Clear"",""search-detached-cancel-button-title"":""Cancel"",""search-submit-button-title"":""Submit"",""search-label"":""Search"",""toggle-section"":""Toggle section"",""toggle-sidebar"":""Toggle sidebar navigation"",""toggle-dark-mode"":""Toggle dark mode"",""toggle-reader-mode"":""Toggle reader mode"",""toggle-navigation"":""Toggle navigation"",""crossref-fig-title"":""Figure"",""crossref-tbl-title"":""Table"",""crossref-lst-title"":""Listing"",""crossref-thm-title"":""Theorem"",""crossref-lem-title"":""Lemma"",""crossref-cor-title"":""Corollary"",""crossref-prp-title"":""Proposition"",""crossref-cnj-title"":""Conjecture"",""crossref-def-title"":""Definition"",""crossref-exm-title"":""Example"",""crossref-exr-title"":""Exercise"",""crossref-ch-prefix"":""Chapter"",""crossref-apx-prefix"":""Appendix"",""crossref-sec-prefix"":""Section"",""crossref-eq-prefix"":""Equation"",""crossref-lof-title"":""List of Figures"",""crossref-lot-title"":""List of Tables"",""crossref-lol-title"":""List of Listings"",""environment-proof-title"":""Proof"",""environment-remark-title"":""Remark"",""environment-solution-title"":""Solution"",""listing-page-order-by"":""Order By"",""listing-page-order-by-default"":""Default"",""listing-page-order-by-date-asc"":""Oldest"",""listing-page-order-by-date-desc"":""Newest"",""listing-page-order-by-number-desc"":""High to Low"",""listing-page-order-by-number-asc"":""Low to High"",""listing-page-field-date"":""Date"",""listing-page-field-title"":""Title"",""listing-page-field-description"":""Description"",""listing-page-field-author"":""Author"",""listing-page-field-filename"":""File Name"",""listing-page-field-filemodified"":""Modified"",""listing-page-field-subtitle"":""Subtitle"",""listing-page-field-readingtime"":""Reading Time"",""listing-page-field-categories"":""Categories"",""listing-page-minutes-compact"":""{0} min"",""listing-page-category-all"":""All"",""listing-page-no-matches"":""No matching items""},""metadata"":{""lang"":""en"",""fig-responsive"":true,""quarto-version"":""1.3.450"",""bibliography"":[""references.bib""],""theme"":""cosmo"",""editor_options"":{""chunk_output_type"":""console""}},""extensions"":{""book"":{""multiFile"":true}}}},""projectFormats"":[""html""]}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/28d59191---
@@ -1 +1 @@
-{""options"":{""chapters"":true},""entries"":[],""headings"":[""preface""]}
\ No newline at end of file
+{""entries"":[],""headings"":[""preface""],""options"":{""chapters"":true}}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/295884aa---
@@ -1 +1 @@
-{""entries"":[{""key"":""exr-lm-fit"",""caption"":""Fitting linear model"",""order"":{""number"":1,""section"":[0,0,0,0,0,0,0]}},{""key"":""exr-lm-hypothesis"",""caption"":""Hypothesis testing"",""order"":{""number"":2,""section"":[0,0,0,0,0,0,0]}},{""key"":""exr-lm-assess-fit"",""caption"":""Evaluate model fit"",""order"":{""number"":3,""section"":[0,0,0,0,0,0,0]}}],""headings"":[""fitting-linear-model"",""hypothesis-testing"",""evaluate-model-fit""],""options"":{""chapters"":true}}
\ No newline at end of file
+{""headings"":[""fitting-linear-model"",""hypothesis-testing"",""evaluate-model-fit""],""options"":{""chapters"":true},""entries"":[{""order"":{""number"":1,""section"":[0,0,0,0,0,0,0]},""caption"":""Fitting linear model"",""key"":""exr-lm-fit""},{""order"":{""number"":2,""section"":[0,0,0,0,0,0,0]},""caption"":""Hypothesis testing"",""key"":""exr-lm-hypothesis""},{""order"":{""number"":3,""section"":[0,0,0,0,0,0,0]},""caption"":""Evaluate model fit"",""key"":""exr-lm-assess-fit""}]}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/4d506fc9---
@@ -1 +1 @@
-{""headings"":[""brozek-score"",""answers-to-selected-exercises""],""entries"":[{""caption"":""Brozek score"",""key"":""exr-brozek"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":1}}],""options"":{""chapters"":true}}
\ No newline at end of file
+{""headings"":[""brozek-score"",""answers-to-selected-exercises""],""entries"":[{""caption"":""Brozek score"",""key"":""exr-brozek"",""order"":{""number"":1,""section"":[0,0,0,0,0,0,0]}}],""options"":{""chapters"":true}}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/5b28360f---
@@ -1 +1 @@
-{""headings"":[""height-weight-gender"",""trout"",""lowering-blood-pressure"",""answers-to-selected-exercises""],""entries"":[{""caption"":""Trout"",""key"":""exr-trout"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":2}},{""caption"":""Height-weight-gender"",""key"":""exr-rerun"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":1}},{""caption"":""Lowering blood pressure"",""key"":""exr-drug"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":3}}],""options"":{""chapters"":true}}
\ No newline at end of file
+{""entries"":[{""order"":{""section"":[0,0,0,0,0,0,0],""number"":2},""caption"":""Trout"",""key"":""exr-trout""},{""order"":{""section"":[0,0,0,0,0,0,0],""number"":1},""caption"":""Height-weight-gender"",""key"":""exr-rerun""},{""order"":{""section"":[0,0,0,0,0,0,0],""number"":3},""caption"":""Lowering blood pressure"",""key"":""exr-drug""}],""headings"":[""height-weight-gender"",""trout"",""lowering-blood-pressure"",""answers-to-selected-exercises""],""options"":{""chapters"":true}}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/691de0dc---
@@ -1 +1 @@
-{""options"":{""chapters"":true},""headings"":[""example-simple-linear-regression"",""example-multiple-regression"",""example-categorical-variable"",""example-categorical-numerical-variables"",""example-interactions"",""example-logistic-regression-with-categorical-variable""],""entries"":[{""key"":""fig-simple-1"",""caption"":""Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model."",""order"":{""number"":1,""section"":[3,1,0,0,0,0,0]}}]}
\ No newline at end of file
+{""entries"":[{""caption"":""Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model."",""key"":""fig-simple-1"",""order"":{""section"":[3,1,0,0,0,0,0],""number"":1}}],""options"":{""chapters"":true},""headings"":[""example-simple-linear-regression"",""example-multiple-regression"",""example-categorical-variable"",""example-categorical-numerical-variables"",""example-interactions"",""example-logistic-regression-with-categorical-variable""]}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/9e3bf041---
@@ -1 +1 @@
-{""options"":{""chapters"":true},""headings"":[""why-linear-models"",""statistical-vs.-deterministic-relationship"",""what-linear-models-are-and-are-not"",""terminology"",""simple-linear-regression"",""weight-and-plasma-volume"",""least-squares"",""least-squares-estimates-for-a-simple-linear-regression"",""least-squares-1"",""intercept-and-slope"",""hypothesis-testing"",""hypothesis-testing-1"",""vector-matrix-notations"",""vector-matrix-form-of-the-linear-model"",""least-squares-in-vector-matrix-notation"",""vector-matrix-notation"",""confidence-intervals-and-prediction-intervals"",""prediction-and-intervals"",""assessing-model-fit"",""r2-summary-of-the-fitted-model"",""r2"",""r2-and-correlation-coefficient"",""r2-1"",""r2adj"",""r2adj-1"",""the-assumptions-of-a-linear-model"",""checking-assumptions"",""influential-observations""],""entries"":[{""key"":""def-r2"",""caption"":""R^2"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":2}},{""key"":""eq-lm"",""caption"":"""",""order"":{""section"":[1,5,0,0,0,0,0],""number"":1}},{""key"":""exm-lss"",""caption"":""Least squares"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":2}},{""key"":""thm-lss"",""caption"":""Least squares estimates for a simple linear regression"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":1}},{""key"":""fig-scatter"",""caption"":""Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA."",""order"":{""section"":[1,1,0,0,0,0,0],""number"":1}},{""key"":""fig-linear-adv"",""caption"":""Examples of a linear models: A) y_i = x_1 + e_i, B) x_1 + I_{x_i} + e_i C) y_i = x_i^2 + e_i, D) y_i = x + x_i^3 + e_i showing that linear models can get more complex and/or capture more than a straight line relationship."",""order"":{""section"":[1,3,0,0,0,0,0],""number"":3}},{""key"":""exm-vector-matrix-notation"",""caption"":""vector-matrix-notation"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":4}},{""key"":""thm-r2adj"",""caption"":""R^2(adj)"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":4}},{""key"":""exm-prediction-and-intervals"",""caption"":""Prediction and intervals"",""order"":{""section"":[1,10,0,0,0,0,0],""number"":5}},{""key"":""thm-r2"",""caption"":""R^2"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":3}},{""key"":""fig-reg-errors"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":6}},{""key"":""exm-simple-lm"",""caption"":""Weight and plasma volume"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":1}},{""key"":""fig-lm-parameters"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)"",""order"":{""section"":[1,7,0,0,0,0,0],""number"":7}},{""key"":""thm-lss-vector-matrix"",""caption"":""Least squares in vector-matrix notation"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":2}},{""key"":""def-vector-matrix-lm"",""caption"":""vector matrix form of the linear model"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":1}},{""key"":""fig-lm-example-reg"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":5}},{""key"":""fig-lm-intro-example"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca."",""order"":{""section"":[1,5,0,0,0,0,0],""number"":4}},{""key"":""fig-relationship"",""caption"":""Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between x and y is not perfect (increasing relationship), c) statistical relationship between x and y is not perfect (decreasing relationship), d) random signal"",""order"":{""section"":[1,2,0,0,0,0,0],""number"":2}},{""key"":""exm-hypothesis-testing"",""caption"":""Hypothesis testing"",""order"":{""section"":[1,8,0,0,0,0,0],""number"":3}}]}
\ No newline at end of file
+{""entries"":[{""order"":{""section"":[1,10,0,0,0,0,0],""number"":5},""key"":""exm-prediction-and-intervals"",""caption"":""Prediction and intervals""},{""order"":{""section"":[1,5,0,0,0,0,0],""number"":1},""key"":""eq-lm"",""caption"":""""},{""order"":{""section"":[1,9,0,0,0,0,0],""number"":1},""key"":""def-vector-matrix-lm"",""caption"":""vector matrix form of the linear model""},{""order"":{""section"":[1,5,0,0,0,0,0],""number"":4},""key"":""fig-lm-intro-example"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca.""},{""order"":{""section"":[1,5,0,0,0,0,0],""number"":5},""key"":""fig-lm-example-reg"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable""},{""order"":{""section"":[1,7,0,0,0,0,0],""number"":7},""key"":""fig-lm-parameters"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)""},{""order"":{""section"":[1,9,0,0,0,0,0],""number"":2},""key"":""thm-lss-vector-matrix"",""caption"":""Least squares in vector-matrix notation""},{""order"":{""section"":[1,11,0,0,0,0,0],""number"":4},""key"":""thm-r2adj"",""caption"":""R^2(adj)""},{""order"":{""section"":[1,6,0,0,0,0,0],""number"":2},""key"":""exm-lss"",""caption"":""Least squares""},{""order"":{""section"":[1,6,0,0,0,0,0],""number"":1},""key"":""thm-lss"",""caption"":""Least squares estimates for a simple linear regression""},{""order"":{""section"":[1,11,0,0,0,0,0],""number"":3},""key"":""thm-r2"",""caption"":""R^2""},{""order"":{""section"":[1,11,0,0,0,0,0],""number"":2},""key"":""def-r2"",""caption"":""R^2""},{""order"":{""section"":[1,3,0,0,0,0,0],""number"":3},""key"":""fig-linear-adv"",""caption"":""Examples of a linear models: A) y_i = x_1 + e_i, B) x_1 + I_{x_i} + e_i C) y_i = x_i^2 + e_i, D) y_i = x + x_i^3 + e_i showing that linear models can get more complex and/or capture more than a straight line relationship.""},{""order"":{""section"":[1,1,0,0,0,0,0],""number"":1},""key"":""fig-scatter"",""caption"":""Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA.""},{""order"":{""section"":[1,9,0,0,0,0,0],""number"":4},""key"":""exm-vector-matrix-notation"",""caption"":""vector-matrix-notation""},{""order"":{""section"":[1,2,0,0,0,0,0],""number"":2},""key"":""fig-relationship"",""caption"":""Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between x and y is not perfect (increasing relationship), c) statistical relationship between x and y is not perfect (decreasing relationship), d) random signal""},{""order"":{""section"":[1,6,0,0,0,0,0],""number"":6},""key"":""fig-reg-errors"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line""},{""order"":{""section"":[1,5,0,0,0,0,0],""number"":1},""key"":""exm-simple-lm"",""caption"":""Weight and plasma volume""},{""order"":{""section"":[1,8,0,0,0,0,0],""number"":3},""key"":""exm-hypothesis-testing"",""caption"":""Hypothesis testing""}],""options"":{""chapters"":true},""headings"":[""why-linear-models"",""statistical-vs.-deterministic-relationship"",""what-linear-models-are-and-are-not"",""terminology"",""simple-linear-regression"",""weight-and-plasma-volume"",""least-squares"",""least-squares-estimates-for-a-simple-linear-regression"",""least-squares-1"",""intercept-and-slope"",""hypothesis-testing"",""hypothesis-testing-1"",""vector-matrix-notations"",""vector-matrix-form-of-the-linear-model"",""least-squares-in-vector-matrix-notation"",""vector-matrix-notation"",""confidence-intervals-and-prediction-intervals"",""prediction-and-intervals"",""assessing-model-fit"",""r2-summary-of-the-fitted-model"",""r2"",""r2-and-correlation-coefficient"",""r2-1"",""r2adj"",""r2adj-1"",""the-assumptions-of-a-linear-model"",""checking-assumptions"",""influential-observations""]}
\ No newline at end of file

---FILE: session-lm/docs/lm-coeff.html---
@@ -437,7 +437,7 @@ <h2 data-number=""3.2"" class=""anchored"" data-anchor-id=""example-multiple-regressi
 </div>
 <ul>
 <li>What happens to <code>BMI</code> if <code>hdl</code> increases by 10?<a href=""#fn12"" class=""footnote-ref"" id=""fnref12"" role=""doc-noteref""><sup>12</sup></a></li>
-<li>What happens to <code>BMI</code> if <code>hdl</code> increases by 10 using the first model again?[decreases by ca. 0.9]</li>
+<li>What happens to <code>BMI</code> if <code>hdl</code> increases by 10 using the first model again?<a href=""#fn13"" class=""footnote-ref"" id=""fnref13"" role=""doc-noteref""><sup>13</sup></a></li>
 <li>How do you explain the difference in <code>BMI</code> changes given these two models?</li>
 </ul>
 <p><strong>Specific interpretation</strong></p>
@@ -482,8 +482,8 @@ <h2 data-number=""3.3"" class=""anchored"" data-anchor-id=""example-categorical-varia
 <p><img src=""lm-coeff_files/figure-html/unnamed-chunk-8-1.png"" class=""img-fluid"" width=""672""></p>
 </div></div></div>
 <p><strong>Model</strong></p>
-<p><span class=""math display"">\[Y_i = \alpha + \beta I_{x_i} + \epsilon_i\]</span> where <span class=""math display"">\[\begin{equation}
-    I_{x_i} =
+<p><span class=""math display"">\[Y_i = \beta_{o} + \beta_{1} I_{x_1,i} + \epsilon_i\]</span> where <span class=""math display"">\[\begin{equation}
+    I_{x_1,i} =
     \left\{
         \begin{array}{cc}
                 1 &amp; \mathrm{if\ } x_i=1 \\
@@ -519,7 +519,7 @@ <h2 data-number=""3.3"" class=""anchored"" data-anchor-id=""example-categorical-varia
 <span id=""cb9-24""><a href=""#cb9-24"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Multiple R-squared:  0.08563,    Adjusted R-squared:  0.07849 </span></span>
 <span id=""cb9-25""><a href=""#cb9-25"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
-<p><strong>Estimates</strong> <span class=""math display"">\[\hat{\alpha} = 27.7674\]</span> <span class=""math display"">\[\hat{\beta} = 3.9396\]</span></p>
+<p><strong>Estimates</strong> <span class=""math display"">\[\hat{\beta_{0}} = 27.7674\]</span> <span class=""math display"">\[\hat{\beta_{1}} = 3.9396\]</span></p>
 <ul>
 <li>The <code>lm()</code> function chooses automatically one of the category as baseline, here <code>females</code>.</li>
 <li>Model summary prints the output of the model with the baseline category <strong>“hidden”</strong>.</li>
@@ -547,7 +547,7 @@ <h2 data-number=""3.3"" class=""anchored"" data-anchor-id=""example-categorical-varia
 <section id=""example-categorical-numerical-variables"" class=""level2"" data-number=""3.4"">
 <h2 data-number=""3.4"" class=""anchored"" data-anchor-id=""example-categorical-numerical-variables""><span class=""header-section-number"">3.4</span> Example: categorical &amp; numerical variables</h2>
 <ul>
-<li>Above we observed a signficant difference in average <code>BMI</code> between men and women among the study participants.</li>
+<li>Above we observed a significant difference in average <code>BMI</code> between men and women among the study participants.</li>
 <li>Can we also observe a significant relationship between <code>BMI</code> and <code>height</code>?</li>
 <li>And if so, does this relationship depend on <code>gender</code>?</li>
 </ul>
@@ -581,8 +581,8 @@ <h2 data-number=""3.4"" class=""anchored"" data-anchor-id=""example-categorical-numer
 </ul>
 <p>To assess the relationship we use a model containing <code>height</code> and <code>gender</code>.</p>
 <p><strong>Model</strong></p>
-<p><span class=""math display"">\[Y_i = \alpha + \beta I_{x_i} + \gamma x_{2,i} + \epsilon_i\]</span> where <span class=""math display"">\[\begin{equation}
-    I_{x_i} =
+<p><span class=""math display"">\[Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} x_{2,i} + \epsilon_i\]</span> where <span class=""math display"">\[\begin{equation}
+    I_{x_1,i} =
     \left\{
         \begin{array}{cc}
                 1 &amp; \mathrm{if\ } \quad person_i\;is\;female \\
@@ -617,7 +617,7 @@ <h2 data-number=""3.4"" class=""anchored"" data-anchor-id=""example-categorical-numer
 <span id=""cb12-22""><a href=""#cb12-22"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p><strong>Model together with estimates</strong></p>
-<p><span class=""math display"">\[Y_i = \alpha + \beta I_{x_i} + \gamma x_{2,i} + \epsilon_i\]</span> where <span class=""math display"">\[\begin{equation}
+<p><span class=""math display"">\[Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} \cdot x_{2,i} + \epsilon_i\]</span> where <span class=""math display"">\[\begin{equation}
     I_{x_i} =
     \left\{
         \begin{array}{cc}
@@ -626,9 +626,9 @@ <h2 data-number=""3.4"" class=""anchored"" data-anchor-id=""example-categorical-numer
         \end{array}
     \right.
 \end{equation}\]</span></p>
-<p>and <span class=""math inline"">\(x_{2,i}\)</span> is the weight of person <span class=""math inline"">\(i\)</span></p>
+<p>and <span class=""math inline"">\(x_{2,i}\)</span> is the height of person <span class=""math inline"">\(i\)</span></p>
 <p><strong>Estimates</strong></p>
-<p><span class=""math display"">\[\hat{\alpha} = 37.743 \]</span> <span class=""math display"">\[\hat{\beta} = 3.163\]</span> <span class=""math display"">\[\hat{\gamma} = -5.719\]</span></p>
+<p><span class=""math display"">\[\hat{\beta_{0}} = 37.743 \]</span> <span class=""math display"">\[\hat{\beta_{1}} = 3.163\]</span> <span class=""math display"">\[\hat{\beta_{2}} = -5.719\]</span></p>
 <ul>
 <li>For instance, using our estimates, for a female who happens to 1.7 m tall we would predict <code>BMI</code> of: <span class=""math display"">\[E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \cdot 1.7) = 31.1837\]</span></li>
 <li>and for a male of height 1.7 m tall we would predict <code>BMI</code> of <span class=""math display"">\[E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \cdot 1.7)  = 28.0207\]</span></li>
@@ -655,12 +655,23 @@ <h2 data-number=""3.5"" class=""anchored"" data-anchor-id=""example-interactions""><sp
 <li>And we then talk about including <strong>interaction effect</strong> since the two lines may interact (cross).</li>
 </ul>
 <p><strong>Model</strong></p>
-<p><span class=""math display"">\[Y_{i,j} = \alpha_i + \beta_ix_{ij} + \epsilon_{i,j}\]</span> where:</p>
+<!-- $$Y_{i,j} = \alpha_i + \beta_i\cdot x_{ij} + \epsilon_{i,j}$$ -->
+<p><span class=""math display"">\[Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} \cdot x_{2,i} + \beta_{3} \cdot I_{x_1,i} \cdot x_{2, i}+ \epsilon_i\]</span> where:</p>
+<ul>
+<li><span class=""math inline"">\(Y_{i}\)</span> is the BMI of person <span class=""math inline"">\(i\)</span></li>
+</ul>
+<p><span class=""math display"">\[\begin{equation}
+    I_{x_1,i} =
+    \left\{
+        \begin{array}{cc}
+                1 &amp; \mathrm{if\ } \quad person_i\;is\;female \\
+                0 &amp; \mathrm{if\ } \quad person_i\;is\;male \\
+        \end{array}
+    \right.
+\end{equation}\]</span></p>
 <ul>
-<li><span class=""math inline"">\(Y_{i,j}\)</span> is the BMI of person <span class=""math inline"">\(j\)</span> of gender <span class=""math inline"">\(i\)</span></li>
-<li><span class=""math inline"">\(x_{ij}\)</span> is the height of person <span class=""math inline"">\(j\)</span> of gender <span class=""math inline"">\(i\)</span></li>
-<li><span class=""math inline"">\(i=1\)</span> corresponds to women in our example (keeping the same coding as above)</li>
-<li><span class=""math inline"">\(i=2\)</span> corresponds to men</li>
+<li><span class=""math inline"">\(x_{1,i}\)</span> is the height of person <span class=""math inline"">\(i\)</span></li>
+<li>and <span class=""math inline"">\(\beta_{3} \cdot I_{x_1,i} \cdot x_{2, i}\)</span> is the interaction term</li>
 </ul>
 <p>In <code>R</code> we define the interaction term with <code>*</code>:</p>
 <div class=""cell"">
@@ -688,11 +699,18 @@ <h2 data-number=""3.5"" class=""anchored"" data-anchor-id=""example-interactions""><sp
 </div>
 <p>Now, based on the regression output we would expect:</p>
 <ul>
-<li>for a woman of height <span class=""math inline"">\(x\)</span>, a BMI value of: <span class=""math display"">\[E(BMI|female\; and \; height=x)=31.222 + 14.219 - 1.981 \cdot x - 6.558 \cdot x = 45.441 -8.539 \cdot x\]</span></li>
+<li>for a woman of height <span class=""math inline"">\(x\)</span>, a BMI value of: <span class=""math display"">\[\begin{align*}
+E(BMI|female\; and \; height=x)=31.222 + 14.219 - 1.981 \cdot x - 6.558 \cdot x = \\ 45.441 -8.539 \cdot x
+\end{align*}\]</span></li>
 <li>for a man of height <span class=""math inline"">\(x\)</span>, a BMI value of <span class=""math display"">\[E(BMI|male\; and \; height=x)=31.222-1.981 \cdot x\]</span></li>
 </ul>
-<p><strong>Estimates</strong> <span class=""math display"">\[\hat{\alpha_1} = 45.441\]</span> <span class=""math display"">\[\hat{\beta_1} = 31.222\]</span></p>
-<p><span class=""math display"">\[\hat{\alpha_2} = 47.34778\]</span> <span class=""math display"">\[\hat{\beta_2} = -1.981\]</span></p>
+<p><strong>Estimates</strong> <span class=""math display"">\[\hat{\beta_0} = 31.22\]</span> <span class=""math display"">\[\hat{\beta_1} = 14.22\]</span></p>
+<p><span class=""math display"">\[\hat{\beta_2} = -1.98\]</span> <span class=""math display"">\[\hat{\beta_3} = -6.558\]</span></p>
+<p>To model relationship between BMI and height in males, the model reduces to: <span class=""math display"">\[Y_i = \beta_{0} + \beta_{2} \cdot x_{2,i} + \epsilon_i = \\ 31.22 - 1.98 \cdot height_i\]</span> To model relationship between BMI and height in females, the models sums up to: <span class=""math display"">\[\begin{align*}
+Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} \cdot x_{2,i} + \beta_{3} \cdot I_{x_1,i} \cdot x_{2, i}+ \epsilon_i = \\ (\beta_{0} + \beta_{1}) + (\beta_{2} + \beta_{3})\cdot height_i = \\  (31.22 + 14.219)  + (-1.98 -6.558)*height_i = \\ 45.44 - 4.58 \cdot height_i
+\end{align*}\]</span></p>
+<p>This lets us model different relationships of BMI and height in both groups, with <strong>individual intercept and slope values</strong>.</p>
+<p>In addition:</p>
 <ul>
 <li>We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term “Genderfemale:height” is equal to zero.</li>
 <li>Or therefore conclude that the relationship between <code>BMI</code> and <code>height</code> is different for men and women.</li>
@@ -740,8 +758,8 @@ <h2 data-number=""3.6"" class=""anchored"" data-anchor-id=""example-logistic-regressi
 <span id=""cb16-27""><a href=""#cb16-27"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Number of Fisher Scoring iterations: 4</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <ul>
-<li>By how much change odds of suffering from obesity when <code>hdl</code> increases by 1?<a href=""#fn13"" class=""footnote-ref"" id=""fnref13"" role=""doc-noteref""><sup>13</sup></a></li>
-<li>What are the odds of suffering from obesity and being a women vs.&nbsp;suffering from obesity and being a man?<a href=""#fn14"" class=""footnote-ref"" id=""fnref14"" role=""doc-noteref""><sup>14</sup></a></li>
+<li>By how much change odds of suffering from obesity when <code>hdl</code> increases by 1?<a href=""#fn14"" class=""footnote-ref"" id=""fnref14"" role=""doc-noteref""><sup>14</sup></a></li>
+<li>What are the odds of suffering from obesity and being a women vs.&nbsp;suffering from obesity and being a man?<a href=""#fn15"" class=""footnote-ref"" id=""fnref15"" role=""doc-noteref""><sup>15</sup></a></li>
 </ul>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb17""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb17-1""><a href=""#cb17-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">ggPredict</span>(m6) <span class=""sc"">+</span> </span>
@@ -780,8 +798,9 @@ <h2 data-number=""3.6"" class=""anchored"" data-anchor-id=""example-logistic-regressi
 <li id=""fn10""><p>decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.<a href=""#fnref10"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
 <li id=""fn11""><p>we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of <span class=""math inline"">\(H_0: \beta=0\)</span> versus <span class=""math inline"">\(H_0: \beta\neq0\)</span> to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of <span class=""math display"">\[H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0\]</span> versus the alternative <span class=""math display"">\[H_a: at \; least \; one \; \beta_j \; is \; non-zero\]</span> This hypothesis test is performed by computing <strong>F-statistics</strong> reported in the model summary and calculated as <span class=""math inline"">\(F = \frac{(TSS - RSS)/p}{RSS/(n-p-1)}\)</span> where <span class=""math inline"">\(TSS = \sum(y_i - \bar{y})^2\)</span> and <span class=""math inline"">\(RSS = \sum(y_i - \hat{y_i})^2\)</span>. Here, the <span class=""math inline"">\(F-statsitics = 3.046\)</span> and the associated <span class=""math inline"">\(p-value &lt; 0.05\)</span> so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.<a href=""#fnref11"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
 <li id=""fn12""><p>decreases by ca. 0.06<a href=""#fnref12"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
-<li id=""fn13""><p>the odds increase by e^{-0.02997} = 0.97<a href=""#fnref13"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
-<li id=""fn14""><p>The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.<a href=""#fnref14"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
+<li id=""fn13""><p>decreases by ca. 0.9<a href=""#fnref13"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
+<li id=""fn14""><p>the odds increase by e^{-0.02997} = 0.97<a href=""#fnref14"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
+<li id=""fn15""><p>The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.<a href=""#fnref15"" class=""footnote-back"" role=""doc-backlink"">↩︎</a></p></li>
 </ol>
 </section>
 

---FILE: session-lm/docs/lm-intro.html---
@@ -332,7 +332,7 @@ <h2 data-number=""1.4"" class=""anchored"" data-anchor-id=""terminology""><span class=
 <ul>
 <li>exposure</li>
 <li>explanatory variable</li>
-<li>dependent variable</li>
+<li>independent variable</li>
 <li>predictor</li>
 <li>covariate</li>
 </ul></li>

---FILE: session-lm/docs/search.json---
@@ -32,7 +32,7 @@
     ""href"": ""lm-intro.html#terminology"",
     ""title"": ""1  Introduction to linear models"",
     ""section"": ""1.4 Terminology"",
-    ""text"": ""1.4 Terminology\nThere are many terms and notations used interchangeably:\n\n\\(y\\) is being called:\n\nresponse\noutcome\ndependent variable\n\n\\(x\\) is being called:\n\nexposure\nexplanatory variable\ndependent variable\npredictor\ncovariate""
+    ""text"": ""1.4 Terminology\nThere are many terms and notations used interchangeably:\n\n\\(y\\) is being called:\n\nresponse\noutcome\ndependent variable\n\n\\(x\\) is being called:\n\nexposure\nexplanatory variable\nindependent variable\npredictor\ncovariate""
   },
   {
     ""objectID"": ""lm-intro.html#simple-linear-regression"",
@@ -172,42 +172,42 @@
     ""href"": ""lm-coeff.html#example-multiple-regression"",
     ""title"": ""3  Common cases"",
     ""section"": ""3.2 Example: multiple regression"",
-    ""text"": ""3.2 Example: multiple regression\nLet’s try to model BMI using more variables\nModel (generic)\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\epsilon_i\\)\n\n\n# fit multiple linear regression and print model summary\nm2 &lt;- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 35.456968   3.149661  11.257  &lt; 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,    Adjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n\nCoefficient interpretations\nUsing the model answer the questions:\n\nwhat would happen to BMI if hdl levels increase by 10?9\nwhat would happen to BMI if age increases by 1 year?10\n\nHypothesis testing\n\noverall, is there a relationship between the response \\(Y\\) (BMI) and predictors?11\n\n Not so easy: alternative model\nLet’s consider another multiple regression model:\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot waist_i + \\epsilon_i\\)\n\nWe fit the model in R and look at the model summary:\n\nm2_alt &lt;- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864, Adjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: &lt; 2.2e-16\n\n\nWhat happens to BMI if hdl increases by 10?12\nWhat happens to BMI if hdl increases by 10 using the first model again?[decreases by ca. 0.9]\nHow do you explain the difference in BMI changes given these two models?\n\nSpecific interpretation\n\nObviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model).\nOur interpretations need to be more specific and we say that a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\)\nOften it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in hdl would also imply a change in total cholesterol chol.\nFurther, our explanation contains no notation of causation.\nWe will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.""
+    ""text"": ""3.2 Example: multiple regression\nLet’s try to model BMI using more variables\nModel (generic)\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\epsilon_i\\)\n\n\n# fit multiple linear regression and print model summary\nm2 &lt;- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 35.456968   3.149661  11.257  &lt; 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,    Adjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n\nCoefficient interpretations\nUsing the model answer the questions:\n\nwhat would happen to BMI if hdl levels increase by 10?9\nwhat would happen to BMI if age increases by 1 year?10\n\nHypothesis testing\n\noverall, is there a relationship between the response \\(Y\\) (BMI) and predictors?11\n\n Not so easy: alternative model\nLet’s consider another multiple regression model:\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot waist_i + \\epsilon_i\\)\n\nWe fit the model in R and look at the model summary:\n\nm2_alt &lt;- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864, Adjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: &lt; 2.2e-16\n\n\nWhat happens to BMI if hdl increases by 10?12\nWhat happens to BMI if hdl increases by 10 using the first model again?13\nHow do you explain the difference in BMI changes given these two models?\n\nSpecific interpretation\n\nObviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model).\nOur interpretations need to be more specific and we say that a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\)\nOften it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in hdl would also imply a change in total cholesterol chol.\nFurther, our explanation contains no notation of causation.\nWe will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.""
   },
   {
     ""objectID"": ""lm-coeff.html#example-categorical-variable"",
     ""href"": ""lm-coeff.html#example-categorical-variable"",
     ""title"": ""3  Common cases"",
     ""section"": ""3.3 Example: categorical variable"",
-    ""text"": ""3.3 Example: categorical variable\n\nWe want to compare the average BMI of men and women.\nWe can do that using linear regression and including gender as binary variable\n\n\n\nCode\nfont.size &lt;- 20\ncol.blue.light &lt;- \""#a6cee3\""\ncol.blue.dark &lt;- \""#1f78b4\""\nmy.ggtheme &lt;- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %&gt;%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\n\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\] for some coding, e.g. we choose to set “Female=1” and “Male=0” or vice versa.\nIn R we write:\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n## [1] \""factor\""\n\n# fit linear regression and print model summary\nm3 &lt;- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n## \n## Call:\n## lm(formula = BMI ~ gender, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -14.167  -4.117  -0.327   3.160  19.273 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   27.7674     0.8527  32.566  &lt; 2e-16 ***\n## genderfemale   3.9396     1.1379   3.462 0.000729 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.437 on 128 degrees of freedom\n## Multiple R-squared:  0.08563,    Adjusted R-squared:  0.07849 \n## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286\n\nEstimates \\[\\hat{\\alpha} = 27.7674\\] \\[\\hat{\\beta} = 3.9396\\]\n\nThe lm() function chooses automatically one of the category as baseline, here females.\nModel summary prints the output of the model with the baseline category “hidden”.\nNotice that the only label we have is “genderfemale”.\nMeaning that we ended-up having a model coded as below: \\[\\begin{equation}\n  I_{x_i} =\n  \\left\\{\n      \\begin{array}{cc}\n              1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n              0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n      \\end{array}\n  \\right.\n\\end{equation}\\]\nConsequently, if observation \\(i\\) is female then the expected value of BMI is: \\[E(BMI_i|female) = 27.7674 + 3.9396 = 31.707\\]\nand if observation \\(i\\) is male then the expected value of BMI is: \\[E(BMI_i|male) = 27.7674\\] We can plot the model in R:\n\n\nggPredict(m3) + \n  my.ggtheme""
+    ""text"": ""3.3 Example: categorical variable\n\nWe want to compare the average BMI of men and women.\nWe can do that using linear regression and including gender as binary variable\n\n\n\nCode\nfont.size &lt;- 20\ncol.blue.light &lt;- \""#a6cee3\""\ncol.blue.dark &lt;- \""#1f78b4\""\nmy.ggtheme &lt;- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %&gt;%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\n\nModel\n\\[Y_i = \\beta_{o} + \\beta_{1} I_{x_1,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\] for some coding, e.g. we choose to set “Female=1” and “Male=0” or vice versa.\nIn R we write:\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n## [1] \""factor\""\n\n# fit linear regression and print model summary\nm3 &lt;- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n## \n## Call:\n## lm(formula = BMI ~ gender, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -14.167  -4.117  -0.327   3.160  19.273 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   27.7674     0.8527  32.566  &lt; 2e-16 ***\n## genderfemale   3.9396     1.1379   3.462 0.000729 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.437 on 128 degrees of freedom\n## Multiple R-squared:  0.08563,    Adjusted R-squared:  0.07849 \n## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286\n\nEstimates \\[\\hat{\\beta_{0}} = 27.7674\\] \\[\\hat{\\beta_{1}} = 3.9396\\]\n\nThe lm() function chooses automatically one of the category as baseline, here females.\nModel summary prints the output of the model with the baseline category “hidden”.\nNotice that the only label we have is “genderfemale”.\nMeaning that we ended-up having a model coded as below: \\[\\begin{equation}\n  I_{x_i} =\n  \\left\\{\n      \\begin{array}{cc}\n              1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n              0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n      \\end{array}\n  \\right.\n\\end{equation}\\]\nConsequently, if observation \\(i\\) is female then the expected value of BMI is: \\[E(BMI_i|female) = 27.7674 + 3.9396 = 31.707\\]\nand if observation \\(i\\) is male then the expected value of BMI is: \\[E(BMI_i|male) = 27.7674\\] We can plot the model in R:\n\n\nggPredict(m3) + \n  my.ggtheme""
   },
   {
     ""objectID"": ""lm-coeff.html#example-categorical-numerical-variables"",
     ""href"": ""lm-coeff.html#example-categorical-numerical-variables"",
     ""title"": ""3  Common cases"",
     ""section"": ""3.4 Example: categorical & numerical variables"",
-    ""text"": ""3.4 Example: categorical & numerical variables\n\nAbove we observed a signficant difference in average BMI between men and women among the study participants.\nCan we also observe a significant relationship between BMI and height?\nAnd if so, does this relationship depend on gender?\n\n\n\nCode\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %&gt;%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\n\n\nFrom the plot we can see that BMI decreases slightly with height.\nOn average, men are taller than women.\nOn average, women have higher BMI than men.\nThe relationship between height and BMI appears to be the same for males and females, i.e. BMI decreases with height for both men and women.\n\nTo assess the relationship we use a model containing height and gender.\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the height of person \\(i\\).\nIn R we write:\n\n# fit linear model and print model summary\nm4 &lt;- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,    Adjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n\nModel together with estimates\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the weight of person \\(i\\)\nEstimates\n\\[\\hat{\\alpha} = 37.743 \\] \\[\\hat{\\beta} = 3.163\\] \\[\\hat{\\gamma} = -5.719\\]\n\nFor instance, using our estimates, for a female who happens to 1.7 m tall we would predict BMI of: \\[E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837\\]\nand for a male of height 1.7 m tall we would predict BMI of \\[E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207\\]\n\nIn R we can plot our data and the fitted model to verify our calculations:\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme""
+    ""text"": ""3.4 Example: categorical & numerical variables\n\nAbove we observed a significant difference in average BMI between men and women among the study participants.\nCan we also observe a significant relationship between BMI and height?\nAnd if so, does this relationship depend on gender?\n\n\n\nCode\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %&gt;%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\n\n\nFrom the plot we can see that BMI decreases slightly with height.\nOn average, men are taller than women.\nOn average, women have higher BMI than men.\nThe relationship between height and BMI appears to be the same for males and females, i.e. BMI decreases with height for both men and women.\n\nTo assess the relationship we use a model containing height and gender.\nModel\n\\[Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the height of person \\(i\\).\nIn R we write:\n\n# fit linear model and print model summary\nm4 &lt;- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,    Adjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n\nModel together with estimates\n\\[Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the height of person \\(i\\)\nEstimates\n\\[\\hat{\\beta_{0}} = 37.743 \\] \\[\\hat{\\beta_{1}} = 3.163\\] \\[\\hat{\\beta_{2}} = -5.719\\]\n\nFor instance, using our estimates, for a female who happens to 1.7 m tall we would predict BMI of: \\[E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837\\]\nand for a male of height 1.7 m tall we would predict BMI of \\[E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207\\]\n\nIn R we can plot our data and the fitted model to verify our calculations:\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme""
   },
   {
     ""objectID"": ""lm-coeff.html#example-interactions"",
     ""href"": ""lm-coeff.html#example-interactions"",
     ""title"": ""3  Common cases"",
     ""section"": ""3.5 Example: interactions"",
-    ""text"": ""3.5 Example: interactions\n\nThe fitted lines in the above example are parallel, the slope is modeled to be the same for men and women, and the intercept denotes the group differences.\nIt is also possible to allow for both intercept and slope being fitted separately for each group.\nThis is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\nAnd we then talk about including interaction effect since the two lines may interact (cross).\n\nModel\n\\[Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}\\] where:\n\n\\(Y_{i,j}\\) is the BMI of person \\(j\\) of gender \\(i\\)\n\\(x_{ij}\\) is the height of person \\(j\\) of gender \\(i\\)\n\\(i=1\\) corresponds to women in our example (keeping the same coding as above)\n\\(i=2\\) corresponds to men\n\nIn R we define the interaction term with *:\n\n# fit linear model with interaction\nm5 &lt;- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n## \n## Call:\n## lm(formula = BMI ~ gender * height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.5564  -4.1137  -0.3072   3.1057  19.2005 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)           31.222     20.318   1.537    0.127\n## genderfemale          14.219     26.032   0.546    0.586\n## height                -1.981     11.638  -0.170    0.865\n## genderfemale:height   -6.558     15.414  -0.425    0.671\n## \n## Residual standard error: 6.469 on 126 degrees of freedom\n## Multiple R-squared:  0.09099,    Adjusted R-squared:  0.06935 \n## F-statistic: 4.204 on 3 and 126 DF,  p-value: 0.007155\n\nNow, based on the regression output we would expect:\n\nfor a woman of height \\(x\\), a BMI value of: \\[E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x\\]\nfor a man of height \\(x\\), a BMI value of \\[E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x\\]\n\nEstimates \\[\\hat{\\alpha_1} = 45.441\\] \\[\\hat{\\beta_1} = 31.222\\]\n\\[\\hat{\\alpha_2} = 47.34778\\] \\[\\hat{\\beta_2} = -1.981\\]\n\nWe can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term “Genderfemale:height” is equal to zero.\nOr therefore conclude that the relationship between BMI and height is different for men and women.\nWe can plot the fitted model and see that the lines are no longer parallel.\n\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme""
+    ""text"": ""3.5 Example: interactions\n\nThe fitted lines in the above example are parallel, the slope is modeled to be the same for men and women, and the intercept denotes the group differences.\nIt is also possible to allow for both intercept and slope being fitted separately for each group.\nThis is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\nAnd we then talk about including interaction effect since the two lines may interact (cross).\n\nModel\n\n\\[Y_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i\\] where:\n\n\\(Y_{i}\\) is the BMI of person \\(i\\)\n\n\\[\\begin{equation}\n    I_{x_1,i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\n\n\\(x_{1,i}\\) is the height of person \\(i\\)\nand \\(\\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}\\) is the interaction term\n\nIn R we define the interaction term with *:\n\n# fit linear model with interaction\nm5 &lt;- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n## \n## Call:\n## lm(formula = BMI ~ gender * height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.5564  -4.1137  -0.3072   3.1057  19.2005 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)           31.222     20.318   1.537    0.127\n## genderfemale          14.219     26.032   0.546    0.586\n## height                -1.981     11.638  -0.170    0.865\n## genderfemale:height   -6.558     15.414  -0.425    0.671\n## \n## Residual standard error: 6.469 on 126 degrees of freedom\n## Multiple R-squared:  0.09099,    Adjusted R-squared:  0.06935 \n## F-statistic: 4.204 on 3 and 126 DF,  p-value: 0.007155\n\nNow, based on the regression output we would expect:\n\nfor a woman of height \\(x\\), a BMI value of: \\[\\begin{align*}\nE(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = \\\\ 45.441 -8.539 \\cdot x\n\\end{align*}\\]\nfor a man of height \\(x\\), a BMI value of \\[E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x\\]\n\nEstimates \\[\\hat{\\beta_0} = 31.22\\] \\[\\hat{\\beta_1} = 14.22\\]\n\\[\\hat{\\beta_2} = -1.98\\] \\[\\hat{\\beta_3} = -6.558\\]\nTo model relationship between BMI and height in males, the model reduces to: \\[Y_i = \\beta_{0} + \\beta_{2} \\cdot x_{2,i} + \\epsilon_i = \\\\ 31.22 - 1.98 \\cdot height_i\\] To model relationship between BMI and height in females, the models sums up to: \\[\\begin{align*}\nY_i = \\beta_{0} + \\beta_{1} I_{x_1,i} + \\beta_{2} \\cdot x_{2,i} + \\beta_{3} \\cdot I_{x_1,i} \\cdot x_{2, i}+ \\epsilon_i = \\\\ (\\beta_{0} + \\beta_{1}) + (\\beta_{2} + \\beta_{3})\\cdot height_i = \\\\  (31.22 + 14.219)  + (-1.98 -6.558)*height_i = \\\\ 45.44 - 4.58 \\cdot height_i\n\\end{align*}\\]\nThis lets us model different relationships of BMI and height in both groups, with individual intercept and slope values.\nIn addition:\n\nWe can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term “Genderfemale:height” is equal to zero.\nOr therefore conclude that the relationship between BMI and height is different for men and women.\nWe can plot the fitted model and see that the lines are no longer parallel.\n\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme""
   },
   {
     ""objectID"": ""lm-coeff.html#example-logistic-regression-with-categorical-variable"",
     ""href"": ""lm-coeff.html#example-logistic-regression-with-categorical-variable"",
     ""title"": ""3  Common cases"",
     ""section"": ""3.6 Example: logistic regression with categorical variable"",
-    ""text"": ""3.6 Example: logistic regression with categorical variable\n\n# recode diabetic status to 1 and 0\ndata_diabetes &lt;- data_diabetes %&gt;%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 &lt;- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n## \n## Call:\n## glm(formula = obese ~ hdl + gender, family = binomial(link = \""logit\""), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept)   0.55047    0.58718   0.937   0.3485   \n## hdl          -0.02997    0.01197  -2.504   0.0123 * \n## genderfemale  1.26586    0.40120   3.155   0.0016 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 164.39  on 127  degrees of freedom\n## AIC: 170.39\n## \n## Number of Fisher Scoring iterations: 4\n\n\nBy how much change odds of suffering from obesity when hdl increases by 1?13\nWhat are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?14\n\n\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\nWe can predict obesity status in R for a man with hdl values of 50:\n\n# define new observation\ndf &lt;- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese &lt;- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n##         1 \n## 0.2792396""
+    ""text"": ""3.6 Example: logistic regression with categorical variable\n\n# recode diabetic status to 1 and 0\ndata_diabetes &lt;- data_diabetes %&gt;%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 &lt;- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n## \n## Call:\n## glm(formula = obese ~ hdl + gender, family = binomial(link = \""logit\""), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept)   0.55047    0.58718   0.937   0.3485   \n## hdl          -0.02997    0.01197  -2.504   0.0123 * \n## genderfemale  1.26586    0.40120   3.155   0.0016 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 164.39  on 127  degrees of freedom\n## AIC: 170.39\n## \n## Number of Fisher Scoring iterations: 4\n\n\nBy how much change odds of suffering from obesity when hdl increases by 1?14\nWhat are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?15\n\n\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\nWe can predict obesity status in R for a man with hdl values of 50:\n\n# define new observation\ndf &lt;- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese &lt;- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n##         1 \n## 0.2792396""
   },
   {
     ""objectID"": ""lm-coeff.html#footnotes"",
     ""href"": ""lm-coeff.html#footnotes"",
     ""title"": ""3  Common cases"",
     ""section"": """",
-    ""text"": ""If the waist increases by 1 cm we would expect our BMI to increase by \\(\\approx 0.35\\) since \\(\\hat{\\beta} = 0.35298\\)↩︎\nIf the waist increases by 10 cm we would expect BMI to increase by \\(0.35298 \\cdot 10 \\approx 3.53\\)↩︎\nNo, as \\(p-value = 0.0633 \\nless 0.05\\)↩︎\nYes, as \\(p-value = 0.0633 &lt; 0.1\\)↩︎\nBMI = -5.12445 + 0.35298 = 21.349↩︎\nBMI = -5.12445 + 0.35298 = 65.47141, however here we have to be careful in predicting outside the model range.↩︎\nIn simple linear regression \\(R^2\\) is the same as \\(r^2\\) and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.↩︎\nThe diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.↩︎\ndecreases by \\(-0.090023 \\cdot 10 = 0.90023\\)↩︎\ndecrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.↩︎\nwe have seen before that in the case of simple linear regression it was enough to test the null hypothesis of \\(H_0: \\beta=0\\) versus \\(H_0: \\beta\\neq0\\) to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of \\[H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\] versus the alternative \\[H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero\\] This hypothesis test is performed by computing F-statistics reported in the model summary and calculated as \\(F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\\) where \\(TSS = \\sum(y_i - \\bar{y})^2\\) and \\(RSS = \\sum(y_i - \\hat{y_i})^2\\). Here, the \\(F-statsitics = 3.046\\) and the associated \\(p-value &lt; 0.05\\) so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.↩︎\ndecreases by ca. 0.06↩︎\nthe odds increase by e^{-0.02997} = 0.97↩︎\nThe odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.↩︎""
+    ""text"": ""If the waist increases by 1 cm we would expect our BMI to increase by \\(\\approx 0.35\\) since \\(\\hat{\\beta} = 0.35298\\)↩︎\nIf the waist increases by 10 cm we would expect BMI to increase by \\(0.35298 \\cdot 10 \\approx 3.53\\)↩︎\nNo, as \\(p-value = 0.0633 \\nless 0.05\\)↩︎\nYes, as \\(p-value = 0.0633 &lt; 0.1\\)↩︎\nBMI = -5.12445 + 0.35298 = 21.349↩︎\nBMI = -5.12445 + 0.35298 = 65.47141, however here we have to be careful in predicting outside the model range.↩︎\nIn simple linear regression \\(R^2\\) is the same as \\(r^2\\) and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.↩︎\nThe diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.↩︎\ndecreases by \\(-0.090023 \\cdot 10 = 0.90023\\)↩︎\ndecrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.↩︎\nwe have seen before that in the case of simple linear regression it was enough to test the null hypothesis of \\(H_0: \\beta=0\\) versus \\(H_0: \\beta\\neq0\\) to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of \\[H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\\] versus the alternative \\[H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero\\] This hypothesis test is performed by computing F-statistics reported in the model summary and calculated as \\(F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}\\) where \\(TSS = \\sum(y_i - \\bar{y})^2\\) and \\(RSS = \\sum(y_i - \\hat{y_i})^2\\). Here, the \\(F-statsitics = 3.046\\) and the associated \\(p-value &lt; 0.05\\) so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.↩︎\ndecreases by ca. 0.06↩︎\ndecreases by ca. 0.9↩︎\nthe odds increase by e^{-0.02997} = 0.97↩︎\nThe odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.↩︎""
   },
   {
     ""objectID"": ""lm-intro-exercises.html"",

---FILE: session-lm/lm-coeff.qmd---
@@ -180,7 +180,7 @@ summary(m2_alt)
 ```
 
 - What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]
-- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]
+- What happens to `BMI` if `hdl` increases by 10 using the first model again?^[decreases by ca. 0.9]
 - How do you explain the difference in `BMI` changes given these two models?
 
 **Specific interpretation**
@@ -226,10 +226,10 @@ data_diabetes %>%
 
 **Model**
 
-$$Y_i = \alpha + \beta I_{x_i} + \epsilon_i$$
+$$Y_i = \beta_{o} + \beta_{1} I_{x_1,i} + \epsilon_i$$
 where
 \begin{equation}
-    I_{x_i} =
+    I_{x_1,i} =
     \left\{
         \begin{array}{cc}
                 1 & \mathrm{if\ } x_i=1 \\
@@ -254,8 +254,8 @@ print(summary(m3))
 ```
 
 **Estimates**
-$$\hat{\alpha} = 27.7674$$
-$$\hat{\beta} = 3.9396$$
+$$\hat{\beta_{0}} = 27.7674$$
+$$\hat{\beta_{1}} = 3.9396$$
 
 - The `lm()` function chooses automatically one of the category as baseline, here `females`.
 - Model summary prints the output of the model with the baseline category **""hidden""**.
@@ -285,7 +285,7 @@ ggPredict(m3) +
 
 ## Example: categorical & numerical variables
 
-- Above we observed a signficant difference in average `BMI` between men and women among the study participants.
+- Above we observed a significant difference in average `BMI` between men and women among the study participants.
 - Can we also observe a significant relationship between `BMI` and `height`?
 - And if so, does this relationship depend on `gender`?
 
@@ -316,10 +316,10 @@ To assess the relationship we use a model containing `height` and `gender`.
 
 **Model**
 
-$$Y_i = \alpha + \beta I_{x_i} + \gamma x_{2,i} + \epsilon_i$$
+$$Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} x_{2,i} + \epsilon_i$$
 where
 \begin{equation}
-    I_{x_i} =
+    I_{x_1,i} =
     \left\{
         \begin{array}{cc}
                 1 & \mathrm{if\ } \quad person_i\;is\;female \\
@@ -342,7 +342,7 @@ print(summary(m4))
 
 **Model together with estimates**
 
-$$Y_i = \alpha + \beta I_{x_i} + \gamma x_{2,i} + \epsilon_i$$
+$$Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} \cdot x_{2,i} + \epsilon_i$$
 where
 \begin{equation}
     I_{x_i} =
@@ -354,14 +354,14 @@ where
     \right.
 \end{equation}
 
-and $x_{2,i}$ is the weight of person $i$
+and $x_{2,i}$ is the height of person $i$
 
 
 **Estimates**
 
-$$\hat{\alpha} = 37.743 $$
-$$\hat{\beta} = 3.163$$
-$$\hat{\gamma} = -5.719$$
+$$\hat{\beta_{0}} = 37.743 $$
+$$\hat{\beta_{1}} = 3.163$$
+$$\hat{\beta_{2}} = -5.719$$
 
 
 - For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:
@@ -389,21 +389,32 @@ ggPredict(m4) +
 
 ## Example: interactions
 
-
 - The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.
 - It is also possible to allow for **both intercept and slope being fitted separately for each group**.
 - This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.
 - And we then talk about including **interaction effect** since the two lines may interact (cross).
 
 **Model**
 
-$$Y_{i,j} = \alpha_i + \beta_ix_{ij} + \epsilon_{i,j}$$
+<!-- $$Y_{i,j} = \alpha_i + \beta_i\cdot x_{ij} + \epsilon_{i,j}$$ -->
+$$Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} \cdot x_{2,i} + \beta_{3} \cdot I_{x_1,i} \cdot x_{2, i}+ \epsilon_i$$
 where:
 
-- $Y_{i,j}$ is the BMI of person $j$ of gender $i$
-- $x_{ij}$ is the height of person $j$ of gender $i$
-- $i=1$ corresponds to women in our example (keeping the same coding as above)
-- $i=2$ corresponds to men
+- $Y_{i}$ is the BMI of person $i$
+
+\begin{equation}
+    I_{x_1,i} =
+    \left\{
+        \begin{array}{cc}
+                1 & \mathrm{if\ } \quad person_i\;is\;female \\
+                0 & \mathrm{if\ } \quad person_i\;is\;male \\
+        \end{array}
+    \right.
+\end{equation}
+
+- $x_{1,i}$ is the height of person $i$
+- and $\beta_{3} \cdot I_{x_1,i} \cdot x_{2, i}$ is the interaction term
+
 
 In `R` we define the interaction term with `*`:
 ```{r}
@@ -419,15 +430,28 @@ print(summary(m5))
 Now, based on the regression output we would expect:
 
 - for a woman of height $x$, a BMI value of:
-$$E(BMI|female\; and \; height=x)=31.222 + 14.219 - 1.981 \cdot x - 6.558 \cdot x = 45.441 -8.539 \cdot x$$
+\begin{align*}
+E(BMI|female\; and \; height=x)=31.222 + 14.219 - 1.981 \cdot x - 6.558 \cdot x = \\ 45.441 -8.539 \cdot x
+\end{align*}
 - for a man of height $x$, a BMI value of $$E(BMI|male\; and \; height=x)=31.222-1.981 \cdot x$$
 
 **Estimates**
-$$\hat{\alpha_1} = 45.441$$
-$$\hat{\beta_1} = 31.222$$
+$$\hat{\beta_0} = 31.22$$
+$$\hat{\beta_1} = 14.22$$
+
+$$\hat{\beta_2} = -1.98$$
+$$\hat{\beta_3} = -6.558$$
+
+To model relationship between BMI and height in males, the model reduces to: 
+$$Y_i = \beta_{0} + \beta_{2} \cdot x_{2,i} + \epsilon_i = \\ 31.22 - 1.98 \cdot height_i$$
+To model relationship between BMI and height in females, the models sums up to: 
+\begin{align*}
+Y_i = \beta_{0} + \beta_{1} I_{x_1,i} + \beta_{2} \cdot x_{2,i} + \beta_{3} \cdot I_{x_1,i} \cdot x_{2, i}+ \epsilon_i = \\ (\beta_{0} + \beta_{1}) + (\beta_{2} + \beta_{3})\cdot height_i = \\  (31.22 + 14.219)  + (-1.98 -6.558)*height_i = \\ 45.44 - 4.58 \cdot height_i
+\end{align*}
+
+This lets us model different relationships of BMI and height in both groups, with **individual intercept and slope values**.
 
-$$\hat{\alpha_2} = 47.34778$$
-$$\hat{\beta_2} = -1.981$$
+In addition: 
 
 - We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term ""Genderfemale:height"" is equal to zero.
 - Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.

---FILE: session-lm/lm-intro.qmd---
@@ -216,7 +216,7 @@ There are many terms and notations used interchangeably:
 - $x$ is being called:
   - exposure
   - explanatory variable
-  - dependent variable
+  - independent variable
   - predictor
   - covariate
 "
NBISweden,workshop-mlbiostatistics,af49d38dd5dfa6e337009937c67b5828b2c68d11,olgadet,,2024-04-24T17:53:43Z,olgadet,,2024-04-24T17:53:43Z,Fix typo,session-lm/.quarto/_freeze/lm-coeff/execute-results/html.json;session-lm/.quarto/_freeze/lm-coeff/figure-html/unnamed-chunk-17-1.png;session-lm/.quarto/_freeze/lm-intro/figure-html/fig-linear-adv-1.png;session-lm/.quarto/_freeze/lm-intro/figure-html/fig-relationship-1.png;session-lm/.quarto/cites/index.json;session-lm/.quarto/idx/lm-coeff.qmd.json;session-lm/.quarto/xref/28d59191;session-lm/.quarto/xref/295884aa;session-lm/.quarto/xref/4d506fc9;session-lm/.quarto/xref/5b28360f;session-lm/.quarto/xref/691de0dc;session-lm/.quarto/xref/9e3bf041;session-lm/docs/lm-coeff.html;session-lm/docs/lm-coeff_files/figure-html/unnamed-chunk-17-1.png;session-lm/docs/lm-intro_files/figure-html/fig-linear-adv-1.png;session-lm/docs/lm-intro_files/figure-html/fig-relationship-1.png;session-lm/docs/lm-reg-cls_files/figure-html/fig-obesity-1.png;session-lm/docs/lm-reg-cls_files/figure-html/unnamed-chunk-12-1.png;session-lm/docs/lm-reg-cls_files/figure-html/unnamed-chunk-9-1.png;session-lm/docs/search.json;session-lm/lm-coeff-demo.qmd;session-lm/lm-coeff.qmd,True,False,True,False,37,45,82,"---FILE: session-lm/.quarto/_freeze/lm-coeff/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""d37aedc9c7f6dc6496a3e05931b657d2"",
+  ""hash"": ""cb19d3d8313ccf769cff4456f54fc54f"",
   ""result"": {
-    ""markdown"": ""---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n```\n:::\n\n\n## Example: simple linear regression\n\n::: {.cell .column-margin fig-cap-location='margin' fig-heigth='4'}\n\n```{.r .cell-code  code-fold=\""false\""}\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n## \n## Call:\n## lm(formula = BMI ~ waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.2374  -2.7689  -0.4532   2.4065  19.3549 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -5.12445    2.73538  -1.873   0.0633 .  \n## waist        0.35298    0.02723  12.965   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.426 on 128 degrees of freedom\n## Multiple R-squared:  0.5677,\tAdjusted R-squared:  0.5643 \n## F-statistic: 168.1 on 1 and 128 DF,  p-value: < 2.2e-16\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n```\n\n::: {.cell-output-display}\n![Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.](lm-coeff_files/figure-html/fig-simple-1-1.png){#fig-simple-1 width=384}\n:::\n:::\n\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n2*pt(12.96291, df=128, lower=F)\n## [1] 4.605102e-25\n```\n:::\n\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n##        1 \n## 30.17348\n```\n:::\n\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""true\""}\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 35.456968   3.149661  11.257  < 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,\tAdjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n```\n:::\n\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864,\tAdjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.9 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code  code-fold=\""true\""}\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n## [1] \""factor\""\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n## \n## Call:\n## lm(formula = BMI ~ gender, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -14.167  -4.117  -0.327   3.160  19.273 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   27.7674     0.8527  32.566  < 2e-16 ***\n## genderfemale   3.9396     1.1379   3.462 0.000729 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.437 on 128 degrees of freedom\n## Multiple R-squared:  0.08563,\tAdjusted R-squared:  0.07849 \n## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286\n```\n:::\n\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m3) + \n  my.ggtheme \n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `heigth` of person $i$.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,\tAdjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n```\n:::\n\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n## \n## Call:\n## lm(formula = BMI ~ gender * height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.5564  -4.1137  -0.3072   3.1057  19.2005 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)\n## (Intercept)           31.222     20.318   1.537    0.127\n## genderfemale          14.219     26.032   0.546    0.586\n## height                -1.981     11.638  -0.170    0.865\n## genderfemale:height   -6.558     15.414  -0.425    0.671\n## \n## Residual standard error: 6.469 on 126 degrees of freedom\n## Multiple R-squared:  0.09099,\tAdjusted R-squared:  0.06935 \n## F-statistic: 4.204 on 3 and 126 DF,  p-value: 0.007155\n```\n:::\n\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n\n## Example: logistic regression with categorical variable\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n## \n## Call:\n## glm(formula = obese ~ hdl + gender, family = binomial(link = \""logit\""), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)   0.55047    0.58718   0.937   0.3485   \n## hdl          -0.02997    0.01197  -2.504   0.0123 * \n## genderfemale  1.26586    0.40120   3.155   0.0016 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 164.39  on 127  degrees of freedom\n## AIC: 170.39\n## \n## Number of Fisher Scoring iterations: 4\n```\n:::\n\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-17-1.png){width=768}\n:::\n:::\n\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n##         1 \n## 0.2792396\n```\n:::\n"",
+    ""markdown"": ""---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n---\n\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n```\n:::\n\n\n## Example: simple linear regression\n\n::: {.cell .column-margin fig-cap-location='margin' fig-heigth='4'}\n\n```{.r .cell-code  code-fold=\""false\""}\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n## \n## Call:\n## lm(formula = BMI ~ waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.2374  -2.7689  -0.4532   2.4065  19.3549 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -5.12445    2.73538  -1.873   0.0633 .  \n## waist        0.35298    0.02723  12.965   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.426 on 128 degrees of freedom\n## Multiple R-squared:  0.5677,\tAdjusted R-squared:  0.5643 \n## F-statistic: 168.1 on 1 and 128 DF,  p-value: < 2.2e-16\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n```\n\n::: {.cell-output-display}\n![Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.](lm-coeff_files/figure-html/fig-simple-1-1.png){#fig-simple-1 width=384}\n:::\n:::\n\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n2*pt(12.96291, df=128, lower=F)\n## [1] 4.605102e-25\n```\n:::\n\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n##        1 \n## 30.17348\n```\n:::\n\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""true\""}\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 35.456968   3.149661  11.257  < 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,\tAdjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n```\n:::\n\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864,\tAdjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n\n::: {.cell .column-margin}\n\n```{.r .cell-code  code-fold=\""true\""}\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n## [1] \""factor\""\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n## \n## Call:\n## lm(formula = BMI ~ gender, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -14.167  -4.117  -0.327   3.160  19.273 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   27.7674     0.8527  32.566  < 2e-16 ***\n## genderfemale   3.9396     1.1379   3.462 0.000729 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.437 on 128 degrees of freedom\n## Multiple R-squared:  0.08563,\tAdjusted R-squared:  0.07849 \n## F-statistic: 11.99 on 1 and 128 DF,  p-value: 0.0007286\n```\n:::\n\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m3) + \n  my.ggtheme \n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,\tAdjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n```\n:::\n\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n## \n## Call:\n## lm(formula = BMI ~ gender * height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.5564  -4.1137  -0.3072   3.1057  19.2005 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)\n## (Intercept)           31.222     20.318   1.537    0.127\n## genderfemale          14.219     26.032   0.546    0.586\n## height                -1.981     11.638  -0.170    0.865\n## genderfemale:height   -6.558     15.414  -0.425    0.671\n## \n## Residual standard error: 6.469 on 126 degrees of freedom\n## Multiple R-squared:  0.09099,\tAdjusted R-squared:  0.06935 \n## F-statistic: 4.204 on 3 and 126 DF,  p-value: 0.007155\n```\n:::\n\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n\n## Example: logistic regression with categorical variable\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n## \n## Call:\n## glm(formula = obese ~ hdl + gender, family = binomial(link = \""logit\""), \n##     data = data_diabetes)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)   0.55047    0.58718   0.937   0.3485   \n## hdl          -0.02997    0.01197  -2.504   0.0123 * \n## genderfemale  1.26586    0.40120   3.155   0.0016 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 178.71  on 129  degrees of freedom\n## Residual deviance: 164.39  on 127  degrees of freedom\n## AIC: 170.39\n## \n## Number of Fisher Scoring iterations: 4\n```\n:::\n\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\n::: {.cell-output-display}\n![](lm-coeff_files/figure-html/unnamed-chunk-17-1.png){width=768}\n:::\n:::\n\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\""false\""}\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n##         1 \n## 0.2792396\n```\n:::\n"",
     ""supporting"": [
       ""lm-coeff_files""
     ],

---FILE: session-lm/.quarto/cites/index.json---
@@ -1 +1 @@
-{""lm-lasso-exercises.qmd"":[],""lm-intro-exercises.qmd"":[],""lm-intro.qmd"":[],""lm-diagn-exercises.qmd"":[],""lm-diagn.qmd"":[],""lm-glm.qmd"":[],""lm-reg-cls.qmd"":[],""lm-coeff-exercises.qmd"":[],""lm-coeff.qmd"":[],""index.qmd"":[],""lm-ml.qmd"":[]}
+{""lm-ml.qmd"":[],""lm-lasso-exercises.qmd"":[],""lm-diagn.qmd"":[],""lm-coeff.qmd"":[],""lm-intro.qmd"":[],""lm-diagn-exercises.qmd"":[],""lm-glm.qmd"":[],""lm-intro-exercises.qmd"":[],""lm-reg-cls.qmd"":[],""index.qmd"":[],""lm-coeff-exercises.qmd"":[]}

---FILE: session-lm/.quarto/idx/lm-coeff.qmd.json---
@@ -1 +1 @@
-{""title"":""Common cases"",""markdown"":{""yaml"":{""output"":""html_document"",""editor_options"":{""chunk_output_type"":""console""}},""headingText"":""Common cases"",""containsRefs"":false,""markdown"":""\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.9 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `heigth` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n"",""srcMarkdownNoYaml"":""\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.9 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `heigth` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n""},""formats"":{""html"":{""identifier"":{""display-name"":""HTML"",""target-format"":""html"",""base-format"":""html""},""execute"":{""fig-width"":7,""fig-height"":5,""fig-format"":""retina"",""fig-dpi"":96,""df-print"":""default"",""error"":false,""eval"":true,""cache"":null,""freeze"":false,""echo"":true,""output"":""html_document"",""warning"":true,""include"":true,""keep-md"":false,""keep-ipynb"":false,""ipynb"":null,""enabled"":null,""daemon"":null,""daemon-restart"":false,""debug"":false,""ipynb-filters"":[],""engine"":""knitr""},""render"":{""keep-tex"":false,""keep-source"":false,""keep-hidden"":false,""prefer-html"":false,""output-divs"":true,""output-ext"":""html"",""fig-align"":""default"",""fig-pos"":null,""fig-env"":null,""code-fold"":true,""code-overflow"":""scroll"",""code-link"":false,""code-line-numbers"":false,""code-tools"":false,""tbl-colwidths"":""auto"",""merge-includes"":true,""inline-includes"":false,""preserve-yaml"":false,""latex-auto-mk"":true,""latex-auto-install"":true,""latex-clean"":true,""latex-max-runs"":10,""latex-makeindex"":""makeindex"",""latex-makeindex-opts"":[],""latex-tlmgr-opts"":[],""latex-input-paths"":[],""latex-output-dir"":null,""link-external-icon"":false,""link-external-newwindow"":false,""self-contained-math"":false,""format-resources"":[],""notebook-links"":true,""format-links"":true},""pandoc"":{""standalone"":true,""wrap"":""none"",""default-image-extension"":""png"",""to"":""html"",""output-file"":""lm-coeff.html""},""language"":{""toc-title-document"":""Table of contents"",""toc-title-website"":""On this page"",""related-formats-title"":""Other Formats"",""related-notebooks-title"":""Notebooks"",""source-notebooks-prefix"":""Source"",""section-title-abstract"":""Abstract"",""section-title-appendices"":""Appendices"",""section-title-footnotes"":""Footnotes"",""section-title-references"":""References"",""section-title-reuse"":""Reuse"",""section-title-copyright"":""Copyright"",""section-title-citation"":""Citation"",""appendix-attribution-cite-as"":""For attribution, please cite this work as:"",""appendix-attribution-bibtex"":""BibTeX citation:"",""title-block-author-single"":""Author"",""title-block-author-plural"":""Authors"",""title-block-affiliation-single"":""Affiliation"",""title-block-affiliation-plural"":""Affiliations"",""title-block-published"":""Published"",""title-block-modified"":""Modified"",""callout-tip-title"":""Tip"",""callout-note-title"":""Note"",""callout-warning-title"":""Warning"",""callout-important-title"":""Important"",""callout-caution-title"":""Caution"",""code-summary"":""Code"",""code-tools-menu-caption"":""Code"",""code-tools-show-all-code"":""Show All Code"",""code-tools-hide-all-code"":""Hide All Code"",""code-tools-view-source"":""View Source"",""code-tools-source-code"":""Source Code"",""code-line"":""Line"",""code-lines"":""Lines"",""copy-button-tooltip"":""Copy to Clipboard"",""copy-button-tooltip-success"":""Copied!"",""repo-action-links-edit"":""Edit this page"",""repo-action-links-source"":""View source"",""repo-action-links-issue"":""Report an issue"",""back-to-top"":""Back to top"",""search-no-results-text"":""No results"",""search-matching-documents-text"":""matching documents"",""search-copy-link-title"":""Copy link to search"",""search-hide-matches-text"":""Hide additional matches"",""search-more-match-text"":""more match in this document"",""search-more-matches-text"":""more matches in this document"",""search-clear-button-title"":""Clear"",""search-detached-cancel-button-title"":""Cancel"",""search-submit-button-title"":""Submit"",""search-label"":""Search"",""toggle-section"":""Toggle section"",""toggle-sidebar"":""Toggle sidebar navigation"",""toggle-dark-mode"":""Toggle dark mode"",""toggle-reader-mode"":""Toggle reader mode"",""toggle-navigation"":""Toggle navigation"",""crossref-fig-title"":""Figure"",""crossref-tbl-title"":""Table"",""crossref-lst-title"":""Listing"",""crossref-thm-title"":""Theorem"",""crossref-lem-title"":""Lemma"",""crossref-cor-title"":""Corollary"",""crossref-prp-title"":""Proposition"",""crossref-cnj-title"":""Conjecture"",""crossref-def-title"":""Definition"",""crossref-exm-title"":""Example"",""crossref-exr-title"":""Exercise"",""crossref-ch-prefix"":""Chapter"",""crossref-apx-prefix"":""Appendix"",""crossref-sec-prefix"":""Section"",""crossref-eq-prefix"":""Equation"",""crossref-lof-title"":""List of Figures"",""crossref-lot-title"":""List of Tables"",""crossref-lol-title"":""List of Listings"",""environment-proof-title"":""Proof"",""environment-remark-title"":""Remark"",""environment-solution-title"":""Solution"",""listing-page-order-by"":""Order By"",""listing-page-order-by-default"":""Default"",""listing-page-order-by-date-asc"":""Oldest"",""listing-page-order-by-date-desc"":""Newest"",""listing-page-order-by-number-desc"":""High to Low"",""listing-page-order-by-number-asc"":""Low to High"",""listing-page-field-date"":""Date"",""listing-page-field-title"":""Title"",""listing-page-field-description"":""Description"",""listing-page-field-author"":""Author"",""listing-page-field-filename"":""File Name"",""listing-page-field-filemodified"":""Modified"",""listing-page-field-subtitle"":""Subtitle"",""listing-page-field-readingtime"":""Reading Time"",""listing-page-field-categories"":""Categories"",""listing-page-minutes-compact"":""{0} min"",""listing-page-category-all"":""All"",""listing-page-no-matches"":""No matching items""},""metadata"":{""lang"":""en"",""fig-responsive"":true,""quarto-version"":""1.3.450"",""bibliography"":[""references.bib""],""theme"":""cosmo"",""editor_options"":{""chunk_output_type"":""console""}},""extensions"":{""book"":{""multiFile"":true}}}},""projectFormats"":[""html""]}
\ No newline at end of file
+{""title"":""Common cases"",""markdown"":{""yaml"":{""output"":""html_document"",""editor_options"":{""chunk_output_type"":""console""}},""headingText"":""Common cases"",""containsRefs"":false,""markdown"":""\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n"",""srcMarkdownNoYaml"":""\n# Common cases\n\nLet's go over some common cases of linear models to clarify their interpretation and usage. We will need to run this code to begin with:\n\n```{r}\n#| message: false\n#| warning: false\n#| code-fold: false\n#| collapse: true\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(faraway)\nlibrary(ggiraphExtra)\n\nfont.size <- 12\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# add obesity and diabetes status to diabetes faraway data\ninch2m <- 2.54/100\ninch2cm <- 2.54\npound2kg <- 0.45\ndata_diabetes <- diabetes %>%\n  mutate(height  = height * inch2m, height = round(height, 2)) %>% \n  mutate(waist = waist * inch2cm) %>%  \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% \n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %>% \n  mutate(diabetic = ifelse(glyhb > 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %>%\n  na.omit()\n\n```\n\n## Example: simple linear regression\n```{r}\n#| warning: false\n#| message: false\n#| label: fig-simple-1\n#| fig-cap: Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model.\n#| fig-cap-location: margin\n#| column: margin\n#| fig-width: 4\n#| fig-heigth: 4\n#| collapse: true\n#| code-fold: false\n\nm1 <- lm(BMI ~ waist, data = data_diabetes)\nsummary(m1)\n\nggPredict(m1) + \n  my.ggtheme + \n  xlab(\""waist [cm]\"")\n\n```\n\n**Model (generic)** \n\n- $Y_i = \\alpha + \\beta \\cdot x_i + \\epsilon_i$ \n\n**Model (fitted)** \n\n- $BMI_i = -5.12 + 0.35 \\cdot waist_i + \\epsilon_i$ \n\n**Slope**\n\n- The value of slope tells us how and by much the outcome changes with a unit change in $x$\n- If the `waist` increases by 1 unit, here in cm, what would be our expected change in `BMI`^[If the waist increases by 1 cm we would expect our `BMI` to increase by $\\approx 0.35$ since $\\hat{\\beta} = 0.35298$]$? \n- And if the `waist` increases by 10 units what would be our expected change in `BMI`^[If the waist increases by 10 cm we would expect `BMI` to increase by $0.35298 \\cdot 10 \\approx  3.53$]$?\n\n**Intercept**\n\n- The **intercept**, often labeled the **constant**, is the value of Y when $x_i=0$.\n- In models where $x_i$ can be equal 0, the intercept is simply the expected mean value of response.\n- In models where $x_i$ cannot be equal 0, like in our `BMI` example where it is not possible to have `BMI` equal to zero, the intercept has no intrinsic meaning.\n- The intercept is thus quite often ignored in linear models, as it is the value of slope that dictates the association between exposure and outcome.\n\n**Hypothesis testing**\n\n- We've seen during the lecture that the check for association between exposure and outcome we check if the we have enough evidence to reject $H_0: \\beta=0$ in favor of the alternative $H_a: \\beta\\neq0$.\n- Here, for the $\\beta$ coefficient we have $t-statistics = 0.35298 / 0.02723 = 12.965$ and a corresponding $p-value = 12.96291$, as $t-statistics \\sim t(130-2) << 0.05$. Such large `t-statsitics` or small p-value means we have enough evidence to reject the null hypothesis and conclude that there is a significant association between waist and BMI. \n- We can double-check R output by calculating p-value ourselves using the Student t distribution:\n```{r}\n#| code-fold: false\n#| collapse: true\n2*pt(12.96291, df=128, lower=F)\n```\n\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 5% significance level?^[No, as $p-value = 0.0633 \\nless 0.05$].\n- Is there enough evidence to reject the null hypothesis of $H_0: \\alpha=0$ in favor of the alternative $H_a: \\alpha\\neq0$ assuming 10% significance level?^[Yes, as $p-value = 0.0633 < 0.1$].\n\n**Predictions**\n\n- Using the model we can predict the `BMI` value for a new observation of `waist`.\n- For instance, we can find expected `BMI` value for someone who measures 100 cm in `waist` by: \n- $BMI = -5.12445 + 0.35298 \\cdot 100 = 30.17355$\n- In R can use `predict()` function:\n\n```{r}\n#| code-fold: false\n#| collapse: true\n# predict BMI for a new value of 100\nnew_data <- data.frame(waist = 100)\npredict(m1, newdata = new_data)\n```\n\n- What would be `BMI` for someone with `waist` measurements of 75?^[BMI = -5.12445 + 0.35298 \\cdot 75 = 21.349]\n- What would be `BMI` for someone with `waist` measurements of 200?^[BMI = -5.12445 + 0.35298 \\cdot 200 = 65.47141, however here we have to be careful in predicting outside the model range.]\n\n**Model fit**\n\n- In simple regression we can use $R^2$ to assess model fit, here $R^2 = 0.5677$.\n- Do you think that the model fits the data well?^[In simple linear regression $R^2$ is the same as $r^2$ and a value of 0.5677 indicates moderate fit, that agrees with the plot above. Since we have more variables in the data set we could try to improve the fit by including more variables.]\n\n**Model assumptions**\n\nWe should also not forget to look at the residual plots to check model assumptions:\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| fig-height: 6\npar(mfrow = c(2,2))\nplot(m1)\n```\n\n- Given the diagnostic plots can we comment about the assumptions of linear models being met?^[The diagnostics do not indicate a serious violation of model assumptions, with no obvious trends of any kind in the residuals plots. Few samples deviate from diagonal line on the Normal Q-Q plot and these could be removed to ensure that the residuals follow normal distribution.]\n\n## Example: multiple regression\n\nLet's try to model BMI using more variables\n\n**Model (generic)**\n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot  age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i  + \\epsilon_i$\n\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit multiple linear regression and print model summary\nm2 <- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n```\n\n**Coefficient interpretations**\n\nUsing the model answer the questions:\n\n- what would happen to `BMI` if `hdl` levels increase by 10?^[decreases by $-0.090023 \\cdot 10 = 0.90023$]\n- what would happen to `BMI` if `age` increases by 1 year?^[decrease by 0.027047, however here we can see that the age coefficient is not significant and therefore we should be careful with our interpretations as there is no evidence that this coefficient is different than 0.]\n\n**Hypothesis testing**\n\n- overall, is there a relationship between the response $Y$ (BMI) and predictors?^[we have seen before that in the case of simple linear regression it was enough to test the null hypothesis of $H_0: \\beta=0$ versus $H_0: \\beta\\neq0$ to answers the question whether there is an overall relationship between response and predictor. In case of multiple regression, with many predictors, we need to test the null hypothesis of $$H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$$ versus the alternative $$H_a: at \\; least \\; one \\; \\beta_j \\; is \\; non-zero$$ This hypothesis test is performed by computing **F-statistics** reported in the model summary and calculated as $F = \\frac{(TSS - RSS)/p}{RSS/(n-p-1)}$ where $TSS = \\sum(y_i - \\bar{y})^2$ and $RSS = \\sum(y_i - \\hat{y_i})^2$. Here, the $F-statsitics = 3.046$ and the associated $p-value < 0.05$ so there is enough evidence to reject the null hypothesis in favor of the alternative and conclude that there is an overall significant relationship between response (BMI) and predictors.]\n\n<br />\n**Not so easy: alternative model**\n\nLet's consider another multiple regression model: \n\n- $Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot  waist_i +  \\epsilon_i$\n\nWe fit the model in `R` and look at the model summary:\n```{r}\n#| collapse: true\n#| code-fold: false\nm2_alt <- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n```\n\n- What happens to `BMI` if `hdl` increases by 10?^[decreases by ca. 0.06]\n- What happens to `BMI` if `hdl` increases by 10 using the first model again?[decreases by ca. 0.9]\n- How do you explain the difference in `BMI` changes given these two models?\n\n**Specific interpretation**\n\n- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). \n- Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\\hat{\\beta}$ in the response $y$**\n- Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. \n- Further, our explanation contains **no notation of causation**.\n- We will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.\n\n\n## Example: categorical variable\n\n- We want to **compare the average BMI of men and women**.\n- We can do that using linear regression and including gender as **binary variable**\n\n```{r}\n#| code-fold: true\n#| collapse: true\n#| column: margin\n#| fig-height: 6\n\nfont.size <- 20\ncol.blue.light <- \""#a6cee3\""\ncol.blue.dark <- \""#1f78b4\""\nmy.ggtheme <- \n  theme_bw() + \n  theme(axis.title = element_text(size = font.size), \n        axis.text = element_text(size = font.size), \n        legend.text = element_text(size = font.size), \n        legend.title = element_blank(), \n        legend.position = \""top\"")\n      \n\n# visualize the data with box plot\ndata_diabetes %>%\n  ggplot(aes(x = gender, y = BMI, fill = gender)) + \n  geom_boxplot() + \n  scale_fill_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } x_i=1 \\\\\n                0 & \\mathrm{if\\ } x_i=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\nfor some coding, e.g. we choose to set \""Female=1\"" and \""Male=0\"" or vice versa.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n\n# Note: check that Gender is indeed non-numeric\nprint(class(data_diabetes$gender))\n\n# fit linear regression and print model summary\nm3 <- lm(BMI ~ gender, data = data_diabetes)\nprint(summary(m3))\n\n```\n\n**Estimates**\n$$\\hat{\\alpha} = 27.7674$$\n$$\\hat{\\beta} = 3.9396$$\n\n- The `lm()` function chooses automatically one of the category as baseline, here `females`.\n- Model summary prints the output of the model with the baseline category **\""hidden\""**.\n- Notice that the only label we have is \""genderfemale\"".\n- Meaning that we ended-up having a model coded as below:\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n- Consequently, if observation $i$ is female then the expected value of `BMI` is:\n$$E(BMI_i|female) = 27.7674 + 3.9396 = 31.707$$\n- and if observation $i$ is male then the expected value of `BMI` is:\n$$E(BMI_i|male) = 27.7674$$\nWe can plot the model in R:\n```{r}\n#| code-fold: false\n#| collapse: false\nggPredict(m3) + \n  my.ggtheme \n```\n\n\n## Example: categorical & numerical variables\n\n- Above we observed a signficant difference in average `BMI` between men and women among the study participants.\n- Can we also observe a significant relationship between `BMI` and `height`?\n- And if so, does this relationship depend on `gender`?\n\n```{r}\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %>%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n- From the plot we can see that `BMI` decreases slightly with `height`.\n- On average, men are taller than women.\n- On average, women have higher BMI than men.\n- The relationship between `height` and `BMI` appears to be the same for males and females, i.e. `BMI` decreases with `height` for both men and women.\n\nTo assess the relationship we use a model containing `height` and `gender`.\n\n**Model**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the `height` of person $i$.\n\nIn `R` we write:\n```{r}\n#| code-fold: false\n#| collapse: true\n# fit linear model and print model summary\nm4 <- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n\n```\n\n**Model together with estimates**\n\n$$Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i$$\nwhere\n\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\n\nand $x_{2,i}$ is the weight of person $i$\n\n\n**Estimates**\n\n$$\\hat{\\alpha} = 37.743 $$\n$$\\hat{\\beta} = 3.163$$\n$$\\hat{\\gamma} = -5.719$$\n\n\n- For instance, using our estimates, for a female who happens to 1.7 m tall we would predict `BMI` of:\n$$E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837$$\n- and for a male of height 1.7 m tall we would predict `BMI` of\n$$E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207$$\n\nIn `R` we can plot our data and the fitted model to verify our calculations:\n```{r}\n#| collapse: true\n#| code-fold: false\n#| message: false\n#| fig-width: 8\n#| fig-height: 6\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n<br />\n\n## Example: interactions\n\n\n- The fitted lines in the above example are **parallel**, the **slope is modeled to be the same for men and women**, and the intercept denotes the group differences.\n- It is also possible to allow for **both intercept and slope being fitted separately for each group**.\n- This is done when we except that the relationships are different in different groups, e.g. increasing in one group and decreasing in the other.\n- And we then talk about including **interaction effect** since the two lines may interact (cross).\n\n**Model**\n\n$$Y_{i,j} = \\alpha_i + \\beta_ix_{ij} + \\epsilon_{i,j}$$\nwhere:\n\n- $Y_{i,j}$ is the BMI of person $j$ of gender $i$\n- $x_{ij}$ is the height of person $j$ of gender $i$\n- $i=1$ corresponds to women in our example (keeping the same coding as above)\n- $i=2$ corresponds to men\n\nIn `R` we define the interaction term with `*`:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# fit linear model with interaction\nm5 <- lm(BMI ~ gender * height, data = data_diabetes)\nprint(summary(m5))\n\n```\n\nNow, based on the regression output we would expect:\n\n- for a woman of height $x$, a BMI value of:\n$$E(BMI|female\\; and \\; height=x)=31.222 + 14.219 - 1.981 \\cdot x - 6.558 \\cdot x = 45.441 -8.539 \\cdot x$$\n- for a man of height $x$, a BMI value of $$E(BMI|male\\; and \\; height=x)=31.222-1.981 \\cdot x$$\n\n**Estimates**\n$$\\hat{\\alpha_1} = 45.441$$\n$$\\hat{\\beta_1} = 31.222$$\n\n$$\\hat{\\alpha_2} = 47.34778$$\n$$\\hat{\\beta_2} = -1.981$$\n\n- We can see from the regression output that there is no evidence to reject the null hypothesis that the interaction term \""Genderfemale:height\"" is equal to zero.\n- Or therefore conclude that the relationship between `BMI` and `height` is different for men and women.\n- We can plot the fitted model and see that the lines are no longer parallel.\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\n\nggPredict(m5) +\n  guides(color=guide_legend(override.aes=list(fill=NA))) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n```\n\n\n## Example: logistic regression with categorical variable\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# recode diabetic status to 1 and 0\ndata_diabetes <- data_diabetes %>%\n  mutate(obese = ifelse(obese == \""Yes\"", 1, 0))\n\n# fit logistic regression using age and gender\nm6 <- glm(obese ~  hdl + gender, family = binomial(link=\""logit\""), data = data_diabetes)\nsummary(m6)\n\n```\n\n- By how much change odds of suffering from obesity when `hdl` increases by 1?^[the odds increase by e^{-0.02997} = 0.97]\n- What are the odds of suffering from obesity and being a women vs. suffering from obesity and being a man?^[The odds of suffering from obesity as a woman are e^{1.26586} = 3.55 times of that suffering from obesity and being a man.]\n\n\n```{r}\n#| code-fold: false\n#| collapse: true\n#| fig-width: 8\n#| fig-height: 6\n#| message: false\n#| warning: false\nggPredict(m6) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n```\n\nWe can predict `obesity status` in R for a man with `hdl` values of 50:\n```{r}\n#| collapse: true\n#| code-fold: false\n\n# define new observation\ndf <- data.frame(hdl = 50, gender = as.factor(\""male\""))\n\n# predict probability of suffering from obesity\nprob_obese <- predict(m6, newdata = df, type = \""response\"")\nprint(prob_obese)\n\n\n```\n\n\n""},""formats"":{""html"":{""identifier"":{""display-name"":""HTML"",""target-format"":""html"",""base-format"":""html""},""execute"":{""fig-width"":7,""fig-height"":5,""fig-format"":""retina"",""fig-dpi"":96,""df-print"":""default"",""error"":false,""eval"":true,""cache"":null,""freeze"":false,""echo"":true,""output"":""html_document"",""warning"":true,""include"":true,""keep-md"":false,""keep-ipynb"":false,""ipynb"":null,""enabled"":null,""daemon"":null,""daemon-restart"":false,""debug"":false,""ipynb-filters"":[],""engine"":""knitr""},""render"":{""keep-tex"":false,""keep-source"":false,""keep-hidden"":false,""prefer-html"":false,""output-divs"":true,""output-ext"":""html"",""fig-align"":""default"",""fig-pos"":null,""fig-env"":null,""code-fold"":true,""code-overflow"":""scroll"",""code-link"":false,""code-line-numbers"":false,""code-tools"":false,""tbl-colwidths"":""auto"",""merge-includes"":true,""inline-includes"":false,""preserve-yaml"":false,""latex-auto-mk"":true,""latex-auto-install"":true,""latex-clean"":true,""latex-max-runs"":10,""latex-makeindex"":""makeindex"",""latex-makeindex-opts"":[],""latex-tlmgr-opts"":[],""latex-input-paths"":[],""latex-output-dir"":null,""link-external-icon"":false,""link-external-newwindow"":false,""self-contained-math"":false,""format-resources"":[],""notebook-links"":true,""format-links"":true},""pandoc"":{""standalone"":true,""wrap"":""none"",""default-image-extension"":""png"",""to"":""html"",""output-file"":""lm-coeff.html""},""language"":{""toc-title-document"":""Table of contents"",""toc-title-website"":""On this page"",""related-formats-title"":""Other Formats"",""related-notebooks-title"":""Notebooks"",""source-notebooks-prefix"":""Source"",""section-title-abstract"":""Abstract"",""section-title-appendices"":""Appendices"",""section-title-footnotes"":""Footnotes"",""section-title-references"":""References"",""section-title-reuse"":""Reuse"",""section-title-copyright"":""Copyright"",""section-title-citation"":""Citation"",""appendix-attribution-cite-as"":""For attribution, please cite this work as:"",""appendix-attribution-bibtex"":""BibTeX citation:"",""title-block-author-single"":""Author"",""title-block-author-plural"":""Authors"",""title-block-affiliation-single"":""Affiliation"",""title-block-affiliation-plural"":""Affiliations"",""title-block-published"":""Published"",""title-block-modified"":""Modified"",""callout-tip-title"":""Tip"",""callout-note-title"":""Note"",""callout-warning-title"":""Warning"",""callout-important-title"":""Important"",""callout-caution-title"":""Caution"",""code-summary"":""Code"",""code-tools-menu-caption"":""Code"",""code-tools-show-all-code"":""Show All Code"",""code-tools-hide-all-code"":""Hide All Code"",""code-tools-view-source"":""View Source"",""code-tools-source-code"":""Source Code"",""code-line"":""Line"",""code-lines"":""Lines"",""copy-button-tooltip"":""Copy to Clipboard"",""copy-button-tooltip-success"":""Copied!"",""repo-action-links-edit"":""Edit this page"",""repo-action-links-source"":""View source"",""repo-action-links-issue"":""Report an issue"",""back-to-top"":""Back to top"",""search-no-results-text"":""No results"",""search-matching-documents-text"":""matching documents"",""search-copy-link-title"":""Copy link to search"",""search-hide-matches-text"":""Hide additional matches"",""search-more-match-text"":""more match in this document"",""search-more-matches-text"":""more matches in this document"",""search-clear-button-title"":""Clear"",""search-detached-cancel-button-title"":""Cancel"",""search-submit-button-title"":""Submit"",""search-label"":""Search"",""toggle-section"":""Toggle section"",""toggle-sidebar"":""Toggle sidebar navigation"",""toggle-dark-mode"":""Toggle dark mode"",""toggle-reader-mode"":""Toggle reader mode"",""toggle-navigation"":""Toggle navigation"",""crossref-fig-title"":""Figure"",""crossref-tbl-title"":""Table"",""crossref-lst-title"":""Listing"",""crossref-thm-title"":""Theorem"",""crossref-lem-title"":""Lemma"",""crossref-cor-title"":""Corollary"",""crossref-prp-title"":""Proposition"",""crossref-cnj-title"":""Conjecture"",""crossref-def-title"":""Definition"",""crossref-exm-title"":""Example"",""crossref-exr-title"":""Exercise"",""crossref-ch-prefix"":""Chapter"",""crossref-apx-prefix"":""Appendix"",""crossref-sec-prefix"":""Section"",""crossref-eq-prefix"":""Equation"",""crossref-lof-title"":""List of Figures"",""crossref-lot-title"":""List of Tables"",""crossref-lol-title"":""List of Listings"",""environment-proof-title"":""Proof"",""environment-remark-title"":""Remark"",""environment-solution-title"":""Solution"",""listing-page-order-by"":""Order By"",""listing-page-order-by-default"":""Default"",""listing-page-order-by-date-asc"":""Oldest"",""listing-page-order-by-date-desc"":""Newest"",""listing-page-order-by-number-desc"":""High to Low"",""listing-page-order-by-number-asc"":""Low to High"",""listing-page-field-date"":""Date"",""listing-page-field-title"":""Title"",""listing-page-field-description"":""Description"",""listing-page-field-author"":""Author"",""listing-page-field-filename"":""File Name"",""listing-page-field-filemodified"":""Modified"",""listing-page-field-subtitle"":""Subtitle"",""listing-page-field-readingtime"":""Reading Time"",""listing-page-field-categories"":""Categories"",""listing-page-minutes-compact"":""{0} min"",""listing-page-category-all"":""All"",""listing-page-no-matches"":""No matching items""},""metadata"":{""lang"":""en"",""fig-responsive"":true,""quarto-version"":""1.3.450"",""bibliography"":[""references.bib""],""theme"":""cosmo"",""editor_options"":{""chunk_output_type"":""console""}},""extensions"":{""book"":{""multiFile"":true}}}},""projectFormats"":[""html""]}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/28d59191---
@@ -1 +1 @@
-{""entries"":[],""options"":{""chapters"":true},""headings"":[""preface""]}
\ No newline at end of file
+{""options"":{""chapters"":true},""entries"":[],""headings"":[""preface""]}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/295884aa---
@@ -1 +1 @@
-{""headings"":[""fitting-linear-model"",""hypothesis-testing"",""evaluate-model-fit""],""entries"":[{""key"":""exr-lm-assess-fit"",""caption"":""Evaluate model fit"",""order"":{""number"":3,""section"":[0,0,0,0,0,0,0]}},{""key"":""exr-lm-fit"",""caption"":""Fitting linear model"",""order"":{""number"":1,""section"":[0,0,0,0,0,0,0]}},{""key"":""exr-lm-hypothesis"",""caption"":""Hypothesis testing"",""order"":{""number"":2,""section"":[0,0,0,0,0,0,0]}}],""options"":{""chapters"":true}}
\ No newline at end of file
+{""entries"":[{""key"":""exr-lm-fit"",""caption"":""Fitting linear model"",""order"":{""number"":1,""section"":[0,0,0,0,0,0,0]}},{""key"":""exr-lm-hypothesis"",""caption"":""Hypothesis testing"",""order"":{""number"":2,""section"":[0,0,0,0,0,0,0]}},{""key"":""exr-lm-assess-fit"",""caption"":""Evaluate model fit"",""order"":{""number"":3,""section"":[0,0,0,0,0,0,0]}}],""headings"":[""fitting-linear-model"",""hypothesis-testing"",""evaluate-model-fit""],""options"":{""chapters"":true}}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/4d506fc9---
@@ -1 +1 @@
-{""entries"":[{""caption"":""Brozek score"",""key"":""exr-brozek"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":1}}],""options"":{""chapters"":true},""headings"":[""brozek-score"",""answers-to-selected-exercises""]}
\ No newline at end of file
+{""headings"":[""brozek-score"",""answers-to-selected-exercises""],""entries"":[{""caption"":""Brozek score"",""key"":""exr-brozek"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":1}}],""options"":{""chapters"":true}}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/5b28360f---
@@ -1 +1 @@
-{""entries"":[{""order"":{""number"":2,""section"":[0,0,0,0,0,0,0]},""caption"":""Trout"",""key"":""exr-trout""},{""order"":{""number"":1,""section"":[0,0,0,0,0,0,0]},""caption"":""Height-weight-gender"",""key"":""exr-rerun""},{""order"":{""number"":3,""section"":[0,0,0,0,0,0,0]},""caption"":""Lowering blood pressure"",""key"":""exr-drug""}],""options"":{""chapters"":true},""headings"":[""height-weight-gender"",""trout"",""lowering-blood-pressure"",""answers-to-selected-exercises""]}
\ No newline at end of file
+{""headings"":[""height-weight-gender"",""trout"",""lowering-blood-pressure"",""answers-to-selected-exercises""],""entries"":[{""caption"":""Trout"",""key"":""exr-trout"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":2}},{""caption"":""Height-weight-gender"",""key"":""exr-rerun"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":1}},{""caption"":""Lowering blood pressure"",""key"":""exr-drug"",""order"":{""section"":[0,0,0,0,0,0,0],""number"":3}}],""options"":{""chapters"":true}}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/691de0dc---
@@ -1 +1 @@
-{""options"":{""chapters"":true},""entries"":[{""order"":{""number"":1,""section"":[3,1,0,0,0,0,0]},""caption"":""Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model."",""key"":""fig-simple-1""}],""headings"":[""example-simple-linear-regression"",""example-multiple-regression"",""example-categorical-variable"",""example-categorical-numerical-variables"",""example-interactions"",""example-logistic-regression-with-categorical-variable""]}
\ No newline at end of file
+{""options"":{""chapters"":true},""headings"":[""example-simple-linear-regression"",""example-multiple-regression"",""example-categorical-variable"",""example-categorical-numerical-variables"",""example-interactions"",""example-logistic-regression-with-categorical-variable""],""entries"":[{""key"":""fig-simple-1"",""caption"":""Scatter plot showing BMI values given waist measurments with a fitted simple linear regression model."",""order"":{""number"":1,""section"":[3,1,0,0,0,0,0]}}]}
\ No newline at end of file

---FILE: session-lm/.quarto/xref/9e3bf041---
@@ -1 +1 @@
-{""options"":{""chapters"":true},""entries"":[{""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable"",""key"":""fig-lm-example-reg"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":5}},{""caption"":""vector-matrix-notation"",""key"":""exm-vector-matrix-notation"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":4}},{""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line"",""key"":""fig-reg-errors"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":6}},{""caption"":""Prediction and intervals"",""key"":""exm-prediction-and-intervals"",""order"":{""section"":[1,10,0,0,0,0,0],""number"":5}},{""caption"":""Least squares in vector-matrix notation"",""key"":""thm-lss-vector-matrix"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":2}},{""caption"":""vector matrix form of the linear model"",""key"":""def-vector-matrix-lm"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":1}},{""caption"":""Hypothesis testing"",""key"":""exm-hypothesis-testing"",""order"":{""section"":[1,8,0,0,0,0,0],""number"":3}},{""caption"":""Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA."",""key"":""fig-scatter"",""order"":{""section"":[1,1,0,0,0,0,0],""number"":1}},{""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca."",""key"":""fig-lm-intro-example"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":4}},{""caption"":""Least squares"",""key"":""exm-lss"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":2}},{""caption"":""Weight and plasma volume"",""key"":""exm-simple-lm"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":1}},{""caption"":""Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between x and y is not perfect (increasing relationship), c) statistical relationship between x and y is not perfect (decreasing relationship), d) random signal"",""key"":""fig-relationship"",""order"":{""section"":[1,2,0,0,0,0,0],""number"":2}},{""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)"",""key"":""fig-lm-parameters"",""order"":{""section"":[1,7,0,0,0,0,0],""number"":7}},{""caption"":""R^2(adj)"",""key"":""thm-r2adj"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":4}},{""caption"":""R^2"",""key"":""def-r2"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":2}},{""caption"":""Least squares estimates for a simple linear regression"",""key"":""thm-lss"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":1}},{""caption"":""R^2"",""key"":""thm-r2"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":3}},{""caption"":""Examples of a linear models: A) y_i = x_1 + e_i, B) x_1 + I_{x_i} + e_i C) y_i = x_i^2 + e_i, D) y_i = x + x_i^3 + e_i showing that linear models can get more complex and/or capture more than a straight line relationship."",""key"":""fig-linear-adv"",""order"":{""section"":[1,3,0,0,0,0,0],""number"":3}},{""caption"":"""",""key"":""eq-lm"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":1}}],""headings"":[""why-linear-models"",""statistical-vs.-deterministic-relationship"",""what-linear-models-are-and-are-not"",""terminology"",""simple-linear-regression"",""weight-and-plasma-volume"",""least-squares"",""least-squares-estimates-for-a-simple-linear-regression"",""least-squares-1"",""intercept-and-slope"",""hypothesis-testing"",""hypothesis-testing-1"",""vector-matrix-notations"",""vector-matrix-form-of-the-linear-model"",""least-squares-in-vector-matrix-notation"",""vector-matrix-notation"",""confidence-intervals-and-prediction-intervals"",""prediction-and-intervals"",""assessing-model-fit"",""r2-summary-of-the-fitted-model"",""r2"",""r2-and-correlation-coefficient"",""r2-1"",""r2adj"",""r2adj-1"",""the-assumptions-of-a-linear-model"",""checking-assumptions"",""influential-observations""]}
\ No newline at end of file
+{""options"":{""chapters"":true},""headings"":[""why-linear-models"",""statistical-vs.-deterministic-relationship"",""what-linear-models-are-and-are-not"",""terminology"",""simple-linear-regression"",""weight-and-plasma-volume"",""least-squares"",""least-squares-estimates-for-a-simple-linear-regression"",""least-squares-1"",""intercept-and-slope"",""hypothesis-testing"",""hypothesis-testing-1"",""vector-matrix-notations"",""vector-matrix-form-of-the-linear-model"",""least-squares-in-vector-matrix-notation"",""vector-matrix-notation"",""confidence-intervals-and-prediction-intervals"",""prediction-and-intervals"",""assessing-model-fit"",""r2-summary-of-the-fitted-model"",""r2"",""r2-and-correlation-coefficient"",""r2-1"",""r2adj"",""r2adj-1"",""the-assumptions-of-a-linear-model"",""checking-assumptions"",""influential-observations""],""entries"":[{""key"":""def-r2"",""caption"":""R^2"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":2}},{""key"":""eq-lm"",""caption"":"""",""order"":{""section"":[1,5,0,0,0,0,0],""number"":1}},{""key"":""exm-lss"",""caption"":""Least squares"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":2}},{""key"":""thm-lss"",""caption"":""Least squares estimates for a simple linear regression"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":1}},{""key"":""fig-scatter"",""caption"":""Scatter plot of weight vs. height for the 130 study participants based on the diabetes data set collected to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia, USA."",""order"":{""section"":[1,1,0,0,0,0,0],""number"":1}},{""key"":""fig-linear-adv"",""caption"":""Examples of a linear models: A) y_i = x_1 + e_i, B) x_1 + I_{x_i} + e_i C) y_i = x_i^2 + e_i, D) y_i = x + x_i^3 + e_i showing that linear models can get more complex and/or capture more than a straight line relationship."",""order"":{""section"":[1,3,0,0,0,0,0],""number"":3}},{""key"":""exm-vector-matrix-notation"",""caption"":""vector-matrix-notation"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":4}},{""key"":""thm-r2adj"",""caption"":""R^2(adj)"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":4}},{""key"":""exm-prediction-and-intervals"",""caption"":""Prediction and intervals"",""order"":{""section"":[1,10,0,0,0,0,0],""number"":5}},{""key"":""thm-r2"",""caption"":""R^2"",""order"":{""section"":[1,11,0,0,0,0,0],""number"":3}},{""key"":""fig-reg-errors"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regrssion gives the equation of the straight line (red) that best describes how the outcome changes with a change of exposure variable. Blue lines represent error terms, the vertical distances to the regression line"",""order"":{""section"":[1,6,0,0,0,0,0],""number"":6}},{""key"":""exm-simple-lm"",""caption"":""Weight and plasma volume"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":1}},{""key"":""fig-lm-parameters"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)"",""order"":{""section"":[1,7,0,0,0,0,0],""number"":7}},{""key"":""thm-lss-vector-matrix"",""caption"":""Least squares in vector-matrix notation"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":2}},{""key"":""def-vector-matrix-lm"",""caption"":""vector matrix form of the linear model"",""order"":{""section"":[1,9,0,0,0,0,0],""number"":1}},{""key"":""fig-lm-example-reg"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca. Linear regression gives the equation of the straight line (red) that best describes how the outcome changes (increase or decreases) with a change of exposure variable"",""order"":{""section"":[1,5,0,0,0,0,0],""number"":5}},{""key"":""fig-lm-intro-example"",""caption"":""Scatter plot of the data shows that high plasma volume tends to be associated with high weight and vice verca."",""order"":{""section"":[1,5,0,0,0,0,0],""number"":4}},{""key"":""fig-relationship"",""caption"":""Deterministic vs. statistical relationship: a) deterministic: equation exactly describes the relationship between the two variables e.g. Ferenheit and Celcius relationship, b) statistical relationship between x and y is not perfect (increasing relationship), c) statistical relationship between x and y is not perfect (decreasing relationship), d) random signal"",""order"":{""section"":[1,2,0,0,0,0,0],""number"":2}},{""key"":""exm-hypothesis-testing"",""caption"":""Hypothesis testing"",""order"":{""section"":[1,8,0,0,0,0,0],""number"":3}}]}
\ No newline at end of file

---FILE: session-lm/docs/lm-coeff.html---
@@ -442,7 +442,7 @@ <h2 data-number=""3.2"" class=""anchored"" data-anchor-id=""example-multiple-regressi
 </ul>
 <p><strong>Specific interpretation</strong></p>
 <ul>
-<li>Obviously there is difference between decrease of 0.9 BMI and decrease of 0.9 in BMI (alternative model).</li>
+<li>Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model).</li>
 <li>Our interpretations need to be more specific and we say that <strong>a unit increase in <span class=""math inline"">\(x\)</span> with other predictors held constant will produce a change equal to <span class=""math inline"">\(\hat{\beta}\)</span> in the response <span class=""math inline"">\(y\)</span></strong></li>
 <li>Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in <code>hdl</code> would also imply a change in total cholesterol <code>chol</code>.</li>
 <li>Further, our explanation contains <strong>no notation of causation</strong>.</li>
@@ -590,7 +590,7 @@ <h2 data-number=""3.4"" class=""anchored"" data-anchor-id=""example-categorical-numer
         \end{array}
     \right.
 \end{equation}\]</span></p>
-<p>and <span class=""math inline"">\(x_{2,i}\)</span> is the <code>heigth</code> of person <span class=""math inline"">\(i\)</span>.</p>
+<p>and <span class=""math inline"">\(x_{2,i}\)</span> is the <code>height</code> of person <span class=""math inline"">\(i\)</span>.</p>
 <p>In <code>R</code> we write:</p>
 <div class=""cell"">
 <div class=""sourceCode cell-code"" id=""cb12""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb12-1""><a href=""#cb12-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># fit linear model and print model summary</span></span>

---FILE: session-lm/docs/search.json---
@@ -172,7 +172,7 @@
     ""href"": ""lm-coeff.html#example-multiple-regression"",
     ""title"": ""3  Common cases"",
     ""section"": ""3.2 Example: multiple regression"",
-    ""text"": ""3.2 Example: multiple regression\nLet’s try to model BMI using more variables\nModel (generic)\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\epsilon_i\\)\n\n\n# fit multiple linear regression and print model summary\nm2 &lt;- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 35.456968   3.149661  11.257  &lt; 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,    Adjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n\nCoefficient interpretations\nUsing the model answer the questions:\n\nwhat would happen to BMI if hdl levels increase by 10?9\nwhat would happen to BMI if age increases by 1 year?10\n\nHypothesis testing\n\noverall, is there a relationship between the response \\(Y\\) (BMI) and predictors?11\n\n Not so easy: alternative model\nLet’s consider another multiple regression model:\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot waist_i + \\epsilon_i\\)\n\nWe fit the model in R and look at the model summary:\n\nm2_alt &lt;- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864, Adjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: &lt; 2.2e-16\n\n\nWhat happens to BMI if hdl increases by 10?12\nWhat happens to BMI if hdl increases by 10 using the first model again?[decreases by ca. 0.9]\nHow do you explain the difference in BMI changes given these two models?\n\nSpecific interpretation\n\nObviously there is difference between decrease of 0.9 BMI and decrease of 0.9 in BMI (alternative model).\nOur interpretations need to be more specific and we say that a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\)\nOften it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in hdl would also imply a change in total cholesterol chol.\nFurther, our explanation contains no notation of causation.\nWe will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.""
+    ""text"": ""3.2 Example: multiple regression\nLet’s try to model BMI using more variables\nModel (generic)\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\epsilon_i\\)\n\n\n# fit multiple linear regression and print model summary\nm2 &lt;- lm(BMI ~ age + chol + hdl,  data = data_diabetes)\nsummary(m2)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl, data = data_diabetes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.074  -4.833  -1.132   3.438  22.032 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 35.456968   3.149661  11.257  &lt; 2e-16 ***\n## age         -0.027047   0.040304  -0.671  0.50340    \n## chol         0.002039   0.012701   0.161  0.87269    \n## hdl         -0.090023   0.032734  -2.750  0.00683 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.552 on 126 degrees of freedom\n## Multiple R-squared:  0.06763,    Adjusted R-squared:  0.04543 \n## F-statistic: 3.046 on 3 and 126 DF,  p-value: 0.03124\n\nCoefficient interpretations\nUsing the model answer the questions:\n\nwhat would happen to BMI if hdl levels increase by 10?9\nwhat would happen to BMI if age increases by 1 year?10\n\nHypothesis testing\n\noverall, is there a relationship between the response \\(Y\\) (BMI) and predictors?11\n\n Not so easy: alternative model\nLet’s consider another multiple regression model:\n\n\\(Y_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot chol_i + \\beta_3 \\cdot hdl_i + \\beta_4 \\cdot waist_i + \\epsilon_i\\)\n\nWe fit the model in R and look at the model summary:\n\nm2_alt &lt;- lm(BMI ~ age + chol + hdl + waist, data = data_diabetes)\nsummary(m2_alt)\n## \n## Call:\n## lm(formula = BMI ~ age + chol + hdl + waist, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.0337  -3.0416  -0.6777   2.2711  18.2894 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.921431   3.588473  -0.257   0.7978    \n## age         -0.050397   0.027016  -1.865   0.0645 .  \n## chol        -0.006250   0.008519  -0.734   0.4645    \n## hdl         -0.006199   0.022890  -0.271   0.7870    \n## waist        0.353256   0.028213  12.521   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.381 on 125 degrees of freedom\n## Multiple R-squared:  0.5864, Adjusted R-squared:  0.5732 \n## F-statistic:  44.3 on 4 and 125 DF,  p-value: &lt; 2.2e-16\n\n\nWhat happens to BMI if hdl increases by 10?12\nWhat happens to BMI if hdl increases by 10 using the first model again?[decreases by ca. 0.9]\nHow do you explain the difference in BMI changes given these two models?\n\nSpecific interpretation\n\nObviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model).\nOur interpretations need to be more specific and we say that a unit increase in \\(x\\) with other predictors held constant will produce a change equal to \\(\\hat{\\beta}\\) in the response \\(y\\)\nOften it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in hdl would also imply a change in total cholesterol chol.\nFurther, our explanation contains no notation of causation.\nWe will learn later how to choose the best model by assessing its fit and including only relevant variable (feature selection), for now we focus on learning how to interpret the coefficients given a fitted model.""
   },
   {
     ""objectID"": ""lm-coeff.html#example-categorical-variable"",
@@ -186,7 +186,7 @@
     ""href"": ""lm-coeff.html#example-categorical-numerical-variables"",
     ""title"": ""3  Common cases"",
     ""section"": ""3.4 Example: categorical & numerical variables"",
-    ""text"": ""3.4 Example: categorical & numerical variables\n\nAbove we observed a signficant difference in average BMI between men and women among the study participants.\nCan we also observe a significant relationship between BMI and height?\nAnd if so, does this relationship depend on gender?\n\n\n\nCode\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %&gt;%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\n\n\nFrom the plot we can see that BMI decreases slightly with height.\nOn average, men are taller than women.\nOn average, women have higher BMI than men.\nThe relationship between height and BMI appears to be the same for males and females, i.e. BMI decreases with height for both men and women.\n\nTo assess the relationship we use a model containing height and gender.\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the heigth of person \\(i\\).\nIn R we write:\n\n# fit linear model and print model summary\nm4 &lt;- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,    Adjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n\nModel together with estimates\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the weight of person \\(i\\)\nEstimates\n\\[\\hat{\\alpha} = 37.743 \\] \\[\\hat{\\beta} = 3.163\\] \\[\\hat{\\gamma} = -5.719\\]\n\nFor instance, using our estimates, for a female who happens to 1.7 m tall we would predict BMI of: \\[E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837\\]\nand for a male of height 1.7 m tall we would predict BMI of \\[E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207\\]\n\nIn R we can plot our data and the fitted model to verify our calculations:\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme""
+    ""text"": ""3.4 Example: categorical & numerical variables\n\nAbove we observed a signficant difference in average BMI between men and women among the study participants.\nCan we also observe a significant relationship between BMI and height?\nAnd if so, does this relationship depend on gender?\n\n\n\nCode\n#|label: fig-htwtgen-plot\n#|fig-cap: Scatter plot showing BMI measurments given height stratified by gender.\n#|fig-cap-location: margin\n#|collapse: true\n#|code-fold: false\n#|fig-width: 5\n#|fig-heigth: 5\n\n# plot the data separately for Male and Female\ndata_diabetes %&gt;%\n  ggplot(aes(x = height, y=BMI, col = gender)) +\n  geom_point(alpha = 0.8, size = 3) +\n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme\n\n\n\n\n\n\nFrom the plot we can see that BMI decreases slightly with height.\nOn average, men are taller than women.\nOn average, women have higher BMI than men.\nThe relationship between height and BMI appears to be the same for males and females, i.e. BMI decreases with height for both men and women.\n\nTo assess the relationship we use a model containing height and gender.\nModel\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the height of person \\(i\\).\nIn R we write:\n\n# fit linear model and print model summary\nm4 &lt;- lm(BMI ~ gender + height, data = data_diabetes)\nprint(summary(m4))\n## \n## Call:\n## lm(formula = BMI ~ gender + height, data = data_diabetes)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -13.7580  -4.2617  -0.3863   3.1646  19.2244 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    37.743     13.294   2.839  0.00527 **\n## genderfemale    3.163      1.538   2.057  0.04172 * \n## height         -5.719      7.606  -0.752  0.45350   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.448 on 127 degrees of freedom\n## Multiple R-squared:  0.08969,    Adjusted R-squared:  0.07535 \n## F-statistic: 6.256 on 2 and 127 DF,  p-value: 0.002562\n\nModel together with estimates\n\\[Y_i = \\alpha + \\beta I_{x_i} + \\gamma x_{2,i} + \\epsilon_i\\] where \\[\\begin{equation}\n    I_{x_i} =\n    \\left\\{\n        \\begin{array}{cc}\n                1 & \\mathrm{if\\ } \\quad person_i\\;is\\;male \\\\\n                0 & \\mathrm{if\\ } \\quad person_i\\;is\\;female \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]\nand \\(x_{2,i}\\) is the weight of person \\(i\\)\nEstimates\n\\[\\hat{\\alpha} = 37.743 \\] \\[\\hat{\\beta} = 3.163\\] \\[\\hat{\\gamma} = -5.719\\]\n\nFor instance, using our estimates, for a female who happens to 1.7 m tall we would predict BMI of: \\[E(BMI_i|female, height = 1.7) = 37.743 + 3.163 + (-5.719 \\cdot 1.7) = 31.1837\\]\nand for a male of height 1.7 m tall we would predict BMI of \\[E(BMI_i|male, height = 1.7) = 37.743 + (-5.719 \\cdot 1.7)  = 28.0207\\]\n\nIn R we can plot our data and the fitted model to verify our calculations:\n\n# plot the data separately for men and women\n# using ggplot() and geom_smooth()\nggPredict(m4) + \n  scale_color_brewer(palette = \""Set2\"") + \n  my.ggtheme""
   },
   {
     ""objectID"": ""lm-coeff.html#example-interactions"",
@@ -224,18 +224,11 @@
     ""text"": ""Answers to selected exercises\n\nSolution. Exercise 1\n\n\n# access and preview data\ndata(fat, package = \""faraway\"")\nhead(fat)\n##   brozek siri density age weight height adipos  free neck chest abdom   hip\n## 1   12.6 12.3  1.0708  23 154.25  67.75   23.7 134.9 36.2  93.1  85.2  94.5\n## 2    6.9  6.1  1.0853  22 173.25  72.25   23.4 161.3 38.5  93.6  83.0  98.7\n## 3   24.6 25.3  1.0414  22 154.00  66.25   24.7 116.0 34.0  95.8  87.9  99.2\n## 4   10.9 10.4  1.0751  26 184.75  72.25   24.9 164.7 37.4 101.8  86.4 101.2\n## 5   27.8 28.7  1.0340  24 184.25  71.25   25.6 133.1 34.4  97.3 100.0 101.9\n## 6   20.6 20.9  1.0502  24 210.25  74.75   26.5 167.0 39.0 104.5  94.4 107.8\n##   thigh knee ankle biceps forearm wrist\n## 1  59.0 37.3  21.9   32.0    27.4  17.1\n## 2  58.7 37.3  23.4   30.5    28.9  18.2\n## 3  59.6 38.9  24.0   28.8    25.2  16.6\n## 4  60.1 37.3  22.8   32.4    29.4  18.2\n## 5  63.2 42.2  24.0   32.2    27.7  17.7\n## 6  66.0 42.0  25.6   35.7    30.6  18.8\n\n# fit linear regression model\nmodel.all &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n\n# print model summary\nprint(summary(model.all))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  &lt; 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: &lt; 2.2e-16\n\n# diagnostics plots\npar(mfrow=c(2,2))\nplot(model.all)\n\n# remove potentially influential observations\nobs &lt;- c(86)\nfat2 &lt;- fat[-obs, ]\n\n# re-fit the model\nmodel.clean &lt;- lm(brozek ~ age + weight + height + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n\n# diagnostics plots\npar(mfrow=c(2,2))\nplot(model.clean)\n\n\n\n\n\n\n\n\n# model summary\nprint(summary(model.clean))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  &lt; 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no height)\nmodel.red1 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\nprint(summary(model.red1))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2830  -2.6162  -0.1017   2.8789   9.3713 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -22.66569   11.97691  -1.892  0.05963 .  \n## age           0.05948    0.02954   2.013  0.04521 *  \n## weight       -0.09829    0.04114  -2.389  0.01765 *  \n## neck         -0.43444    0.21445  -2.026  0.04389 *  \n## abdom         0.88762    0.06839  12.979  &lt; 2e-16 ***\n## hip          -0.17180    0.12919  -1.330  0.18483    \n## thigh         0.25327    0.12960   1.954  0.05183 .  \n## knee         -0.02318    0.22128  -0.105  0.91665    \n## ankle         0.17300    0.20411   0.848  0.39752    \n## biceps        0.15695    0.15715   0.999  0.31894    \n## forearm       0.43091    0.18239   2.363  0.01895 *  \n## wrist        -1.51011    0.49120  -3.074  0.00235 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.976 on 240 degrees of freedom\n## Multiple R-squared:  0.7484, Adjusted R-squared:  0.7369 \n## F-statistic:  64.9 on 11 and 240 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no knee)\nmodel.red2 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh + ankle + biceps + forearm + wrist, data = fat)\nprint(summary(model.red2))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2552  -2.5979  -0.1133   2.8693   9.3584 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -23.08716   11.25781  -2.051  0.04137 *  \n## age           0.05875    0.02864   2.051  0.04134 *  \n## weight       -0.09965    0.03897  -2.557  0.01117 *  \n## neck         -0.43088    0.21131  -2.039  0.04253 *  \n## abdom         0.88875    0.06740  13.186  &lt; 2e-16 ***\n## hip          -0.17231    0.12884  -1.337  0.18234    \n## thigh         0.24942    0.12403   2.011  0.04544 *  \n## ankle         0.16946    0.20089   0.844  0.39974    \n## biceps        0.15847    0.15616   1.015  0.31123    \n## forearm       0.42946    0.18150   2.366  0.01876 *  \n## wrist        -1.51470    0.48823  -3.102  0.00215 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.968 on 241 degrees of freedom\n## Multiple R-squared:  0.7484, Adjusted R-squared:  0.738 \n## F-statistic: 71.69 on 10 and 241 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no ankle)\nmodel.red3 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh  + biceps + forearm + wrist, data = fat)\nprint(summary(model.red3))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0740  -2.5615  -0.1021   2.7999   9.3199 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -20.61247   10.86240  -1.898   0.0589 .  \n## age           0.05727    0.02857   2.004   0.0461 *  \n## weight       -0.09141    0.03770  -2.424   0.0161 *  \n## neck         -0.45458    0.20931  -2.172   0.0308 *  \n## abdom         0.88098    0.06673  13.203   &lt;2e-16 ***\n## hip          -0.17575    0.12870  -1.366   0.1733    \n## thigh         0.25504    0.12378   2.061   0.0404 *  \n## biceps        0.15178    0.15587   0.974   0.3311    \n## forearm       0.42805    0.18138   2.360   0.0191 *  \n## wrist        -1.40948    0.47175  -2.988   0.0031 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.965 on 242 degrees of freedom\n## Multiple R-squared:  0.7477, Adjusted R-squared:  0.7383 \n## F-statistic: 79.67 on 9 and 242 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no biceps)\nmodel.red4 &lt;- lm(brozek ~ age + weight + neck + abdom + hip + thigh  + forearm + wrist, data = fat)\nprint(summary(model.red4))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + hip + thigh + \n##     forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0574  -2.7411  -0.1912   2.6929   9.4977 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -20.06213   10.84654  -1.850  0.06558 .  \n## age           0.05922    0.02850   2.078  0.03876 *  \n## weight       -0.08414    0.03695  -2.277  0.02366 *  \n## neck         -0.43189    0.20799  -2.077  0.03889 *  \n## abdom         0.87721    0.06661  13.170  &lt; 2e-16 ***\n## hip          -0.18641    0.12821  -1.454  0.14727    \n## thigh         0.28644    0.11949   2.397  0.01727 *  \n## forearm       0.48255    0.17251   2.797  0.00557 ** \n## wrist        -1.40487    0.47167  -2.978  0.00319 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.965 on 243 degrees of freedom\n## Multiple R-squared:  0.7467, Adjusted R-squared:  0.7383 \n## F-statistic: 89.53 on 8 and 243 DF,  p-value: &lt; 2.2e-16\n\n# re-fit the model (no hip)\nmodel.red5 &lt;- lm(brozek ~ age + weight + neck + abdom  + thigh  + forearm + wrist, data = fat)\nprint(summary(model.red5))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + \n##     wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0193  -2.8016  -0.1234   2.9387   9.0019 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -30.17420    8.34200  -3.617 0.000362 ***\n## age           0.06149    0.02852   2.156 0.032047 *  \n## weight       -0.11236    0.03151  -3.565 0.000437 ***\n## neck         -0.37203    0.20434  -1.821 0.069876 .  \n## abdom         0.85152    0.06437  13.229  &lt; 2e-16 ***\n## thigh         0.20973    0.10745   1.952 0.052099 .  \n## forearm       0.51824    0.17115   3.028 0.002726 ** \n## wrist        -1.40081    0.47274  -2.963 0.003346 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.974 on 244 degrees of freedom\n## Multiple R-squared:  0.7445, Adjusted R-squared:  0.7371 \n## F-statistic: 101.6 on 7 and 244 DF,  p-value: &lt; 2.2e-16\n\n# compare model.clean and final model\nprint(summary(model.clean))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + height + neck + abdom + \n##     hip + thigh + knee + ankle + biceps + forearm + wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.2664  -2.5658  -0.0798   2.8976   9.3204 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -17.063433  14.489336  -1.178  0.24011    \n## age           0.056520   0.029888   1.891  0.05983 .  \n## weight       -0.085513   0.045170  -1.893  0.05954 .  \n## height       -0.059703   0.086695  -0.689  0.49171    \n## neck         -0.439315   0.214802  -2.045  0.04193 *  \n## abdom         0.875779   0.070589  12.407  &lt; 2e-16 ***\n## hip          -0.192118   0.132655  -1.448  0.14885    \n## thigh         0.237304   0.131793   1.801  0.07303 .  \n## knee         -0.006595   0.222832  -0.030  0.97642    \n## ankle         0.164831   0.204681   0.805  0.42144    \n## biceps        0.149530   0.157693   0.948  0.34397    \n## forearm       0.424885   0.182801   2.324  0.02095 *  \n## wrist        -1.474317   0.494475  -2.982  0.00316 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.98 on 239 degrees of freedom\n## Multiple R-squared:  0.7489, Adjusted R-squared:  0.7363 \n## F-statistic:  59.4 on 12 and 239 DF,  p-value: &lt; 2.2e-16\nprint(summary(model.red5))\n## \n## Call:\n## lm(formula = brozek ~ age + weight + neck + abdom + thigh + forearm + \n##     wrist, data = fat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.0193  -2.8016  -0.1234   2.9387   9.0019 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -30.17420    8.34200  -3.617 0.000362 ***\n## age           0.06149    0.02852   2.156 0.032047 *  \n## weight       -0.11236    0.03151  -3.565 0.000437 ***\n## neck         -0.37203    0.20434  -1.821 0.069876 .  \n## abdom         0.85152    0.06437  13.229  &lt; 2e-16 ***\n## thigh         0.20973    0.10745   1.952 0.052099 .  \n## forearm       0.51824    0.17115   3.028 0.002726 ** \n## wrist        -1.40081    0.47274  -2.963 0.003346 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.974 on 244 degrees of freedom\n## Multiple R-squared:  0.7445, Adjusted R-squared:  0.7371 \n## F-statistic: 101.6 on 7 and 244 DF,  p-value: &lt; 2.2e-16\n\nNote: we have just run a very simple feature selection using stepwise regression. In this method, using backward elimination, we build a model containing all the variables and remove them one by one based on defined criteria (here we have used p-values) and we stop when we have a justifiable model or when removing a predictor does not change the chosen criterion significantly.""
   },
   {
-    ""objectID"": ""lm-lasso-exercises.html"",
-    ""href"": ""lm-lasso-exercises.html"",
+    ""objectID"": ""lm-lasso-exercises.html#load-data-reformat-data"",
+    ""href"": ""lm-lasso-exercises.html#load-data-reformat-data"",
     ""title"": ""Exercises (regularization)"",
-    ""section"": """",
-    ""text"": ""Data for exercises are on Canvas under Files -&gt; data_exercises –&gt; linear-models\n\nExercise 1 Follow and try to run the code below for fitting Lasso model to predict BMI given all the numerical variables in the diabetes data set.\n\nLoad data & reformat data\n\n# load libraries\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(caret)\n\nLoading required package: lattice\n\n\nWarning: package 'lattice' was built under R version 4.3.2\n\n\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(splitTools)\n\n# import raw data\ninput_diabetes &lt;- read_csv(\""data/data-diabetes.csv\"")\n\nRows: 403 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"",\""\nchr  (3): location, gender, frame\ndbl (16): id, chol, stab.glu, hdl, ratio, glyhb, age, height, weight, bp.1s,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# preview data\nglimpse(input_diabetes)\n\nRows: 403\nColumns: 19\n$ id       &lt;dbl&gt; 1000, 1001, 1002, 1003, 1005, 1008, 1011, 1015, 1016, 1022, 1…\n$ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 227, 177, 263, 242, 215, 23…\n$ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 75, 87, 89, 82, 128, 75, 79, 76, …\n$ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 4…\n$ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 5.2, 3.6, 6.6, 4.5, 6.3, 6…\n$ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 3.94, 4.84, 5.78, 4…\n$ location &lt;chr&gt; \""Buckingham\"", \""Buckingham\"", \""Buckingham\"", \""Buckingham\"", \""Buck…\n$ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 3…\n$ gender   &lt;chr&gt; \""female\"", \""female\"", \""female\"", \""male\"", \""male\"", \""male\"", \""male\"",…\n$ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 59, 69, 63, 65, 58, 60, 59, 69, 6…\n$ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 170, 166, 202, 156, 195, 1…\n$ frame    &lt;chr&gt; \""medium\"", \""large\"", \""large\"", \""large\"", \""medium\"", \""large\"", \""medi…\n$ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, NA, 160, 108, 130, 102, 13…\n$ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, NA, 80, 72, 90, 68, 80, NA, 66, …\n$ bp.2s    &lt;dbl&gt; NA, NA, 185, NA, NA, NA, 161, NA, 128, NA, 130, NA, NA, NA, N…\n$ bp.2d    &lt;dbl&gt; NA, NA, 92, NA, NA, NA, 112, NA, 86, NA, 90, NA, NA, NA, NA, …\n$ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 34, 45, 39, 42, 35, 37, 36, 3…\n$ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 39, 40, 50, 45, 50, 41, 43, 40, 4…\n$ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 1020, 300, 240, 300, 90, 7…\n\n# run basic feature engieering of the data\n# exclude bp.2s, pb.2d due to large number of missing data \n# create BMI based on weight and height\n# keep only numerical variables\n# keep samples for which none missing data is present (complete case analysis)\n\n# define numerical predictors\npred_numeric &lt;- c(\""chol\"", \""stab.glu\"", \""hdl\"", \""ratio\"", \""glyhb\"", \""age\"", \""height\"", \""weight\"", \""bp.1s\"", \""bp.1d\"", \""waist\"", \""hip\"", \""time.ppn\"")\n\n# and only numerical predictors\nconv_factor &lt;- 703 # conversion factor to calculate BMI from inches and pounds BMI = weight (lb) / [height (in)]2 x 703\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(BMI = weight / height^2 * 703, BMI = round(BMI, 2)) %&gt;%\n  relocate(BMI, .after = id) %&gt;%\n  dplyr::select(-bp.2s, -bp.2d) %&gt;%\n  select(all_of(c(\""BMI\"", pred_numeric))) %&gt;%\n  na.omit()\n  \n# preview reformatted data\nglimpse(data_diabetes)\n\nRows: 375\nColumns: 14\n$ BMI      &lt;dbl&gt; 22.13, 37.42, 48.37, 18.64, 27.82, 26.50, 28.20, 24.51, 35.78…\n$ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 177, 263, 242, 215, 238, 19…\n$ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 87, 89, 82, 128, 75, 76, 83, 78, …\n$ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 49, 40, 54, 34, 36, 30, 47, 38, 6…\n$ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 3.6, 6.6, 4.5, 6.3, 6.6, 6…\n$ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 4.84, 5.78, 4.77, 4…\n$ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 45, 55, 60, 38, 27, 36, 33, 50, 2…\n$ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 69, 63, 65, 58, 60, 69, 65, 65, 6…\n$ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 166, 202, 156, 195, 170, 1…\n$ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, 160, 108, 130, 102, 130, 1…\n$ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, 80, 72, 90, 68, 80, 66, 90, 100,…\n$ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 45, 39, 42, 35, 36, 37, 37, 3…\n$ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 40, 50, 45, 50, 41, 40, 41, 43, 3…\n$ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 300, 240, 300, 90, 720, 22…\n\n\n\nsplit data and scale\n\n\nCode\n# split data into train (70%) and test (30%)\nset.seed(123)\ninds &lt;- partition(data_diabetes$BMI, p = c(train = 0.7, test = 0.3))\n\ndata_train &lt;- data_diabetes %&gt;%\n  slice(inds$train)\n\ndata_test &lt;- data_diabetes %&gt;%\n  slice(inds$test)\n\n# standardize variables\n# Note: the correct approach is to scale the non-test data (train) \n# and then scale the test data using the parameters of the train data (e.g. mean or sd)\n# Here we will use preProcess() function from caret package to do that\n# We do not scale response (BMI)\n\nscaling &lt;- preProcess(data_train[, pred_numeric], method = c(\""center\"", \""scale\""))\ndata_train[, pred_numeric] &lt;- predict(scaling, data_train[, pred_numeric])\ndata_test[, pred_numeric] &lt;- predict(scaling, data_test[, pred_numeric])\n\n\nFitting Lasso\n\n# define x (predictors) and y (response)\nx &lt;- model.matrix(BMI ~.-1, data = data_train)\ny &lt;- data_train$BMI\n\n# we use glmnet function to fit Lasso, alpha = 1 for Lasso, alpha = 0 for Ridge\nfit &lt;- glmnet(x, y, alpha=1) \n\n# by default this returns many Lasso models, fitted across a range of lambda values\n# and we can plot a whole path of models, to see how coefficients change as a function of lambda\nplot(fit, xvar=\""lambda\"", label=TRUE)\n\n\n\n\nTrain Lasso model\n\n# To train Lasso model we can use k-fold cross validation\n# We can use cv.glmnet() function for that\n# Alternatively, we could try writing our code from scratch doing more data splits...\ncv &lt;- cv.glmnet(x, y, alpha = 1)\n\n# plot MSE as a function of the lambda\nplot(cv)\n\n\n\n\nLasso: final model\n\n# value of lambda for which MSE is smallest\ncv$lambda.min\n\n[1] 0.03005165\n\n# fit the final model with the selected lambda value\nfit_final &lt;- glmnet(x, y, alpha = 1, lambda =cv$lambda.min) \n\n# check model coefficients. \"".\"" indicates 0\ncoef(fit_final)\n\n14 x 1 sparse Matrix of class \""dgCMatrix\""\n                     s0\n(Intercept) 28.82120155\nchol        -0.01914479\nstab.glu    -0.03099725\nhdl          .         \nratio        .         \nglyhb        .         \nage          .         \nheight      -3.18952689\nweight       6.27956187\nbp.1s        0.02225091\nbp.1d        .         \nwaist        .         \nhip          0.47057628\ntime.ppn    -0.00453767\n\n\nWe can see that the largest coefficients (in terms of absolute values) are for weight and height, which of course it makes sense as we are predicting BMI that was defined based on weight and height in the first place. This is naturally for demonstration purposes only.\nLasso predictions\n\n# Let's see how the model performs on new unseen test data\nx_test &lt;- model.matrix(BMI ~.-1, data_test)\ny_pred &lt;- fit_final %&gt;% predict(x_test) %&gt;% as.vector()\n\nplot(data_test$BMI, y_pred, xlab = \""BMI\"", ylab = \""predicted BMI\"")\n\n\n\ncor(data_test$BMI, y_pred)\n\n[1] 0.9945698""
-  },
-  {
-    ""objectID"": ""lm-coeff-exercises.html#answers-to-selected-exercises"",
-    ""href"": ""lm-coeff-exercises.html#answers-to-selected-exercises"",
-    ""title"": ""Exercises (regression coefficients)"",
-    ""section"": ""Answers to selected exercises"",
-    ""text"": ""Answers to selected exercises\n\nSolution. Exercise 1\n\n\n\n\n\nhtwtgen &lt;- read.csv(\""data/lm/heights_weights_genders.csv\"")\nhead(htwtgen)\n\n  Gender   Height   Weight\n1   Male 73.84702 241.8936\n2   Male 68.78190 162.3105\n3   Male 74.11011 212.7409\n4   Male 71.73098 220.0425\n5   Male 69.88180 206.3498\n6   Male 67.25302 152.2122\n\n# a)\nmodel1 &lt;- lm(Height ~ Gender, data = htwtgen)\nmodel2 &lt;- lm(Height ~ Gender + Weight, data = htwtgen)\nmodel3 &lt;- lm(Height ~ Gender * Weight, data = htwtgen)\n\n# print(summary(model1))\n# print(summary(model2))\n# print(summary(model3))\n\n\nuse equations to find the height for men and women respectively: \\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\] \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\]\n\n\n\n# for men\nnew.obs &lt;- data.frame(Weight=120, Gender=\""Male\"")\npredict(model3, newdata = new.obs)\n\n       1 \n60.65427 \n\n# for female\nnew.obs &lt;- data.frame(Weight=120, Gender=\""Female\"")\npredict(model3, newdata = new.obs)\n\n       1 \n61.79882 \n\n\n\nSolution. Exercise 2\n\n\n# read in data and show preview\ntrout &lt;- read.csv(\""data/lm/trout.csv\"")\n\n# recode the Group variable and treat like categories (factor)\ntrout$Group &lt;- factor(trout$Group, labels=c(\""Dominant\"", \""Subordinate\""))\nhead(trout)\n##   Energy Ration    Group\n## 1  44.26  81.35 Dominant\n## 2  67.16  91.68 Dominant\n## 3  48.15  58.00 Dominant\n## 4  34.53  58.63 Dominant\n## 5  67.93  91.93 Dominant\n## 6  72.45  96.56 Dominant\n\n# plot data\n# boxplots of Energy and Ration per group\nboxplot(trout$Energy ~ trout$Group, xlab=\""\"", ylab=\""Energy\"")\n\n\n\nboxplot(trout$Ration ~ trout$Group, xlab=\""\"", ylab=\""Ration\"")\n\n\n\n\n# scatter plot of Ration vs. Energy\nplot(trout$Ration, trout$Energy, pch=19, xlab=\""Ration\"", ylab=\""Energy\"")\n\n\n\n\n\nFrom the exploratory plots we see that there is some sort of relationship between ratio and energy, i.e. energy increase while ration obtained increases\nFrom box plots we see that the ration obtained may be different in two groups\n\n\n# Is there a relationship between ration obtained and energy expenditure\nmodel1 &lt;- lm(Energy ~ Ration, data = trout)\nprint(summary(model1))\n## \n## Call:\n## lm(formula = Energy ~ Ration, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.704  -4.703  -0.578   2.432  33.506 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   4.3037    12.5156   0.344 0.734930    \n## Ration        0.7211     0.1716   4.203 0.000535 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.05 on 18 degrees of freedom\n## Multiple R-squared:  0.4953, Adjusted R-squared:  0.4673 \n## F-statistic: 17.66 on 1 and 18 DF,  p-value: 0.0005348\n# from the regression output we can see that yes, a unit increase in ratio increase energy expenditure by 0.72\n\n# Is there a relationship between ration obtained and energy expenditure different for each type of fish?\n# we first check if there is a group effect\nmodel2 &lt;- lm(Energy ~ Ration + Group, data = trout)\nprint(summary(model2))\n## \n## Call:\n## lm(formula = Energy ~ Ration + Group, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.130  -5.139  -0.870   2.199  25.622 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      -24.8506    13.3031  -1.868  0.07910 .  \n## Ration             1.0109     0.1626   6.218 9.36e-06 ***\n## GroupSubordinate  17.0120     5.1075   3.331  0.00396 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.647 on 17 degrees of freedom\n## Multiple R-squared:  0.6946, Adjusted R-squared:  0.6587 \n## F-statistic: 19.33 on 2 and 17 DF,  p-value: 4.182e-05\nggPredict(model2) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\n# and whether there is an interaction effect\nmodel3 &lt;- lm(Energy ~ Ration * Group, data = trout)\nprint(summary(model3))\n## \n## Call:\n## lm(formula = Energy ~ Ration * Group, data = trout)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.7951  -6.0981  -0.1554   3.9612  23.5946 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              -9.2330    15.9394  -0.579 0.570483    \n## Ration                    0.8149     0.1968   4.141 0.000767 ***\n## GroupSubordinate        -18.9558    22.6934  -0.835 0.415848    \n## Ration:GroupSubordinate   0.5200     0.3204   1.623 0.124148    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.214 on 16 degrees of freedom\n## Multiple R-squared:  0.7378, Adjusted R-squared:  0.6886 \n## F-statistic:    15 on 3 and 16 DF,  p-value: 6.537e-05\nggPredict(model3) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\nBased on the regression output and plots we can say:\n\nthere is a relationship between ration obtained and energy expenditure\nthat this relationship is the same in the two groups although the energy expenditure is higher in the dominant fish\n\n\nSolution. Exercise 3\n\n\n\n\nYes. The redn and initial were significantly associated (p-value = 0.00312, linear regression).\n\nmodel1 &lt;- lm(redn ~ initial, data = blooddrug)\nsummary(model1)\n## \n## Call:\n## lm(formula = redn ~ initial, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -23.476 -11.705   1.558   9.197  24.392 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) -72.7302    29.1879  -2.492  0.02036 * \n## initial       0.5902     0.1788   3.301  0.00312 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.79 on 23 degrees of freedom\n## Multiple R-squared:  0.3214, Adjusted R-squared:  0.2919 \n## F-statistic: 10.89 on 1 and 23 DF,  p-value: 0.003125\n\n\n\n\nNo. The drug2 and drug3 were not significantly different from drug1 (p-value = 0.714 and p-value = 0.628, respectively). The patients of the drug 1 group had 2.750 higher blood pressure drop (redn) than those of the drug 2 group. However, the difference was relatively small comparing to the standard error of the estimate, which was 7.402. The difference between drug 1 and 3 was relatively small, too.\n\nmodel2 &lt;- lm(redn ~ drug, data = blooddrug)\nsummary(model2)\n## \n## Call:\n## lm(formula = redn ~ drug, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -32.000  -9.286   0.000  12.714  26.000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   23.250      5.517   4.214 0.000358 ***\n## drug2          2.750      7.402   0.372 0.713796    \n## drug3         -3.964      8.076  -0.491 0.628379    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 15.6 on 22 degrees of freedom\n## Multiple R-squared:  0.03349,    Adjusted R-squared:  -0.05437 \n## F-statistic: 0.3812 on 2 and 22 DF,  p-value: 0.6875\n\n\n\n\nYes. The redn of the drug2 group was significantly higher than that of the drug1 group after adjustment for the effects of the initial (P = 0.018). The reduction of the patients who got the drug 2 was much higher (13.6906) than the drug 1, comparing to the standard error of the difference (5.3534) after accounting for initial blood pressure.\n\nmodel3 &lt;- lm(redn ~ drug + initial, data = blooddrug)\nsummary(model3)\n## \n## Call:\n## lm(formula = redn ~ drug + initial, data = blooddrug)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.8114 -10.5842  -0.4959   6.2834  16.4265 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -124.8488    28.0674  -4.448 0.000223 ***\n## drug2         13.6906     5.3534   2.557 0.018346 *  \n## drug3         -7.2045     5.4275  -1.327 0.198625    \n## initial        0.8895     0.1671   5.323 2.81e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.42 on 21 degrees of freedom\n## Multiple R-squared:  0.5886, Adjusted R-squared:  0.5298 \n## F-statistic: 10.01 on 3 and 21 DF,  p-value: 0.0002666""
+    ""section"": ""Load data & reformat data"",
+    ""text"": ""Load data & reformat data\n\n# load libraries\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(splitTools)\n\n# import raw data\ninput_diabetes &lt;- read_csv(\""data/data-diabetes.csv\"")\n\n# preview data\nglimpse(input_diabetes)\n\nRows: 403\nColumns: 19\n$ id       &lt;dbl&gt; 1000, 1001, 1002, 1003, 1005, 1008, 1011, 1015, 1016, 1022, 1…\n$ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 227, 177, 263, 242, 215, 23…\n$ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 75, 87, 89, 82, 128, 75, 79, 76, …\n$ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 4…\n$ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 5.2, 3.6, 6.6, 4.5, 6.3, 6…\n$ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 3.94, 4.84, 5.78, 4…\n$ location &lt;chr&gt; \""Buckingham\"", \""Buckingham\"", \""Buckingham\"", \""Buckingham\"", \""Buck…\n$ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 3…\n$ gender   &lt;chr&gt; \""female\"", \""female\"", \""female\"", \""male\"", \""male\"", \""male\"", \""male\"",…\n$ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 59, 69, 63, 65, 58, 60, 59, 69, 6…\n$ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 170, 166, 202, 156, 195, 1…\n$ frame    &lt;chr&gt; \""medium\"", \""large\"", \""large\"", \""large\"", \""medium\"", \""large\"", \""medi…\n$ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, NA, 160, 108, 130, 102, 13…\n$ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, NA, 80, 72, 90, 68, 80, NA, 66, …\n$ bp.2s    &lt;dbl&gt; NA, NA, 185, NA, NA, NA, 161, NA, 128, NA, 130, NA, NA, NA, N…\n$ bp.2d    &lt;dbl&gt; NA, NA, 92, NA, NA, NA, 112, NA, 86, NA, 90, NA, NA, NA, NA, …\n$ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 34, 45, 39, 42, 35, 37, 36, 3…\n$ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 39, 40, 50, 45, 50, 41, 43, 40, 4…\n$ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 1020, 300, 240, 300, 90, 7…\n\n# run basic feature engieering of the data\n# exclude bp.2s, pb.2d due to large number of missing data \n# create BMI based on weight and height\n# keep only numerical variables\n# keep samples for which none missing data is present (complete case analysis)\n\n# define numerical predictors\npred_numeric &lt;- c(\""chol\"", \""stab.glu\"", \""hdl\"", \""ratio\"", \""glyhb\"", \""age\"", \""height\"", \""weight\"", \""bp.1s\"", \""bp.1d\"", \""waist\"", \""hip\"", \""time.ppn\"")\n\n# and only numerical predictors\nconv_factor &lt;- 703 # conversion factor to calculate BMI from inches and pounds BMI = weight (lb) / [height (in)]2 x 703\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(BMI = weight / height^2 * 703, BMI = round(BMI, 2)) %&gt;%\n  relocate(BMI, .after = id) %&gt;%\n  dplyr::select(-bp.2s, -bp.2d) %&gt;%\n  select(all_of(c(\""BMI\"", pred_numeric))) %&gt;%\n  na.omit()\n  \n# preview reformatted data\nglimpse(data_diabetes)\n\nRows: 375\nColumns: 14\n$ BMI      &lt;dbl&gt; 22.13, 37.42, 48.37, 18.64, 27.82, 26.50, 28.20, 24.51, 35.78…\n$ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 177, 263, 242, 215, 238, 19…\n$ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 87, 89, 82, 128, 75, 76, 83, 78, …\n$ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 49, 40, 54, 34, 36, 30, 47, 38, 6…\n$ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 3.6, 6.6, 4.5, 6.3, 6.6, 6…\n$ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 4.84, 5.78, 4.77, 4…\n$ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 45, 55, 60, 38, 27, 36, 33, 50, 2…\n$ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 69, 63, 65, 58, 60, 69, 65, 65, 6…\n$ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 166, 202, 156, 195, 170, 1…\n$ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, 160, 108, 130, 102, 130, 1…\n$ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, 80, 72, 90, 68, 80, 66, 90, 100,…\n$ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 45, 39, 42, 35, 36, 37, 37, 3…\n$ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 40, 50, 45, 50, 41, 40, 41, 43, 3…\n$ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 300, 240, 300, 90, 720, 22…""
   },
   {
     ""objectID"": ""lm-lasso-exercises.html#split-data-and-scale"",
@@ -273,10 +266,10 @@
     ""text"": ""predict BMI\n\n# Let's see how the model performs on new unseen test data\nx_test &lt;- model.matrix(BMI ~.-1, data_test)\ny_pred &lt;- fit_final %&gt;% predict(x_test) %&gt;% as.vector()\n\nplot(data_test$BMI, y_pred, xlab = \""BMI\"", ylab = \""predicted BMI\"")\n\n\n\ncor(data_test$BMI, y_pred)\n\n[1] 0.9945698""
   },
   {
-    ""objectID"": ""lm-lasso-exercises.html#load-data-reformat-data"",
-    ""href"": ""lm-lasso-exercises.html#load-data-reformat-data"",
-    ""title"": ""Exercises (regularization)"",
-    ""section"": ""Load data & reformat data"",
-    ""text"": ""Load data & reformat data\n\n# load libraries\nlibrary(tidyverse)\nlibrary(glmnet)\nlibrary(caret)\nlibrary(splitTools)\n\n# import raw data\ninput_diabetes &lt;- read_csv(\""data/data-diabetes.csv\"")\n\n# preview data\nglimpse(input_diabetes)\n\nRows: 403\nColumns: 19\n$ id       &lt;dbl&gt; 1000, 1001, 1002, 1003, 1005, 1008, 1011, 1015, 1016, 1022, 1…\n$ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 227, 177, 263, 242, 215, 23…\n$ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 75, 87, 89, 82, 128, 75, 79, 76, …\n$ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 4…\n$ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 5.2, 3.6, 6.6, 4.5, 6.3, 6…\n$ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 3.94, 4.84, 5.78, 4…\n$ location &lt;chr&gt; \""Buckingham\"", \""Buckingham\"", \""Buckingham\"", \""Buckingham\"", \""Buck…\n$ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 3…\n$ gender   &lt;chr&gt; \""female\"", \""female\"", \""female\"", \""male\"", \""male\"", \""male\"", \""male\"",…\n$ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 59, 69, 63, 65, 58, 60, 59, 69, 6…\n$ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 170, 166, 202, 156, 195, 1…\n$ frame    &lt;chr&gt; \""medium\"", \""large\"", \""large\"", \""large\"", \""medium\"", \""large\"", \""medi…\n$ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, NA, 160, 108, 130, 102, 13…\n$ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, NA, 80, 72, 90, 68, 80, NA, 66, …\n$ bp.2s    &lt;dbl&gt; NA, NA, 185, NA, NA, NA, 161, NA, 128, NA, 130, NA, NA, NA, N…\n$ bp.2d    &lt;dbl&gt; NA, NA, 92, NA, NA, NA, 112, NA, 86, NA, 90, NA, NA, NA, NA, …\n$ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 34, 45, 39, 42, 35, 37, 36, 3…\n$ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 39, 40, 50, 45, 50, 41, 43, 40, 4…\n$ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 1020, 300, 240, 300, 90, 7…\n\n# run basic feature engieering of the data\n# exclude bp.2s, pb.2d due to large number of missing data \n# create BMI based on weight and height\n# keep only numerical variables\n# keep samples for which none missing data is present (complete case analysis)\n\n# define numerical predictors\npred_numeric &lt;- c(\""chol\"", \""stab.glu\"", \""hdl\"", \""ratio\"", \""glyhb\"", \""age\"", \""height\"", \""weight\"", \""bp.1s\"", \""bp.1d\"", \""waist\"", \""hip\"", \""time.ppn\"")\n\n# and only numerical predictors\nconv_factor &lt;- 703 # conversion factor to calculate BMI from inches and pounds BMI = weight (lb) / [height (in)]2 x 703\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(BMI = weight / height^2 * 703, BMI = round(BMI, 2)) %&gt;%\n  relocate(BMI, .after = id) %&gt;%\n  dplyr::select(-bp.2s, -bp.2d) %&gt;%\n  select(all_of(c(\""BMI\"", pred_numeric))) %&gt;%\n  na.omit()\n  \n# preview reformatted data\nglimpse(data_diabetes)\n\nRows: 375\nColumns: 14\n$ BMI      &lt;dbl&gt; 22.13, 37.42, 48.37, 18.64, 27.82, 26.50, 28.20, 24.51, 35.78…\n$ chol     &lt;dbl&gt; 203, 165, 228, 78, 249, 248, 195, 177, 263, 242, 215, 238, 19…\n$ stab.glu &lt;dbl&gt; 82, 97, 92, 93, 90, 94, 92, 87, 89, 82, 128, 75, 76, 83, 78, …\n$ hdl      &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 49, 40, 54, 34, 36, 30, 47, 38, 6…\n$ ratio    &lt;dbl&gt; 3.6, 6.9, 6.2, 6.5, 8.9, 3.6, 4.8, 3.6, 6.6, 4.5, 6.3, 6.6, 6…\n$ glyhb    &lt;dbl&gt; 4.31, 4.44, 4.64, 4.63, 7.72, 4.81, 4.84, 4.84, 5.78, 4.77, 4…\n$ age      &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 45, 55, 60, 38, 27, 36, 33, 50, 2…\n$ height   &lt;dbl&gt; 62, 64, 61, 67, 68, 71, 69, 69, 63, 65, 58, 60, 69, 65, 65, 6…\n$ weight   &lt;dbl&gt; 121, 218, 256, 119, 183, 190, 191, 166, 202, 156, 195, 170, 1…\n$ bp.1s    &lt;dbl&gt; 118, 112, 190, 110, 138, 132, 161, 160, 108, 130, 102, 130, 1…\n$ bp.1d    &lt;dbl&gt; 59, 68, 92, 50, 80, 86, 112, 80, 72, 90, 68, 80, 66, 90, 100,…\n$ waist    &lt;dbl&gt; 29, 46, 49, 33, 44, 36, 46, 34, 45, 39, 42, 35, 36, 37, 37, 3…\n$ hip      &lt;dbl&gt; 38, 48, 57, 38, 41, 42, 49, 40, 50, 45, 50, 41, 40, 41, 43, 3…\n$ time.ppn &lt;dbl&gt; 720, 360, 180, 480, 300, 195, 720, 300, 240, 300, 90, 720, 22…""
+    ""objectID"": ""lm-coeff-exercises.html#answers-to-selected-exercises"",
+    ""href"": ""lm-coeff-exercises.html#answers-to-selected-exercises"",
+    ""title"": ""Exercises (regression coefficients)"",
+    ""section"": ""Answers to selected exercises"",
+    ""text"": ""Answers to selected exercises\n\nSolution. Exercise 1\n\n\n\n\n\nhtwtgen &lt;- read.csv(\""data/lm/heights_weights_genders.csv\"")\nhead(htwtgen)\n\n  Gender   Height   Weight\n1   Male 73.84702 241.8936\n2   Male 68.78190 162.3105\n3   Male 74.11011 212.7409\n4   Male 71.73098 220.0425\n5   Male 69.88180 206.3498\n6   Male 67.25302 152.2122\n\n# a)\nmodel1 &lt;- lm(Height ~ Gender, data = htwtgen)\nmodel2 &lt;- lm(Height ~ Gender + Weight, data = htwtgen)\nmodel3 &lt;- lm(Height ~ Gender * Weight, data = htwtgen)\n\n# print(summary(model1))\n# print(summary(model2))\n# print(summary(model3))\n\n\nuse equations to find the height for men and women respectively: \\[E(height|male\\; and \\; weight=x)=47.34778 - 1.68367 + 0.12043x + 0.00449x = 45.7 + 0.125x\\] \\[E(height|female\\; and \\; weight=x)=47.34778 + 0.12043x\\]\n\n\n\n# for men\nnew.obs &lt;- data.frame(Weight=120, Gender=\""Male\"")\npredict(model3, newdata = new.obs)\n\n       1 \n60.65427 \n\n# for female\nnew.obs &lt;- data.frame(Weight=120, Gender=\""Female\"")\npredict(model3, newdata = new.obs)\n\n       1 \n61.79882 \n\n\n\nSolution. Exercise 2\n\n\n# read in data and show preview\ntrout &lt;- read.csv(\""data/lm/trout.csv\"")\n\n# recode the Group variable and treat like categories (factor)\ntrout$Group &lt;- factor(trout$Group, labels=c(\""Dominant\"", \""Subordinate\""))\nhead(trout)\n##   Energy Ration    Group\n## 1  44.26  81.35 Dominant\n## 2  67.16  91.68 Dominant\n## 3  48.15  58.00 Dominant\n## 4  34.53  58.63 Dominant\n## 5  67.93  91.93 Dominant\n## 6  72.45  96.56 Dominant\n\n# plot data\n# boxplots of Energy and Ration per group\nboxplot(trout$Energy ~ trout$Group, xlab=\""\"", ylab=\""Energy\"")\n\n\n\nboxplot(trout$Ration ~ trout$Group, xlab=\""\"", ylab=\""Ration\"")\n\n\n\n\n# scatter plot of Ration vs. Energy\nplot(trout$Ration, trout$Energy, pch=19, xlab=\""Ration\"", ylab=\""Energy\"")\n\n\n\n\n\nFrom the exploratory plots we see that there is some sort of relationship between ratio and energy, i.e. energy increase while ration obtained increases\nFrom box plots we see that the ration obtained may be different in two groups\n\n\n# Is there a relationship between ration obtained and energy expenditure\nmodel1 &lt;- lm(Energy ~ Ration, data = trout)\nprint(summary(model1))\n## \n## Call:\n## lm(formula = Energy ~ Ration, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.704  -4.703  -0.578   2.432  33.506 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   4.3037    12.5156   0.344 0.734930    \n## Ration        0.7211     0.1716   4.203 0.000535 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.05 on 18 degrees of freedom\n## Multiple R-squared:  0.4953, Adjusted R-squared:  0.4673 \n## F-statistic: 17.66 on 1 and 18 DF,  p-value: 0.0005348\n# from the regression output we can see that yes, a unit increase in ratio increase energy expenditure by 0.72\n\n# Is there a relationship between ration obtained and energy expenditure different for each type of fish?\n# we first check if there is a group effect\nmodel2 &lt;- lm(Energy ~ Ration + Group, data = trout)\nprint(summary(model2))\n## \n## Call:\n## lm(formula = Energy ~ Ration + Group, data = trout)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -13.130  -5.139  -0.870   2.199  25.622 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      -24.8506    13.3031  -1.868  0.07910 .  \n## Ration             1.0109     0.1626   6.218 9.36e-06 ***\n## GroupSubordinate  17.0120     5.1075   3.331  0.00396 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.647 on 17 degrees of freedom\n## Multiple R-squared:  0.6946, Adjusted R-squared:  0.6587 \n## F-statistic: 19.33 on 2 and 17 DF,  p-value: 4.182e-05\nggPredict(model2) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\n# and whether there is an interaction effect\nmodel3 &lt;- lm(Energy ~ Ration * Group, data = trout)\nprint(summary(model3))\n## \n## Call:\n## lm(formula = Energy ~ Ration * Group, data = trout)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -12.7951  -6.0981  -0.1554   3.9612  23.5946 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              -9.2330    15.9394  -0.579 0.570483    \n## Ration                    0.8149     0.1968   4.141 0.000767 ***\n## GroupSubordinate        -18.9558    22.6934  -0.835 0.415848    \n## Ration:GroupSubordinate   0.5200     0.3204   1.623 0.124148    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.214 on 16 degrees of freedom\n## Multiple R-squared:  0.7378, Adjusted R-squared:  0.6886 \n## F-statistic:    15 on 3 and 16 DF,  p-value: 6.537e-05\nggPredict(model3) +\n  theme_light() +\n  guides(color=guide_legend(override.aes=list(fill=NA)))\n\n\n\n\nBased on the regression output and plots we can say:\n\nthere is a relationship between ration obtained and energy expenditure\nthat this relationship is the same in the two groups although the energy expenditure is higher in the dominant fish\n\n\nSolution. Exercise 3\n\n\n\n\nYes. The redn and initial were significantly associated (p-value = 0.00312, linear regression).\n\nmodel1 &lt;- lm(redn ~ initial, data = blooddrug)\nsummary(model1)\n## \n## Call:\n## lm(formula = redn ~ initial, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -23.476 -11.705   1.558   9.197  24.392 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept) -72.7302    29.1879  -2.492  0.02036 * \n## initial       0.5902     0.1788   3.301  0.00312 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 12.79 on 23 degrees of freedom\n## Multiple R-squared:  0.3214, Adjusted R-squared:  0.2919 \n## F-statistic: 10.89 on 1 and 23 DF,  p-value: 0.003125\n\n\n\n\nNo. The drug2 and drug3 were not significantly different from drug1 (p-value = 0.714 and p-value = 0.628, respectively). The patients of the drug 1 group had 2.750 higher blood pressure drop (redn) than those of the drug 2 group. However, the difference was relatively small comparing to the standard error of the estimate, which was 7.402. The difference between drug 1 and 3 was relatively small, too.\n\nmodel2 &lt;- lm(redn ~ drug, data = blooddrug)\nsummary(model2)\n## \n## Call:\n## lm(formula = redn ~ drug, data = blooddrug)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -32.000  -9.286   0.000  12.714  26.000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   23.250      5.517   4.214 0.000358 ***\n## drug2          2.750      7.402   0.372 0.713796    \n## drug3         -3.964      8.076  -0.491 0.628379    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 15.6 on 22 degrees of freedom\n## Multiple R-squared:  0.03349,    Adjusted R-squared:  -0.05437 \n## F-statistic: 0.3812 on 2 and 22 DF,  p-value: 0.6875\n\n\n\n\nYes. The redn of the drug2 group was significantly higher than that of the drug1 group after adjustment for the effects of the initial (P = 0.018). The reduction of the patients who got the drug 2 was much higher (13.6906) than the drug 1, comparing to the standard error of the difference (5.3534) after accounting for initial blood pressure.\n\nmodel3 &lt;- lm(redn ~ drug + initial, data = blooddrug)\nsummary(model3)\n## \n## Call:\n## lm(formula = redn ~ drug + initial, data = blooddrug)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.8114 -10.5842  -0.4959   6.2834  16.4265 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -124.8488    28.0674  -4.448 0.000223 ***\n## drug2         13.6906     5.3534   2.557 0.018346 *  \n## drug3         -7.2045     5.4275  -1.327 0.198625    \n## initial        0.8895     0.1671   5.323 2.81e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.42 on 21 degrees of freedom\n## Multiple R-squared:  0.5886, Adjusted R-squared:  0.5298 \n## F-statistic: 10.01 on 3 and 21 DF,  p-value: 0.0002666""
   }
 ]
\ No newline at end of file

---FILE: session-lm/lm-coeff-demo.qmd---
@@ -45,7 +45,8 @@ data_diabetes <- diabetes %>%
 
 ```
 
-<!-- Simple regression -->
+## Simple regression
+
 ```{r}
 #| label: simple regression examples
 #| eval: false
@@ -76,7 +77,7 @@ ggPredict(m1)
 
 ```
 
-<!-- Multiple regression -->
+## Multiple regression
 ```{r}
 #| label: multiple regression examples
 #| eval: false
@@ -86,8 +87,8 @@ ggPredict(m1)
 m2 <- lm(BMI ~ age + height + chol + stab.glu, data = data_diabetes)
 summary(m2)
 
-m2 <- lm(BMI ~ age, data = data_diabetes)
-summary(m2)
+#m2 <- lm(BMI ~ age, data = data_diabetes)
+#summary(m2)
 
 m2 <- lm(BMI ~ age + chol + hdl, data = data_diabetes)
 summary(m2)
@@ -96,7 +97,8 @@ m2 <- lm(BMI ~ age + waist + chol + hdl, data = data_diabetes)
 summary(m2)
 ```
 
-<!-- Category -->
+
+## Category
 ```{r}
 #| eval: false
 #| include: false
@@ -110,7 +112,7 @@ summary(m3)
 ggPredict(m3)
 ```
 
-<!-- Category + numerical -->
+## Category + numerical
 ```{r}
 #| eval: false
 #| include: false
@@ -125,7 +127,7 @@ ggPredict(m4)
 
 ```
 
-<!-- Category + numerical: interactions -->
+## Category + numerical: interactions
 ```{r}
 #| eval: false
 #| include: false
@@ -143,20 +145,17 @@ summary(m5)
 ggPredict(m5)
 ```
 
-<!-- Logistic regression -->
+## Logistic regression
 ```{r}
 #| eval: false
 #| include: false
 
 data_diabetes <- data_diabetes %>%
-  mutate(diabetic = ifelse(diabetic == ""Yes"", 1, 0))
+  mutate(obese = ifelse(obese== ""Yes"", 1, 0))
 
-m6 <- glm(diabetic ~ age, family = binomial(link=""logit""), data = data_diabetes)
+m6 <- glm(obese ~  hdl + gender, family = binomial(link=""logit""), data = data_diabetes)
 summary(m6)
 ggPredict(m6)
 
-m6 <- glm(diabetic ~ age + gender, family = binomial(link=""logit""), data = data_diabetes)
-summary(m6)
-ggPredict(m6)
 
 ```

---FILE: session-lm/lm-coeff.qmd---
@@ -185,7 +185,7 @@ summary(m2_alt)
 
 **Specific interpretation**
 
-- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.9 in BMI (alternative model). 
+- Obviously there is difference between decrease of 0.9 BMI and decrease of 0.06 in BMI (alternative model). 
 - Our interpretations need to be more specific and we say that **a unit increase in $x$ with other predictors held constant will produce a change equal to $\hat{\beta}$ in the response $y$**
 - Often it may be quite unrealistic to be able to control other variables and keep them constant and for our alternative model, a change in `hdl` would also imply a change in total cholesterol `chol`. 
 - Further, our explanation contains **no notation of causation**.
@@ -328,7 +328,7 @@ where
     \right.
 \end{equation}
 
-and $x_{2,i}$ is the `heigth` of person $i$.
+and $x_{2,i}$ is the `height` of person $i$.
 
 In `R` we write:
 ```{r}"
NBISweden,workshop-mlbiostatistics,32931796b18c9fd4d124d694ed2c22d95b7974de,olgadet,,2024-04-18T18:13:10Z,olgadet,,2024-04-18T18:13:10Z,"Update supervise session, rm regression",olga-notes.md;session-supervise-presentation/.gitignore;session-supervise-presentation/.quarto/_freeze/session-supervise-presentation/execute-results/html.json;session-supervise-presentation/.quarto/xref/c3b72353;session-supervise-presentation/images/supervised-02.png;session-supervise-presentation/session-supervise-presentation.html;session-supervise-presentation/session-supervise-presentation.qmd;session-supervise/KNN-demo.qmd;session-supervise/docs/KNN-demo.html;session-supervise/docs/exercises.html;session-supervise/docs/images/supervised.png;session-supervise/docs/index.html;session-supervise/docs/intro.html;session-supervise/docs/search.json;session-supervise/docs/site_libs/bootstrap/bootstrap.min.css;session-supervise/docs/site_libs/quarto-html/quarto.js;session-supervise/exercises.qmd;session-supervise/images/supervised.png;session-supervise/intro.qmd,True,False,True,False,662,425,1087,"---FILE: olga-notes.md---
@@ -13,8 +13,154 @@
 
 ## Descriptive statistics
 
-- check if code is available
-- add gtsummary() examples and to exercises 
+- (done) check if code is available
+- (done) add gtsummary() examples and to exercises 
+
+## Wednesday
+
+10:00 - 11:00 Introduction to supervised learning with KNN-classifier
+
+Objectives:
+
+- understand difference between supervised vs. unsupervised learning
+- being able to run KNN classifier with data splitting and outcome evaluation
+
+11:00 - 12:00 Introduction to linear models
+
+- what linear models are
+- simple linear regression demo
+- slope / intercept
+- hypothesis testing
+- vector-matrix notations
+
+12:00 - 13:00 lunch
+
+13:00 - 14:30 Linear models: regression and classification
+
+- checking model assumptions
+- assessing model fit
+- linear model selection & regularization
+- GLM: logistic regression, Poisson regression
+- Logistics Lasso?
+
+14:30 - 15:00 break
+15:00 - 16:00 Linear models exercises
+
+- common cases
+- putting into ML context (regression, regularized regression)
+
+## Thursday
+
+10:00 - 12:00 Linear models: common cases (go through together)
+12:00 - 13:00 lunch
+13:00 - 14:30 PCA
+14:30 - 15:00 break
+15:00 - 16:30 Clustering
+
+## Friday
+
+10:00 - 12:00 Putting everything together with tidymodels
+12:00 - 13:00 lunch
+13:00 - 14:30 Random Forest
+
+ChatGPT
+
+Creating a structured and engaging 30-minute presentation on an introduction to supervised learning can provide a solid foundation for understanding this key concept in machine learning. Here’s an outline that covers the basics, introduces core concepts, and includes examples to help illustrate the principles:
+
+### Presentation Outline: Introduction to Supervised Learning
+
+#### Slide 2: Overview of Machine Learning
+- Definition of machine learning
+- Brief description of the types of machine learning: Supervised, Unsupervised, and Reinforcement Learning
+
+#### Slide 3: What is Supervised Learning?
+- Definition of supervised learning
+- Key characteristics of supervised learning
+- Contrast with unsupervised learning
+
+#### Slide 4: How Supervised Learning Works
+- Overview of the process (Training Data -> Learning Algorithm -> Model)
+- Explanation of training data (features and labels)
+
+#### Slide 5: Types of Supervised Learning
+- Classification
+  - Definition and when to use it
+  - Example: Email spam detection
+- Regression
+  - Definition and when to use it
+  - Example: Housing price prediction
+
+#### Slide 6: Data Collection and Preparation
+- Importance of data in machine learning
+- Steps in data collection and cleaning
+- Example of a simple dataset
+
+#### Slide 7: Splitting Data
+- Explanation of training set and test set
+- Importance of data splitting
+- Cross-validation (brief mention)
+
+#### Slide 8: Choosing the Right Algorithm
+- Factors influencing the choice of algorithm
+- Brief overview of popular algorithms:
+  - Linear regression
+  - Logistic regression
+  - Decision trees
+
+#### Slide 9: Model Training
+- Explanation of how models are trained
+- Importance of feature selection
+- Overfitting vs. underfitting
+
+#### Slide 10: Model Evaluation
+- Metrics for performance evaluation
+  - Accuracy, precision, recall for classification
+  - MSE, RMSE for regression
+- Importance of model evaluation
+
+#### Slide 11: Improving Model Performance
+- Techniques for improving performance
+  - Feature engineering
+  - Hyperparameter tuning
+
+#### Slide 12: Practical Example: Building a Simple Classifier
+- Step-by-step example using a simple dataset (e.g., Iris dataset)
+- Tools and code snippets (e.g., Python with scikit-learn)
+
+#### Slide 13: Applications of Supervised Learning
+- Real-world applications in various industries
+  - Healthcare, finance, marketing, etc.
+
+#### Slide 14: Challenges in Supervised Learning
+- Data quality and quantity issues
+- Ethical considerations (bias, privacy)
+
+#### Slide 15: Future Trends and Conclusion
+- Brief discussion on the future of supervised learning
+- Recap of key points
+- Q&A
+
+#### Slide 16: Additional Resources
+- Books, courses, and websites for further learning
+- Contact information for follow-up questions
+
+### Tips for Presentation:
+- Use clear, concise language and avoid jargon when possible.
+- Include visual aids like diagrams and flowcharts to explain concepts.
+- Provide real-world examples to help the audience relate to the material.
+- Engage the audience with questions or quick activities related to the examples.
+
+This outline will help you craft a comprehensive introduction to supervised learning that is informative and engaging for your audience.
+
+
+
+
+- Linear: remove what linear models are and are not? Missing plus, and indices; change term to least-squares models?
+- Linear: checking assumptions plots from lm() method, maybe focus more on that, explain Cook’s distance; add more examples, correlated measurements.
+- Splitting presentation: checking assumptions, logistics regressions into two entries under Schedule 
+- Linear: bring the same challenge from Tuesday but add age now
+
+
 
 
 
@@ -24,10 +170,8 @@
 - Remove non-parametric session, move the correlation back to main on Tuesday main or Wednesday, add link to chapter.
 - Tuesday competition: simulated data, find DEGs, log it, t-test, confidence intervals and/or demonstration
 - Check quizzes for Tuesday after removing non-parametric session.
-- Linear: remove what linear models are and are not? Missing plus, and indices; change term to least-squares models?
-- Linear: checking assumptions plots from lm() method, maybe focus more on that, explain Cook’s distance; add more examples, correlated measurements.
-- Splitting presentation: checking assumptions, logistics regressions into two entries under Schedule 
-- Linear: bring the same challenge from Tuesday but add age now
+  
+
 - Feature engineering & selection: check tidymodels
 - Move intro to supervised to Wednesday morning, finish linear models with supervised example
 - Thursday afternoon: feature selection, expand Lasso, logistic-Lasso; Friday more practical example with tidymodels, maybe not with tidymodels.

---FILE: session-supervise-presentation/.gitignore---
@@ -0,0 +1 @@
+/.quarto/

---FILE: session-supervise-presentation/.quarto/_freeze/session-supervise-presentation/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""ffd1302b90686b5a6b14a8b8c0e14e0b"",
+  ""hash"": ""264c5bdcfb0eeac35e28d8e7a39c2f46"",
   ""result"": {
-    ""markdown"": ""---\ntitle: \""Supervised learning\""\n# author: Olga Dethlefsen\nformat: \n  revealjs:\n    slide-number: true\n    theme: [default, custom.scss]\n    view-distance: 10\n    chalkboard: \n      buttons: true\n  html:\n    code-fold: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n# We will discuss\n\n-   What supervised learning is.\n-   Data splitting.\n-   How to evaluate classification and regression ML models.\n-   How to put together a minimum working example of supervised learning with KNN classifier.\n\n## What is supervised learning\n\n*Definition*\n\n<br>\n\n\n```{mermaid}\nflowchart TD\n  A(Machine learning) --> B(unsupervised learning)\n  A --> C(supervised learning)\n```\n\n\n<br>\n\n::: incremental\n-   Supervised learning can be used for **classification** and **regression**.\n    -   For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)\n    -   For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)\n-   In supervised learning we are using sample **labels** to **train** (build) a model.\n-   We then use the trained model for **interpretation** and **prediction**.\n:::\n\n## What is supervised learning?\n\n*Definition* <br>\n\n::: incremental\n-   **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).\n-   Common supervised machine learning algorithms include:\n    -   K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN).\n-   Many of the ML methods can be implemented to work both for classifying samples and forecasting numeric outcome.\n:::\n\n## Supervised learning\n\n*Outline* <br>\n\nAcross many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:\n\n::: incremental\n-   deciding on the task: classification or regression\\\n-   **splitting data** to keep part of data for training and part for testing\n-   selecting supervised machine learning algorithms to be trained (or a set of these)\n-   deciding on the training strategy, i.e. which **performance metrics** to use and how to search for the best model parameters\n-   running **feature engineering**: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables\n-   performing **feature selection**: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present\n:::\n\n## Supervised learning {.scrollable}\n\n*Outline* <br>\n\nThe diagram below shows a basic strategy on how to train KNN for classification, given a data set with $n$ samples, $p$ variables and $y$ categorical outcome\n\n\n```{mermaid}\n\nflowchart TD\n  A([data]) -. split data \\n e.g. basic, stratified, grouped -.-> B([non-test set])\n  A([data]) -.-> C([test set])\n  B -.-> D(choose algorithm \\n e.g. KNN)\n  D -.-> E(choose evaluation metric \\n e.g. overall accuracy)\n  E -.-> F(feature engineering & selection)\n  F -.-> G(prepare parameter space, e.g. odd k-values from 3 to 30)\n  G -. split non-test -.-> H([train set & validation set])\n  H -.-> J(fit model on train set)\n  J -.-> K(collect evaluation metric on validation set)\n  K -.-> L{all values checked? \\n e.g. k more than 30}\n  L -. No .-> J\n  L -. Yes .-> M(select best parameter values)\n  M -.-> N(fit model on all non-test data)\n  N -.-> O(assess model on test data)\n  C -.-> O\n\n```\n\n\n## Supervised learning\n\n*Outline* <br>\n\nBefore we see how this training may look like in `R`, let's talk more about\n\n-   **KNN**, K-nearest neighbor algorithm\n-   **data splitting** and\n-   **performance metrics** useful for evaluating models\n\n## Classification\n\n*Definition* <br>\n\n::: incremental\n-   Classification methods are algorithms used to categorize (classify) objects based on their measurements.\n-   They belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the label (class) of.\n-   Let's for each observations $i$ collect pair of information $\\{\\mathbf{x_i}, g_i\\}$\n-   where $\\{\\mathbf{x_i}\\}$ is a set of exploratory variables e.g. a gene expression data\n-   and $g_i \\in \\{1, \\dots, G\\}$ is the class label for each observation (known), e.g. cancer stage I, II, III or IV\n-   Then we want to find a **classification rule** $f(.)$ (model) such that $$f(\\mathbf{x_i})=g_i$$\n:::\n\n## KNN\n\n*example of a classification algorithm*\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-00-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-01-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-02-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-03-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n\n*example of a classification algorithm*\n\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-10-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-20-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-30-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-40-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n\n*Algorithm* <br>\n\n::: incremental\n-   Decide on the value of $k$.\n-   Calculate the distance between the query-instance (observations for new sample) and all the training samples.\n-   Sort the distances and determine the nearest neighbors based on the $k$-th minimum distance.\n-   Gather the categories of the nearest neighbors.\n-   Use majority voting of the categories of the nearest neighbors as the prediction value for the new sample.\n-   Note: *Euclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.*\n:::\n\n## Data splitting\n\n*Why* <br>\n\n-   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\n-   As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.\n-   To deal with overconfident estimation of future performance we can implement various data splitting strategies.\n\n## Data splitting\n\n*train, validation & test sets*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   **Training data**: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule.\n-   **Validation data**: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters.\n-   **Test data**: this data is used to give an estimate of future prediction performance for the model and parameters chosen.\n-   Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\n\n## Data splitting\n\n*cross validation & repeated cross validation*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-kfolds-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   In **k-fold cross-validation** we split data into $k$ roughly equal-sized parts.\n-   We start by setting the validation data to be the first set of data and the training data to be all other sets.\n-   We estimate the validation error rate / correct classification rate for the split.\n-   We then repeat the process $k-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data.\n-   We finish with $k$ different error or correct classification rates.\n-   In this way, every data point has its class membership predicted once.\n-   The final reported error rate is usually the average of $k$ error rates.\n\n## Data splitting\n\n*Leave-one-out cross-validation* <br>\n\n-   Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n::: {.cell .fig-cap-location-margin layout-align=\""center\""}\n::: {.cell-output-display}\n![Example of LOOCV, leave-one-out cross validation](figures/data-split-loocv-02.png){#fig-data-split-loocv fig-align='center' width=100%}\n:::\n:::\n\n\n## Performance metrics\n\n*Evaluating classification* <br>\n\n-   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.\n-   There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\n-   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\n## Evaluating classification\n\n<br> <br> **Correct (miss)classification rate**\n\n-   The simplest way to evaluate in which we count for all the $n$ predictions how many times we got the classification right. $$Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where $1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\n**Missclassification Rate**\n\nMissclassification Rate = 1 - Correct Classification Rate\n\n## Evaluating classification {.smaller}\n\n<br>\n\n**Confusion matrix**\n\nConfusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes.\n\n|                     | Predicted Positive  | Predicted Negative  |\n|---------------------|---------------------|---------------------|\n| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n\n-   **Accuracy**: measures the proportion of correctly classified samples over the total number of samples. $$ACC = \\frac{TP+TN}{TP+TN+FP+FN}$$\n\n-   **Sensitivity**: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as **true positive rate** and **recall**. $$TPR = \\frac{TP}{TP + FN}$$\n\n-   **Specificity**: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as **true negative rate** and **selectivity**. $$TNR = \\frac{TN}{TN+FP}$$\n\n-   **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as **positive predictive value**. $$PPV = \\frac{TP}{TP + FP}$$\n\n## Performance metrics\n\n*Evaluating regression* <br>\n\n::: incremental\n-   The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models.\n-   For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements.\n-   In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model.\n:::\n\n## Evaluating regression {.smaller}\n\n<br>\n\n-   **R-squared**: As seen in the linear regression session. $$\n    R^2=1-\\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n    $$\n-   **Adjusted R-squared**: seen before $$\n    R_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\frac{n-1}{n-p-1}\n    $$\n-   **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. $$MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2$$\n-   **Root Mean Squared Error (RMSE)**: square root of the MSE $$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}$$\n-   **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|$$\n-   **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.\n\n## Live demo\n\n*KNN model for classification*\n"",
+    ""markdown"": ""---\ntitle: \""Supervised learning\""\n# author: Olga Dethlefsen\nformat: \n  revealjs:\n    slide-number: true\n    theme: [default, custom.scss]\n    view-distance: 10\n    chalkboard: \n      buttons: true\n  html:\n    code-fold: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n# We will discuss\n\n-   What supervised learning is.\n-   Data splitting.\n-   How to evaluate classification models.\n-   How to put together a minimum working example of supervised learning with KNN classifier.\n\n## What is supervised learning\n\n*Definition*\n\n<br>\n\n\n```{mermaid}\nflowchart TD\n  A(Machine learning) --> B(unsupervised learning)\n  A --> C(supervised learning)\n```\n\n\n<br>\n\n::: incremental\n-   Supervised learning can be used for **classification** and **regression**.\n    -   For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)\n    -   For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)\n:::\n\n\n## What is supervised learning?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/supervised-02.png)\n:::\n:::\n\n\n\n-   In supervised learning we are using sample **labels** to **train** (build) a model.\n-   We then use the trained model for **interpretation** and **prediction**.\n\n## What is supervised learning?\n\n*Definition* <br>\n\n::: incremental\n-   **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).\n-   Common supervised machine learning algorithms include:\n    -   K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN).\n-   Many of the ML methods can be implemented to work both for classifying samples and forecasting numeric outcome.\n:::\n\n## Supervised learning\n\n*Outline* <br>\n\nAcross many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:\n\n::: incremental\n-   deciding on the task: classification or regression\\\n-   **splitting data** to keep part of data for training and part for testing\n-   selecting supervised machine learning algorithms to be trained (or a set of these)\n-   deciding on the training strategy, i.e. which **performance metrics** to use and how to search for the best model parameters\n-   running **feature engineering**: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables\n-   performing **feature selection**: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present\n:::\n\n## Supervised learning {.scrollable}\n\n*Outline* <br>\n\nThe diagram below shows a basic strategy on how to train KNN for classification, given a data set with $n$ samples, $p$ variables and $y$ categorical outcome\n\n\n```{mermaid}\n\nflowchart TD\n  A([data]) -. split data \\n e.g. basic, stratified, grouped -.-> B([non-test set])\n  A([data]) -.-> C([test set])\n  B -.-> D(choose algorithm \\n e.g. KNN)\n  D -.-> E(choose evaluation metric \\n e.g. overall accuracy)\n  E -.-> F(feature engineering & selection)\n  F -.-> G(prepare parameter space, e.g. odd k-values from 3 to 30)\n  G -. split non-test -.-> H([train set & validation set])\n  H -.-> J(fit model on train set)\n  J -.-> K(collect evaluation metric on validation set)\n  K -.-> L{all values checked? \\n e.g. k more than 30}\n  L -. No .-> J\n  L -. Yes .-> M(select best parameter values)\n  M -.-> N(fit model on all non-test data)\n  N -.-> O(assess model on test data)\n  C -.-> O\n\n```\n\n\n## Supervised learning\n\n*Outline* <br>\n\nBefore we see how this training may look like in `R`, let's talk more about\n\n-   **KNN**, K-nearest neighbor algorithm\n-   **data splitting** and\n-   **performance metrics** useful for evaluating models\n\n## Classification\n\n*Definition* <br>\n\n::: incremental\n-   Classification methods are algorithms used to categorize (classify) objects based on their measurements.\n-   They belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the label (class) of.\n-   Let's for each observations $i$ collect pair of information $\\{\\mathbf{x_i}, g_i\\}$\n-   where $\\{\\mathbf{x_i}\\}$ is a set of exploratory variables e.g. a gene expression data\n-   and $g_i \\in \\{1, \\dots, G\\}$ is the class label for each observation (known), e.g. cancer stage I, II, III or IV\n-   Then we want to find a **classification rule** $f(.)$ (model) such that $$f(\\mathbf{x_i})=g_i$$\n:::\n\n## KNN\n\n*example of a classification algorithm*\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-00-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-01-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-02-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-03-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n\n*example of a classification algorithm*\n\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-10-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-20-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-30-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-40-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n\n*Algorithm* <br>\n\n::: incremental\n-   Decide on the value of $k$.\n-   Calculate the distance between the query-instance (observations for new sample) and all the training samples.\n-   Sort the distances and determine the nearest neighbors based on the $k$-th minimum distance.\n-   Gather the categories of the nearest neighbors.\n-   Use majority voting of the categories of the nearest neighbors as the prediction value for the new sample.\n-   Note: *Euclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.*\n:::\n\n## Data splitting\n\n*Why* <br>\n\n-   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\n-   As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.\n-   To deal with overconfident estimation of future performance we can implement various data splitting strategies.\n\n## Data splitting\n\n*train, validation & test sets*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   **Training data**: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule.\n-   **Validation data**: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters.\n-   **Test data**: this data is used to give an estimate of future prediction performance for the model and parameters chosen.\n-   Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\n\n## Data splitting\n\n*cross validation & repeated cross validation*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-kfolds-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   In **k-fold cross-validation** we split data into $k$ roughly equal-sized parts.\n-   We start by setting the validation data to be the first set of data and the training data to be all other sets.\n-   We estimate the validation error rate / correct classification rate for the split.\n-   We then repeat the process $k-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data.\n-   We finish with $k$ different error or correct classification rates.\n-   In this way, every data point has its class membership predicted once.\n-   The final reported error rate is usually the average of $k$ error rates.\n\n## Data splitting\n\n*Leave-one-out cross-validation* <br>\n\n-   Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n::: {.cell layout-align=\""center\"" fig-cap-location='margin'}\n::: {.cell-output-display}\n![Example of LOOCV, leave-one-out cross validation](figures/data-split-loocv-02.png){#fig-data-split-loocv fig-align='center' width=100%}\n:::\n:::\n\n\n## Performance metrics\n\n*Evaluating classification* <br>\n\n-   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.\n-   There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\n-   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\n## Evaluating classification\n\n<br> <br> **Correct (miss)classification rate**\n\n-   The simplest way to evaluate in which we count for all the $n$ predictions how many times we got the classification right. $$Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where $1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\n**Missclassification Rate**\n\nMissclassification Rate = 1 - Correct Classification Rate\n\n## Evaluating classification {.smaller}\n\n<br>\n\n**Confusion matrix**\n\nConfusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes.\n\n|                     | Predicted Positive  | Predicted Negative  |\n|---------------------|---------------------|---------------------|\n| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n\n-   **Accuracy**: measures the proportion of correctly classified samples over the total number of samples. $$ACC = \\frac{TP+TN}{TP+TN+FP+FN}$$\n\n-   **Sensitivity**: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as **true positive rate** and **recall**. $$TPR = \\frac{TP}{TP + FN}$$\n\n-   **Specificity**: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as **true negative rate** and **selectivity**. $$TNR = \\frac{TN}{TN+FP}$$\n\n-   **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as **positive predictive value**. $$PPV = \\frac{TP}{TP + FP}$$\n\n<!-- ## Performance metrics -->\n\n<!-- *Evaluating regression* <br> -->\n\n<!-- ::: incremental -->\n<!-- -   The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. -->\n<!-- -   For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. -->\n<!-- -   In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. -->\n<!-- ::: -->\n\n\n<!-- ## Evaluating regression {.smaller} -->\n\n<!-- <br> -->\n\n<!-- -   **R-squared**: As seen in the linear regression session. $$ -->\n<!--     R^2=1-\\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} -->\n<!--     $$ -->\n<!-- -   **Adjusted R-squared**: seen before $$ -->\n<!--     R_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\frac{n-1}{n-p-1} -->\n<!--     $$ -->\n<!-- -   **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. $$MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2$$ -->\n<!-- -   **Root Mean Squared Error (RMSE)**: square root of the MSE $$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}$$ -->\n<!-- -   **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|$$ -->\n<!-- -   **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values. -->\n\n## Live demo\n\n*KNN model for classification*\n"",
     ""supporting"": [
       ""session-supervise-presentation_files""
     ],

---FILE: session-supervise-presentation/.quarto/xref/c3b72353---
@@ -1 +1 @@
-{""headings"":[""we-will-discuss"",""what-is-supervised-learning"",""what-is-supervised-learning-1"",""supervised-learning"",""supervised-learning-1"",""supervised-learning-2"",""classification"",""knn"",""knn-1"",""knn-2"",""data-splitting"",""data-splitting-1"",""data-splitting-2"",""data-splitting-3"",""performance-metrics"",""evaluating-classification"",""evaluating-classification-1"",""performance-metrics-1"",""evaluating-regression"",""live-demo""],""entries"":[{""order"":{""number"":1,""section"":[1,13,0,0,0,0,0]},""caption"":""Example of LOOCV, leave-one-out cross validation"",""key"":""fig-data-split-loocv""}]}
\ No newline at end of file
+{""headings"":[""we-will-discuss"",""what-is-supervised-learning"",""what-is-supervised-learning-1"",""what-is-supervised-learning-2"",""supervised-learning"",""supervised-learning-1"",""supervised-learning-2"",""classification"",""knn"",""knn-1"",""knn-2"",""data-splitting"",""data-splitting-1"",""data-splitting-2"",""data-splitting-3"",""performance-metrics"",""evaluating-classification"",""evaluating-classification-1"",""live-demo""],""entries"":[{""order"":{""number"":1,""section"":[1,14,0,0,0,0,0]},""caption"":""Example of LOOCV, leave-one-out cross validation"",""key"":""fig-data-split-loocv""}]}
\ No newline at end of file

---FILE: session-supervise-presentation/session-supervise-presentation.html---
@@ -8,7 +8,7 @@
 <link href=""session-supervise-presentation_files/libs/quarto-html/light-border.css"" rel=""stylesheet"">
 <link href=""session-supervise-presentation_files/libs/quarto-html/quarto-html.min.css"" rel=""stylesheet"" data-mode=""light"">
 <link href=""session-supervise-presentation_files/libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles""><meta charset=""utf-8"">
-  <meta name=""generator"" content=""quarto-1.3.333"">
+  <meta name=""generator"" content=""quarto-1.3.450"">
 
   <title>Supervised learning</title>
   <meta name=""apple-mobile-web-app-capable"" content=""yes"">
@@ -29,7 +29,7 @@
       vertical-align: middle;
     }
   </style>
-  <link rel=""stylesheet"" href=""session-supervise-presentation_files/libs/revealjs/dist/theme/quarto.css"" id=""theme"">
+  <link rel=""stylesheet"" href=""session-supervise-presentation_files/libs/revealjs/dist/theme/quarto.css"">
   <link href=""session-supervise-presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css"" rel=""stylesheet"">
   <link href=""session-supervise-presentation_files/libs/revealjs/plugin/reveal-menu/menu.css"" rel=""stylesheet"">
   <link href=""session-supervise-presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css"" rel=""stylesheet"">
@@ -341,7 +341,7 @@ <h1>We will discuss</h1>
 <ul>
 <li>What supervised learning is.</li>
 <li>Data splitting.</li>
-<li>How to evaluate classification and regression ML models.</li>
+<li>How to evaluate classification models.</li>
 <li>How to put together a minimum working example of supervised learning with KNN classifier.</li>
 </ul>
 </section>
@@ -369,13 +369,19 @@ <h2>What is supervised learning</h2>
 <li class=""fragment"">For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)</li>
 <li class=""fragment"">For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)</li>
 </ul></li>
-<li class=""fragment"">In supervised learning we are using sample <strong>labels</strong> to <strong>train</strong> (build) a model.</li>
-<li class=""fragment"">We then use the trained model for <strong>interpretation</strong> and <strong>prediction</strong>.</li>
 </ul>
 </div>
 </section>
 <section id=""what-is-supervised-learning-1"" class=""slide level2"">
 <h2>What is supervised learning?</h2>
+
+<img data-src=""images/supervised-02.png"" class=""r-stretch""><ul>
+<li>In supervised learning we are using sample <strong>labels</strong> to <strong>train</strong> (build) a model.</li>
+<li>We then use the trained model for <strong>interpretation</strong> and <strong>prediction</strong>.</li>
+</ul>
+</section>
+<section id=""what-is-supervised-learning-2"" class=""slide level2"">
+<h2>What is supervised learning?</h2>
 <p><em>Definition</em> <br></p>
 <div>
 <ul>
@@ -534,7 +540,7 @@ <h2>Data splitting</h2>
 <li>Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.</li>
 </ul>
 
-<img data-src=""figures/data-split-loocv-02.png"" class=""margin-caption r-stretch quarto-figure-center"" style=""width:100.0%""><p class=""caption"">Figure&nbsp;1: Example of LOOCV, leave-one-out cross validation</p></section>
+<img data-src=""figures/data-split-loocv-02.png"" style=""width:100.0%"" class=""r-stretch quarto-figure-center""><p class=""caption"">Figure&nbsp;1: Example of LOOCV, leave-one-out cross validation</p></section>
 <section id=""performance-metrics"" class=""slide level2"">
 <h2>Performance metrics</h2>
 <p><em>Evaluating classification</em> <br></p>
@@ -585,33 +591,25 @@ <h2>Evaluating classification</h2>
 <li><p><strong>Specificity</strong>: measures the proportion of true negatives over all actual negative samples, i.e.&nbsp;how well the classifier is able to avoid false negatives. It is also known as <strong>true negative rate</strong> and <strong>selectivity</strong>. <span class=""math display"">\[TNR = \frac{TN}{TN+FP}\]</span></p></li>
 <li><p><strong>Precision</strong>: measures the proportion of true positives over all positive predictions made by the classifier, i.e.&nbsp;how well the classifier is able to avoid false positives. It is also known as <strong>positive predictive value</strong>. <span class=""math display"">\[PPV = \frac{TP}{TP + FP}\]</span></p></li>
 </ul>
-</section>
-<section id=""performance-metrics-1"" class=""slide level2"">
-<h2>Performance metrics</h2>
-<p><em>Evaluating regression</em> <br></p>
-<div>
-<ul>
-<li class=""fragment"">The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models.</li>
-<li class=""fragment"">For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements.</li>
-<li class=""fragment"">In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model.</li>
-</ul>
-</div>
-</section>
-<section id=""evaluating-regression"" class=""slide level2 smaller"">
-<h2>Evaluating regression</h2>
-<p><br></p>
-<ul>
-<li><strong>R-squared</strong>: As seen in the linear regression session. <span class=""math display"">\[
-R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
-\]</span></li>
-<li><strong>Adjusted R-squared</strong>: seen before <span class=""math display"">\[
-R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
-\]</span></li>
-<li><strong>Mean Squared Error (MSE)</strong>: average squared difference between the predicted values and the actual values. <span class=""math display"">\[MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2\]</span></li>
-<li><strong>Root Mean Squared Error (RMSE)</strong>: square root of the MSE <span class=""math display"">\[RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}\]</span></li>
-<li><strong>MAE</strong>: average absolute difference between the predicted values and the actual values <span class=""math display"">\[MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|\]</span></li>
-<li><strong>Mean Absolute Percentage Error (MAPE)</strong>: average percentage difference between the predicted values and the actual values.</li>
-</ul>
+<!-- ## Performance metrics -->
+<!-- *Evaluating regression* <br> -->
+<!-- ::: incremental -->
+<!-- -   The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. -->
+<!-- -   For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. -->
+<!-- -   In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. -->
+<!-- ::: -->
+<!-- ## Evaluating regression {.smaller} -->
+<!-- <br> -->
+<!-- -   **R-squared**: As seen in the linear regression session. $$ -->
+<!--     R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} -->
+<!--     $$ -->
+<!-- -   **Adjusted R-squared**: seen before $$ -->
+<!--     R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1} -->
+<!--     $$ -->
+<!-- -   **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. $$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$ -->
+<!-- -   **Root Mean Squared Error (RMSE)**: square root of the MSE $$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$ -->
+<!-- -   **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$ -->
+<!-- -   **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values. -->
 </section>
 <section id=""live-demo"" class=""slide level2"">
 <h2>Live demo</h2>

---FILE: session-supervise-presentation/session-supervise-presentation.qmd---
@@ -19,7 +19,7 @@ bibliography: references.bib
 
 -   What supervised learning is.
 -   Data splitting.
--   How to evaluate classification and regression ML models.
+-   How to evaluate classification models.
 -   How to put together a minimum working example of supervised learning with KNN classifier.
 
 ## What is supervised learning
@@ -40,9 +40,21 @@ flowchart TD
 -   Supervised learning can be used for **classification** and **regression**.
     -   For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)
     -   For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)
+:::
+
+
+## What is supervised learning?
+
+```{r}
+#| fig-width: 12
+
+library(knitr)
+include_graphics(""images/supervised-02.png"")
+```
+
+
 -   In supervised learning we are using sample **labels** to **train** (build) a model.
 -   We then use the trained model for **interpretation** and **prediction**.
-:::
 
 ## What is supervised learning?
 
@@ -412,30 +424,31 @@ Confusion matrix allows us to compare between actual and predicted values. It is
 
 -   **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as **positive predictive value**. $$PPV = \frac{TP}{TP + FP}$$
 
-## Performance metrics
+<!-- ## Performance metrics -->
 
-*Evaluating regression* <br>
+<!-- *Evaluating regression* <br> -->
 
-::: incremental
--   The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models.
--   For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements.
--   In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model.
-:::
+<!-- ::: incremental -->
+<!-- -   The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. -->
+<!-- -   For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. -->
+<!-- -   In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. -->
+<!-- ::: -->
 
-## Evaluating regression {.smaller}
 
-<br>
+<!-- ## Evaluating regression {.smaller} -->
+
+<!-- <br> -->
 
--   **R-squared**: As seen in the linear regression session. $$
-    R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
-    $$
--   **Adjusted R-squared**: seen before $$
-    R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
-    $$
--   **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. $$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$
--   **Root Mean Squared Error (RMSE)**: square root of the MSE $$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$
--   **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$
--   **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.
+<!-- -   **R-squared**: As seen in the linear regression session. $$ -->
+<!--     R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} -->
+<!--     $$ -->
+<!-- -   **Adjusted R-squared**: seen before $$ -->
+<!--     R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1} -->
+<!--     $$ -->
+<!-- -   **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. $$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$ -->
+<!-- -   **Root Mean Squared Error (RMSE)**: square root of the MSE $$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$ -->
+<!-- -   **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$ -->
+<!-- -   **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values. -->
 
 ## Live demo
 

---FILE: session-supervise/KNN-demo.qmd---
@@ -5,7 +5,7 @@ editor_options:
 ---
 
 
-# Demo: KNN model for classification
+# Demo: k-nearest neighbors (KNN)
 
 Let's try to build a classifier to predict obesity (Obese vs Non-obese) given our diabetes data set. To start simple:
 

---FILE: session-supervise/docs/KNN-demo.html---
@@ -2,12 +2,12 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.333"">
+<meta name=""generator"" content=""quarto-1.3.450"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
 
-<title>Introduction to supervised learning - 2&nbsp; Demo: KNN model for classification</title>
+<title>Introduction to supervised learning - 2&nbsp; Demo: k-nearest neighbors (KNN)</title>
 <style>
 code{white-space: pre-wrap;}
 span.smallcaps{font-variant: small-caps;}
@@ -91,7 +91,8 @@
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
     ""search-detached-cancel-button-title"": ""Cancel"",
-    ""search-submit-button-title"": ""Submit""
+    ""search-submit-button-title"": ""Submit"",
+    ""search-label"": ""Search""
   }
 }</script>
 
@@ -100,7 +101,7 @@
 
 </head>
 
-<body class=""nav-sidebar floating slimcontent"">
+<body class=""nav-sidebar floating"">
 
 <div id=""quarto-search-results""></div>
   <header id=""quarto-header"" class=""headroom fixed-top"">
@@ -109,10 +110,10 @@
       <button type=""button"" class=""quarto-btn-toggle btn"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">
         <i class=""bi bi-layout-text-sidebar-reverse""></i>
       </button>
-      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./KNN-demo.html""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></a></li></ol></nav>
+      <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./KNN-demo.html""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></a></li></ol></nav>
       <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
       </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
+      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -149,7 +150,7 @@
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
   <a href=""./KNN-demo.html"" class=""sidebar-item-text sidebar-link active"">
- <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></span></a>
+ <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></span></a>
   </div>
 </li>
         <li class=""sidebar-item"">
@@ -167,11 +168,11 @@
         
     </div>
 <!-- main -->
-<main class=""content page-columns page-full"" id=""quarto-document-content"">
+<main class=""content"" id=""quarto-document-content"">
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
-<h1 class=""title""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></h1>
+<h1 class=""title""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></h1>
 </div>
 
 
@@ -336,14 +337,14 @@ <h1 class=""title""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapt
 <span id=""cb3-25""><a href=""#cb3-25"" aria-hidden=""true"" tabindex=""-1""></a>}</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
 <p>Selecting best <span class=""math inline"">\(k\)</span></p>
-<div class=""cell page-columns page-full"" data-layout-align=""center"">
+<div class=""cell"" data-layout-align=""center"" data-fig-cap-location=""margin"">
 <div class=""sourceCode cell-code"" id=""cb4""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb4-1""><a href=""#cb4-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># plot classification rate as a function of k</span></span>
 <span id=""cb4-2""><a href=""#cb4-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">plot</span>(k_values, cls_rate, <span class=""at"">type=</span><span class=""st"">""l""</span>, <span class=""at"">xlab=</span><span class=""st"">""k""</span>, <span class=""at"">ylab=</span><span class=""st"">""cls rate""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-<div class=""cell-output-display page-columns page-full"">
-<div class=""quarto-figure quarto-figure-center page-columns page-full"">
-<figure class=""figure page-columns page-full"">
+<div class=""cell-output-display"">
+<div class=""quarto-figure quarto-figure-center"">
+<figure class=""figure"">
 <p><img src=""KNN-demo_files/figure-html/knn-best-parameters-1.png"" class=""img-fluid figure-img"" width=""672""></p>
-<figcaption class=""figure-caption margin-caption"">Overall classification rate as a function of k</figcaption>
+<figcaption class=""figure-caption"">Overall classification rate as a function of k</figcaption>
 </figure>
 </div>
 </div>

---FILE: session-supervise/docs/exercises.html---
@@ -2,7 +2,7 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.333"">
+<meta name=""generator"" content=""quarto-1.3.450"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
@@ -90,7 +90,8 @@
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
     ""search-detached-cancel-button-title"": ""Cancel"",
-    ""search-submit-button-title"": ""Submit""
+    ""search-submit-button-title"": ""Submit"",
+    ""search-label"": ""Search""
   }
 }</script>
 
@@ -111,7 +112,7 @@
       <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./exercises.html"">Exercises</a></li></ol></nav>
       <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
       </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
+      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -148,7 +149,7 @@
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
   <a href=""./KNN-demo.html"" class=""sidebar-item-text sidebar-link"">
- <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></span></a>
+ <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></span></a>
   </div>
 </li>
         <li class=""sidebar-item"">
@@ -195,15 +196,14 @@ <h1 class=""title"">Exercises</h1>
 <p><span class=""theorem-title""><strong>Exercise 1 (KNN with k-fold cross-validation) </strong></span>Modify the demo example to select best value of <span class=""math inline"">\(k\)</span> using 5-fold cross validation. Do we get a different value of the best <span class=""math inline"">\(k\)</span>? Does it improve the performance on the test data?</p>
 <p>Hint: you can use “create_folds()” function from “library(splitTools)” to create the folds.</p>
 </div>
-<div id=""exr-lm"" class=""theorem exercise"">
-<p><span class=""theorem-title""><strong>Exercise 2 (Supervised regression) </strong></span>Let’s revisit regression in a context of supervised learning. Using the <code>diabetes</code> data set and data splitting find the best model to predict BMI scores.</p>
-<p>Split the data into train (60%), validation (20%) and test (20%). Assess three regression models on the validation set using RMSE. Which model seems to be the best in terms of predicting BMI? What would be the expected performance on the new unseen data?</p>
-<ol type=""1"">
-<li>Model 1: use <code>age</code> as only as predictor</li>
-<li>Model 2: use <code>age</code>, <code>hdl</code> as predictors</li>
-<li>Model 3: use <code>age</code>, <code>hdl</code> and <code>waist</code> as predictors</li>
-</ol>
-</div>
+<!-- ::: {#exr-lm} -->
+<!-- ## Supervised regression -->
+<!-- Let's revisit regression in a context of supervised learning. Using the `diabetes` data set and data splitting find the best model to predict BMI scores.  -->
+<!-- Split the data into train (60%), validation (20%) and test (20%). Assess three regression models on the validation set using RMSE. Which model seems to be the best in terms of predicting BMI? What would be the expected performance on the new unseen data? -->
+<!-- 1. Model 1: use `age` as only as predictor -->
+<!-- 2. Model 2: use `age`, `hdl` as predictors -->
+<!-- 3. Model 3: use `age`, `hdl` and `waist` as predictors -->
+<!-- ::: -->
 <section id=""answers-to-exercises"" class=""level2 unnumbered"">
 <h2 class=""unnumbered anchored"" data-anchor-id=""answers-to-exercises"">Answers to exercises</h2>
 <div class=""solution proof"">
@@ -226,7 +226,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""answers-to-exercises"">Answers to
 <span id=""cb1-14""><a href=""#cb1-14"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">waist =</span> waist <span class=""sc"">*</span> inch2cm) <span class=""sc"">%&gt;%</span> </span>
 <span id=""cb1-15""><a href=""#cb1-15"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">weight =</span> weight <span class=""sc"">*</span> pound2kg, <span class=""at"">weight =</span> <span class=""fu"">round</span>(weight, <span class=""dv"">2</span>)) <span class=""sc"">%&gt;%</span></span>
 <span id=""cb1-16""><a href=""#cb1-16"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">BMI =</span> weight <span class=""sc"">/</span> height<span class=""sc"">^</span><span class=""dv"">2</span>, <span class=""at"">BMI =</span> <span class=""fu"">round</span>(BMI, <span class=""dv"">2</span>)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb1-17""><a href=""#cb1-17"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">obese=</span> <span class=""fu"">cut</span>(BMI, <span class=""at"">breaks =</span> <span class=""fu"">c</span>(<span class=""dv"">0</span>, <span class=""fl"">29.9</span>, <span class=""dv"">100</span>), <span class=""at"">labels =</span> <span class=""fu"">c</span>(<span class=""st"">""No""</span>, <span class=""st"">""Yes""</span>))) <span class=""sc"">%&gt;%</span></span>
+<span id=""cb1-17""><a href=""#cb1-17"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">obese =</span> <span class=""fu"">cut</span>(BMI, <span class=""at"">breaks =</span> <span class=""fu"">c</span>(<span class=""dv"">0</span>, <span class=""fl"">29.9</span>, <span class=""dv"">100</span>), <span class=""at"">labels =</span> <span class=""fu"">c</span>(<span class=""st"">""No""</span>, <span class=""st"">""Yes""</span>))) <span class=""sc"">%&gt;%</span></span>
 <span id=""cb1-18""><a href=""#cb1-18"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">diabetic =</span> <span class=""fu"">ifelse</span>(glyhb <span class=""sc"">&gt;</span> <span class=""dv"">7</span>, <span class=""st"">""Yes""</span>, <span class=""st"">""No""</span>), <span class=""at"">diabetic =</span> <span class=""fu"">factor</span>(diabetic, <span class=""at"">levels =</span> <span class=""fu"">c</span>(<span class=""st"">""No""</span>, <span class=""st"">""Yes""</span>))) <span class=""sc"">%&gt;%</span></span>
 <span id=""cb1-19""><a href=""#cb1-19"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">location =</span> <span class=""fu"">factor</span>(location)) <span class=""sc"">%&gt;%</span></span>
 <span id=""cb1-20""><a href=""#cb1-20"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">frame =</span> <span class=""fu"">factor</span>(frame)) <span class=""sc"">%&gt;%</span></span>
@@ -315,92 +315,77 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""answers-to-exercises"">Answers to
 <span id=""cb2-13""><a href=""#cb2-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(cls_rate)</span>
 <span id=""cb2-14""><a href=""#cb2-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## [1] 0.8481013</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
 </div>
-<div class=""solution proof"">
-<p><span class=""proof-title""><em>Solution</em>. </span><a href=""#exr-lm"">Exercise&nbsp;<span>2</span></a></p>
-</div>
-<div class=""cell"" data-layout-align=""center"">
-<div class=""sourceCode cell-code"" id=""cb3""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb3-1""><a href=""#cb3-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># input and clean data</span></span>
-<span id=""cb3-2""><a href=""#cb3-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># load libraries</span></span>
-<span id=""cb3-3""><a href=""#cb3-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(tidyverse)</span>
-<span id=""cb3-4""><a href=""#cb3-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(splitTools)</span>
-<span id=""cb3-5""><a href=""#cb3-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(kknn)</span>
-<span id=""cb3-6""><a href=""#cb3-6"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-7""><a href=""#cb3-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># input data</span></span>
-<span id=""cb3-8""><a href=""#cb3-8"" aria-hidden=""true"" tabindex=""-1""></a>input_diabetes <span class=""ot"">&lt;-</span> <span class=""fu"">read_csv</span>(<span class=""st"">""data/data-diabetes.csv""</span>)</span>
-<span id=""cb3-9""><a href=""#cb3-9"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-10""><a href=""#cb3-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># clean data</span></span>
-<span id=""cb3-11""><a href=""#cb3-11"" aria-hidden=""true"" tabindex=""-1""></a>inch2cm <span class=""ot"">&lt;-</span> <span class=""fl"">2.54</span></span>
-<span id=""cb3-12""><a href=""#cb3-12"" aria-hidden=""true"" tabindex=""-1""></a>pound2kg <span class=""ot"">&lt;-</span> <span class=""fl"">0.45</span></span>
-<span id=""cb3-13""><a href=""#cb3-13"" aria-hidden=""true"" tabindex=""-1""></a>data_diabetes <span class=""ot"">&lt;-</span> input_diabetes <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-14""><a href=""#cb3-14"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">height  =</span> height <span class=""sc"">*</span> inch2cm <span class=""sc"">/</span> <span class=""dv"">100</span>, <span class=""at"">height =</span> <span class=""fu"">round</span>(height, <span class=""dv"">2</span>)) <span class=""sc"">%&gt;%</span> </span>
-<span id=""cb3-15""><a href=""#cb3-15"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">waist =</span> waist <span class=""sc"">*</span> inch2cm) <span class=""sc"">%&gt;%</span> </span>
-<span id=""cb3-16""><a href=""#cb3-16"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">weight =</span> weight <span class=""sc"">*</span> pound2kg, <span class=""at"">weight =</span> <span class=""fu"">round</span>(weight, <span class=""dv"">2</span>)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-17""><a href=""#cb3-17"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">BMI =</span> weight <span class=""sc"">/</span> height<span class=""sc"">^</span><span class=""dv"">2</span>, <span class=""at"">BMI =</span> <span class=""fu"">round</span>(BMI, <span class=""dv"">2</span>)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-18""><a href=""#cb3-18"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">obese=</span> <span class=""fu"">cut</span>(BMI, <span class=""at"">breaks =</span> <span class=""fu"">c</span>(<span class=""dv"">0</span>, <span class=""fl"">29.9</span>, <span class=""dv"">100</span>), <span class=""at"">labels =</span> <span class=""fu"">c</span>(<span class=""st"">""No""</span>, <span class=""st"">""Yes""</span>))) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-19""><a href=""#cb3-19"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">diabetic =</span> <span class=""fu"">ifelse</span>(glyhb <span class=""sc"">&gt;</span> <span class=""dv"">7</span>, <span class=""st"">""Yes""</span>, <span class=""st"">""No""</span>), <span class=""at"">diabetic =</span> <span class=""fu"">factor</span>(diabetic, <span class=""at"">levels =</span> <span class=""fu"">c</span>(<span class=""st"">""No""</span>, <span class=""st"">""Yes""</span>))) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-20""><a href=""#cb3-20"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">location =</span> <span class=""fu"">factor</span>(location)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-21""><a href=""#cb3-21"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">frame =</span> <span class=""fu"">factor</span>(frame)) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-22""><a href=""#cb3-22"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">mutate</span>(<span class=""at"">gender =</span> <span class=""fu"">factor</span>(gender))</span>
-<span id=""cb3-23""><a href=""#cb3-23"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-24""><a href=""#cb3-24"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># select relevant data and remove missing data</span></span>
-<span id=""cb3-25""><a href=""#cb3-25"" aria-hidden=""true"" tabindex=""-1""></a>data_input <span class=""ot"">&lt;-</span> </span>
-<span id=""cb3-26""><a href=""#cb3-26"" aria-hidden=""true"" tabindex=""-1""></a>  data_diabetes <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-27""><a href=""#cb3-27"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">select</span>(BMI, age, hdl, waist) <span class=""sc"">%&gt;%</span></span>
-<span id=""cb3-28""><a href=""#cb3-28"" aria-hidden=""true"" tabindex=""-1""></a>  <span class=""fu"">na.omit</span>()</span>
-<span id=""cb3-29""><a href=""#cb3-29"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">glimpse</span>(data_input)</span>
-<span id=""cb3-30""><a href=""#cb3-30"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Rows: 394</span></span>
-<span id=""cb3-31""><a href=""#cb3-31"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Columns: 4</span></span>
-<span id=""cb3-32""><a href=""#cb3-32"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## $ BMI   &lt;dbl&gt; 22.09, 36.92, 47.95, 18.53, 27.52, 26.39, 28.07, 34.00, 24.39, 3…</span></span>
-<span id=""cb3-33""><a href=""#cb3-33"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## $ age   &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 33, …</span></span>
-<span id=""cb3-34""><a href=""#cb3-34"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## $ hdl   &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 47, …</span></span>
-<span id=""cb3-35""><a href=""#cb3-35"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## $ waist &lt;dbl&gt; 73.66, 116.84, 124.46, 83.82, 111.76, 91.44, 116.84, 86.36, 86.3…</span></span>
-<span id=""cb3-36""><a href=""#cb3-36"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-37""><a href=""#cb3-37"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># define calculate_rmse() function</span></span>
-<span id=""cb3-38""><a href=""#cb3-38"" aria-hidden=""true"" tabindex=""-1""></a>calculate_rmse <span class=""ot"">&lt;-</span> <span class=""cf"">function</span>(y_true, y_pred){</span>
-<span id=""cb3-39""><a href=""#cb3-39"" aria-hidden=""true"" tabindex=""-1""></a>  rmse <span class=""ot"">=</span> <span class=""fu"">sqrt</span>(<span class=""fu"">mean</span>((y_true <span class=""sc"">-</span> y_pred)<span class=""sc"">^</span><span class=""dv"">2</span>))</span>
-<span id=""cb3-40""><a href=""#cb3-40"" aria-hidden=""true"" tabindex=""-1""></a>}</span>
-<span id=""cb3-41""><a href=""#cb3-41"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-42""><a href=""#cb3-42"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># split into train, validation and test: stratify by BMI</span></span>
-<span id=""cb3-43""><a href=""#cb3-43"" aria-hidden=""true"" tabindex=""-1""></a>randseed <span class=""ot"">&lt;-</span> <span class=""dv"">123</span></span>
-<span id=""cb3-44""><a href=""#cb3-44"" aria-hidden=""true"" tabindex=""-1""></a>inds <span class=""ot"">&lt;-</span> <span class=""fu"">partition</span>(data_input<span class=""sc"">$</span>BMI, </span>
-<span id=""cb3-45""><a href=""#cb3-45"" aria-hidden=""true"" tabindex=""-1""></a>                  <span class=""at"">p =</span> <span class=""fu"">c</span>(<span class=""at"">train =</span> <span class=""fl"">0.6</span>, <span class=""at"">valid =</span> <span class=""fl"">0.2</span>, <span class=""at"">test =</span> <span class=""fl"">0.2</span>),</span>
-<span id=""cb3-46""><a href=""#cb3-46"" aria-hidden=""true"" tabindex=""-1""></a>                  <span class=""at"">seed =</span> randseed)</span>
-<span id=""cb3-47""><a href=""#cb3-47"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-48""><a href=""#cb3-48"" aria-hidden=""true"" tabindex=""-1""></a>data_train <span class=""ot"">&lt;-</span> data_input[inds<span class=""sc"">$</span>train, ]</span>
-<span id=""cb3-49""><a href=""#cb3-49"" aria-hidden=""true"" tabindex=""-1""></a>data_valid <span class=""ot"">&lt;-</span> data_input[inds<span class=""sc"">$</span>valid,]</span>
-<span id=""cb3-50""><a href=""#cb3-50"" aria-hidden=""true"" tabindex=""-1""></a>data_test <span class=""ot"">&lt;-</span> data_input[inds<span class=""sc"">$</span>test, ]</span>
-<span id=""cb3-51""><a href=""#cb3-51"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-52""><a href=""#cb3-52"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Model 1</span></span>
-<span id=""cb3-53""><a href=""#cb3-53"" aria-hidden=""true"" tabindex=""-1""></a>m1 <span class=""ot"">&lt;-</span> <span class=""fu"">lm</span>(BMI <span class=""sc"">~</span> age, <span class=""at"">data =</span> data_train) <span class=""co""># fit model on train</span></span>
-<span id=""cb3-54""><a href=""#cb3-54"" aria-hidden=""true"" tabindex=""-1""></a>pred_bmi_1 <span class=""ot"">&lt;-</span> <span class=""fu"">predict</span>(m1, <span class=""at"">newdata =</span> data_valid) <span class=""co""># predict BMI using validation set</span></span>
-<span id=""cb3-55""><a href=""#cb3-55"" aria-hidden=""true"" tabindex=""-1""></a>m1_rmse <span class=""ot"">&lt;-</span> <span class=""fu"">calculate_rmse</span>(data_valid<span class=""sc"">$</span>BMI, pred_bmi_1) <span class=""co""># calculate RMSE</span></span>
-<span id=""cb3-56""><a href=""#cb3-56"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-57""><a href=""#cb3-57"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Model 2</span></span>
-<span id=""cb3-58""><a href=""#cb3-58"" aria-hidden=""true"" tabindex=""-1""></a>m2 <span class=""ot"">&lt;-</span> <span class=""fu"">lm</span>(BMI <span class=""sc"">~</span> age <span class=""sc"">+</span> hdl, <span class=""at"">data =</span> data_train) </span>
-<span id=""cb3-59""><a href=""#cb3-59"" aria-hidden=""true"" tabindex=""-1""></a>pred_bmi_2 <span class=""ot"">&lt;-</span> <span class=""fu"">predict</span>(m2, <span class=""at"">newdata =</span> data_valid) </span>
-<span id=""cb3-60""><a href=""#cb3-60"" aria-hidden=""true"" tabindex=""-1""></a>m2_rmse <span class=""ot"">&lt;-</span> <span class=""fu"">calculate_rmse</span>(data_valid<span class=""sc"">$</span>BMI, pred_bmi_2) </span>
-<span id=""cb3-61""><a href=""#cb3-61"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-62""><a href=""#cb3-62"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Model 3</span></span>
-<span id=""cb3-63""><a href=""#cb3-63"" aria-hidden=""true"" tabindex=""-1""></a>m3 <span class=""ot"">&lt;-</span> <span class=""fu"">lm</span>(BMI <span class=""sc"">~</span> age <span class=""sc"">+</span> hdl <span class=""sc"">+</span> waist, <span class=""at"">data =</span> data_train) </span>
-<span id=""cb3-64""><a href=""#cb3-64"" aria-hidden=""true"" tabindex=""-1""></a>pred_bmi_3 <span class=""ot"">&lt;-</span> <span class=""fu"">predict</span>(m3, <span class=""at"">newdata =</span> data_valid) </span>
-<span id=""cb3-65""><a href=""#cb3-65"" aria-hidden=""true"" tabindex=""-1""></a>m3_rmse <span class=""ot"">&lt;-</span> <span class=""fu"">calculate_rmse</span>(data_valid<span class=""sc"">$</span>BMI, pred_bmi_3) </span>
-<span id=""cb3-66""><a href=""#cb3-66"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-67""><a href=""#cb3-67"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Compare models</span></span>
-<span id=""cb3-68""><a href=""#cb3-68"" aria-hidden=""true"" tabindex=""-1""></a>rmse <span class=""ot"">&lt;-</span> <span class=""fu"">data.frame</span>(<span class=""at"">model =</span> <span class=""fu"">c</span>(<span class=""st"">""Model 1""</span>, <span class=""st"">""Model 2""</span>, <span class=""st"">""Model 3""</span>), <span class=""at"">rmse =</span> <span class=""fu"">c</span>(m1_rmse, m2_rmse, m3_rmse))</span>
-<span id=""cb3-69""><a href=""#cb3-69"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(rmse)</span>
-<span id=""cb3-70""><a href=""#cb3-70"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     model     rmse</span></span>
-<span id=""cb3-71""><a href=""#cb3-71"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 1 Model 1 6.206929</span></span>
-<span id=""cb3-72""><a href=""#cb3-72"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2 Model 2 5.879602</span></span>
-<span id=""cb3-73""><a href=""#cb3-73"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 3 Model 3 3.418270</span></span>
-<span id=""cb3-74""><a href=""#cb3-74"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-75""><a href=""#cb3-75"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Out of the three models, Model 3, has the smallest RMSE and is thus selected as best</span></span>
-<span id=""cb3-76""><a href=""#cb3-76"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb3-77""><a href=""#cb3-77"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Expected performance on the test data</span></span>
-<span id=""cb3-78""><a href=""#cb3-78"" aria-hidden=""true"" tabindex=""-1""></a>pred_bmi_final <span class=""ot"">&lt;-</span> <span class=""fu"">predict</span>(m3, <span class=""at"">newdata =</span> data_test)</span>
-<span id=""cb3-79""><a href=""#cb3-79"" aria-hidden=""true"" tabindex=""-1""></a>rmse_final <span class=""ot"">&lt;-</span> <span class=""fu"">calculate_rmse</span>(data_test<span class=""sc"">$</span>BMI, pred_bmi_final)</span>
-<span id=""cb3-80""><a href=""#cb3-80"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(rmse_final)</span>
-<span id=""cb3-81""><a href=""#cb3-81"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## [1] 3.180441</span></span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
-</div>
+<!-- ::: {.solution} -->
+<!-- @exr-lm -->
+<!-- ::: -->
+<!-- ```{r} -->
+<!-- #| label: supervised-regression -->
+<!-- #| warning: false -->
+<!-- #| message: false -->
+<!-- #| fig-width: 7 -->
+<!-- #| fig-height: 8 -->
+<!-- #| fig-align: center -->
+<!-- #| code-fold: false -->
+<!-- #| collapse: true -->
+<!-- # input and clean data -->
+<!-- # load libraries -->
+<!-- library(tidyverse) -->
+<!-- library(splitTools) -->
+<!-- library(kknn) -->
+<!-- # input data -->
+<!-- input_diabetes <- read_csv(""data/data-diabetes.csv"") -->
+<!-- # clean data -->
+<!-- inch2cm <- 2.54 -->
+<!-- pound2kg <- 0.45 -->
+<!-- data_diabetes <- input_diabetes %>% -->
+<!--   mutate(height  = height * inch2cm / 100, height = round(height, 2)) %>%  -->
+<!--   mutate(waist = waist * inch2cm) %>%  -->
+<!--   mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>% -->
+<!--   mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% -->
+<!--   mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(""No"", ""Yes""))) %>% -->
+<!--   mutate(diabetic = ifelse(glyhb > 7, ""Yes"", ""No""), diabetic = factor(diabetic, levels = c(""No"", ""Yes""))) %>% -->
+<!--   mutate(location = factor(location)) %>% -->
+<!--   mutate(frame = factor(frame)) %>% -->
+<!--   mutate(gender = factor(gender)) -->
+<!-- # select relevant data and remove missing data -->
+<!-- data_input <-  -->
+<!--   data_diabetes %>% -->
+<!--   select(BMI, age, hdl, waist) %>% -->
+<!--   na.omit() -->
+<!-- glimpse(data_input) -->
+<!-- # define calculate_rmse() function -->
+<!-- calculate_rmse <- function(y_true, y_pred){ -->
+<!--   rmse = sqrt(mean((y_true - y_pred)^2)) -->
+<!-- } -->
+<!-- # split into train, validation and test: stratify by BMI -->
+<!-- randseed <- 123 -->
+<!-- inds <- partition(data_input$BMI,  -->
+<!--                   p = c(train = 0.6, valid = 0.2, test = 0.2), -->
+<!--                   seed = randseed) -->
+<!-- data_train <- data_input[inds$train, ] -->
+<!-- data_valid <- data_input[inds$valid,] -->
+<!-- data_test <- data_input[inds$test, ] -->
+<!-- # Model 1 -->
+<!-- m1 <- lm(BMI ~ age, data = data_train) # fit model on train -->
+<!-- pred_bmi_1 <- predict(m1, newdata = data_valid) # predict BMI using validation set -->
+<!-- m1_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_1) # calculate RMSE -->
+<!-- # Model 2 -->
+<!-- m2 <- lm(BMI ~ age + hdl, data = data_train)  -->
+<!-- pred_bmi_2 <- predict(m2, newdata = data_valid)  -->
+<!-- m2_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_2)  -->
+<!-- # Model 3 -->
+<!-- m3 <- lm(BMI ~ age + hdl + waist, data = data_train)  -->
+<!-- pred_bmi_3 <- predict(m3, newdata = data_valid)  -->
+<!-- m3_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_3)  -->
+<!-- # Compare models -->
+<!-- rmse <- data.frame(model = c(""Model 1"", ""Model 2"", ""Model 3""), rmse = c(m1_rmse, m2_rmse, m3_rmse)) -->
+<!-- print(rmse) -->
+<!-- # Out of the three models, Model 3, has the smallest RMSE and is thus selected as best -->
+<!-- # Expected performance on the test data -->
+<!-- pred_bmi_final <- predict(m3, newdata = data_test) -->
+<!-- rmse_final <- calculate_rmse(data_test$BMI, pred_bmi_final) -->
+<!-- print(rmse_final) -->
+<!-- ``` -->
 
 
 </section>
@@ -642,7 +627,7 @@ <h2 class=""unnumbered anchored"" data-anchor-id=""answers-to-exercises"">Answers to
 <nav class=""page-navigation"">
   <div class=""nav-page nav-page-previous"">
       <a href=""./KNN-demo.html"" class=""pagination-link"">
-        <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></span>
+        <i class=""bi bi-arrow-left-short""></i> <span class=""nav-page-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></span>
       </a>          
   </div>
   <div class=""nav-page nav-page-next"">

---FILE: session-supervise/docs/index.html---
@@ -2,7 +2,7 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.333"">
+<meta name=""generator"" content=""quarto-1.3.450"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
@@ -57,7 +57,8 @@
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
     ""search-detached-cancel-button-title"": ""Cancel"",
-    ""search-submit-button-title"": ""Submit""
+    ""search-submit-button-title"": ""Submit"",
+    ""search-label"": ""Search""
   }
 }</script>
 
@@ -78,7 +79,7 @@
       <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./index.html"">Preface</a></li></ol></nav>
       <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
       </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
+      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -115,7 +116,7 @@
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
   <a href=""./KNN-demo.html"" class=""sidebar-item-text sidebar-link"">
- <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></span></a>
+ <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></span></a>
   </div>
 </li>
         <li class=""sidebar-item"">

---FILE: session-supervise/docs/intro.html---
@@ -2,7 +2,7 @@
 <html xmlns=""http://www.w3.org/1999/xhtml"" lang=""en"" xml:lang=""en""><head>
 
 <meta charset=""utf-8"">
-<meta name=""generator"" content=""quarto-1.3.333"">
+<meta name=""generator"" content=""quarto-1.3.450"">
 
 <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, user-scalable=yes"">
 
@@ -20,6 +20,40 @@
   margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
   vertical-align: middle;
 }
+/* CSS for syntax highlighting */
+pre > code.sourceCode { white-space: pre; position: relative; }
+pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
+pre > code.sourceCode > span:empty { height: 1.2em; }
+.sourceCode { overflow: visible; }
+code.sourceCode > span { color: inherit; text-decoration: inherit; }
+div.sourceCode { margin: 1em 0; }
+pre.sourceCode { margin: 0; }
+@media screen {
+div.sourceCode { overflow: auto; }
+}
+@media print {
+pre > code.sourceCode { white-space: pre-wrap; }
+pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
+}
+pre.numberSource code
+  { counter-reset: source-line 0; }
+pre.numberSource code > span
+  { position: relative; left: -4em; counter-increment: source-line; }
+pre.numberSource code > span > a:first-child::before
+  { content: counter(source-line);
+    position: relative; left: -1em; text-align: right; vertical-align: baseline;
+    border: none; display: inline-block;
+    -webkit-touch-callout: none; -webkit-user-select: none;
+    -khtml-user-select: none; -moz-user-select: none;
+    -ms-user-select: none; user-select: none;
+    padding: 0 4px; width: 4em;
+  }
+pre.numberSource { margin-left: 3em;  padding-left: 4px; }
+div.sourceCode
+  {   }
+@media screen {
+pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
+}
 </style>
 
 
@@ -57,7 +91,8 @@
     ""search-more-matches-text"": ""more matches in this document"",
     ""search-clear-button-title"": ""Clear"",
     ""search-detached-cancel-button-title"": ""Cancel"",
-    ""search-submit-button-title"": ""Submit""
+    ""search-submit-button-title"": ""Submit"",
+    ""search-label"": ""Search""
   }
 }</script>
 <meta name=""mermaid-theme"" content=""forest"">
@@ -70,7 +105,7 @@
 
 </head>
 
-<body class=""nav-sidebar floating slimcontent"">
+<body class=""nav-sidebar floating"">
 
 <div id=""quarto-search-results""></div>
   <header id=""quarto-header"" class=""headroom fixed-top"">
@@ -82,7 +117,7 @@
       <nav class=""quarto-page-breadcrumbs"" aria-label=""breadcrumb""><ol class=""breadcrumb""><li class=""breadcrumb-item""><a href=""./intro.html""><span class=""chapter-number"">1</span>&nbsp; <span class=""chapter-title"">Supervised learning</span></a></li></ol></nav>
       <a class=""flex-grow-1"" role=""button"" data-bs-toggle=""collapse"" data-bs-target=""#quarto-sidebar,#quarto-sidebar-glass"" aria-controls=""quarto-sidebar"" aria-expanded=""false"" aria-label=""Toggle sidebar navigation"" onclick=""if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"">      
       </a>
-      <button type=""button"" class=""btn quarto-search-button"" aria-label=""Search"" onclick=""window.quartoOpenSearch();"">
+      <button type=""button"" class=""btn quarto-search-button"" aria-label="""" onclick=""window.quartoOpenSearch();"">
         <i class=""bi bi-search""></i>
       </button>
     </div>
@@ -119,7 +154,7 @@
         <li class=""sidebar-item"">
   <div class=""sidebar-item-container""> 
   <a href=""./KNN-demo.html"" class=""sidebar-item-text sidebar-link"">
- <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></span></a>
+ <span class=""menu-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></span></a>
   </div>
 </li>
         <li class=""sidebar-item"">
@@ -138,24 +173,24 @@
     <h2 id=""toc-title"">Table of contents</h2>
    
   <ul>
-  <li><a href=""#what-is-supervised-learning"" id=""toc-what-is-supervised-learning"" class=""nav-link active"" data-scroll-target=""#what-is-supervised-learning""><span class=""header-section-number"">1.1</span> What is supervised learning?</a></li>
-  <li><a href=""#outline"" id=""toc-outline"" class=""nav-link"" data-scroll-target=""#outline""><span class=""header-section-number"">1.2</span> Outline</a></li>
-  <li><a href=""#classification"" id=""toc-classification"" class=""nav-link"" data-scroll-target=""#classification""><span class=""header-section-number"">1.3</span> Classification</a></li>
-  <li><a href=""#knn-example"" id=""toc-knn-example"" class=""nav-link"" data-scroll-target=""#knn-example""><span class=""header-section-number"">1.4</span> KNN example</a></li>
-  <li><a href=""#data-splitting"" id=""toc-data-splitting"" class=""nav-link"" data-scroll-target=""#data-splitting""><span class=""header-section-number"">1.5</span> Data splitting</a>
+  <li><a href=""#examples-of-supervised-learning"" id=""toc-examples-of-supervised-learning"" class=""nav-link active"" data-scroll-target=""#examples-of-supervised-learning""><span class=""header-section-number"">1.1</span> Examples of supervised learning</a></li>
+  <li><a href=""#what-is-supervised-learning"" id=""toc-what-is-supervised-learning"" class=""nav-link"" data-scroll-target=""#what-is-supervised-learning""><span class=""header-section-number"">1.2</span> What is supervised learning?</a></li>
+  <li><a href=""#outline"" id=""toc-outline"" class=""nav-link"" data-scroll-target=""#outline""><span class=""header-section-number"">1.3</span> Outline</a></li>
+  <li><a href=""#classification"" id=""toc-classification"" class=""nav-link"" data-scroll-target=""#classification""><span class=""header-section-number"">1.4</span> Classification</a></li>
+  <li><a href=""#knn-example"" id=""toc-knn-example"" class=""nav-link"" data-scroll-target=""#knn-example""><span class=""header-section-number"">1.5</span> KNN example</a></li>
+  <li><a href=""#data-splitting"" id=""toc-data-splitting"" class=""nav-link"" data-scroll-target=""#data-splitting""><span class=""header-section-number"">1.6</span> Data splitting</a>
   <ul class=""collapse"">
   <li><a href=""#train-validation-test-sets"" id=""toc-train-validation-test-sets"" class=""nav-link"" data-scroll-target=""#train-validation-test-sets"">train, validation &amp; test sets</a></li>
   <li><a href=""#cross-validation"" id=""toc-cross-validation"" class=""nav-link"" data-scroll-target=""#cross-validation"">cross validation</a></li>
   <li><a href=""#repeated-cross-validation"" id=""toc-repeated-cross-validation"" class=""nav-link"" data-scroll-target=""#repeated-cross-validation"">repeated cross validation</a></li>
   <li><a href=""#leave-one-out-cross-validation"" id=""toc-leave-one-out-cross-validation"" class=""nav-link"" data-scroll-target=""#leave-one-out-cross-validation"">Leave-one-out cross-validation</a></li>
   </ul></li>
-  <li><a href=""#evaluating-classification"" id=""toc-evaluating-classification"" class=""nav-link"" data-scroll-target=""#evaluating-classification""><span class=""header-section-number"">1.6</span> Evaluating classification</a></li>
-  <li><a href=""#evaluating-regression"" id=""toc-evaluating-regression"" class=""nav-link"" data-scroll-target=""#evaluating-regression""><span class=""header-section-number"">1.7</span> Evaluating regression</a></li>
+  <li><a href=""#evaluating-classification"" id=""toc-evaluating-classification"" class=""nav-link"" data-scroll-target=""#evaluating-classification""><span class=""header-section-number"">1.7</span> Evaluating classification</a></li>
   </ul>
 </nav>
     </div>
 <!-- main -->
-<main class=""content page-columns page-full"" id=""quarto-document-content"">
+<main class=""content"" id=""quarto-document-content"">
 
 <header id=""title-block-header"" class=""quarto-title-block default"">
 <div class=""quarto-title"">
@@ -174,18 +209,47 @@ <h1 class=""title""><span class=""chapter-number"">1</span>&nbsp; <span class=""chapt
 
 </header>
 
-<section id=""what-is-supervised-learning"" class=""level2"" data-number=""1.1"">
-<h2 data-number=""1.1"" class=""anchored"" data-anchor-id=""what-is-supervised-learning""><span class=""header-section-number"">1.1</span> What is supervised learning?</h2>
+<section id=""examples-of-supervised-learning"" class=""level2"" data-number=""1.1"">
+<h2 data-number=""1.1"" class=""anchored"" data-anchor-id=""examples-of-supervised-learning""><span class=""header-section-number"">1.1</span> Examples of supervised learning</h2>
+<ul>
+<li>Supervised learning can be used for <strong>classification</strong> tasks.
+<ul>
+<li>E.g. given a large amount of images of biopsies, where each image is marked as benign or malignant, we have trained a classification model. Now, given a new biopsy sample we can diagnose cancer as benign or malignant.</li>
+</ul></li>
+<li>And supervised learning can be used for <strong>regression</strong> tasks.
+<ul>
+<li>E.g. using DNA samples from a diverse group of individuals across a wide range of ages, we measured methylation levels at thousands of CpG sites across the genome and trained a regression model by selecting hundreds of sites based on their potential relevance to aging. Now, given a new DNA samples and methylation measurements at these specific CpG sites, we can use the model to forecast individual’s age.</li>
+</ul></li>
+</ul>
+</section>
+<section id=""what-is-supervised-learning"" class=""level2"" data-number=""1.2"">
+<h2 data-number=""1.2"" class=""anchored"" data-anchor-id=""what-is-supervised-learning""><span class=""header-section-number"">1.2</span> What is supervised learning?</h2>
 <ul>
-<li>Supervised learning can be used for classification, e.g.&nbsp;given a new biopsy sample we want to tell whether it contains tumor tissue (Yes/No) and for regression, e.g.&nbsp;given a new measurements of the methylation sites we want to forecast epigenomic age.</li>
 <li>In supervised learning we are using sample <strong>labels</strong> to <strong>train</strong> (build) a model. We then use the trained model for interpretation and <strong>prediction</strong>.</li>
-<li>This is in contrast to previously discussed unsupervised learning such as clustering or PCA - methods that we were using to find patterns in the data. We treated data set a a whole, using measurements for all samples but not the samples labels such as sample groups to find the components with the highest variables (PCA) or the optimal number of clusters (k-means).</li>
+<li>This is in contrast to unsupervised learning, like clustering or Principal Component Analysis (PCA), that aims to discover patterns or groupings in the data without any predefined labels, such as identifying subtypes of a disease based on genetic variations.</li>
+</ul>
+<div class=""cell"">
+<details>
+<summary>Code</summary>
+<div class=""sourceCode cell-code"" id=""cb1""><pre class=""sourceCode r code-with-copy""><code class=""sourceCode r""><span id=""cb1-1""><a href=""#cb1-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(knitr)</span>
+<span id=""cb1-2""><a href=""#cb1-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">include_graphics</span>(<span class=""st"">""images/supervised.png""</span>)</span></code><button title=""Copy to Clipboard"" class=""code-copy-button""><i class=""bi""></i></button></pre></div>
+</details>
+<div class=""cell-output-display"">
+<div id=""fig-supervised"" class=""quarto-figure quarto-figure-center anchored"">
+<figure class=""figure"">
+<p><img src=""images/supervised.png"" class=""img-fluid figure-img""></p>
+<figcaption class=""figure-caption"">Figure&nbsp;1.1: Ilustration of supervised learning that focuses on training models to make predictions or decisions based on labeled training data.</figcaption>
+</figure>
+</div>
+</div>
+</div>
+<ul>
 <li><strong>Training</strong> a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).</li>
-<li>Common supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome.</li>
+<li>Common supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome, i.e.&nbsp;classification and regression tasks.</li>
 </ul>
 </section>
-<section id=""outline"" class=""level2"" data-number=""1.2"">
-<h2 data-number=""1.2"" class=""anchored"" data-anchor-id=""outline""><span class=""header-section-number"">1.2</span> Outline</h2>
+<section id=""outline"" class=""level2"" data-number=""1.3"">
+<h2 data-number=""1.3"" class=""anchored"" data-anchor-id=""outline""><span class=""header-section-number"">1.3</span> Outline</h2>
 <p>Across many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:</p>
 <ul>
 <li>deciding on the task: classification or regression<br>
@@ -233,22 +297,22 @@ <h2 data-number=""1.2"" class=""anchored"" data-anchor-id=""outline""><span class=""hea
 </ul>
 <p>And we will leave feature engineering and feature selection for the next session.</p>
 </section>
-<section id=""classification"" class=""level2"" data-number=""1.3"">
-<h2 data-number=""1.3"" class=""anchored"" data-anchor-id=""classification""><span class=""header-section-number"">1.3</span> Classification</h2>
+<section id=""classification"" class=""level2"" data-number=""1.4"">
+<h2 data-number=""1.4"" class=""anchored"" data-anchor-id=""classification""><span class=""header-section-number"">1.4</span> Classification</h2>
 <ul>
 <li>Classification methods are algorithms used to categorize (classify) objects based on their measurements.</li>
 <li>They belong under <strong>supervised learning</strong> as we usually start off with <strong>labeled</strong> data, i.e.&nbsp;observations with measurements for which we know the label (class) of.</li>
 <li>If we have a pair <span class=""math inline"">\(\{\mathbf{x_i}, g_i\}\)</span> for each observation <span class=""math inline"">\(i\)</span>, with <span class=""math inline"">\(g_i \in \{1, \dots, G\}\)</span> being the class label, where <span class=""math inline"">\(G\)</span> is the number of different classes and <span class=""math inline"">\(\mathbf{x_i}\)</span> a set of exploratory variables, that can be continuous, categorical or a mix of both, then we want to find a <strong>classification rule</strong> <span class=""math inline"">\(f(.)\)</span> (model) such that <span class=""math display"">\[f(\mathbf{x_i})=g_i\]</span></li>
 </ul>
 </section>
-<section id=""knn-example"" class=""level2 page-columns page-full"" data-number=""1.4"">
-<h2 data-number=""1.4"" class=""anchored"" data-anchor-id=""knn-example""><span class=""header-section-number"">1.4</span> KNN example</h2>
-<div class=""cell page-columns page-full"" data-layout-align=""center"">
-<div class=""cell-output-display page-columns page-full"">
-<div id=""fig-knn"" class=""quarto-figure quarto-figure-center anchored page-columns page-full"">
-<figure class=""figure page-columns page-full"">
+<section id=""knn-example"" class=""level2"" data-number=""1.5"">
+<h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""knn-example""><span class=""header-section-number"">1.5</span> KNN example</h2>
+<div class=""cell"" data-layout-align=""center"" data-fig-cap-location=""margin"">
+<div class=""cell-output-display"">
+<div id=""fig-knn"" class=""quarto-figure quarto-figure-center anchored"">
+<figure class=""figure"">
 <p><img src=""intro_files/figure-html/fig-knn-1.png"" class=""img-fluid figure-img"" width=""768""></p>
-<figcaption class=""figure-caption margin-caption"">Figure&nbsp;1.1: An example of k-nearest neighbors algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangle samples based on its gene A and gene B measurements and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurements and is thus classified by majority vote as a black dot (D).</figcaption>
+<figcaption class=""figure-caption"">Figure&nbsp;1.2: An example of k-nearest neighbors algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangle samples based on its gene A and gene B measurements and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurements and is thus classified by majority vote as a black dot (D).</figcaption>
 </figure>
 </div>
 </div>
@@ -263,33 +327,33 @@ <h2 data-number=""1.4"" class=""anchored"" data-anchor-id=""knn-example""><span class=
 </ul>
 <p><em>Euclidean distance is a classic distance used with KNN; other distance measures are also used incl.&nbsp;weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.</em></p>
 </section>
-<section id=""data-splitting"" class=""level2 page-columns page-full"" data-number=""1.5"">
-<h2 data-number=""1.5"" class=""anchored"" data-anchor-id=""data-splitting""><span class=""header-section-number"">1.5</span> Data splitting</h2>
+<section id=""data-splitting"" class=""level2"" data-number=""1.6"">
+<h2 data-number=""1.6"" class=""anchored"" data-anchor-id=""data-splitting""><span class=""header-section-number"">1.6</span> Data splitting</h2>
 <ul>
 <li>Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.</li>
 <li>As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to <strong>overfitting</strong>.</li>
 <li>To deal with overconfident estimation of future performance we can implement various data splitting strategies.</li>
 </ul>
-<section id=""train-validation-test-sets"" class=""level3 unnumbered page-columns page-full"">
+<section id=""train-validation-test-sets"" class=""level3 unnumbered"">
 <h3 class=""unnumbered anchored"" data-anchor-id=""train-validation-test-sets"">train, validation &amp; test sets</h3>
 <ul>
 <li>Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively</li>
 <li><strong>Training data</strong>: this is data used to fit (train) the classification or regression model, i.e.&nbsp;derive the classification rule</li>
 <li><strong>Validation data</strong>: this is data used to select which parameters or types of model perform best, i.e.&nbsp;to validate the performance of model parameters</li>
 <li><strong>Test data</strong>: this data is used to give an estimate of future prediction performance for the model and parameters chosen</li>
 </ul>
-<div class=""cell page-columns page-full"" data-layout-align=""center"">
-<div class=""cell-output-display page-columns page-full"">
-<div id=""fig-data-split"" class=""quarto-figure quarto-figure-center anchored page-columns page-full"">
-<figure class=""figure page-columns page-full"">
+<div class=""cell"" data-layout-align=""center"" data-fig-cap-location=""margin"">
+<div class=""cell-output-display"">
+<div id=""fig-data-split"" class=""quarto-figure quarto-figure-center anchored"">
+<figure class=""figure"">
 <p><img src=""figures/data-split-02.png"" class=""img-fluid figure-img"" style=""width:100.0%""></p>
-<figcaption class=""figure-caption margin-caption"">Figure&nbsp;1.2: Example of splitting data into train (50%), validation (25%) and test (25%) set</figcaption>
+<figcaption class=""figure-caption"">Figure&nbsp;1.3: Example of splitting data into train (50%), validation (25%) and test (25%) set</figcaption>
 </figure>
 </div>
 </div>
 </div>
 </section>
-<section id=""cross-validation"" class=""level3 unnumbered page-columns page-full"">
+<section id=""cross-validation"" class=""level3 unnumbered"">
 <h3 class=""unnumbered anchored"" data-anchor-id=""cross-validation"">cross validation</h3>
 <ul>
 <li>It can happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data. e.g.&nbsp;gets all the difficult observation to classify.</li>
@@ -302,12 +366,12 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""cross-validation"">cross validati
 <li>In this way, every data point has its class membership predicted once.</li>
 <li>The final reported error rate is usually the average of <span class=""math inline"">\(k\)</span> error rates.</li>
 </ul>
-<div class=""cell page-columns page-full"" data-layout-align=""center"">
-<div class=""cell-output-display page-columns page-full"">
-<div id=""fig-data-split-kfold"" class=""quarto-figure quarto-figure-center anchored page-columns page-full"">
-<figure class=""figure page-columns page-full"">
+<div class=""cell"" data-layout-align=""center"" data-fig-cap-location=""margin"">
+<div class=""cell-output-display"">
+<div id=""fig-data-split-kfold"" class=""quarto-figure quarto-figure-center anchored"">
+<figure class=""figure"">
 <p><img src=""figures/data-split-kfolds-02.png"" class=""img-fluid figure-img"" style=""width:100.0%""></p>
-<figcaption class=""figure-caption margin-caption"">Figure&nbsp;1.3: Example of k-fold cross validation split (k = 3)</figcaption>
+<figcaption class=""figure-caption"">Figure&nbsp;1.4: Example of k-fold cross validation split (k = 3)</figcaption>
 </figure>
 </div>
 </div>
@@ -319,25 +383,25 @@ <h3 class=""unnumbered anchored"" data-anchor-id=""repeated-cross-validation"">repea
 <li>In repeated cross-validation we are repeating the cross-validation many times, e.g.&nbsp;we can create 5 validation folds 3 times</li>
 </ul>
 </section>
-<section id=""leave-one-out-cross-validation"" class=""level3 unnumbered page-columns page-full"">
+<section id=""leave-one-out-cross-validation"" class=""level3 unnumbered"">
 <h3 class=""unnumbered anchored"" data-anchor-id=""leave-one-out-cross-validation"">Leave-one-out cross-validation</h3>
 <ul>
 <li>Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.</li>
 </ul>
-<div class=""cell page-columns page-full"" data-layout-align=""center"">
-<div class=""cell-output-display page-columns page-full"">
-<div id=""fig-data-split-loocv"" class=""quarto-figure quarto-figure-center anchored page-columns page-full"">
-<figure class=""figure page-columns page-full"">
+<div class=""cell"" data-layout-align=""center"" data-fig-cap-location=""margin"">
+<div class=""cell-output-display"">
+<div id=""fig-data-split-loocv"" class=""quarto-figure quarto-figure-center anchored"">
+<figure class=""figure"">
 <p><img src=""figures/data-split-loocv-02.png"" class=""img-fluid figure-img"" style=""width:100.0%""></p>
-<figcaption class=""figure-caption margin-caption"">Figure&nbsp;1.4: Example of LOOCV, leave-one-out cross validation</figcaption>
+<figcaption class=""figure-caption"">Figure&nbsp;1.5: Example of LOOCV, leave-one-out cross validation</figcaption>
 </figure>
 </div>
 </div>
 </div>
 </section>
 </section>
-<section id=""evaluating-classification"" class=""level2"" data-number=""1.6"">
-<h2 data-number=""1.6"" class=""anchored"" data-anchor-id=""evaluating-classification""><span class=""header-section-number"">1.6</span> Evaluating classification</h2>
+<section id=""evaluating-classification"" class=""level2"" data-number=""1.7"">
+<h2 data-number=""1.7"" class=""anchored"" data-anchor-id=""evaluating-classification""><span class=""header-section-number"">1.7</span> Evaluating classification</h2>
 <ul>
 <li>To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g.&nbsp;change the value of <span class=""math inline"">\(k\)</span> in KNN.</li>
 <li>There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.</li>
@@ -380,22 +444,22 @@ <h2 data-number=""1.6"" class=""anchored"" data-anchor-id=""evaluating-classification
 <li><p><strong>Precision</strong>: measures the proportion of true positives over all positive predictions made by the classifier, i.e.&nbsp;how well the classifier is able to avoid false positives. It is also known as <strong>positive predictive value</strong> <span class=""math display"">\[PPV = \frac{TP}{TP + FP}\]</span></p></li>
 <li><p><strong>ROC AUC</strong>: the receiver operating characteristic (ROC) curve is a graphical representation of the trade off between sensitivity and specificity for different threshold values. The area under the ROC curve (AUC) is a performance metric that ranges from 0 to 1, with a higher value indicating better performance. AUC is a measure of how well the classifier is able to distinguish between positive and negative samples.</p></li>
 </ul>
-</section>
-<section id=""evaluating-regression"" class=""level2"" data-number=""1.7"">
-<h2 data-number=""1.7"" class=""anchored"" data-anchor-id=""evaluating-regression""><span class=""header-section-number"">1.7</span> Evaluating regression</h2>
-<p>The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include:</p>
-<ul>
-<li><strong>R-squared</strong>: As seen in the linear regression session. <span class=""math display"">\[
-R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
-\]</span></li>
-<li><strong>Adjusted R-squared</strong>: seen before <span class=""math display"">\[
-R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
-\]</span></li>
-<li><strong>Mean Squared Error (MSE)</strong>: average squared difference between the predicted values and the actual values. <span class=""math display"">\[MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2\]</span></li>
-<li><strong>Root Mean Squared Error (RMSE)</strong>: square root of the MSE <span class=""math display"">\[RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}\]</span></li>
-<li><strong>MAE</strong>: average absolute difference between the predicted values and the actual values <span class=""math display"">\[MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|\]</span></li>
-<li><strong>Mean Absolute Percentage Error (MAPE)</strong>: average percentage difference between the predicted values and the actual values.</li>
-</ul>
+<!-- ## Evaluating regression -->
+<!-- The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include:  -->
+<!-- - **R-squared**: As seen in the linear regression session. -->
+<!-- $$ -->
+<!-- R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} -->
+<!-- $$ -->
+<!-- - **Adjusted R-squared**: seen before -->
+<!-- $$ -->
+<!-- R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1} -->
+<!-- $$ -->
+<!-- - **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values.  -->
+<!-- $$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$ -->
+<!-- - **Root Mean Squared Error (RMSE)**: square root of the MSE  -->
+<!-- $$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$ -->
+<!-- - **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$ -->
+<!-- - **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values. -->
 
 
 </section>
@@ -642,7 +706,7 @@ <h2 data-number=""1.7"" class=""anchored"" data-anchor-id=""evaluating-regression""><s
   </div>
   <div class=""nav-page nav-page-next"">
       <a href=""./KNN-demo.html"" class=""pagination-link"">
-        <span class=""nav-page-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: KNN model for classification</span></span> <i class=""bi bi-arrow-right-short""></i>
+        <span class=""nav-page-text""><span class=""chapter-number"">2</span>&nbsp; <span class=""chapter-title"">Demo: k-nearest neighbors (KNN)</span></span> <i class=""bi bi-arrow-right-short""></i>
       </a>
   </div>
 </nav>

---FILE: session-supervise/docs/search.json---
@@ -6,59 +6,59 @@
     ""section"": """",
     ""text"": ""Preface\nAims\n\nto introduce supervised learning for classification and regression\n\nLearning outcomes\n\nto be able to explain supervised learning\nto be able to split data into training, validation and test sets\nto be able to explain basic performance metrics for classification and regression\nto be able to use kknn() function to select the optimal value of \\(k\\) and build KNN classifier\n\nDo you see a mistake or a typo? We would be grateful if you let us know via edu.ml-biostats@nbis.se\nThis repository contains teaching and learning materials prepared for and used during “Introduction to biostatistics and Machine Learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use the basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/""
   },
+  {
+    ""objectID"": ""intro.html#examples-of-supervised-learning"",
+    ""href"": ""intro.html#examples-of-supervised-learning"",
+    ""title"": ""1  Supervised learning"",
+    ""section"": ""1.1 Examples of supervised learning"",
+    ""text"": ""1.1 Examples of supervised learning\n\nSupervised learning can be used for classification tasks.\n\nE.g. given a large amount of images of biopsies, where each image is marked as benign or malignant, we have trained a classification model. Now, given a new biopsy sample we can diagnose cancer as benign or malignant.\n\nAnd supervised learning can be used for regression tasks.\n\nE.g. using DNA samples from a diverse group of individuals across a wide range of ages, we measured methylation levels at thousands of CpG sites across the genome and trained a regression model by selecting hundreds of sites based on their potential relevance to aging. Now, given a new DNA samples and methylation measurements at these specific CpG sites, we can use the model to forecast individual’s age.""
+  },
   {
     ""objectID"": ""intro.html#what-is-supervised-learning"",
     ""href"": ""intro.html#what-is-supervised-learning"",
     ""title"": ""1  Supervised learning"",
-    ""section"": ""1.1 What is supervised learning?"",
-    ""text"": ""1.1 What is supervised learning?\n\nSupervised learning can be used for classification, e.g. given a new biopsy sample we want to tell whether it contains tumor tissue (Yes/No) and for regression, e.g. given a new measurements of the methylation sites we want to forecast epigenomic age.\nIn supervised learning we are using sample labels to train (build) a model. We then use the trained model for interpretation and prediction.\nThis is in contrast to previously discussed unsupervised learning such as clustering or PCA - methods that we were using to find patterns in the data. We treated data set a a whole, using measurements for all samples but not the samples labels such as sample groups to find the components with the highest variables (PCA) or the optimal number of clusters (k-means).\nTraining a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).\nCommon supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome.""
+    ""section"": ""1.2 What is supervised learning?"",
+    ""text"": ""1.2 What is supervised learning?\n\nIn supervised learning we are using sample labels to train (build) a model. We then use the trained model for interpretation and prediction.\nThis is in contrast to unsupervised learning, like clustering or Principal Component Analysis (PCA), that aims to discover patterns or groupings in the data without any predefined labels, such as identifying subtypes of a disease based on genetic variations.\n\n\n\nCode\nlibrary(knitr)\ninclude_graphics(\""images/supervised.png\"")\n\n\n\n\n\nFigure 1.1: Ilustration of supervised learning that focuses on training models to make predictions or decisions based on labeled training data.\n\n\n\n\n\nTraining a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).\nCommon supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome, i.e. classification and regression tasks.""
   },
   {
     ""objectID"": ""intro.html#outline"",
     ""href"": ""intro.html#outline"",
     ""title"": ""1  Supervised learning"",
-    ""section"": ""1.2 Outline"",
-    ""text"": ""1.2 Outline\nAcross many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:\n\ndeciding on the task: classification or regression\n\nsplitting data to keep part of data for training and part for testing\nselecting supervised machine learning algorithms to be trained (or a set of these)\ndeciding on the training strategy, i.e. which performance metrics to use and how to search for the best model parameters\nrunning feature engineering: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables\nperforming feature selection: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present\n\nThe diagram below shows a basic strategy on how to train KNN for classification, given a data set with \\(n\\) samples, \\(p\\) variables and \\(y\\) categorical outcome\n\n\n\n\nflowchart TD\n  A([data]) -. split data \\n e.g. basic, stratified, grouped -.-&gt; B([non-test set])\n  A([data]) -.-&gt; C([test set])\n  B -.-&gt; D(choose algorithm \\n e.g. KNN)\n  D -.-&gt; E(choose evaluation metric \\n e.g. overall accuracy)\n  E -.-&gt; F(feature engineering & selection)\n  F -.-&gt; G(prepare parameter space, e.g. odd k-values from 3 to 30)\n  G -. split non-test -.-&gt; H([train set & validation set])\n  H -.-&gt; J(fit model on train set)\n  J -.-&gt; K(collect evaluation metric on validation set)\n  K -.-&gt; L{all values checked? \\n e.g. k more than 30}\n  L -. No .-&gt; J\n  L -. Yes .-&gt; M(select best parameter values)\n  M -.-&gt; N(fit model on all non-test data)\n  N -.-&gt; O(assess model on test data)\n  C -.-&gt; O\n  \n\n\n\n\n\n\n \nBefore we see how this training may look like in R, let’s talk more about\n\nKNN, K-nearest neighbor algorithm\ndata splitting and\nperformance metrics useful for evaluating models\n\nAnd we will leave feature engineering and feature selection for the next session.""
+    ""section"": ""1.3 Outline"",
+    ""text"": ""1.3 Outline\nAcross many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:\n\ndeciding on the task: classification or regression\n\nsplitting data to keep part of data for training and part for testing\nselecting supervised machine learning algorithms to be trained (or a set of these)\ndeciding on the training strategy, i.e. which performance metrics to use and how to search for the best model parameters\nrunning feature engineering: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables\nperforming feature selection: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present\n\nThe diagram below shows a basic strategy on how to train KNN for classification, given a data set with \\(n\\) samples, \\(p\\) variables and \\(y\\) categorical outcome\n\n\n\n\nflowchart TD\n  A([data]) -. split data \\n e.g. basic, stratified, grouped -.-&gt; B([non-test set])\n  A([data]) -.-&gt; C([test set])\n  B -.-&gt; D(choose algorithm \\n e.g. KNN)\n  D -.-&gt; E(choose evaluation metric \\n e.g. overall accuracy)\n  E -.-&gt; F(feature engineering & selection)\n  F -.-&gt; G(prepare parameter space, e.g. odd k-values from 3 to 30)\n  G -. split non-test -.-&gt; H([train set & validation set])\n  H -.-&gt; J(fit model on train set)\n  J -.-&gt; K(collect evaluation metric on validation set)\n  K -.-&gt; L{all values checked? \\n e.g. k more than 30}\n  L -. No .-&gt; J\n  L -. Yes .-&gt; M(select best parameter values)\n  M -.-&gt; N(fit model on all non-test data)\n  N -.-&gt; O(assess model on test data)\n  C -.-&gt; O\n  \n\n\n\n\n\n\n \nBefore we see how this training may look like in R, let’s talk more about\n\nKNN, K-nearest neighbor algorithm\ndata splitting and\nperformance metrics useful for evaluating models\n\nAnd we will leave feature engineering and feature selection for the next session.""
   },
   {
     ""objectID"": ""intro.html#classification"",
     ""href"": ""intro.html#classification"",
     ""title"": ""1  Supervised learning"",
-    ""section"": ""1.3 Classification"",
-    ""text"": ""1.3 Classification\n\nClassification methods are algorithms used to categorize (classify) objects based on their measurements.\nThey belong under supervised learning as we usually start off with labeled data, i.e. observations with measurements for which we know the label (class) of.\nIf we have a pair \\(\\{\\mathbf{x_i}, g_i\\}\\) for each observation \\(i\\), with \\(g_i \\in \\{1, \\dots, G\\}\\) being the class label, where \\(G\\) is the number of different classes and \\(\\mathbf{x_i}\\) a set of exploratory variables, that can be continuous, categorical or a mix of both, then we want to find a classification rule \\(f(.)\\) (model) such that \\[f(\\mathbf{x_i})=g_i\\]""
+    ""section"": ""1.4 Classification"",
+    ""text"": ""1.4 Classification\n\nClassification methods are algorithms used to categorize (classify) objects based on their measurements.\nThey belong under supervised learning as we usually start off with labeled data, i.e. observations with measurements for which we know the label (class) of.\nIf we have a pair \\(\\{\\mathbf{x_i}, g_i\\}\\) for each observation \\(i\\), with \\(g_i \\in \\{1, \\dots, G\\}\\) being the class label, where \\(G\\) is the number of different classes and \\(\\mathbf{x_i}\\) a set of exploratory variables, that can be continuous, categorical or a mix of both, then we want to find a classification rule \\(f(.)\\) (model) such that \\[f(\\mathbf{x_i})=g_i\\]""
   },
   {
     ""objectID"": ""intro.html#knn-example"",
     ""href"": ""intro.html#knn-example"",
     ""title"": ""1  Supervised learning"",
-    ""section"": ""1.4 KNN example"",
-    ""text"": ""1.4 KNN example\n\n\n\n\n\nFigure 1.1: An example of k-nearest neighbors algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangle samples based on its gene A and gene B measurements and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurements and is thus classified by majority vote as a black dot (D).\n\n\n\n\nAlgorithm\n\nDecide on the value of \\(k\\)\nCalculate the distance between the query-instance (observations for new sample) and all the training samples\nSort the distances and determine the nearest neighbors based on the \\(k\\)-th minimum distance\nGather the categories of the nearest neighbors\nUse majority voting of the categories of the nearest neighbors as the prediction value for the new sample\n\nEuclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.""
+    ""section"": ""1.5 KNN example"",
+    ""text"": ""1.5 KNN example\n\n\n\n\n\nFigure 1.2: An example of k-nearest neighbors algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangle samples based on its gene A and gene B measurements and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurements and is thus classified by majority vote as a black dot (D).\n\n\n\n\nAlgorithm\n\nDecide on the value of \\(k\\)\nCalculate the distance between the query-instance (observations for new sample) and all the training samples\nSort the distances and determine the nearest neighbors based on the \\(k\\)-th minimum distance\nGather the categories of the nearest neighbors\nUse majority voting of the categories of the nearest neighbors as the prediction value for the new sample\n\nEuclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.""
   },
   {
     ""objectID"": ""intro.html#data-splitting"",
     ""href"": ""intro.html#data-splitting"",
     ""title"": ""1  Supervised learning"",
-    ""section"": ""1.5 Data splitting"",
-    ""text"": ""1.5 Data splitting\n\nPart of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\nAs a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to overfitting.\nTo deal with overconfident estimation of future performance we can implement various data splitting strategies.\n\n\ntrain, validation & test sets\n\nCommon split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\nTraining data: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule\nValidation data: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters\nTest data: this data is used to give an estimate of future prediction performance for the model and parameters chosen\n\n\n\n\n\n\nFigure 1.2: Example of splitting data into train (50%), validation (25%) and test (25%) set\n\n\n\n\n\n\ncross validation\n\nIt can happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data. e.g. gets all the difficult observation to classify.\nOr that we do not have enough data in each subset after performing the split.\nIn k-fold cross-validation we split data into \\(k\\) roughly equal-sized parts.\nWe start by setting the validation data to be the first set of data and the training data to be all other sets.\nWe estimate the validation error rate / correct classification rate for the split.\nWe then repeat the process \\(k-1\\) times, each time with a different part of the data set to be the validation data and the remainder being the training data.\nWe finish with \\(k\\) different error or correct classification rates.\nIn this way, every data point has its class membership predicted once.\nThe final reported error rate is usually the average of \\(k\\) error rates.\n\n\n\n\n\n\nFigure 1.3: Example of k-fold cross validation split (k = 3)\n\n\n\n\n\n\nrepeated cross validation\n\nIn repeated cross-validation we are repeating the cross-validation many times, e.g. we can create 5 validation folds 3 times\n\n\n\nLeave-one-out cross-validation\n\nLeave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n\n\n\n\nFigure 1.4: Example of LOOCV, leave-one-out cross validation""
+    ""section"": ""1.6 Data splitting"",
+    ""text"": ""1.6 Data splitting\n\nPart of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\nAs a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to overfitting.\nTo deal with overconfident estimation of future performance we can implement various data splitting strategies.\n\n\ntrain, validation & test sets\n\nCommon split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\nTraining data: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule\nValidation data: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters\nTest data: this data is used to give an estimate of future prediction performance for the model and parameters chosen\n\n\n\n\n\n\nFigure 1.3: Example of splitting data into train (50%), validation (25%) and test (25%) set\n\n\n\n\n\n\ncross validation\n\nIt can happen that despite random splitting in train/validation/test dataset one of the subsets does not represent data. e.g. gets all the difficult observation to classify.\nOr that we do not have enough data in each subset after performing the split.\nIn k-fold cross-validation we split data into \\(k\\) roughly equal-sized parts.\nWe start by setting the validation data to be the first set of data and the training data to be all other sets.\nWe estimate the validation error rate / correct classification rate for the split.\nWe then repeat the process \\(k-1\\) times, each time with a different part of the data set to be the validation data and the remainder being the training data.\nWe finish with \\(k\\) different error or correct classification rates.\nIn this way, every data point has its class membership predicted once.\nThe final reported error rate is usually the average of \\(k\\) error rates.\n\n\n\n\n\n\nFigure 1.4: Example of k-fold cross validation split (k = 3)\n\n\n\n\n\n\nrepeated cross validation\n\nIn repeated cross-validation we are repeating the cross-validation many times, e.g. we can create 5 validation folds 3 times\n\n\n\nLeave-one-out cross-validation\n\nLeave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n\n\n\n\nFigure 1.5: Example of LOOCV, leave-one-out cross validation""
   },
   {
     ""objectID"": ""intro.html#evaluating-classification"",
     ""href"": ""intro.html#evaluating-classification"",
     ""title"": ""1  Supervised learning"",
-    ""section"": ""1.6 Evaluating classification"",
-    ""text"": ""1.6 Evaluating classification\n\nTo train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of \\(k\\) in KNN.\nThere are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\nCommon measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\nCorrect (miss)classification rate\n\nThe simplest way to evaluate in which we count for all the \\(n\\) predictions how many times we got the classification right. \\[Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}\\] where \\(1[]\\) is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\nMissclassification Rate\nMissclassification Rate = 1 - Correct Classification Rate\nConfusion matrix\nConfusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes. For a binary classifier we have:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nBased on the confusion matrix, we can derive common performance metrics of a binary classifier:\n\nAccuracy: measures the proportion of correctly classified samples over the total number of samples. \\[ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\].\nSensitivity: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as true positive rate and recall. \\[TPR = \\frac{TP}{TP + FN}\\]\nSpecificity: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as true negative rate and selectivity. \\[TNR = \\frac{TN}{TN+FP}\\]\nPrecision: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as positive predictive value \\[PPV = \\frac{TP}{TP + FP}\\]\nROC AUC: the receiver operating characteristic (ROC) curve is a graphical representation of the trade off between sensitivity and specificity for different threshold values. The area under the ROC curve (AUC) is a performance metric that ranges from 0 to 1, with a higher value indicating better performance. AUC is a measure of how well the classifier is able to distinguish between positive and negative samples.""
-  },
-  {
-    ""objectID"": ""intro.html#evaluating-regression"",
-    ""href"": ""intro.html#evaluating-regression"",
-    ""title"": ""1  Supervised learning"",
-    ""section"": ""1.7 Evaluating regression"",
-    ""text"": ""1.7 Evaluating regression\nThe idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include:\n\nR-squared: As seen in the linear regression session. \\[\nR^2=1-\\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n\\]\nAdjusted R-squared: seen before \\[\nR_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\frac{n-1}{n-p-1}\n\\]\nMean Squared Error (MSE): average squared difference between the predicted values and the actual values. \\[MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2\\]\nRoot Mean Squared Error (RMSE): square root of the MSE \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}\\]\nMAE: average absolute difference between the predicted values and the actual values \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|\\]\nMean Absolute Percentage Error (MAPE): average percentage difference between the predicted values and the actual values.""
+    ""section"": ""1.7 Evaluating classification"",
+    ""text"": ""1.7 Evaluating classification\n\nTo train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of \\(k\\) in KNN.\nThere are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\nCommon measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\nCorrect (miss)classification rate\n\nThe simplest way to evaluate in which we count for all the \\(n\\) predictions how many times we got the classification right. \\[Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}\\] where \\(1[]\\) is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\nMissclassification Rate\nMissclassification Rate = 1 - Correct Classification Rate\nConfusion matrix\nConfusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes. For a binary classifier we have:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nBased on the confusion matrix, we can derive common performance metrics of a binary classifier:\n\nAccuracy: measures the proportion of correctly classified samples over the total number of samples. \\[ACC = \\frac{TP+TN}{TP+TN+FP+FN}\\].\nSensitivity: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as true positive rate and recall. \\[TPR = \\frac{TP}{TP + FN}\\]\nSpecificity: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as true negative rate and selectivity. \\[TNR = \\frac{TN}{TN+FP}\\]\nPrecision: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as positive predictive value \\[PPV = \\frac{TP}{TP + FP}\\]\nROC AUC: the receiver operating characteristic (ROC) curve is a graphical representation of the trade off between sensitivity and specificity for different threshold values. The area under the ROC curve (AUC) is a performance metric that ranges from 0 to 1, with a higher value indicating better performance. AUC is a measure of how well the classifier is able to distinguish between positive and negative samples.""
   },
   {
     ""objectID"": ""KNN-demo.html"",
     ""href"": ""KNN-demo.html"",
-    ""title"": ""2  Demo: KNN model for classification"",
+    ""title"": ""2  Demo: k-nearest neighbors (KNN)"",
     ""section"": """",
     ""text"": ""Let’s try to build a classifier to predict obesity (Obese vs Non-obese) given our diabetes data set. To start simple:\n\nwe will see how well we can predict obesity given waist and hdl variables\nwe will use data splitting into train, validation and test, i.e. not cross-validation with the help of splitTools() library\nwe will use KNN algorithm as implemented in kknn() function in library(kknn)\n\nReading in data\n\n# load libraries\nlibrary(tidyverse)\nlibrary(splitTools)\nlibrary(kknn)\n\n# input data\ninput_diabetes &lt;- read_csv(\""data/data-diabetes.csv\"")\n\n# clean data\ninch2cm &lt;- 2.54\npound2kg &lt;- 0.45\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2cm) %&gt;% \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;%\n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(diabetic = ifelse(glyhb &gt; 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(location = factor(location)) %&gt;%\n  mutate(frame = factor(frame)) %&gt;%\n  mutate(gender = factor(gender))\n  \n# select data for KNN\ndata_input &lt;- data_diabetes %&gt;%\n  select(obese, waist, hdl) %&gt;%\n  na.omit()\n\n# How many obese and non-obese in our data set?\ndata_input %&gt;%\n  count(obese)\n## # A tibble: 2 × 2\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No      250\n## 2 Yes     144\n\n# preview data\nglimpse(data_input)\n## Rows: 394\n## Columns: 3\n## $ obese &lt;fct&gt; No, Yes, Yes, No, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, N…\n## $ waist &lt;dbl&gt; 73.66, 116.84, 124.46, 83.82, 111.76, 91.44, 116.84, 86.36, 86.3…\n## $ hdl   &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 47, …\n\ndata_input %&gt;%\n  ggplot(aes(x = waist, y = hdl, fill = obese)) + \n  geom_point(shape=21, alpha = 0.7, size = 2) + \n  theme_bw() + \n  scale_fill_manual(values = c(\""blue3\"", \""brown1\"")) + \n  theme(legend.position = \""top\"")\n\n\n\n\n\n\n\n\nSplitting data\n\n# split data into train (40%), validation (40%) and test (20%)\n# stratify by obese\nrandseed &lt;- 123\nset.seed(randseed)\ninds &lt;- partition(data_input$obese, p = c(train = 0.4, valid = 0.4, test = 0.2), seed = randseed)\nstr(inds)\n## List of 3\n##  $ train: int [1:158] 6 8 11 16 17 22 26 28 32 35 ...\n##  $ valid: int [1:157] 1 3 5 7 9 10 13 14 15 24 ...\n##  $ test : int [1:79] 2 4 12 18 19 20 21 23 27 30 ...\ndata_train &lt;- data_input[inds$train, ]\ndata_valid &lt;- data_input[inds$valid,]\ndata_test &lt;- data_input[inds$test, ]\n\n# check dimensions of data\ndata_train %&gt;% dim()\n## [1] 158   3\ndata_valid %&gt;% dim()\n## [1] 157   3\ndata_test %&gt;% dim()\n## [1] 79  3\n\n# check distribution of obese and non-obese\ndata_train %&gt;%\n  group_by(obese) %&gt;%\n  count()\n## # A tibble: 2 × 2\n## # Groups:   obese [2]\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No      100\n## 2 Yes      58\n\ndata_valid %&gt;%\n  group_by(obese) %&gt;%\n  count()\n## # A tibble: 2 × 2\n## # Groups:   obese [2]\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No      100\n## 2 Yes      57\n\ndata_test %&gt;%\n  group_by(obese) %&gt;%\n  count()\n## # A tibble: 2 × 2\n## # Groups:   obese [2]\n##   obese     n\n##   &lt;fct&gt; &lt;int&gt;\n## 1 No       50\n## 2 Yes      29\n\nTraining KNN\n\n# prepare parameters search space\nn &lt;- nrow(data_train)\nk_values &lt;- seq(1, n-1, 2) # check every odd value of k between 1 and number of samples-1\n\n# allocate empty vector to collect overall classification rate for each k\ncls_rate &lt;- rep(0, length(k_values)) \n\nfor (l in seq_along(k_values))\n{\n  \n  # fit model given k value\n  model &lt;- kknn(obese ~., data_train, data_valid, \n                k = k_values[l], \n                kernel = \""rectangular\"")\n  \n  # extract predicted class (predicted obesity status)\n  cls_pred &lt;- model$fitted.values\n  \n  # define actual class (actual obesity status)\n  cls_true &lt;- data_valid$obese\n  \n  # calculate overall classification rate\n  cls_rate[l] &lt;- sum((cls_pred == cls_true))/length(cls_pred)\n  \n}\n\nSelecting best \\(k\\)\n\n# plot classification rate as a function of k\nplot(k_values, cls_rate, type=\""l\"", xlab=\""k\"", ylab=\""cls rate\"")\n\n\n\n\nOverall classification rate as a function of k\n\n\n\n# For which value of k do we reach the highest classification rate?\nk_best &lt;- k_values[which.max(cls_rate)]\nprint(k_best)\n## [1] 51\n\nFinal model and performance on future unseen data (test data)\n\n# How would our model perform on the future data using the optimal k?\nmodel_final &lt;- kknn(obese ~., data_train, data_test, k=k_best, kernel = \""rectangular\"")\ncls_pred &lt;- model_final$fitted.values\ncls_true &lt;- data_test$obese\n\ncls_rate &lt;- sum((cls_pred == cls_true))/length(cls_pred)\nprint(cls_rate)\n## [1] 0.8481013""
   },
@@ -67,13 +67,6 @@
     ""href"": ""exercises.html#answers-to-exercises"",
     ""title"": ""Exercises"",
     ""section"": ""Answers to exercises"",
-    ""text"": ""Answers to exercises\n\nSolution. Exercise 1\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(splitTools)\nlibrary(kknn)\n\n# input data\ninput_diabetes &lt;- read_csv(\""data/data-diabetes.csv\"")\n\n# clean data\ninch2cm &lt;- 2.54\npound2kg &lt;- 0.45\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2cm) %&gt;% \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;%\n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(diabetic = ifelse(glyhb &gt; 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(location = factor(location)) %&gt;%\n  mutate(frame = factor(frame)) %&gt;%\n  mutate(gender = factor(gender))\n  \n# select data for KNN\ndata_input &lt;- data_diabetes %&gt;%\n  select(obese, waist, hdl) %&gt;%\n  na.omit()\n\n# set random seed\nrandseed &lt;- 123\nset.seed(randseed)\n\n# split data into other (non-test) and test\nsplits &lt;- partition(data_input$obese, p = c(other = 0.80, test = 0.20))\ndata_test &lt;- data_input[splits$test, ]\ndata_other &lt;- data_input[splits$other, ]\n\n# create train and validation folds\nkfolds_train &lt;- create_folds(data_other$obese, k = 5, seed = randseed)\nkfolds_valid &lt;- create_folds(data_other$obese, k = 5, \n                             invert = TRUE, # gives back indices of the validation samples\n                             seed = randseed) # OBS! use the same seed as above in kfolds_train()\n\n\n# prepare parameters search space\nn &lt;- nrow(data_other)\nk_values &lt;- seq(1, 100, 2) # check every odd value of k between 1 and 50\n\n# allocate empty matrix to collect overall classification rate for each k and 5-folds\nfolds &lt;- 5\ncls_rate &lt;- matrix(data = NA, ncol = folds, nrow = length(k_values))\ncolnames(cls_rate) &lt;- paste(\""kfold\"", 1:folds, sep=\""\"")\nrownames(cls_rate) &lt;- paste(\""k\"", k_values, sep=\""\"")\n\nfor (k in seq_along(k_values))\n{\n  # for every value of k \n  # fit model on every train fold and evaluate on every validation fold\n  for (f in 1:folds){\n    \n    data_train &lt;- data_other[kfolds_train[[f]], ]\n    data_valid &lt;- data_other[kfolds_valid[[f]], ]\n    \n    # fit model given k value\n    model &lt;- kknn(obese ~., data_train, data_valid, \n                k = k_values[k], \n                kernel = \""rectangular\"")\n    \n    # extract predicted class (predicted obesity status)\n    cls_pred &lt;- model$fitted.values\n  \n    # define actual class (actual obesity status)\n    cls_true &lt;- data_valid$obese\n  \n    # calculate overall classification rate\n    cls_rate[k, f] &lt;- sum((cls_pred == cls_true))/length(cls_pred)\n    \n  }\n  \n}\n\n# plot average classification rate (across folds) as a function of k\ncls_rate_avg &lt;- apply(cls_rate, 1, mean)\nplot(k_values, cls_rate_avg, type=\""l\"", xlab=\""k\"", ylab=\""cls rate (avg)\"")\n\n\n\n\n\n\n\n\n# For which value of k do we reach the highest classification rate?\nk_best &lt;- k_values[which.max(cls_rate_avg)]\nprint(k_best)\n## [1] 71\n\n# How would our model perform on the future data using the optimal k?\nmodel_final &lt;- kknn(obese ~., data_other, data_test, k=k_best, kernel = \""rectangular\"")\ncls_pred &lt;- model_final$fitted.values\ncls_true &lt;- data_test$obese\n\ncls_rate &lt;- sum((cls_pred == cls_true))/length(cls_pred)\nprint(cls_rate)\n## [1] 0.8481013\n\n\nSolution. Exercise 2\n\n\n# input and clean data\n# load libraries\nlibrary(tidyverse)\nlibrary(splitTools)\nlibrary(kknn)\n\n# input data\ninput_diabetes &lt;- read_csv(\""data/data-diabetes.csv\"")\n\n# clean data\ninch2cm &lt;- 2.54\npound2kg &lt;- 0.45\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2cm) %&gt;% \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;%\n  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(diabetic = ifelse(glyhb &gt; 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(location = factor(location)) %&gt;%\n  mutate(frame = factor(frame)) %&gt;%\n  mutate(gender = factor(gender))\n\n# select relevant data and remove missing data\ndata_input &lt;- \n  data_diabetes %&gt;%\n  select(BMI, age, hdl, waist) %&gt;%\n  na.omit()\nglimpse(data_input)\n## Rows: 394\n## Columns: 4\n## $ BMI   &lt;dbl&gt; 22.09, 36.92, 47.95, 18.53, 27.52, 26.39, 28.07, 34.00, 24.39, 3…\n## $ age   &lt;dbl&gt; 46, 29, 58, 67, 64, 34, 30, 37, 45, 55, 60, 38, 27, 40, 36, 33, …\n## $ hdl   &lt;dbl&gt; 56, 24, 37, 12, 28, 69, 41, 44, 49, 40, 54, 34, 36, 46, 30, 47, …\n## $ waist &lt;dbl&gt; 73.66, 116.84, 124.46, 83.82, 111.76, 91.44, 116.84, 86.36, 86.3…\n\n# define calculate_rmse() function\ncalculate_rmse &lt;- function(y_true, y_pred){\n  rmse = sqrt(mean((y_true - y_pred)^2))\n}\n\n# split into train, validation and test: stratify by BMI\nrandseed &lt;- 123\ninds &lt;- partition(data_input$BMI, \n                  p = c(train = 0.6, valid = 0.2, test = 0.2),\n                  seed = randseed)\n\ndata_train &lt;- data_input[inds$train, ]\ndata_valid &lt;- data_input[inds$valid,]\ndata_test &lt;- data_input[inds$test, ]\n\n# Model 1\nm1 &lt;- lm(BMI ~ age, data = data_train) # fit model on train\npred_bmi_1 &lt;- predict(m1, newdata = data_valid) # predict BMI using validation set\nm1_rmse &lt;- calculate_rmse(data_valid$BMI, pred_bmi_1) # calculate RMSE\n\n# Model 2\nm2 &lt;- lm(BMI ~ age + hdl, data = data_train) \npred_bmi_2 &lt;- predict(m2, newdata = data_valid) \nm2_rmse &lt;- calculate_rmse(data_valid$BMI, pred_bmi_2) \n\n# Model 3\nm3 &lt;- lm(BMI ~ age + hdl + waist, data = data_train) \npred_bmi_3 &lt;- predict(m3, newdata = data_valid) \nm3_rmse &lt;- calculate_rmse(data_valid$BMI, pred_bmi_3) \n\n# Compare models\nrmse &lt;- data.frame(model = c(\""Model 1\"", \""Model 2\"", \""Model 3\""), rmse = c(m1_rmse, m2_rmse, m3_rmse))\nprint(rmse)\n##     model     rmse\n## 1 Model 1 6.206929\n## 2 Model 2 5.879602\n## 3 Model 3 3.418270\n\n# Out of the three models, Model 3, has the smallest RMSE and is thus selected as best\n\n# Expected performance on the test data\npred_bmi_final &lt;- predict(m3, newdata = data_test)\nrmse_final &lt;- calculate_rmse(data_test$BMI, pred_bmi_final)\nprint(rmse_final)\n## [1] 3.180441""
-  },
-  {
-    ""objectID"": ""intro.html#live-demo"",
-    ""href"": ""intro.html#live-demo"",
-    ""title"": ""1  Supervised learning"",
-    ""section"": ""1.8 Live demo"",
-    ""text"": ""1.8 Live demo""
+    ""text"": ""Answers to exercises\n\nSolution. Exercise 1\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(splitTools)\nlibrary(kknn)\n\n# input data\ninput_diabetes &lt;- read_csv(\""data/data-diabetes.csv\"")\n\n# clean data\ninch2cm &lt;- 2.54\npound2kg &lt;- 0.45\ndata_diabetes &lt;- input_diabetes %&gt;%\n  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %&gt;% \n  mutate(waist = waist * inch2cm) %&gt;% \n  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %&gt;%\n  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %&gt;%\n  mutate(obese = cut(BMI, breaks = c(0, 29.9, 100), labels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(diabetic = ifelse(glyhb &gt; 7, \""Yes\"", \""No\""), diabetic = factor(diabetic, levels = c(\""No\"", \""Yes\""))) %&gt;%\n  mutate(location = factor(location)) %&gt;%\n  mutate(frame = factor(frame)) %&gt;%\n  mutate(gender = factor(gender))\n  \n# select data for KNN\ndata_input &lt;- data_diabetes %&gt;%\n  select(obese, waist, hdl) %&gt;%\n  na.omit()\n\n# set random seed\nrandseed &lt;- 123\nset.seed(randseed)\n\n# split data into other (non-test) and test\nsplits &lt;- partition(data_input$obese, p = c(other = 0.80, test = 0.20))\ndata_test &lt;- data_input[splits$test, ]\ndata_other &lt;- data_input[splits$other, ]\n\n# create train and validation folds\nkfolds_train &lt;- create_folds(data_other$obese, k = 5, seed = randseed)\nkfolds_valid &lt;- create_folds(data_other$obese, k = 5, \n                             invert = TRUE, # gives back indices of the validation samples\n                             seed = randseed) # OBS! use the same seed as above in kfolds_train()\n\n\n# prepare parameters search space\nn &lt;- nrow(data_other)\nk_values &lt;- seq(1, 100, 2) # check every odd value of k between 1 and 50\n\n# allocate empty matrix to collect overall classification rate for each k and 5-folds\nfolds &lt;- 5\ncls_rate &lt;- matrix(data = NA, ncol = folds, nrow = length(k_values))\ncolnames(cls_rate) &lt;- paste(\""kfold\"", 1:folds, sep=\""\"")\nrownames(cls_rate) &lt;- paste(\""k\"", k_values, sep=\""\"")\n\nfor (k in seq_along(k_values))\n{\n  # for every value of k \n  # fit model on every train fold and evaluate on every validation fold\n  for (f in 1:folds){\n    \n    data_train &lt;- data_other[kfolds_train[[f]], ]\n    data_valid &lt;- data_other[kfolds_valid[[f]], ]\n    \n    # fit model given k value\n    model &lt;- kknn(obese ~., data_train, data_valid, \n                k = k_values[k], \n                kernel = \""rectangular\"")\n    \n    # extract predicted class (predicted obesity status)\n    cls_pred &lt;- model$fitted.values\n  \n    # define actual class (actual obesity status)\n    cls_true &lt;- data_valid$obese\n  \n    # calculate overall classification rate\n    cls_rate[k, f] &lt;- sum((cls_pred == cls_true))/length(cls_pred)\n    \n  }\n  \n}\n\n# plot average classification rate (across folds) as a function of k\ncls_rate_avg &lt;- apply(cls_rate, 1, mean)\nplot(k_values, cls_rate_avg, type=\""l\"", xlab=\""k\"", ylab=\""cls rate (avg)\"")\n\n\n\n\n\n\n\n\n# For which value of k do we reach the highest classification rate?\nk_best &lt;- k_values[which.max(cls_rate_avg)]\nprint(k_best)\n## [1] 71\n\n# How would our model perform on the future data using the optimal k?\nmodel_final &lt;- kknn(obese ~., data_other, data_test, k=k_best, kernel = \""rectangular\"")\ncls_pred &lt;- model_final$fitted.values\ncls_true &lt;- data_test$obese\n\ncls_rate &lt;- sum((cls_pred == cls_true))/length(cls_pred)\nprint(cls_rate)\n## [1] 0.8481013""
   }
 ]
\ No newline at end of file

---FILE: session-supervise/docs/site_libs/quarto-html/quarto.js---
@@ -5,7 +5,52 @@ const sectionChanged = new CustomEvent(""quarto-sectionChanged"", {
   composed: false,
 });
 
+const layoutMarginEls = () => {
+  // Find any conflicting margin elements and add margins to the
+  // top to prevent overlap
+  const marginChildren = window.document.querySelectorAll(
+    "".column-margin.column-container > * ""
+  );
+
+  let lastBottom = 0;
+  for (const marginChild of marginChildren) {
+    if (marginChild.offsetParent !== null) {
+      // clear the top margin so we recompute it
+      marginChild.style.marginTop = null;
+      const top = marginChild.getBoundingClientRect().top + window.scrollY;
+      console.log({
+        childtop: marginChild.getBoundingClientRect().top,
+        scroll: window.scrollY,
+        top,
+        lastBottom,
+      });
+      if (top < lastBottom) {
+        const margin = lastBottom - top;
+        marginChild.style.marginTop = `${margin}px`;
+      }
+      const styles = window.getComputedStyle(marginChild);
+      const marginTop = parseFloat(styles[""marginTop""]);
+
+      console.log({
+        top,
+        height: marginChild.getBoundingClientRect().height,
+        marginTop,
+        total: top + marginChild.getBoundingClientRect().height + marginTop,
+      });
+      lastBottom = top + marginChild.getBoundingClientRect().height + marginTop;
+    }
+  }
+};
+
 window.document.addEventListener(""DOMContentLoaded"", function (_event) {
+  // Recompute the position of margin elements anytime the body size changes
+  if (window.ResizeObserver) {
+    const resizeObserver = new window.ResizeObserver(
+      throttle(layoutMarginEls, 50)
+    );
+    resizeObserver.observe(window.document.body);
+  }
+
   const tocEl = window.document.querySelector('nav.toc-active[role=""doc-toc""]');
   const sidebarEl = window.document.getElementById(""quarto-sidebar"");
   const leftTocEl = window.document.getElementById(""quarto-sidebar-toc-left"");
@@ -431,32 +476,6 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
     };
   };
 
-  // Find any conflicting margin elements and add margins to the
-  // top to prevent overlap
-  const marginChildren = window.document.querySelectorAll(
-    "".column-margin.column-container > * ""
-  );
-
-  const layoutMarginEls = () => {
-    let lastBottom = 0;
-    for (const marginChild of marginChildren) {
-      if (marginChild.offsetParent !== null) {
-        // clear the top margin so we recompute it
-        marginChild.style.marginTop = null;
-        const top = marginChild.getBoundingClientRect().top + window.scrollY;
-        if (top < lastBottom) {
-          const margin = lastBottom - top;
-          marginChild.style.marginTop = `${margin}px`;
-        }
-        const styles = window.getComputedStyle(marginChild);
-        const marginTop = parseFloat(styles[""marginTop""]);
-        lastBottom =
-          top + marginChild.getBoundingClientRect().height + marginTop;
-      }
-    }
-  };
-  nexttick(layoutMarginEls);
-
   const tabEls = document.querySelectorAll('a[data-bs-toggle=""tab""]');
   for (const tabEl of tabEls) {
     const id = tabEl.getAttribute(""data-bs-target"");
@@ -466,7 +485,6 @@ window.document.addEventListener(""DOMContentLoaded"", function (_event) {
       );
       if (columnEl)
         tabEl.addEventListener(""shown.bs.tab"", function (event) {
-
           const el = event.srcElement;
           if (el) {
             const visibleCls = `${el.id}-margin-content`;

---FILE: session-supervise/exercises.qmd---
@@ -15,19 +15,19 @@ Hint: you can use ""create_folds()"" function from ""library(splitTools)"" to create
 
 :::
 
-::: {#exr-lm}
+<!-- ::: {#exr-lm} -->
 
-## Supervised regression
+<!-- ## Supervised regression -->
 
-Let's revisit regression in a context of supervised learning. Using the `diabetes` data set and data splitting find the best model to predict BMI scores. 
+<!-- Let's revisit regression in a context of supervised learning. Using the `diabetes` data set and data splitting find the best model to predict BMI scores.  -->
 
-Split the data into train (60%), validation (20%) and test (20%). Assess three regression models on the validation set using RMSE. Which model seems to be the best in terms of predicting BMI? What would be the expected performance on the new unseen data?
- 
-1. Model 1: use `age` as only as predictor
-2. Model 2: use `age`, `hdl` as predictors
-3. Model 3: use `age`, `hdl` and `waist` as predictors
+<!-- Split the data into train (60%), validation (20%) and test (20%). Assess three regression models on the validation set using RMSE. Which model seems to be the best in terms of predicting BMI? What would be the expected performance on the new unseen data? -->
 
-:::
+<!-- 1. Model 1: use `age` as only as predictor -->
+<!-- 2. Model 2: use `age`, `hdl` as predictors -->
+<!-- 3. Model 3: use `age`, `hdl` and `waist` as predictors -->
+
+<!-- ::: -->
 
 
 ## Answers to exercises {.unnumbered}
@@ -62,7 +62,7 @@ data_diabetes <- input_diabetes %>%
   mutate(waist = waist * inch2cm) %>% 
   mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%
   mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>%
-  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(""No"", ""Yes""))) %>%
+  mutate(obese = cut(BMI, breaks = c(0, 29.9, 100), labels = c(""No"", ""Yes""))) %>%
   mutate(diabetic = ifelse(glyhb > 7, ""Yes"", ""No""), diabetic = factor(diabetic, levels = c(""No"", ""Yes""))) %>%
   mutate(location = factor(location)) %>%
   mutate(frame = factor(frame)) %>%
@@ -145,90 +145,90 @@ print(cls_rate)
 ```
 
 
-::: {.solution}
-@exr-lm
-:::
-
-```{r}
-#| label: supervised-regression
-#| warning: false
-#| message: false
-#| fig-width: 7
-#| fig-height: 8
-#| fig-align: center
-#| code-fold: false
-#| collapse: true
-
-# input and clean data
-# load libraries
-library(tidyverse)
-library(splitTools)
-library(kknn)
-
-# input data
-input_diabetes <- read_csv(""data/data-diabetes.csv"")
-
-# clean data
-inch2cm <- 2.54
-pound2kg <- 0.45
-data_diabetes <- input_diabetes %>%
-  mutate(height  = height * inch2cm / 100, height = round(height, 2)) %>% 
-  mutate(waist = waist * inch2cm) %>% 
-  mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>%
-  mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>%
-  mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(""No"", ""Yes""))) %>%
-  mutate(diabetic = ifelse(glyhb > 7, ""Yes"", ""No""), diabetic = factor(diabetic, levels = c(""No"", ""Yes""))) %>%
-  mutate(location = factor(location)) %>%
-  mutate(frame = factor(frame)) %>%
-  mutate(gender = factor(gender))
-
-# select relevant data and remove missing data
-data_input <- 
-  data_diabetes %>%
-  select(BMI, age, hdl, waist) %>%
-  na.omit()
-glimpse(data_input)
-
-# define calculate_rmse() function
-calculate_rmse <- function(y_true, y_pred){
-  rmse = sqrt(mean((y_true - y_pred)^2))
-}
-
-# split into train, validation and test: stratify by BMI
-randseed <- 123
-inds <- partition(data_input$BMI, 
-                  p = c(train = 0.6, valid = 0.2, test = 0.2),
-                  seed = randseed)
-
-data_train <- data_input[inds$train, ]
-data_valid <- data_input[inds$valid,]
-data_test <- data_input[inds$test, ]
-
-# Model 1
-m1 <- lm(BMI ~ age, data = data_train) # fit model on train
-pred_bmi_1 <- predict(m1, newdata = data_valid) # predict BMI using validation set
-m1_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_1) # calculate RMSE
-
-# Model 2
-m2 <- lm(BMI ~ age + hdl, data = data_train) 
-pred_bmi_2 <- predict(m2, newdata = data_valid) 
-m2_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_2) 
-
-# Model 3
-m3 <- lm(BMI ~ age + hdl + waist, data = data_train) 
-pred_bmi_3 <- predict(m3, newdata = data_valid) 
-m3_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_3) 
-
-# Compare models
-rmse <- data.frame(model = c(""Model 1"", ""Model 2"", ""Model 3""), rmse = c(m1_rmse, m2_rmse, m3_rmse))
-print(rmse)
-
-# Out of the three models, Model 3, has the smallest RMSE and is thus selected as best
-
-# Expected performance on the test data
-pred_bmi_final <- predict(m3, newdata = data_test)
-rmse_final <- calculate_rmse(data_test$BMI, pred_bmi_final)
-print(rmse_final)
-
-```
+<!-- ::: {.solution} -->
+<!-- @exr-lm -->
+<!-- ::: -->
+
+<!-- ```{r} -->
+<!-- #| label: supervised-regression -->
+<!-- #| warning: false -->
+<!-- #| message: false -->
+<!-- #| fig-width: 7 -->
+<!-- #| fig-height: 8 -->
+<!-- #| fig-align: center -->
+<!-- #| code-fold: false -->
+<!-- #| collapse: true -->
+
+<!-- # input and clean data -->
+<!-- # load libraries -->
+<!-- library(tidyverse) -->
+<!-- library(splitTools) -->
+<!-- library(kknn) -->
+
+<!-- # input data -->
+<!-- input_diabetes <- read_csv(""data/data-diabetes.csv"") -->
+
+<!-- # clean data -->
+<!-- inch2cm <- 2.54 -->
+<!-- pound2kg <- 0.45 -->
+<!-- data_diabetes <- input_diabetes %>% -->
+<!--   mutate(height  = height * inch2cm / 100, height = round(height, 2)) %>%  -->
+<!--   mutate(waist = waist * inch2cm) %>%  -->
+<!--   mutate(weight = weight * pound2kg, weight = round(weight, 2)) %>% -->
+<!--   mutate(BMI = weight / height^2, BMI = round(BMI, 2)) %>% -->
+<!--   mutate(obese= cut(BMI, breaks = c(0, 29.9, 100), labels = c(""No"", ""Yes""))) %>% -->
+<!--   mutate(diabetic = ifelse(glyhb > 7, ""Yes"", ""No""), diabetic = factor(diabetic, levels = c(""No"", ""Yes""))) %>% -->
+<!--   mutate(location = factor(location)) %>% -->
+<!--   mutate(frame = factor(frame)) %>% -->
+<!--   mutate(gender = factor(gender)) -->
+
+<!-- # select relevant data and remove missing data -->
+<!-- data_input <-  -->
+<!--   data_diabetes %>% -->
+<!--   select(BMI, age, hdl, waist) %>% -->
+<!--   na.omit() -->
+<!-- glimpse(data_input) -->
+
+<!-- # define calculate_rmse() function -->
+<!-- calculate_rmse <- function(y_true, y_pred){ -->
+<!--   rmse = sqrt(mean((y_true - y_pred)^2)) -->
+<!-- } -->
+
+<!-- # split into train, validation and test: stratify by BMI -->
+<!-- randseed <- 123 -->
+<!-- inds <- partition(data_input$BMI,  -->
+<!--                   p = c(train = 0.6, valid = 0.2, test = 0.2), -->
+<!--                   seed = randseed) -->
+
+<!-- data_train <- data_input[inds$train, ] -->
+<!-- data_valid <- data_input[inds$valid,] -->
+<!-- data_test <- data_input[inds$test, ] -->
+
+<!-- # Model 1 -->
+<!-- m1 <- lm(BMI ~ age, data = data_train) # fit model on train -->
+<!-- pred_bmi_1 <- predict(m1, newdata = data_valid) # predict BMI using validation set -->
+<!-- m1_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_1) # calculate RMSE -->
+
+<!-- # Model 2 -->
+<!-- m2 <- lm(BMI ~ age + hdl, data = data_train)  -->
+<!-- pred_bmi_2 <- predict(m2, newdata = data_valid)  -->
+<!-- m2_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_2)  -->
+
+<!-- # Model 3 -->
+<!-- m3 <- lm(BMI ~ age + hdl + waist, data = data_train)  -->
+<!-- pred_bmi_3 <- predict(m3, newdata = data_valid)  -->
+<!-- m3_rmse <- calculate_rmse(data_valid$BMI, pred_bmi_3)  -->
+
+<!-- # Compare models -->
+<!-- rmse <- data.frame(model = c(""Model 1"", ""Model 2"", ""Model 3""), rmse = c(m1_rmse, m2_rmse, m3_rmse)) -->
+<!-- print(rmse) -->
+
+<!-- # Out of the three models, Model 3, has the smallest RMSE and is thus selected as best -->
+
+<!-- # Expected performance on the test data -->
+<!-- pred_bmi_final <- predict(m3, newdata = data_test) -->
+<!-- rmse_final <- calculate_rmse(data_test$BMI, pred_bmi_final) -->
+<!-- print(rmse_final) -->
+
+<!-- ``` -->
 

---FILE: session-supervise/intro.qmd---
@@ -6,13 +6,32 @@ editor_options:
 
 # Supervised learning
 
+## Examples of supervised learning
+
+-   Supervised learning can be used for **classification** tasks. 
+    - E.g. given a large amount of images of biopsies, where each image is marked as benign or malignant, we have trained a classification model. Now, given a new biopsy sample we can diagnose cancer as benign or malignant. 
+-   And supervised learning can be used for **regression** tasks. 
+    - E.g. using DNA samples from a diverse group of individuals across a wide range of ages, we measured methylation levels at thousands of CpG sites across the genome and trained a regression model by selecting hundreds of sites based on their potential relevance to aging. Now, given a new DNA samples and methylation measurements at these specific CpG sites, we can use the model to forecast individual's age. 
+    
+
 ## What is supervised learning?
 
--   Supervised learning can be used for classification, e.g. given a new biopsy sample we want to tell whether it contains tumor tissue (Yes/No) and for regression, e.g. given a new measurements of the methylation sites we want to forecast epigenomic age.
--   In supervised learning we are using sample **labels** to **train** (build) a model. We then use the trained model for interpretation and **prediction**.
--   This is in contrast to previously discussed unsupervised learning such as clustering or PCA - methods that we were using to find patterns in the data. We treated data set a a whole, using measurements for all samples but not the samples labels such as sample groups to find the components with the highest variables (PCA) or the optimal number of clusters (k-means).
--   **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).
--   Common supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome.
+- In supervised learning we are using sample **labels** to **train** (build) a model. We then use the trained model for interpretation and **prediction**.
+- This is in contrast to unsupervised learning, like clustering or Principal Component Analysis (PCA), that aims to discover patterns or groupings in the data without any predefined labels, such as identifying subtypes of a disease based on genetic variations. 
+
+
+```{r}
+#| label: fig-supervised
+#| fig-cap: ""Ilustration of supervised learning that focuses on training models to make predictions or decisions based on labeled training data.""
+#| fig-width: 12
+
+library(knitr)
+include_graphics(""images/supervised.png"")
+```
+
+- **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).
+-   Common supervised machine learning algorithms include K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). Many can be implemented to work both for classifying samples and forecasting numeric outcome, i.e. classification and regression tasks.
+
 
 ## Outline
 
@@ -245,21 +264,21 @@ Based on the confusion matrix, we can derive common performance metrics of a bin
 
 - **ROC AUC**: the receiver operating characteristic (ROC) curve is a graphical representation of the trade off between sensitivity and specificity for different threshold values. The area under the ROC curve (AUC) is a performance metric that ranges from 0 to 1, with a higher value indicating better performance. AUC is a measure of how well the classifier is able to distinguish between positive and negative samples.
 
-## Evaluating regression
-The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include: 
-
-- **R-squared**: As seen in the linear regression session.
-$$
-R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
-$$
-- **Adjusted R-squared**: seen before
-$$
-R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
-$$
-- **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. 
-$$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$
-- **Root Mean Squared Error (RMSE)**: square root of the MSE 
-$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$
-- **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$
-- **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.
+<!-- ## Evaluating regression -->
+<!-- The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include:  -->
+
+<!-- - **R-squared**: As seen in the linear regression session. -->
+<!-- $$ -->
+<!-- R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} -->
+<!-- $$ -->
+<!-- - **Adjusted R-squared**: seen before -->
+<!-- $$ -->
+<!-- R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1} -->
+<!-- $$ -->
+<!-- - **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values.  -->
+<!-- $$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$ -->
+<!-- - **Root Mean Squared Error (RMSE)**: square root of the MSE  -->
+<!-- $$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$ -->
+<!-- - **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$ -->
+<!-- - **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values. -->
 "
NBISweden,workshop-mlbiostatistics,183e774bb04a45b1fd7b0bf64ca32951a96759a2,evaf,eva@freyhult.net,2024-04-09T08:13:12Z,evaf,eva@freyhult.net,2024-04-09T08:13:12Z,Fix typos,session-clustering-anew/intro.qmd,True,False,True,False,18,15,33,"---FILE: session-clustering-anew/intro.qmd---
@@ -208,7 +208,7 @@ include_graphics(""images/distances.png"")
 1. $d_{ij}$ must be 0 or positive (objects are identical, $d_{ij} = 0$, or they are different, $d_{ij} > 0$)
 2. $d_{ij} = d_{ji}$, distance from A to B is the same as from B to A
 3. $d_{jj} = 0$, an object is identical to itself
-4. $d_{ik} \le d_{ij} + d_{jk}$, when considering three objects the distance between any two of them cannot exceed the sum of the distances between the other two pairs. In other words, the distances can be constructed a a triangule.
+4. $d_{ik} \le d_{ij} + d_{jk}$, when considering three objects the distance between any two of them cannot exceed the sum of the distances between the other two pairs. In other words, the distances can construct a triangle.
 
 - The **Manhattan** or City Block metric is an example of this type of distance metrics. 
 $$distance (A,B) = \sum|a_i - b_i|$$
@@ -242,6 +242,7 @@ include_graphics(""images/cosine.png"")
 #### Squared Euclidean distance {.unnumbered}
 
 - The sum of the squared differences between scores for two cases on all variables, i.e. the squared length of the hypotenuse. This measure magnifies distances between cases that are further apart. 
+- Note that the squared Euclidean distance is not a metric as it does not fulfill the triangle inequality.
 
 #### Chebychev {.unnumbered}
 
@@ -254,7 +255,7 @@ $$distance(A, B) = max |a_i - b_i|$$
 #### City block or Manhattan distance {.unnumbered}
 
 - A distance that follows a route along the non-hypotenuse sides of a triangle. The name refers to the grid-like layout of most American cities which makes it impossible to go directly between two points. 
-- This metric is hence less affected by outliers than the Euclidean and squared Euclidean metrics. 
+- This metric is hence less affected by outliers than the Euclidean and squared Euclidean distances. 
 $$distance (A,B) = \sum|a_i - b_i|$$
 - Is preferred over Euclidean distance when there is a high dimensionality in the data.
 - May not be the best choice, if one wants to perform space rotation or wants a smooth and differentiable function (converge more easily).
@@ -268,7 +269,7 @@ where $S^{-1}$ is the inverse covariance matrix$
 
 #### Minkowski {.unnumbered}
 
-- Minkowski distance is ageneralization of both the Euclidean distance and the Manhattan distance in a normed vector space. 
+- Minkowski distance is a generalization of both the Euclidean distance and the Manhattan distance in a normed vector space. 
 - It can be thought of as a way of measuring the length of the path between two points when moving along axes aligned with the coordinate system. 
 - The formula for the Minkowski distance of order $p$ between two points $A = (a_1, a_2, ..., a_n)$ and $B = (b_1, b_2, ..., b_n)$ in an $n$-dimensional space is given by $$distance(A,B) = \left(\sum_{i=1}^{n} |a_i - b_i|^p\right)^{1/p}$$ 
 where  $p$ determines the form of the distance: for $p=1$, it becomes the Manhattan distance (sum of the absolute differences of their coordinates); for $p=2$, it yields the Euclidean distance (the shortest direct line between two points); and for $p=\infty$, it approaches the Chebyshev distance (the maximum difference along any coordinate dimension). 
@@ -303,9 +304,9 @@ x %>%
 
 where: 
 
-- $a$) is the number of cases which both share the attribute
-- $d$) number of cases which neither have the attribute
-- $b$ and $c$) are the number of cases in which only one of the pair has the attribute. 
+- $a$ is the number of cases which both share the attribute
+- $d$ number of cases which neither have the attribute
+- $b$ and $c$ dare the number of cases in which only one of the pair has the attribute. 
 - Note: $a + b + c + d = n$, the sample size
 
 #### Dice {.unnumbered}
@@ -370,14 +371,14 @@ Interactive visualization of k-means algorithm can be found [here](https://hckr.
 
 **When to use and not to use k-means**
 
-- k-means is relatively fast and scalable, making it a good choice for large data sets. It works well with continuous, numeric data since it relies on Euclidean distances. Results are easy to interpret due to the simple assingment of data points to clusters. It works good if one knows the number of clusters or can get that the number of clusters for internal or external validation. 
+- k-means is relatively fast and scalable, making it a good choice for large data sets. It works well with continuous, numeric data since it relies on Euclidean distances. Results are easy to interpret due to the simple assignment of data points to clusters. It works good if one knows the number of clusters or can get that the number of clusters for internal or external validation. 
 - On the downside, k-means is sensitive to the initial placement of cluster centroids. It can sometimes converge to local minima, resulting in suboptimal cluster assignments. Outliers can also significantly impact the position of centroids and lead to poor cluster assignments. 
 
 ### PAM, partition around medoids
 
 - PAM is very similar to k-means, but instead of using centroids we use **medoids** to represent clusters. 
 - **Medoid** is centrally located objects within the cluster. 
-- This makes PAM more roboust when compare to k-means, but with higher computational complexity. 
+- This makes PAM more robust when compare to k-means, but with higher computational complexity. 
 
 The algorithm can be described in few steps: 
 
@@ -416,7 +417,7 @@ x <- df %>%
 res_kmeans <- kmeans(x, centers = 3)
 res_pam <- pam(x, k = 3)
 
-# preprae data frames for plotting
+# prepare data frames for plotting
 df_kmeans <- df %>%
   mutate(cluster = res_kmeans$cluster) %>%
   mutate(cluster = as.factor(cluster))
@@ -464,7 +465,7 @@ p1 + p2
 ## HCL, hierarchical clustering
 
 - Hierarchical clustering does not require the number of clusters to be specified. Instead of creating a single set of clusters it creates a hierarchy of clusterings based on pairwise dissimilarities.
-- These hierarchy of clusteres is often represented as **dendrogram**.
+- This hierarchy of clusters is often represented as a **dendrogram**.
 
 ```{r fig-hcl-dendrogram}
 #| label: fig-dend
@@ -565,11 +566,11 @@ ggplot(df_cl, aes(x, y, color = Cluster)) +
 - With $n$ objects to cluster both strategies will produce a dendrogram representing the $n-1$ levels in the hierarchy.
 - Each level represent a specific clustering of the objects into disjoint clusters.
 - The heights of the branches in the dendrogram are proportional to the dissimilarity of the merged/split clusters.
-- The divisive approach is used less often due to the computational complexity. Here, we fill focus more on the commonly used agglomerative approach. 
+- The divisive approach is used less often due to the computational complexity. Here, we will focus more on the commonly used agglomerative approach. 
 
 ### Linkage & Agglomerative clustering
 
-- Agglomerative clustering starts with all objects in separate clusters and at each level merge the pair of clusters with the smallest dissimilarity. The pairwise dissilimarities between objects can be computed according to the distance measures discussed above. 
+- Agglomerative clustering starts with all objects in separate clusters and at each level merge the pair of clusters with the smallest dissimilarity. The pairwise dissimilarities between objects can be computed according to the distance measures discussed above. 
 - We need one more ingredient for hierarchical clustering, that is a method for computing dissimilarity between clusters, known as **linkage method**. There are several linkage methods, as illustrated below. 
 
 ```{r}
@@ -605,7 +606,7 @@ $$d_{al}(A, B) = \frac{1}{n_A n_B}\sum_i\sum_j d(a_i, b_j)$$
 $$d_{wl}(A, B) = \sum_{i=1}^{n_A} (a_i - m_{A \cup B})^2 + \sum_{i=1}^{n_B} (b_i - m_{A \cup B})^2 - \sum_{i=1}^{n_A} (a_i - m_{A})^2 - \sum_{i=1}^{n_B} (b_i - m_{B})^2$$
 where $m_A, m_B, m_{A \cup B}$ are the center of the clusters $A$, $B$ and $A \cup B$, respectively. 
 
-- *Note* that Ward's linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function `hclust` either the Euclidean or squared Euclidean distance can be used in combination with the linkage `method='ward.D'` or `method='ward.D2`, respectively.
+- *Note* that Ward's linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function `hclust` either the Euclidean or squared Euclidean distance can be used in combination with the linkage `method='ward.D2'` or `method='ward.D`, respectively.
 
 
 ```{r}
@@ -621,7 +622,9 @@ d <- dist(df) # calculate distance
 h_single <- hclust(d, method = ""single"")
 h_average <- hclust(d, method = ""average"")
 h_complete <- hclust(d, method = ""complete"")
-h_ward <- hclust(d, method = ""ward.D"")
+h_ward <- hclust(d, method = ""ward.D2"")
+## Alternatively
+## h_ward <- hclust(d^2, method = ""ward.D"")
 
 dend_single <- as.dendrogram(h_single) %>% set(""branches_lwd"", 3)
 dend_average <- as.dendrogram(h_average) %>% set(""branches_lwd"", 3)
@@ -843,7 +846,7 @@ Heatmap(data_heatmap,
 ## Additional comments & resources
 
 - In addition to partitioning and hierarchical clustering methods there is a wide range of other algorithms. Common ones include model-, density, and grid-based methods. Some of them are explained in [this presentation](https://docs.google.com/presentation/d/1OgFOM7Zo9rQRiXB0r7EBuLg7gFIgaDwYd3pY2EwxWlg/edit?usp=sharing). Here, you can also find more details about **external validation** of cluster analysis.
-- Although historically it was not recommended to apply cluster analysis to **mixed data types** (e.g. both binary and continuous), some distance metrics have been developed, e.g. Gower's general coefficient to calculate similarity between different data types. Lately, the principles of decision trees have been also used for computing yet anther metric for mixed data types, **unsupervised extra trees dissimilarity, UET**. For an interesting study comparing clustering methods for heterogeneous data, where more details about Grower's and UET can be found, see @Preud2021. 
+- Although historically it was not recommended to apply cluster analysis to **mixed data types** (e.g. both binary and continuous), some distance metrics have been developed, e.g. Gower's general coefficient to calculate similarity between different data types. Lately, the principles of decision trees have been also used for computing yet another metric for mixed data types, **unsupervised extra trees dissimilarity, UET**. For an interesting study comparing clustering methods for heterogeneous data, where more details about Grower's and UET can be found, see @Preud2021. 
 - For a more applied reading in health research, reviewing distance metrics and clustering algorithms see @Gao2023. 
 - In scRNA-seq analysis it is common to use clustering based on the graph data. In particular, more details about Leiden algorithms can be found [here](https://www.sc-best-practices.org/cellular_structure/clustering.html) or in the [@Traag2019].
 "
NBISweden,workshop-mlbiostatistics,ee9239dc50aec0198e24c11b87bc7babdddfba78,olgadet,,2023-04-28T06:12:30Z,olgadet,,2023-04-28T06:12:30Z,Fix typo in engineering,session-feature-selection-presentation/session-feature-selection.html;session-feature-selection-presentation/session-feature-selection.qmd,True,False,True,False,31,31,62,"---FILE: session-feature-selection-presentation/session-feature-selection.html---
@@ -10,7 +10,7 @@
 <link href=""session-feature-selection_files/libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles""><meta charset=""utf-8"">
   <meta name=""generator"" content=""quarto-1.3.333"">
 
-  <title>Feature engieering &amp; selection</title>
+  <title>Feature engineering &amp; selection</title>
   <meta name=""apple-mobile-web-app-capable"" content=""yes"">
   <meta name=""apple-mobile-web-app-status-bar-style"" content=""black-translucent"">
   <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"">
@@ -348,7 +348,7 @@
     <div class=""slides"">
 
 <section id=""title-slide"" class=""quarto-title-block center"">
-  <h1 class=""title"">Feature engieering &amp; selection</h1>
+  <h1 class=""title"">Feature engineering &amp; selection</h1>
 
 <div class=""quarto-title-authors"">
 </div>
@@ -375,13 +375,13 @@ <h3 id=""lets-discuss"">Let’s discuss:</h3>
 </ul>
 </div>
 </section>
-<section id=""feature-engieering"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>Definition</em> <br></p>
 <p>Feature engineering refers to techniques in machine learning that are used to prepare data for modeling and in turn improve the performance of machine learning models.</p>
 </section>
-<section id=""feature-engieering-1"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering-1"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>scaling &amp; normalization</em> <br></p>
 <div>
 <ul>
@@ -401,8 +401,8 @@ <h2>Feature engieering</h2>
 <p><br></p>
 </div>
 </section>
-<section id=""feature-engieering-2"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering-2"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>common transformations</em> <br></p>
 <p><strong>square-root for moderate skew</strong></p>
 <ul>
@@ -420,8 +420,8 @@ <h2>Feature engieering</h2>
 <li>1/(max(x+1) - x) for negatively skewed data</li>
 </ul>
 </section>
-<section id=""feature-engieering-3"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering-3"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>dummy variables</em> <br></p>
 <div class=""columns"">
 <div class=""column"" style=""width:40%;"">
@@ -485,8 +485,8 @@ <h2>Feature engieering</h2>
 </div>
 </div>
 </section>
-<section id=""feature-engieering-4"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering-4"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>missing data</em> <br></p>
 <ul>
 <li><strong>handling missing data</strong> via
@@ -497,8 +497,8 @@ <h2>Feature engieering</h2>
 </ul></li>
 </ul>
 </section>
-<section id=""feature-engieering-5"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering-5"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>Rubin’s (1976) missing data classification system</em> <br></p>
 <div class=""columns"">
 <div class=""column"" style=""width:50%;"">
@@ -515,16 +515,16 @@ <h3 id=""mar"">MAR</h3>
 <h3 id=""mnar"">MNAR</h3>
 <ul>
 <li>missing not at random</li>
-<li>omitting two higest values for Test 2</li>
+<li>omitting two highest values for Test 2</li>
 <li>when the missing values on a variable are related to the values of that variable itself</li>
 </ul>
 </div><div class=""column"" style=""width:50%;"">
-<p><img data-src=""figures/missing-data.jpg""> <span class=""citation"" data-cites=""missing2008""><span>“Missing Data: A Gentle Introduction by Patrick E. McKnight, Katherine M. McKnight, Souraya Sidani, and Aurelio Jose Figueredo”</span> (<a href=""#/references"" role=""doc-biblioref"" onclick="""">2008</a>)</span>]</p>
+<p><img data-src=""figures/missing-data.jpg""> <span class=""citation"" data-cites=""missing2008""><span>“Missing Data: A Gentle Introduction by Patrick E. McKnight, Katherine M. McKnight, Souraya Sidani, and Aurelio Jose Figueredo”</span> (<a href=""#/references"" role=""doc-biblioref"" onclick="""">2008</a>)</span></p>
 </div>
 </div>
 </section>
-<section id=""feature-engieering-6"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering-6"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>handling imbalanced data</em> <br></p>
 <div class=""columns"">
 <div class=""column"" style=""width:50%;"">
@@ -549,8 +549,8 @@ <h2>Feature engieering</h2>
 </div>
 </div>
 </section>
-<section id=""feature-engieering-7"" class=""slide level2"">
-<h2>Feature engieering</h2>
+<section id=""feature-engineering-7"" class=""slide level2"">
+<h2>Feature engineering</h2>
 <p><em>misc</em> <br></p>
 <div>
 <ul>

---FILE: session-feature-selection-presentation/session-feature-selection.qmd---
@@ -1,5 +1,5 @@
 ---
-title: ""Feature engieering & selection""
+title: ""Feature engineering & selection""
 # author: Olga Dethlefsen
 format: 
   revealjs:
@@ -34,7 +34,7 @@ $p \gg n$ <br>
     -   regularized regression
 -   and learn how to use `tidymodels` framework for supervised learning projects
 
-## Feature engieering
+## Feature engineering
 
 *Definition* <br>
 
@@ -70,7 +70,7 @@ data_diabetes <- input_diabetes %>%
 
 Feature engineering refers to techniques in machine learning that are used to prepare data for modeling and in turn improve the performance of machine learning models.
 
-## Feature engieering
+## Feature engineering
 
 *scaling & normalization* <br>
 
@@ -87,7 +87,7 @@ Feature engineering refers to techniques in machine learning that are used to pr
 
 <br>
 
-## Feature engieering
+## Feature engineering
 
 *common transformations* <br>
 
@@ -106,7 +106,7 @@ Feature engineering refers to techniques in machine learning that are used to pr
 -   1/x for positively skewed data
 -   1/(max(x+1) - x) for negatively skewed data
 
-## Feature engieering
+## Feature engineering
 
 *dummy variables* <br>
 
@@ -144,7 +144,7 @@ data_dummy %>%
 :::
 :::
 
-## Feature engieering
+## Feature engineering
 
 *missing data* <br>
 
@@ -153,7 +153,7 @@ data_dummy %>%
     -   deleting strategies such as list-wise deletion (complete-case analysis) or pair-wise deletion (available-case analysis)
     -   choosing algorithms that can handle some extent of missing data, e.g. Random Forest, Naive Bayes
 
-## Feature engieering
+## Feature engineering
 
 *Rubin's (1976) missing data classification system* <br>
 
@@ -172,17 +172,17 @@ data_dummy %>%
 ### MNAR
 
 -   missing not at random
--   omitting two higest values for Test 2
+-   omitting two highest values for Test 2
 -   when the missing values on a variable are related to the values of that variable itself
 :::
 
 ::: {.column width=""50%""}
 ![](figures/missing-data.jpg)
-@missing2008]
+@missing2008
 :::
 :::
 
-## Feature engieering
+## Feature engineering
 
 *handling imbalanced data* <br>
 
@@ -228,7 +228,7 @@ data_diabetes %>%
 
 
 
-## Feature engieering
+## Feature engineering
 
 *misc* <br>
 "
NBISweden,workshop-mlbiostatistics,d8e959d1e9c8e973a745a835a70a9a9593f69b98,olgadet,,2023-04-27T07:54:54Z,olgadet,,2023-04-27T07:54:54Z,Fix typo,session-supervise-presentation/.quarto/_freeze/session-supervise-presentation/execute-results/html.json;session-supervise-presentation/.quarto/xref/c3b72353;session-supervise-presentation/session-supervise-presentation.html;session-supervise-presentation/session-supervise-presentation.qmd,True,False,True,False,68,79,147,"---FILE: session-supervise-presentation/.quarto/_freeze/session-supervise-presentation/execute-results/html.json---
@@ -1,7 +1,7 @@
 {
-  ""hash"": ""f82cebeed48696c9eb88920fec0d6b5e"",
+  ""hash"": ""ffd1302b90686b5a6b14a8b8c0e14e0b"",
   ""result"": {
-    ""markdown"": ""---\ntitle: \""Supervised learning\""\n# author: Olga Dethlefsen\nformat: \n  revealjs:\n    slide-number: true\n    theme: [default, custom.scss]\n    view-distance: 10\n    chalkboard: \n      buttons: true\n  html:\n    code-fold: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n# We will discuss\n\n- What supervised learning is.\n- Data splitting.\n- How to evaluate classification and regression ML models.\n- How to put together a minimum working example of supervised learning with KNN classifier.\n\n## What is supervised learning\n*Definition*\n\n<br>\n\n\n```{mermaid}\nflowchart TD\n  A(Machine n learning) --> B(unsupervised n learning)\n  A --> C(supervised n learning)\n```\n\n\n<br>\n\n::: incremental\n\n-   Supervised learning can be used for **classification** and **regression**.\n    - For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)\n    - For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)\n- In supervised learning we are using sample **labels** to **train** (build) a model. \n- We then use the trained model for **interpretation** and **prediction**.\n\n:::\n\n## What is supervised learning?\n*Definition*\n<br>\n\n::: incremental\n-   **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).\n-   Common supervised machine learning algorithms include:\n    - K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). \n- Many of the ML methods can be implemented to work both for classifying samples and forecasting numeric outcome.\n:::\n\n## Supervised learning\n*Outline*\n<br>\n\nAcross many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:\n\n::: incremental\n\n- deciding on the task: classification or regression  \n- **splitting data** to keep part of data for training and part for testing\n-   selecting supervised machine learning algorithms to be trained (or a set of these)\n-   deciding on the training strategy, i.e. which **performance metrics** to use and how to search for the best model parameters\n-   running **feature engineering**: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables\n-   performing **feature selection**: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present\n:::\n\n## Supervised learning {.scrollable}\n*Outline*\n<br>\n\nThe diagram below shows a basic strategy on how to train KNN for classification, given a data set with $n$ samples, $p$ variables and $y$ categorical outcome\n\n\n```{mermaid}\n\nflowchart TD\n  A([data]) -. split data \\n e.g. basic, stratified, grouped -.-> B([non-test set])\n  A([data]) -.-> C([test set])\n  B -.-> D(choose algorithm \\n e.g. KNN)\n  D -.-> E(choose evaluation metric \\n e.g. overall accuracy)\n  E -.-> F(feature engineering & selection)\n  F -.-> G(prepare parameter space, e.g. odd k-values from 3 to 30)\n  G -. split non-test -.-> H([train set & validation set])\n  H -.-> J(fit model on train set)\n  J -.-> K(collect evaluation metric on validation set)\n  K -.-> L{all values checked? \\n e.g. k more than 30}\n  L -. No .-> J\n  L -. Yes .-> M(select best parameter values)\n  M -.-> N(fit model on all non-test data)\n  N -.-> O(assess model on test data)\n  C -.-> O\n\n```\n\n\n## Supervised learning\n*Outline*\n<br>\n\nBefore we see how this training may look like in `R`, let's talk more about\n\n-   **KNN**, K-nearest neighbor algorithm\n-   **data splitting** and\n-   **performance metrics** useful for evaluating models\n\n## Classification\n*Definition*\n<br>\n\n::: incremental\n\n-   Classification methods are algorithms used to categorize (classify) objects based on their measurements.\n-   They belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the label (class) of.\n-   Let's for each observations $i$ collect pair of information $\\{\\mathbf{x_i}, g_i\\}$\n-   where $\\{\\mathbf{x_i}\\}$ is a set of exploratory variables e.g. a gene expression data\n-   and $g_i \\in \\{1, \\dots, G\\}$ is the class label for each observation (known), e.g. cancer stage I, II, III or IV\n-   Then we want to find a **classification rule** $f(.)$ (model) such that $$f(\\mathbf{x_i})=g_i$$\n\n:::\n\n\n\n\n\n\n## KNN \n\n*example of a classification algorithm*\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-00-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-01-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-02-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-03-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n\n*example of a classification algorithm*\n\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-10-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-20-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-30-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-40-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n*Algorithm*\n<br>\n\n::: incremental\n-   Decide on the value of $k$.\n-   Calculate the distance between the query-instance (observations for new sample) and all the training samples.\n-   Sort the distances and determine the nearest neighbors based on the $k$-th minimum distance.\n-   Gather the categories of the nearest neighbors.\n-   Use majority voting of the categories of the nearest neighbors as the prediction value for the new sample.\n- Note: *Euclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.*\n:::\n\n\n## Data splitting\n*Why*\n<br>\n\n-   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\n-   As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.\n-   To deal with overconfident estimation of future performance we can implement various data splitting strategies.\n\n## Data splitting\n*train, validation & test sets*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   **Training data**: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule.\n-   **Validation data**: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters.\n-   **Test data**: this data is used to give an estimate of future prediction performance for the model and parameters chosen.\n-   Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\n\n## Data splitting\n*cross validation & repeated cross validation*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-kfolds-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   In **k-fold cross-validation** we split data into $k$ roughly equal-sized parts.\n-   We start by setting the validation data to be the first set of data and the training data to be all other sets.\n-   We estimate the validation error rate / correct classification rate for the split.\n-   We then repeat the process $k-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data.\n-   We finish with $k$ different error or correct classification rates.\n-   In this way, every data point has its class membership predicted once.\n-   The final reported error rate is usually the average of $k$ error rates.\n\n## Data splitting\n*Leave-one-out cross-validation*\n<br>\n\n-   Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n::: {.cell .fig-cap-location-margin layout-align=\""center\""}\n::: {.cell-output-display}\n![Example of LOOCV, leave-one-out cross validation](figures/data-split-loocv-02.png){#fig-data-split-loocv fig-align='center' width=100%}\n:::\n:::\n\n\n## Performance metrics\n*Evaluating classification*\n<br>\n\n-   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.\n-   There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\n-   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\n## Evaluating classification\n<br>\n<br>\n**Correct (miss)classification rate**\n\n-   The simplest way to evaluate in which we count for all the $n$ predictions how many times we got the classification right. $$Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where $1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\n**Missclassification Rate**\n\nMissclassification Rate = 1 - Correct Classification Rate\n\n## Evaluating classification {.smaller}\n<br>\n\n**Confusion matrix**\n\nConfusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes. \n\n|                     | Predicted Positive  | Predicted Negative  |\n|---------------------|---------------------|---------------------|\n| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n\n- **Accuracy**: measures the proportion of correctly classified samples over the total number of samples. $$ACC = \\frac{TP+TN}{TP+TN+FP+FN}$$\n- **Sensitivity**: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as **true positive rate** and **recall**.\n $$TPR = \\frac{TP}{TP + FN}$$\n\n- **Specificity**: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as **true negative rate** and **selectivity**. $$TNR = \\frac{TN}{TN+FP}$$\n\n- **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives.  It is also known as **positive predictive value**. $$PPV = \\frac{TP}{TP + FP}$$ \n\n## Performance metrics\n*Evaluating regression*\n<br>\n\n::: incremental\n- The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. \n- For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. \n- In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. \n:::\n\n## Evaluating regression {.smaller}\n<br>\n\n\n- **R-squared**: As seen in the linear regression session.\n$$\nR^2=1-\\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n$$\n- **Adjusted R-squared**: seen before\n$$\nR_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\frac{n-1}{n-p-1}\n$$\n- **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. \n$$MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2$$\n- **Root Mean Squared Error (RMSE)**: square root of the MSE \n$$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}$$\n- **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|$$\n- **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.\n\n## Live demo\n*KNN model for classification*\n"",
+    ""markdown"": ""---\ntitle: \""Supervised learning\""\n# author: Olga Dethlefsen\nformat: \n  revealjs:\n    slide-number: true\n    theme: [default, custom.scss]\n    view-distance: 10\n    chalkboard: \n      buttons: true\n  html:\n    code-fold: false\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n# We will discuss\n\n-   What supervised learning is.\n-   Data splitting.\n-   How to evaluate classification and regression ML models.\n-   How to put together a minimum working example of supervised learning with KNN classifier.\n\n## What is supervised learning\n\n*Definition*\n\n<br>\n\n\n```{mermaid}\nflowchart TD\n  A(Machine learning) --> B(unsupervised learning)\n  A --> C(supervised learning)\n```\n\n\n<br>\n\n::: incremental\n-   Supervised learning can be used for **classification** and **regression**.\n    -   For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)\n    -   For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)\n-   In supervised learning we are using sample **labels** to **train** (build) a model.\n-   We then use the trained model for **interpretation** and **prediction**.\n:::\n\n## What is supervised learning?\n\n*Definition* <br>\n\n::: incremental\n-   **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).\n-   Common supervised machine learning algorithms include:\n    -   K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN).\n-   Many of the ML methods can be implemented to work both for classifying samples and forecasting numeric outcome.\n:::\n\n## Supervised learning\n\n*Outline* <br>\n\nAcross many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:\n\n::: incremental\n-   deciding on the task: classification or regression\\\n-   **splitting data** to keep part of data for training and part for testing\n-   selecting supervised machine learning algorithms to be trained (or a set of these)\n-   deciding on the training strategy, i.e. which **performance metrics** to use and how to search for the best model parameters\n-   running **feature engineering**: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables\n-   performing **feature selection**: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present\n:::\n\n## Supervised learning {.scrollable}\n\n*Outline* <br>\n\nThe diagram below shows a basic strategy on how to train KNN for classification, given a data set with $n$ samples, $p$ variables and $y$ categorical outcome\n\n\n```{mermaid}\n\nflowchart TD\n  A([data]) -. split data \\n e.g. basic, stratified, grouped -.-> B([non-test set])\n  A([data]) -.-> C([test set])\n  B -.-> D(choose algorithm \\n e.g. KNN)\n  D -.-> E(choose evaluation metric \\n e.g. overall accuracy)\n  E -.-> F(feature engineering & selection)\n  F -.-> G(prepare parameter space, e.g. odd k-values from 3 to 30)\n  G -. split non-test -.-> H([train set & validation set])\n  H -.-> J(fit model on train set)\n  J -.-> K(collect evaluation metric on validation set)\n  K -.-> L{all values checked? \\n e.g. k more than 30}\n  L -. No .-> J\n  L -. Yes .-> M(select best parameter values)\n  M -.-> N(fit model on all non-test data)\n  N -.-> O(assess model on test data)\n  C -.-> O\n\n```\n\n\n## Supervised learning\n\n*Outline* <br>\n\nBefore we see how this training may look like in `R`, let's talk more about\n\n-   **KNN**, K-nearest neighbor algorithm\n-   **data splitting** and\n-   **performance metrics** useful for evaluating models\n\n## Classification\n\n*Definition* <br>\n\n::: incremental\n-   Classification methods are algorithms used to categorize (classify) objects based on their measurements.\n-   They belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the label (class) of.\n-   Let's for each observations $i$ collect pair of information $\\{\\mathbf{x_i}, g_i\\}$\n-   where $\\{\\mathbf{x_i}\\}$ is a set of exploratory variables e.g. a gene expression data\n-   and $g_i \\in \\{1, \\dots, G\\}$ is the class label for each observation (known), e.g. cancer stage I, II, III or IV\n-   Then we want to find a **classification rule** $f(.)$ (model) such that $$f(\\mathbf{x_i})=g_i$$\n:::\n\n## KNN\n\n*example of a classification algorithm*\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-00-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-01-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-02-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-03-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n\n*example of a classification algorithm*\n\n\n\n\n\n\n\n\n\n\n\n::: r-stack\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-10-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-20-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-30-1.png){.fragment width=\""700\"" height=\""600\""}\n\n![](session-supervise-presentation_files/figure-revealjs/fig-knn-40-1.png){.fragment width=\""700\"" height=\""600\""}\n:::\n\n## KNN\n\n*Algorithm* <br>\n\n::: incremental\n-   Decide on the value of $k$.\n-   Calculate the distance between the query-instance (observations for new sample) and all the training samples.\n-   Sort the distances and determine the nearest neighbors based on the $k$-th minimum distance.\n-   Gather the categories of the nearest neighbors.\n-   Use majority voting of the categories of the nearest neighbors as the prediction value for the new sample.\n-   Note: *Euclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.*\n:::\n\n## Data splitting\n\n*Why* <br>\n\n-   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.\n-   As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.\n-   To deal with overconfident estimation of future performance we can implement various data splitting strategies.\n\n## Data splitting\n\n*train, validation & test sets*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   **Training data**: this is data used to fit (train) the classification or regression model, i.e. derive the classification rule.\n-   **Validation data**: this is data used to select which parameters or types of model perform best, i.e. to validate the performance of model parameters.\n-   **Test data**: this data is used to give an estimate of future prediction performance for the model and parameters chosen.\n-   Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively\n\n## Data splitting\n\n*cross validation & repeated cross validation*\n\n\n::: {.cell layout-align=\""center\""}\n::: {.cell-output-display}\n![](figures/data-split-kfolds-02.png){fig-align='center' width=100%}\n:::\n:::\n\n\n-   In **k-fold cross-validation** we split data into $k$ roughly equal-sized parts.\n-   We start by setting the validation data to be the first set of data and the training data to be all other sets.\n-   We estimate the validation error rate / correct classification rate for the split.\n-   We then repeat the process $k-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data.\n-   We finish with $k$ different error or correct classification rates.\n-   In this way, every data point has its class membership predicted once.\n-   The final reported error rate is usually the average of $k$ error rates.\n\n## Data splitting\n\n*Leave-one-out cross-validation* <br>\n\n-   Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.\n\n\n::: {.cell .fig-cap-location-margin layout-align=\""center\""}\n::: {.cell-output-display}\n![Example of LOOCV, leave-one-out cross validation](figures/data-split-loocv-02.png){#fig-data-split-loocv fig-align='center' width=100%}\n:::\n:::\n\n\n## Performance metrics\n\n*Evaluating classification* <br>\n\n-   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.\n-   There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.\n-   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.\n\n## Evaluating classification\n\n<br> <br> **Correct (miss)classification rate**\n\n-   The simplest way to evaluate in which we count for all the $n$ predictions how many times we got the classification right. $$Correct\\; Classifcation \\; Rate = \\frac{\\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where $1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise\n\n**Missclassification Rate**\n\nMissclassification Rate = 1 - Correct Classification Rate\n\n## Evaluating classification {.smaller}\n\n<br>\n\n**Confusion matrix**\n\nConfusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes.\n\n|                     | Predicted Positive  | Predicted Negative  |\n|---------------------|---------------------|---------------------|\n| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n\n-   **Accuracy**: measures the proportion of correctly classified samples over the total number of samples. $$ACC = \\frac{TP+TN}{TP+TN+FP+FN}$$\n\n-   **Sensitivity**: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as **true positive rate** and **recall**. $$TPR = \\frac{TP}{TP + FN}$$\n\n-   **Specificity**: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as **true negative rate** and **selectivity**. $$TNR = \\frac{TN}{TN+FP}$$\n\n-   **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as **positive predictive value**. $$PPV = \\frac{TP}{TP + FP}$$\n\n## Performance metrics\n\n*Evaluating regression* <br>\n\n::: incremental\n-   The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models.\n-   For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements.\n-   In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model.\n:::\n\n## Evaluating regression {.smaller}\n\n<br>\n\n-   **R-squared**: As seen in the linear regression session. $$\n    R^2=1-\\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n    $$\n-   **Adjusted R-squared**: seen before $$\n    R_{adj}^2=1-\\frac{RSS}{TSS}\\frac{n-1}{n-p-1} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\frac{n-1}{n-p-1}\n    $$\n-   **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. $$MSE = \\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2$$\n-   **Root Mean Squared Error (RMSE)**: square root of the MSE $$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({y_i}-\\hat{y}_i)^2}$$\n-   **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|{y_i}-\\hat{y}_i|$$\n-   **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.\n\n## Live demo\n\n*KNN model for classification*\n"",
     ""supporting"": [
       ""session-supervise-presentation_files""
     ],

---FILE: session-supervise-presentation/.quarto/xref/c3b72353---
@@ -1 +1 @@
-{""headings"":[""we-will-discuss"",""what-is-supervised-learning"",""what-is-supervised-learning-1"",""supervised-learning"",""supervised-learning-1"",""supervised-learning-2"",""classification"",""knn"",""knn-1"",""knn-2"",""data-splitting"",""data-splitting-1"",""data-splitting-2"",""data-splitting-3"",""performance-metrics"",""evaluating-classification"",""evaluating-classification-1"",""performance-metrics-1"",""evaluating-regression"",""live-demo""],""entries"":[{""order"":{""section"":[1,13,0,0,0,0,0],""number"":1},""caption"":""Example of LOOCV, leave-one-out cross validation"",""key"":""fig-data-split-loocv""}]}
\ No newline at end of file
+{""headings"":[""we-will-discuss"",""what-is-supervised-learning"",""what-is-supervised-learning-1"",""supervised-learning"",""supervised-learning-1"",""supervised-learning-2"",""classification"",""knn"",""knn-1"",""knn-2"",""data-splitting"",""data-splitting-1"",""data-splitting-2"",""data-splitting-3"",""performance-metrics"",""evaluating-classification"",""evaluating-classification-1"",""performance-metrics-1"",""evaluating-regression"",""live-demo""],""entries"":[{""order"":{""number"":1,""section"":[1,13,0,0,0,0,0]},""caption"":""Example of LOOCV, leave-one-out cross validation"",""key"":""fig-data-split-loocv""}]}
\ No newline at end of file

---FILE: session-supervise-presentation/session-supervise-presentation.html---
@@ -354,8 +354,8 @@ <h2>What is supervised learning</h2>
 <div>
 <div>
 <pre class=""mermaid mermaid-js"">flowchart TD
-  A(Machine n learning) --&gt; B(unsupervised n learning)
-  A --&gt; C(supervised n learning)
+  A(Machine learning) --&gt; B(unsupervised learning)
+  A --&gt; C(supervised learning)
 </pre>
 </div>
 </div>

---FILE: session-supervise-presentation/session-supervise-presentation.qmd---
@@ -17,64 +17,62 @@ bibliography: references.bib
 
 # We will discuss
 
-- What supervised learning is.
-- Data splitting.
-- How to evaluate classification and regression ML models.
-- How to put together a minimum working example of supervised learning with KNN classifier.
+-   What supervised learning is.
+-   Data splitting.
+-   How to evaluate classification and regression ML models.
+-   How to put together a minimum working example of supervised learning with KNN classifier.
 
 ## What is supervised learning
+
 *Definition*
 
 <br>
 
 ```{mermaid}
 flowchart TD
-  A(Machine n learning) --> B(unsupervised n learning)
-  A --> C(supervised n learning)
+  A(Machine learning) --> B(unsupervised learning)
+  A --> C(supervised learning)
 ```
 
 <br>
 
 ::: incremental
-
 -   Supervised learning can be used for **classification** and **regression**.
-    - For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)
-    - For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)
-- In supervised learning we are using sample **labels** to **train** (build) a model. 
-- We then use the trained model for **interpretation** and **prediction**.
-
+    -   For instance given a new biopsy sample we want to tell whether it contains tumor tissue (classification)
+    -   For instance given a new measurements of the methylation sites we want to forecast epigenomic age (regression)
+-   In supervised learning we are using sample **labels** to **train** (build) a model.
+-   We then use the trained model for **interpretation** and **prediction**.
 :::
 
 ## What is supervised learning?
-*Definition*
-<br>
+
+*Definition* <br>
 
 ::: incremental
 -   **Training** a model means selecting the best values for the model attributes (algorithm parameters) that allow linking the input data with the desired output task (classification or regression).
 -   Common supervised machine learning algorithms include:
-    - K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN). 
-- Many of the ML methods can be implemented to work both for classifying samples and forecasting numeric outcome.
+    -   K-Nearest Neighbor (KNN), Support Vector Machines (SVM), Random Forest (RF) or Artificial Neural Networks (ANN).
+-   Many of the ML methods can be implemented to work both for classifying samples and forecasting numeric outcome.
 :::
 
 ## Supervised learning
-*Outline*
-<br>
+
+*Outline* <br>
 
 Across many algorithms and applications we can distinguish some common steps when using supervised learning. These steps include:
 
 ::: incremental
-
-- deciding on the task: classification or regression  
-- **splitting data** to keep part of data for training and part for testing
+-   deciding on the task: classification or regression\
+-   **splitting data** to keep part of data for training and part for testing
 -   selecting supervised machine learning algorithms to be trained (or a set of these)
 -   deciding on the training strategy, i.e. which **performance metrics** to use and how to search for the best model parameters
 -   running **feature engineering**: depending on the data and algorithms chosen, we may need to normalize or transform variables, reduce dimensionality or re-code categorical variables
 -   performing **feature selection**: reducing number of features by keeping only the relevant ones, e.g. by filtering zero and near-zero variance features, removing highly correlated features or features with large amount of missing data present
 :::
 
 ## Supervised learning {.scrollable}
-*Outline*
-<br>
+
+*Outline* <br>
 
 The diagram below shows a basic strategy on how to train KNN for classification, given a data set with $n$ samples, $p$ variables and $y$ categorical outcome
 
@@ -100,8 +98,8 @@ flowchart TD
 ```
 
 ## Supervised learning
-*Outline*
-<br>
+
+*Outline* <br>
 
 Before we see how this training may look like in `R`, let's talk more about
 
@@ -110,26 +108,19 @@ Before we see how this training may look like in `R`, let's talk more about
 -   **performance metrics** useful for evaluating models
 
 ## Classification
-*Definition*
-<br>
 
-::: incremental
+*Definition* <br>
 
+::: incremental
 -   Classification methods are algorithms used to categorize (classify) objects based on their measurements.
 -   They belong under **supervised learning** as we usually start off with **labeled** data, i.e. observations with measurements for which we know the label (class) of.
 -   Let's for each observations $i$ collect pair of information $\{\mathbf{x_i}, g_i\}$
 -   where $\{\mathbf{x_i}\}$ is a set of exploratory variables e.g. a gene expression data
 -   and $g_i \in \{1, \dots, G\}$ is the class label for each observation (known), e.g. cancer stage I, II, III or IV
 -   Then we want to find a **classification rule** $f(.)$ (model) such that $$f(\mathbf{x_i})=g_i$$
-
 :::
 
-
-
-
-
-
-## KNN 
+## KNN
 
 *example of a classification algorithm*
 
@@ -307,28 +298,28 @@ points(x[n.idx2[2:3]], y[n.idx2[2:3]], pch=19, col=""black"")
 :::
 
 ## KNN
-*Algorithm*
-<br>
+
+*Algorithm* <br>
 
 ::: incremental
 -   Decide on the value of $k$.
 -   Calculate the distance between the query-instance (observations for new sample) and all the training samples.
 -   Sort the distances and determine the nearest neighbors based on the $k$-th minimum distance.
 -   Gather the categories of the nearest neighbors.
 -   Use majority voting of the categories of the nearest neighbors as the prediction value for the new sample.
-- Note: *Euclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.*
+-   Note: *Euclidean distance is a classic distance used with KNN; other distance measures are also used incl. weighted Euclidean distance, Mahalanobis distance, Manhattan distance, maximum distance etc.*
 :::
 
-
 ## Data splitting
-*Why*
-<br>
+
+*Why* <br>
 
 -   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.
 -   As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.
 -   To deal with overconfident estimation of future performance we can implement various data splitting strategies.
 
 ## Data splitting
+
 *train, validation & test sets*
 
 ```{r}
@@ -345,6 +336,7 @@ knitr::include_graphics(""figures/data-split-02.png"")
 -   Common split strategies include 50%/25%/25% and 33%/33%/33% splits for training/validation/test respectively
 
 ## Data splitting
+
 *cross validation & repeated cross validation*
 
 ```{r}
@@ -365,8 +357,8 @@ knitr::include_graphics(""figures/data-split-kfolds-02.png"")
 -   The final reported error rate is usually the average of $k$ error rates.
 
 ## Data splitting
-*Leave-one-out cross-validation*
-<br>
+
+*Leave-one-out cross-validation* <br>
 
 -   Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.
 
@@ -382,17 +374,16 @@ knitr::include_graphics(""figures/data-split-loocv-02.png"")
 ```
 
 ## Performance metrics
-*Evaluating classification*
-<br>
+
+*Evaluating classification* <br>
 
 -   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.
 -   There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.
 -   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.
 
 ## Evaluating classification
-<br>
-<br>
-**Correct (miss)classification rate**
+
+<br> <br> **Correct (miss)classification rate**
 
 -   The simplest way to evaluate in which we count for all the $n$ predictions how many times we got the classification right. $$Correct\; Classifcation \; Rate = \frac{\sum_{i=1}^{n}1[f(x_i)=g_i]}{n}$$ where $1[]$ is an indicator function equal to 1 if the statement in the bracket is true and 0 otherwise
 
@@ -401,53 +392,51 @@ knitr::include_graphics(""figures/data-split-loocv-02.png"")
 Missclassification Rate = 1 - Correct Classification Rate
 
 ## Evaluating classification {.smaller}
+
 <br>
 
 **Confusion matrix**
 
-Confusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes. 
+Confusion matrix allows us to compare between actual and predicted values. It is a N x N matrix, where N is the number of classes.
 
 |                     | Predicted Positive  | Predicted Negative  |
 |---------------------|---------------------|---------------------|
 | **Actual Positive** | True Positive (TP)  | False Negative (FN) |
 | **Actual Negative** | False Positive (FP) | True Negative (TN)  |
 
-- **Accuracy**: measures the proportion of correctly classified samples over the total number of samples. $$ACC = \frac{TP+TN}{TP+TN+FP+FN}$$
-- **Sensitivity**: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as **true positive rate** and **recall**.
- $$TPR = \frac{TP}{TP + FN}$$
+-   **Accuracy**: measures the proportion of correctly classified samples over the total number of samples. $$ACC = \frac{TP+TN}{TP+TN+FP+FN}$$
+
+-   **Sensitivity**: measures the proportion of true positives over all actual positive samples, i.e. how well the classifier is able to detect positive samples. It is also known as **true positive rate** and **recall**. $$TPR = \frac{TP}{TP + FN}$$
 
-- **Specificity**: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as **true negative rate** and **selectivity**. $$TNR = \frac{TN}{TN+FP}$$
+-   **Specificity**: measures the proportion of true negatives over all actual negative samples, i.e. how well the classifier is able to avoid false negatives. It is also known as **true negative rate** and **selectivity**. $$TNR = \frac{TN}{TN+FP}$$
 
-- **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives.  It is also known as **positive predictive value**. $$PPV = \frac{TP}{TP + FP}$$ 
+-   **Precision**: measures the proportion of true positives over all positive predictions made by the classifier, i.e. how well the classifier is able to avoid false positives. It is also known as **positive predictive value**. $$PPV = \frac{TP}{TP + FP}$$
 
 ## Performance metrics
-*Evaluating regression*
-<br>
+
+*Evaluating regression* <br>
 
 ::: incremental
-- The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. 
-- For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. 
-- In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. 
+-   The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models.
+-   For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements.
+-   In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model.
 :::
 
 ## Evaluating regression {.smaller}
-<br>
 
+<br>
 
-- **R-squared**: As seen in the linear regression session.
-$$
-R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
-$$
-- **Adjusted R-squared**: seen before
-$$
-R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
-$$
-- **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. 
-$$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$
-- **Root Mean Squared Error (RMSE)**: square root of the MSE 
-$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$
-- **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$
-- **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.
+-   **R-squared**: As seen in the linear regression session. $$
+    R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
+    $$
+-   **Adjusted R-squared**: seen before $$
+    R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
+    $$
+-   **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. $$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$
+-   **Root Mean Squared Error (RMSE)**: square root of the MSE $$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$
+-   **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$
+-   **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.
 
 ## Live demo
+
 *KNN model for classification*"
NBISweden,workshop-mlbiostatistics,6c9c4c072bbabae2752e751bbba48195d9123319,evaf,eva@freyhult.net,2023-03-13T14:11:42Z,evaf,eva@freyhult.net,2023-03-13T14:11:42Z,Fixed typos in supervise and feature selection,session-feature-selection/case-study.qmd;session-feature-selection/intro.qmd;session-supervise/KNN-demo.qmd;session-supervise/exercises.qmd;session-supervise/intro.qmd,True,False,True,False,34,27,61,"---FILE: session-feature-selection/case-study.qmd---
@@ -84,7 +84,7 @@ data_cor <- data_diabetes %>%
 # visualize correlation via heatmap
 ggcorrplot(data_cor, hc.order = TRUE, lab = FALSE)
 
-# bass on the number of missing data, let's delete bp.2s, bp.2d
+# based on the number of missing data, let's delete bp.2s, bp.2d
 # and use complete-cases analysis 
 data_diabetes_narm <- data_diabetes %>%
   dplyr::select(-bp.2s, -bp.2d) %>%

---FILE: session-feature-selection/intro.qmd---
@@ -6,9 +6,9 @@ editor_options:
 
 # Introduction
 
--   Quite often we are not only interested in building the best predictive model but we would also like to know which features are the key ones, e.g. which genes measurements allow us to tell healthy and tumor tissues apart.
+-   Quite often we are not only interested in building the best predictive model but we would also like to know which features are the key ones, e.g. which gene measurements allow us to tell healthy and tumor tissues apart.
 -   We have already seen some examples of feature selection when we talked about regression (e.g. forward selection) but these may not be best in the context of omics data, where typically number of features exceeds the number of samples the features are measures for ($p \gg n$).
--   Additionally, features selection often goes hand in hand with the feature engineering part of the supervised learning.
+-   Additionally, feature selection often goes hand in hand with the feature engineering part of the supervised learning.
 -   Let's explain briefly what is feature engineering is, define main groups of feature selection and dive into regularized regression, one of the embedded methods of feature selection. Finally, we will put everything together into a more realistic predictive modeling case study using `tidymodels` framework.
 
 ## Feature engineering
@@ -45,7 +45,7 @@ data_diabetes <- input_diabetes %>%
 
 Feature engineering refers to techniques in machine learning that are used to prepare data for modeling and in turn improve the performance of machine learning models. Depending on the data, question of interest and modeling strategy such as chosen algorithm, these techniques may include:
 
--   **scaling** of numerical features, e.g. scaling to 0 and 1 scale to prevent features with larger scales dominating the model. By default we used scaling with `kknn()` function as it is based on calculating Euclidean distance.
+-   **scaling** of numerical features, e.g. scaling to mean 0 and standard deviation 1 scale to prevent features with larger scales dominating the model. By default we used scaling with `kknn()` function as it is based on calculating Euclidean distance.
 -   **normalization** and/or **transformations**
 -   representing categorical variables with **dummy variables** or **one-hot encoding** to create numerical features. For instance a categorical variable `obese` with three possible vales (underweight, healthy, overweight) can be transformed into two binary variables: ""is_healthy"", and ""is_overweight"", where the value of each variable is 1 if the observation belongs to that category and 0 otherwise. Only $k-1$ binary variables to encode $k$ categories. In one-hot encoding $k$ binary variables are created.
 
@@ -73,7 +73,7 @@ data_dummy %>%
 
 ```
 
--   **handing missing data** via imputations (mean, median, KNN-based) or deleting strategies such as list-wise deletion (complete-case analysis) or pair-wise deletion (available-case analysis)
+-   **handling missing data** via imputations (mean, median, KNN-based) or deleting strategies such as list-wise deletion (complete-case analysis) or pair-wise deletion (available-case analysis)
 - **handling imbalanced data** e.g. via down-sampling and up-sampling strategies or generating synthetic instances e.g. with SMOTE [@fernandez2018smote] or ADASYN [@4633969]
 ```{r}
 #| label: imbalanced-data
@@ -123,9 +123,9 @@ Feature selection is the process of selecting the most relevant and informative
 
 Regularized regression expands on the regression by adding a penalty term or terms to shrink the model coefficients of less important features towards zero. This can help to prevent overfitting and improve the accuracy of the predictive model. Depending on the penalty added, we talk about **Ridge**, **Lasso** or **Elastic Nets** regression.
 
-Previously when talking about regression, we saw that the least squares fitting procedure estimates model coefficients $\beta_0, \beta_1, \cdots, \beta_p$ using the values that minimize the residual sum of squares: $$RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{i=1}^{p}\beta_jx_{ij} \right)^2$$ {#eq-lm}
+Previously when talking about regression, we saw that the least squares fitting procedure estimates model coefficients $\beta_0, \beta_1, \cdots, \beta_p$ using the values that minimize the residual sum of squares: $$RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2$$ {#eq-lm}
 
-In **regularized regression** the coefficients are estimated by minimizing slightly different quantity. In **Ridge regression** we estimate $\hat\beta^{L}$ that minimizes $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{i=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2$$ {#eq-ridge}
+In **regularized regression** the coefficients are estimated by minimizing slightly different quantity. In **Ridge regression** we estimate $\hat\beta^{L}$ that minimizes $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2$$ {#eq-ridge}
 
 where:
 
@@ -215,19 +215,19 @@ data_plot %>%
 
 ## Bias-variance trade-off
 
-Ridge regression's advantages over least squares estimates stems from **bias-variance trade-off**, another fundamental concept in machine learning that.
+Ridge regression's advantages over least squares estimates stems from **bias-variance trade-off**, another fundamental concept in machine learning.
 
 -   The bias-variance trade-off describes the relationship between model complexity, prediction accuracy, and the ability of the model to generalize to new data.
 -   **Bias** refers to the error that is introduced by approximating a real-life problem with a simplified model. A high bias model is one that makes overly simplistic assumptions about the underlying data, resulting in *under-fitting* and poor accuracy.
 -   **Variance** refers to the sensitivity of a model to fluctuations in the training data. A high variance model is one that is overly complex and captures noise in the training data, resulting in *overfitting* and poor generalization to new data.
 -   The goal of machine learning is to find a model with **the right balance between bias and variance**, which can generalize well to new data.
 -   The bias-variance trade-off can be visualized in terms of MSE, means squared error of the model. The **MSE** can be decomposed into: $$MSE(\hat\beta) := bias^2(\hat\beta) + Var(\hat\beta) + noise$$
 -   The irreducible error is the inherent noise in the data that cannot be reduced by any model, while the bias and variance terms can be reduced by choosing an appropriate model complexity. The trade-off lies in finding the right balance between bias and variance that minimizes the total MSE.
--   In practice, this trade-off can be addressed by **regularizing the model**, selecting an appropriate model complexity, or by using ensemble methods that combine multiple models to reduce the variance (e.g. Random Forest). Ultimately, the goal is to find a model that is both accurate and generalization.
+-   In practice, this trade-off can be addressed by **regularizing the model**, selecting an appropriate model complexity, or by using ensemble methods that combine multiple models to reduce the variance (e.g. Random Forest). Ultimately, the goal is to find a model that is both accurate and generalizing.
 
 ```{r}
 #| label: fig-bias-variance
-#| fig-cap: Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. all, A Introduction to statistical learning
+#| fig-cap: Squared bias, variance and test mean squared error for ridge regression predictions on a simulated data as a function of lambda demonstrating bias-variance trade-off. Based on Gareth James et. al, A Introduction to statistical learning
 #| fig-cap-location: margin
 #| fig-align: center
 #| echo: false
@@ -238,9 +238,9 @@ knitr::include_graphics(""figures/bias-variance.png"")
 
 ## Ridge, Lasso and Elastic Nets
 
-In **Ridge** regression we minimize: $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{i=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2$$ {#eq-ridge2} where $\lambda \sum_{j=1}^{p}\beta_j^2$ is also known as **L2** regularization element or $l_2$ penalty
+In **Ridge** regression we minimize: $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}\beta_j^2$$ {#eq-ridge2} where $\lambda \sum_{j=1}^{p}\beta_j^2$ is also known as **L2** regularization element or $l_2$ penalty
 
-In **Lasso** regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{i=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| = RSS + \lambda \sum_{j=1}^{p}|\beta_j|$$ {#eq-lasso} where $\lambda \sum_{j=1}^{p}|\beta_j|$ is also known as *L1* regularization element or $l_1$ penalty
+In **Lasso** regression, that is Least Absolute Shrinkage and Selection Operator regression we change penalty term to absolute value of the regression coefficients: $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| = RSS + \lambda \sum_{j=1}^{p}|\beta_j|$$ {#eq-lasso} where $\lambda \sum_{j=1}^{p}|\beta_j|$ is also known as **L1** regularization element or $l_1$ penalty
 
 Lasso regression was introduced to help model interpretation. With Ridge regression we improve model performance but unless $\lambda = \infty$ all beta coefficients are non-zero, hence all variables remain in the model. By using $l_1$ penalty we can force some of the coefficients estimates to be exactly equal to 0, hence perform **variable selection**
 
@@ -284,7 +284,7 @@ data_plot %>%
 
 ```
 
-**Elastic Net** use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{i=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 $$ {#eq-elastic-net}
+**Elastic Net** use both L1 and L2 penalties to try to find middle grounds by performing parameter shrinkage and variable selection. $$\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 = RSS + \lambda \sum_{j=1}^{p}|\beta_j| + \lambda \sum_{j=1}^{p}\beta_j^2 $$ {#eq-elastic-net}
 
 In the `glmnet` library we can fit Elastic Net by setting parameters $\alpha$. Actually, under the hood `glmnet` minimizes a cost function: $$\sum_{i_=1}^{n}(y_i-\hat y_i)^2 + \lambda \left ( (1-\alpha) \sum_{j=1}^{p}\beta_j^2 + \alpha \sum_{j=1}^{p}|\beta_j|\right )$$ where:
 

---FILE: session-supervise/KNN-demo.qmd---
@@ -10,7 +10,7 @@ editor_options:
 Let's try to build a classifier to predict obesity (Obese vs Non-obese) given our diabetes data set. To start simple:
 
 - we will see how well we can predict obesity given `waist` and `hdl` variables
-- we will use data splitting into train, validation and test, i.e. not cross-validation with th help of `splitTools()` library
+- we will use data splitting into train, validation and test, i.e. not cross-validation with the help of `splitTools()` library
 - we will use KNN algorithm as implemented in `kknn()` function in `library(kknn)`
 
 
@@ -127,12 +127,12 @@ k_values <- seq(1, n-1, 2) # check every odd value of k between 1 and number of
 # allocate empty vector to collect overall classification rate for each k
 cls_rate <- rep(0, length(k_values)) 
 
-for (k in seq_along(k_values))
+for (l in seq_along(k_values))
 {
   
   # fit model given k value
   model <- kknn(obese ~., data_train, data_valid, 
-                k = k_values[k], 
+                k = k_values[l], 
                 kernel = ""rectangular"")
   
   # extract predicted class (predicted obesity status)
@@ -142,7 +142,7 @@ for (k in seq_along(k_values))
   cls_true <- data_valid$obese
   
   # calculate overall classification rate
-  cls_rate[k] <- sum((cls_pred == cls_true))/length(cls_pred)
+  cls_rate[l] <- sum((cls_pred == cls_true))/length(cls_pred)
   
 }
 

---FILE: session-supervise/exercises.qmd---
@@ -19,7 +19,7 @@ Hint: you can use ""create_folds()"" function from ""library(splitTools)"" to create
 
 ## Supervised regression
 
-Let's revisit regression in a context of supervised learning. Using the `diabetets` data set and data splitting find the best model to predict BMI scores. 
+Let's revisit regression in a context of supervised learning. Using the `diabetes` data set and data splitting find the best model to predict BMI scores. 
 
 Split the data into train (60%), validation (20%) and test (20%). Assess three regression models on the validation set using RMSE. Which model seems to be the best in terms of predicting BMI? What would be the expected performance on the new unseen data?
  

---FILE: session-supervise/intro.qmd---
@@ -69,7 +69,7 @@ And we will leave feature engineering and feature selection for the next session
 
 ```{r knn-example}
 #| label: fig-knn
-#| fig-cap: An example of k-nearest neighbours algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangales samples based on its gene A and gene B measurments and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurments and is thus classified by majority vote as a black dot (D).
+#| fig-cap: An example of k-nearest neighbours algorithm with k=3; A) in the top a new sample (blue) is closest to three red triangle samples based on its gene A and gene B measurments and thus is classified as a red (B); in the bottom (C), a new sample (blue) is closest to 2 black dots and 1 red triangle based on its gene A and B measurments and is thus classified by majority vote as a black dot (D).
 #| fig-width: 8
 #| fig-height: 8
 #| fig-align: center
@@ -146,7 +146,7 @@ points(x[n.idx2[2:3]], y[n.idx2[2:3]], pch=19, col=""black"")
 ## Data splitting
 
 -   Part of the issue of fitting complex models to data is that the model can be continually tweaked to adapt as well as possible.
--   As a results the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.
+-   As a result the trained model may not generalize well on future data due to the added complexity that only works for a given unique data set, leading to **overfitting**.
 -   To deal with overconfident estimation of future performance we can implement various data splitting strategies.
 
 ### train, validation & test sets {.unnumbered}
@@ -175,7 +175,7 @@ knitr::include_graphics(""figures/data-split.png"")
 -   We start by setting the validation data to be the first set of data and the training data to be all other sets.
 -   We estimate the validation error rate / correct classification rate for the split.
 -   We then repeat the process $k-1$ times, each time with a different part of the data set to be the validation data and the remainder being the training data.
--   We finish with $k$ different error of correct classification rates.
+-   We finish with $k$ different error or correct classification rates.
 -   In this way, every data point has its class membership predicted once.
 -   The final reported error rate is usually the average of $k$ error rates.
 
@@ -188,6 +188,7 @@ knitr::include_graphics(""figures/data-split.png"")
 #| out-width: 100%
 #| 
 knitr::include_graphics(""figures/data-split-kfolds.png"")
+##Eva: k=1,2,3 look identical. Can the splits be indicated somehow, circle the validation fold or something similar? I see 20 different objects, so I would expect 6 or 7 objects in each validation set. Use 'validation set' and 'training set' instead of 'validation fold' etc.
 ```
 
 ### repeated cross validation {.unnumbered}
@@ -211,7 +212,7 @@ knitr::include_graphics(""figures/data-split-loocv.png"")
 ## Evaluating classification
 
 -   To train the model we need some way of evaluating how well it works so we know how to tune the model parameters, e.g. change the value of $k$ in KNN.
--   There are few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.
+-   There are a few measures being used that involve looking at the truth (labels) and comparing it to what was predicted by the model.
 -   Common measures include: correct (overall) classification rate, missclassification rate, class specific rates, cross classification tables, sensitivity and specificity and ROC curves.
 
 **Correct (miss)classification rate**
@@ -245,15 +246,21 @@ Based on the confusion matrix, we can derive common performance metrics of a bin
 - **ROC AUC**: the receiver operating characteristic (ROC) curve is a graphical representation of the trade off between sensitivity and specificity for different threshold values. The area under the ROC curve (AUC) is a performance metric that ranges from 0 to 1, with a higher value indicating better performance. AUC is a measure of how well the classifier is able to distinguish between positive and negative samples.
 
 ## Evaluating regression
-The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression mode; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include: 
-
+The idea of using data splits to train the model holds for fitting regression models. We can use data splits to train and assess regression models. For instance thinking back about the regression examples we have seen in previous section, we could try to find the best regression model to predict BMI given all other variables in the diabetes data set such as age, waist or cholesterol measurements. In the next section we will also learn about regularized regression where a penalty term is added to improve the generalization of a regression model; the penalty term(s) is optimized during the training of the model. Some common performance metric used in supervised regression include: 
 
+- **R-squared**: As seen in the linear regression session.
+$$
+R^2=1-\frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
+$$
 - **Adjusted R-squared**: seen before
+$$
+R_{adj}^2=1-\frac{RSS}{TSS}\frac{n-1}{n-p-1} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}\frac{n-1}{n-p-1}
+$$
 - **Mean Squared Error (MSE)**: average squared difference between the predicted values and the actual values. 
-$$RMSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y_i})^2$$
+$$MSE = \frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2$$
 - **Root Mean Squared Error (RMSE)**: square root of the MSE 
-$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y_i})^2}$$
-- **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y_i}|$$
+$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}({y_i}-\hat{y}_i)^2}$$
+- **MAE**: average absolute difference between the predicted values and the actual values $$MAE = \frac{1}{N}\sum_{i=1}^{N}|{y_i}-\hat{y}_i|$$
 - **Mean Absolute Percentage Error (MAPE)**: average percentage difference between the predicted values and the actual values.
 
 "
NBISweden,workshop-mlbiostatistics,c3de0be5c4f2ce56c103ef4a29e2f4c5bdeb308b,evaf,eva@freyhult.net,2023-03-10T14:33:59Z,evaf,eva@freyhult.net,2023-03-10T14:33:59Z,Fix precourse typos,session-precourse-math/math-differentiation.qmd;session-precourse-math/math-functions.qmd;session-precourse-math/math-matrix.qmd;session-precourse-math/math-notations.qmd;session-precourse-math/math-sets.qmd,True,False,True,False,19,21,40,"---FILE: session-precourse-math/math-differentiation.qmd---
@@ -38,13 +38,13 @@ The function $f(x) = x^4 - 4x^3 - x^2 - e^{-x}$ changes at different rates for d
 
 - between $x \in (-10, -9)$ the $f(x)$ is increasing at slightly higher pace than $x \in (5,6)$ 
 - between $x \in (-7, -5)$ the $f(x)$ is decreasing and
-- between $x \in (0, 1)$ the $f(x)$ is not changing
+- between $x \in (0, 1)$ the $f(x)$ is (almost) not changing
 - to be able to talk more precisely about the rate of change than just saying ""large and positive"" or ""small and negative"" change we need to quantify the changes, i.e. assign the rate of change an exact value
 - **Differentiation** is a technique for calculating the rate of change of any function
 
 ## Average rate of change across an interval
 ```{r}
-#| label: diff-01
+#| label: fig-diff-01
 #| echo: false
 #| out-width: 8in
 #| out-height: 5in
@@ -56,15 +56,13 @@ knitr::include_graphics(""figures/precourse/math-differentiation-01.png"")
 
 ```
 
-To dive further into calculating the rate of change let's look at Figure \@ref(fig:diff-01) and define the *average rate of change* of a function across an interval. Figure \@ref(fig:diff-01) shows a function $f(x)$ with two possible argument values $a$ and $b$ marked and their corresponding function values $f(a)$ and $f(b)$. 
+To dive further into calculating the rate of change let's look at @fig-diff-01 and define the *average rate of change* of a function across an interval. @fig-diff-01 shows a function $f(x)$ with two possible argument values $a$ and $b$ marked and their corresponding function values $f(a)$ and $f(b)$. 
 
 Consider that $x$ is increasing from $a$ to $b$. The change in $x$ is $b-a$, i.e. as $x$ increases from $a$ to $b$ the function $f(x)$ increase from $f(a)$ to $f(b)$. The change in $f(x)$ is $f(b)-f(a)$ and the average rate of change of $y$ across the $[a,b]$ interval is:
 
-\begin{equation}
+$$
 \frac{change\:in\:y}{change\:in\:x}=\frac{f(b)-f(a)}{b-a}
-(\#eq:diff-point)
-\end{equation}
-
+$$ {#eq-diff-point}
 
 E.g. let's take a quadratic function $f(x)=x^2$ and calculate the average rate of change across the interval $[1, 4]$. 
 
@@ -90,12 +88,12 @@ plot(x,y, type=""l"", las=1, ylab=""f(x)"")
 
 ## Rate of change at a point
 - We often need to know the rate of change of a function at a point, and not simply an average rate of change across an interval.
-- Figure \@ref(fig:diff-02), similar to Figure \@ref(fig:diff-01), shows, instead of two points $a$ and $b$, point $a$ and a second point defined in terms of its distance from the first point $a$. Thus, the two points are now $a$ and $a + h$ and the distance between the two points is equal to $h$.
+- @fig-diff-02, similar to @fig-diff-01, shows, instead of two points $a$ and $b$, point $a$ and a second point defined in terms of its distance from the first point $a$. Thus, the two points are now $a$ and $a + h$ and the distance between the two points is equal to $h$.
 - Now we can write that:
 $$\frac{change\:in\:y}{change\:in\:x}=\frac{f(a+h)-f(a)}{a+h-a} = \frac{f(a+h)-f(a)}{h}$$
 
 ```{r}
-#| label: diff-02
+#| label: fig-diff-02
 #| echo: false
 #| fig-dpi: 300
 #| out-width: 8in
@@ -116,10 +114,9 @@ Further:
 - the term $\delta$ reads as ""delta"" and represents a small change
 
 We can thus continue and write that a **rate of change of a function at a point** is given by
-\begin{equation}
+$$
 \frac{small\:change\:in\:y}{small\:change\:in\:x} = \lim_{h\to0}\frac{f(a+h)-f(a)}{h}
-(\#eq:diff-point-2)
-\end{equation}
+$$ {#eq-diff-point-2}
 
 <br />
 

---FILE: session-precourse-math/math-functions.qmd---
@@ -96,7 +96,7 @@ plot(x, y, xlab=""temperature [Celsius]"", ylab=""temperature [Farenheit]"", type=""b
 
 - constant function $f(x) = a$
 - identity function $f(x) = x$
-- linear function $f(x) = ax + b$
+- linear function $f(x) = a + bx$
 - quadratic function $f(x) = a + bx + cx^2$
 - cubic function $fx() = a + bx + cx^2 + dx^3$
 

---FILE: session-precourse-math/math-matrix.qmd---
@@ -123,9 +123,9 @@ $$\mathbf{C} = \mathbf{A} \cdot \mathbf{B}  = \begin{bmatrix}
 \end{bmatrix}$$
 
 ## Inverse of a matrix 
-For a square matrix $\mathbf{A}$ there may exist a matrix $\mathbf{B}$ such that $\mathbf{A} \cdot \mathbf{B} = \mathbf{I}$. An **inverse**, if it exists, is denoted as $\mathbf{A}^{-1}$ and we can rewrite the definition as $$\mathbf{A} \cdot \mathbf{A}^{-1} = \mathbf{I}$$ where $\mathbf{I}$ is an identify matrix (equivalent to 1). There is no division for matrices, instead we can use inverse to multiply the matrix by an inverse, similar to when instead of dividing the number $a$ by $b$ we multiply $a$ by reciprocal of $b = \frac{1}{b}$
+For a square matrix $\mathbf{A}$ there may exist a matrix $\mathbf{B}$ such that $\mathbf{A} \cdot \mathbf{B} = \mathbf{I}$. An **inverse**, if it exists, is denoted as $\mathbf{A}^{-1}$ and we can rewrite the definition as $$\mathbf{A} \cdot \mathbf{A}^{-1} = \mathbf{I}$$ where $\mathbf{I}$ is an identity matrix (equivalent to 1). There is no division for matrices, instead we can use inverse to multiply the matrix by an inverse, similar to when instead of dividing the number $a$ by $b$ we multiply $a$ by reciprocal of $b = \frac{1}{b}$
 
-For a 2-dimensional matrix we can follow the below formula for obtaining the inverse 
+For a $2 \times 2$ matrix we can follow the below formula for obtaining the inverse 
 $$\begin{bmatrix}
   x_{11} & x_{12}   \\
   x_{21} & x_{22} 

---FILE: session-precourse-math/math-notations.qmd---
@@ -28,7 +28,7 @@ Mathematics gives us a precise language to communicate different concepts and id
 
 - **variables**: things that can vary, e.g. temperature and time
 - **constants**: fixed and unchanging quantities used in certain calculations, e.g. 3.14159
-- in principle one could freely choose letters and symbols to represent variables and constants, but it is helpful and choose letters and symbols that have meaning in a particular context. Hence, we
+- in principle one could freely choose letters and symbols to represent variables and constants, but it is helpful and choose letters and symbols that have meaning in a particular context. Hence,
 - $x, y, z$, the end of the alphabet is reserved for variables
 - $a, b, c$, the beginning of the alphabet is used to represent constants
 - $\pi$, $\omega$ and Greek letters below are used frequently used to represent common constant, e.g. $\pi = 3.14159$
@@ -62,7 +62,8 @@ If the letters $x$ and $y$ represent two numbers, then:
 - their **sum** is written as $x + y$
 - subtracting $y$ from $x$ is $x - y$, known also as **difference**
 - to multiply $x$ and $y$ we written as $x \cdot y$ or also with the multiplication signed omitted as $xy$. The quantity is known as **product of x and y**
-- multiplication is **associative**, e.g. when we multiply three numbers together, $x \cdot y \cdot z$, the order of multiplication does not matter, so $x \cdot y \cdot z$ is the same as $z \cdot x \cdot y$ or $y \cdot z \cdot x$
+- multiplication is **commutative**, e.g. when we multiply three numbers together, $x \cdot y \cdot z$, the order of multiplication does not matter, so $x \cdot y \cdot z$ is the same as $z \cdot x \cdot y$ or $y \cdot z \cdot x$
+<!-- Isn't the associative law that (x*y)*z=x*(y*z), whereas the described above is commutative? -->
 - division is denoted by $\frac{x}{y}$ and means that $x$ is divided by $y$. In this expression $x$, on the top, is called **numerator** and $y$, on the bottom, is called **denominator**
 - division by 1 leaves any number unchanged, e.g. $\frac{x}{1}=x$ and division by 0 is not allowed
 
@@ -94,7 +95,7 @@ Given any two real numbers $a$ and $b$ there are three mutually exclusive possib
 **Strict and weak**
 
 - inequality in $a > b$ and $a < b$ is **strict**
-- as oppose to **weak** inequality denoted as $a \ge b$ or $a \le b$
+- as opposed to **weak** inequality denoted as $a \ge b$ or $a \le b$
 
 <br />
 Some useful relations are:

---FILE: session-precourse-math/math-sets.qmd---
@@ -32,8 +32,8 @@ library(""ggvenn"")
 
 <br />
 
--   **subset,** $\subseteq$: if every element of set A is also in B, then A is said to be a subset of B, written as $A \subseteq B$ and pronounced A is contained in B, e.g. $A \subseteq B$, when $A = \{2, 4, 6\}$ and \$ = $B = \{2, 4, 6, 8, 10\}$. Every set is a subset of itself.
--   **superset**: for our outs $A$ and $B$ we can also say that $B$ is a **superset** of $A$ and write $B \supset A$
+-   **subset,** $\subseteq$: if every element of set A is also in B, then A is said to be a subset of B, written as $A \subseteq B$ and pronounced A is contained in B, e.g. $A \subseteq B$, when $A = \{2, 4, 6\}$ and $B = \{2, 4, 6, 8, 10\}$. Every set is a subset of itself.
+-   **superset**: for our sets $A$ and $B$ we can also say that $B$ is a **superset** of $A$ and write $B \supset A$
 
 <br />
 
@@ -43,7 +43,7 @@ library(""ggvenn"")
 ## Basic set operations
 
 -   **union of two sets,** $\cup$ : two sets can be ""added"" together, the union of A and B, written as $A \cup B$, e.g. $\{1, 2\} \cup \{2, 3\} = \{1, 2, 3\}$ or $\{1, 2, 3\} \cup \{1, 4, 5, 6\} = \{1, 2, 3, 4, 5, 6\}$
--   **intersection of two sets,** $\cap$: a new set can be constructed by taking members of two sets that are ""in common"", written as $A \cap B$, e.g. $\{1, 2, 3, 4, 5, 6\} \cap \{2, 3, 7\} = \{2, 3\}$ or $\{1, 2, 3\} \cap \{7 \} = \{\emptyset \}$
+-   **intersection of two sets,** $\cap$: a new set can be constructed by taking members of two sets that are ""in common"", written as $A \cap B$, e.g. $\{1, 2, 3, 4, 5, 6\} \cap \{2, 3, 7\} = \{2, 3\}$ or $\{1, 2, 3\} \cap \{7 \} = \emptyset$
 
 <br/>
 "
NBISweden,workshop-mlbiostatistics,247f8546b3884be48569e89b2486505e64e01d7c,evaf,eva@freyhult.net,2023-01-12T13:39:00Z,evaf,eva@freyhult.net,2023-01-12T17:41:47Z,Fix typo in ?sample question,precourse-R-quiz/R-quiz.qmd,True,False,True,False,2,2,4,"---FILE: precourse-R-quiz/R-quiz.qmd---
@@ -47,9 +47,9 @@ Use help() or ?() to find out more about `read.csv()` function. Which of the bel
 - footer (correct)
 
 ### Q
-Use help() or ?() to find out more about `sample()` function. Which of the below is **NOT** an argument of the `read.csv()`?
+Use help() or ?() to find out more about `sample()` function. Which of the below is **NOT** an argument of the `sample()`?
 
-- n, the number of items to choose from
+- x, the elements to choose from
 - size, a non-negative integer giving the number of items to choose
 - replace, logical data type, TRUE for sampling with replacement
 - useHash, logical data type, TRUE for using hash-version of the algorithm (correct)"
NBISweden,workshop-mlbiostatistics,8b4f11b03bb9d1f37695f6165a2539493cef2763,Mun-Gwan Hong,mungwan.hong@nbis.se,2022-09-14T21:58:14Z,Mun-Gwan Hong,mungwan.hong@nbis.se,2022-09-14T21:58:14Z,fix the name of the author,session-pca/exercises-solutions.Rmd,True,False,True,False,1,1,2,"---FILE: session-pca/exercises-solutions.Rmd---
@@ -1,6 +1,6 @@
 ---
 title: ""Exercises and solutions: PCA""
-author: ""Eva Freyhult, Mun-Gwan Hong""
+author: ""Mun-Gwan Hong""
 date: ""2022-09-15""
 output:
   html_document:"
NBISweden,workshop-mlbiostatistics,bb3459fb39e64ab2bf05faddf4eb6ad448184027,Mun-Gwan Hong,mungwan.hong@nbis.se,2022-09-14T21:37:02Z,Mun-Gwan Hong,mungwan.hong@nbis.se,2022-09-14T21:37:02Z,Fix adjust_html_for_canvas.sh for session-pca,session-pca/html/adjust_html_for_canvas.sh,False,False,False,False,1,1,2,"---FILE: session-pca/html/adjust_html_for_canvas.sh---
@@ -1,5 +1,5 @@
 #!/bin/zsh
-COURSE_GITHUB_IO=""https://nbisweden.github.io/workshop-mlbiostatistics/session-clustering/html""
+COURSE_GITHUB_IO=""https://nbisweden.github.io/workshop-mlbiostatistics/session-pca/html""
 
 for html_out in chapters exercises exercises-solutions
 do"
NBISweden,workshop-mlbiostatistics,23654347775654fa503689e9a14699533c5ad069,Julie Lorent,5382593+jlorent@users.noreply.github.com,2022-09-14T10:02:37Z,Julie Lorent,5382593+jlorent@users.noreply.github.com,2022-09-14T10:02:37Z,fix typos and info data and exercises files,session-glm/.Rprofile;session-glm/304-linear-GLM.Rmd;session-glm/_bookdown_files/304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png;session-glm/_bookdown_files/304-linear-GLM_files/figure-html/unnamed-chunk-6-1.png;session-glm/docs/304-linear-GLM.md;session-glm/docs/304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png;session-glm/docs/304-linear-GLM_files/figure-html/unnamed-chunk-6-1.png;session-glm/docs/404.html;session-glm/docs/generalized-linear-models.html;session-glm/docs/index.html;session-glm/docs/index.md;session-glm/docs/search_index.json;session-glm/index.Rmd;session-glm/renv/.gitignore;session-glm/renv/activate.R,True,False,True,False,599,92,691,"---FILE: session-glm/.Rprofile---
@@ -0,0 +1 @@
+source(""renv/activate.R"")

---FILE: session-glm/304-linear-GLM.Rmd---
@@ -72,8 +72,8 @@ $$log(\frac{p_i}{1-p_i})=\beta_0 + \beta_1x_i$$
 and given the properties of logarithms this is also equivalent to:
 $$p_i = \frac{exp(\beta_0 + \beta_1x_i)}{1 + exp(\beta_0 + \beta_1x_i)}$$
 - In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a **link function**.
-- Here, the **link function** $log(\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (hearing Yanny) the linear predictor (age) 
-- Thus **GLM model** can be written as $$g(\mu_i)=\mathbf{X}\boldsymbol\beta$$ where `g()` is the link function.
+- Here, the **link function** $log(\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (hearing Yanny) and the linear predictor (age) 
+- Thus the **GLM model** can be written as $$g(\mu_i)=\mathbf{X}\boldsymbol\beta$$ where `g()` is the link function.
 - We use `glm()` function in R to fit GLM models
 
 ```{r, fig.align=""center"", fig.width=4, fig.height=3, fig.cap=""Fitted logistic model to the Yanny and Laurel data""}
@@ -117,7 +117,7 @@ qchisq(df=1, p=0.95)
 
 - In logistic regression we often interpret the model coefficients by taking $e^{\hat{\beta}}$
 - and we talk about **odd ratios**
-- e.g. we can say, given our above model, $e^{-0.02444} = 0.9758562$ that for each unit increase in age the odds of hearing Laurel get multiplied by 0.98
+- e.g. we can say, given our above model, $e^{-0.02444} = 0.9758562$ that for each unit increase in age the odds of hearing Yanny get multiplied by 0.98
 
 **Other covariates**
 
@@ -204,13 +204,13 @@ Suppose we wish to model $Y_i$ the number of cancer cases in the i-th intermedia
 - E\_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013
 - pm10: air pollution
 - smoke: percentage of people in an area that smoke
-- ethic: percentage of people who are non-white
-- logpice: natural log of average house price
+- ethnic: percentage of people who are non-white
+- log.price: natural log of average house price
 - easting and northing: co-ordinates of the central point of the IG divided by 10000
 
 We can model the **rate of occurrence of cancer** using the very same `glm` function:¨
 - now we use **poisson family distribution** to model counts
-- and we will include an **offset term** to model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions
+- and we will include an **offset term** to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions
 
 ```{r, collapse=TRUE}
 # Read in and preview data
@@ -231,9 +231,9 @@ print(summary(epid1))
 
 **Rate ratio**
 
-- similarly to logistic regression it common to look at the $e^\beta$
+- similarly to logistic regression, it is common to look at the $e^\beta$
 - for instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient
-- coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air poluttion
+- coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air pollution
 - the rate ratio allows us to quantify by how much, here by a factor of $e^{0.0500269} = 1.05$ 
 
 -----
@@ -243,6 +243,7 @@ print(summary(epid1))
 ```{exercise, ""glm-rerun""}
 
 Make sure you can run and understand the above code for logistic and Poisson regression
+The data files can be downloaded in Canvas from Files/data-exercises/linear-models. An Rmd file with the code from the book chapter is provided in Files/exercises/GLM
 
 ```
 

---FILE: session-glm/docs/304-linear-GLM.md---
@@ -72,8 +72,8 @@ $$log(\frac{p_i}{1-p_i})=\beta_0 + \beta_1x_i$$
 and given the properties of logarithms this is also equivalent to:
 $$p_i = \frac{exp(\beta_0 + \beta_1x_i)}{1 + exp(\beta_0 + \beta_1x_i)}$$
 - In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a **link function**.
-- Here, the **link function** $log(\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (hearing Yanny) the linear predictor (age) 
-- Thus **GLM model** can be written as $$g(\mu_i)=\mathbf{X}\boldsymbol\beta$$ where `g()` is the link function.
+- Here, the **link function** $log(\frac{p_i}{1-p_i})$ provides the link between the binomial distribution of $Y_i$ (hearing Yanny) and the linear predictor (age) 
+- Thus the **GLM model** can be written as $$g(\mu_i)=\mathbf{X}\boldsymbol\beta$$ where `g()` is the link function.
 - We use `glm()` function in R to fit GLM models
 
 
@@ -206,7 +206,7 @@ qchisq(df=1, p=0.95)
 
 - In logistic regression we often interpret the model coefficients by taking $e^{\hat{\beta}}$
 - and we talk about **odd ratios**
-- e.g. we can say, given our above model, $e^{-0.02444} = 0.9758562$ that for each unit increase in age the odds of hearing Laurel get multiplied by 0.98
+- e.g. we can say, given our above model, $e^{-0.02444} = 0.9758562$ that for each unit increase in age the odds of hearing Yanny get multiplied by 0.98
 
 **Other covariates**
 
@@ -355,13 +355,13 @@ Suppose we wish to model $Y_i$ the number of cancer cases in the i-th intermedia
 - E\_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013
 - pm10: air pollution
 - smoke: percentage of people in an area that smoke
-- ethic: percentage of people who are non-white
-- logpice: natural log of average house price
+- ethnic: percentage of people who are non-white
+- log.price: natural log of average house price
 - easting and northing: co-ordinates of the central point of the IG divided by 10000
 
 We can model the **rate of occurrence of cancer** using the very same `glm` function:¨
 - now we use **poisson family distribution** to model counts
-- and we will include an **offset term** to model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions
+- and we will include an **offset term** to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions
 
 
 ```r
@@ -417,9 +417,9 @@ print(summary(epid1))
 
 **Rate ratio**
 
-- similarly to logistic regression it common to look at the $e^\beta$
+- similarly to logistic regression, it is common to look at the $e^\beta$
 - for instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient
-- coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air poluttion
+- coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air pollution
 - the rate ratio allows us to quantify by how much, here by a factor of $e^{0.0500269} = 1.05$ 
 
 -----
@@ -428,6 +428,7 @@ print(summary(epid1))
 
 \BeginKnitrBlock{exercise}<div class=""exercise""><span class=""exercise"" id=""exr:glm-rerun""><strong>(\#exr:glm-rerun) </strong></span>
 Make sure you can run and understand the above code for logistic and Poisson regression
+The data files can be downloaded in Canvas from Files/data-exercises/linear-models. An Rmd file with the code from the book chapter is provided in Files/exercises/GLM
 </div>\EndKnitrBlock{exercise}
 
 

---FILE: session-glm/docs/404.html---
@@ -21,10 +21,10 @@
   <meta name=""twitter:description"" content=""Page not found | Introduction to GLM"" />
   
 
-<meta name=""author"" content=""Olga Dethlefsen"" />
+<meta name=""author"" content=""Olga Dethlefsen, Julie Lorent"" />
 
 
-<meta name=""date"" content=""2022-09-11"" />
+<meta name=""date"" content=""2022-09-14"" />
 
   <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
   <meta name=""apple-mobile-web-app-capable"" content=""yes"" />

---FILE: session-glm/docs/generalized-linear-models.html---
@@ -21,10 +21,10 @@
   <meta name=""twitter:description"" content=""Chapter 1 Generalized linear models | Introduction to GLM"" />
   
 
-<meta name=""author"" content=""Olga Dethlefsen"" />
+<meta name=""author"" content=""Olga Dethlefsen, Julie Lorent"" />
 
 
-<meta name=""date"" content=""2022-09-11"" />
+<meta name=""date"" content=""2022-09-14"" />
 
   <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
   <meta name=""apple-mobile-web-app-capable"" content=""yes"" />
@@ -231,8 +231,8 @@ <h2><span class=""header-section-number"">1.2</span> Logistic regression</h2>
 and given the properties of logarithms this is also equivalent to:
 <span class=""math display"">\[p_i = \frac{exp(\beta_0 + \beta_1x_i)}{1 + exp(\beta_0 + \beta_1x_i)}\]</span></li>
 <li>In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a <strong>link function</strong>.</li>
-<li>Here, the <strong>link function</strong> <span class=""math inline"">\(log(\frac{p_i}{1-p_i})\)</span> provides the link between the binomial distribution of <span class=""math inline"">\(Y_i\)</span> (hearing Yanny) the linear predictor (age)</li>
-<li>Thus <strong>GLM model</strong> can be written as <span class=""math display"">\[g(\mu_i)=\mathbf{X}\boldsymbol\beta\]</span> where <code>g()</code> is the link function.</li>
+<li>Here, the <strong>link function</strong> <span class=""math inline"">\(log(\frac{p_i}{1-p_i})\)</span> provides the link between the binomial distribution of <span class=""math inline"">\(Y_i\)</span> (hearing Yanny) and the linear predictor (age)</li>
+<li>Thus the <strong>GLM model</strong> can be written as <span class=""math display"">\[g(\mu_i)=\mathbf{X}\boldsymbol\beta\]</span> where <code>g()</code> is the link function.</li>
 <li>We use <code>glm()</code> function in R to fit GLM models</li>
 </ul>
 <div class=""sourceCode"" id=""cb2""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb2-1""><a href=""generalized-linear-models.html#cb2-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># fit logistic regression model</span></span>
@@ -350,7 +350,7 @@ <h2><span class=""header-section-number"">1.2</span> Logistic regression</h2>
 <ul>
 <li>In logistic regression we often interpret the model coefficients by taking <span class=""math inline"">\(e^{\hat{\beta}}\)</span></li>
 <li>and we talk about <strong>odd ratios</strong></li>
-<li>e.g. we can say, given our above model, <span class=""math inline"">\(e^{-0.02444} = 0.9758562\)</span> that for each unit increase in age the odds of hearing Laurel get multiplied by 0.98</li>
+<li>e.g. we can say, given our above model, <span class=""math inline"">\(e^{-0.02444} = 0.9758562\)</span> that for each unit increase in age the odds of hearing Yanny get multiplied by 0.98</li>
 </ul>
 <p><strong>Other covariates</strong></p>
 <ul>
@@ -487,13 +487,13 @@ <h2><span class=""header-section-number"">1.3</span> Poisson regression</h2>
 <li>E_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013</li>
 <li>pm10: air pollution</li>
 <li>smoke: percentage of people in an area that smoke</li>
-<li>ethic: percentage of people who are non-white</li>
-<li>logpice: natural log of average house price</li>
+<li>ethnic: percentage of people who are non-white</li>
+<li>log.price: natural log of average house price</li>
 <li>easting and northing: co-ordinates of the central point of the IG divided by 10000</li>
 </ul>
 <p>We can model the <strong>rate of occurrence of cancer</strong> using the very same <code>glm</code> function:¨
 - now we use <strong>poisson family distribution</strong> to model counts
-- and we will include an <strong>offset term</strong> to model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions</p>
+- and we will include an <strong>offset term</strong> to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions</p>
 <div class=""sourceCode"" id=""cb11""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb11-1""><a href=""generalized-linear-models.html#cb11-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Read in and preview data</span></span>
 <span id=""cb11-2""><a href=""generalized-linear-models.html#cb11-2"" aria-hidden=""true"" tabindex=""-1""></a>cancer <span class=""ot"">&lt;-</span> <span class=""fu"">read.csv</span>(<span class=""st"">&quot;data/lm/cancer.csv&quot;</span>)</span>
 <span id=""cb11-3""><a href=""generalized-linear-models.html#cb11-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">head</span>(cancer)</span>
@@ -544,9 +544,9 @@ <h2><span class=""header-section-number"">1.3</span> Poisson regression</h2>
 </ul>
 <p><strong>Rate ratio</strong></p>
 <ul>
-<li>similarly to logistic regression it common to look at the <span class=""math inline"">\(e^\beta\)</span></li>
+<li>similarly to logistic regression, it is common to look at the <span class=""math inline"">\(e^\beta\)</span></li>
 <li>for instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient</li>
-<li>coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air poluttion</li>
+<li>coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air pollution</li>
 <li>the rate ratio allows us to quantify by how much, here by a factor of <span class=""math inline"">\(e^{0.0500269} = 1.05\)</span></li>
 </ul>
 <hr />
@@ -556,7 +556,8 @@ <h2><span class=""header-section-number"">1.4</span> Exercises (GLMs)</h2>
 
 <div class=""exercise"">
 <p><span id=""exr:glm-rerun"" class=""exercise""><strong>Exercise 1.1  </strong></span>
-Make sure you can run and understand the above code for logistic and Poisson regression</p>
+Make sure you can run and understand the above code for logistic and Poisson regression
+The data files can be downloaded in Canvas from Files/data-exercises/linear-models. An Rmd file with the code from the book chapter is provided in Files/exercises/GLM</p>
 </div>
 
 <div class=""exercise"">

---FILE: session-glm/docs/index.html---
@@ -21,10 +21,10 @@
   <meta name=""twitter:description"" content=""Introduction to GLM"" />
   
 
-<meta name=""author"" content=""Olga Dethlefsen"" />
+<meta name=""author"" content=""Olga Dethlefsen, Julie Lorent"" />
 
 
-<meta name=""date"" content=""2022-09-11"" />
+<meta name=""date"" content=""2022-09-14"" />
 
   <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
   <meta name=""apple-mobile-web-app-capable"" content=""yes"" />
@@ -164,8 +164,8 @@ <h1>
             <section class=""normal"" id=""section-"">
 <div id=""header"">
 <h1 class=""title"">Introduction to GLM</h1>
-<p class=""author""><em>Olga Dethlefsen</em></p>
-<p class=""date""><em>2022-09-11</em></p>
+<p class=""author""><em>Olga Dethlefsen, Julie Lorent</em></p>
+<p class=""date""><em>2022-09-14</em></p>
 </div>
 <div id=""preface"" class=""section level1 unnumbered"">
 <h1>Preface</h1>

---FILE: session-glm/docs/index.md---
@@ -1,9 +1,9 @@
 ---
 title: ""Introduction to GLM""
-author: ""Olga Dethlefsen""
+author: ""Olga Dethlefsen, Julie Lorent""
 site: bookdown::bookdown_site
 documentclass: book
-date: ""2022-09-11""  
+date: ""2022-09-14""  
 bibliography: [book.bib]
 biblio-style: apalike
 link-citations: yes

---FILE: session-glm/docs/search_index.json---
@@ -1 +1 @@
-[[""index.html"", ""Introduction to GLM Preface"", "" Introduction to GLM Olga Dethlefsen 2022-09-11 Preface Generalized Linear Models, GLMs, extend linear model framework to outcome variables that do not follow normal distribution. They are most frequently used to model binary, categorical or count data. Do you see a mistake or a typo? I would be grateful if you let me know via edu.ml-biostats@nbis.se This repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/ ""],[""generalized-linear-models.html"", ""Chapter 1 Generalized linear models 1.1 Why Generalized Linear Models (GLMs) 1.2 Logistic regression 1.3 Poisson regression 1.4 Exercises (GLMs)"", "" Chapter 1 Generalized linear models Aims to briefly introduce GLMs via examples of modeling binary and count response Learning outcomes to understand the limits of linear regression and the application of GLMs to be able to use glm() function to fit and interpret logistic and Poisson regression 1.1 Why Generalized Linear Models (GLMs) GLMs extend linear model framework to outcome variables that do not follow normal distribution They are most frequently used to model binary, categorical or count data In the Galapagos Island example we have tried to model Species using linear model It kind of worked but the predicted counts were not counts (natural numbers) but rational numbers instead that make no sense when taking about count data Similarly, fitting a regression line to binary data yields predicted values that could take any value, including \\(&lt;0\\) not to mention that it is hard to argue that the values of 0 and 1s are normally distributed Figure 1.1: Example of fitting linear model to binary data, to model the acceptance to medical school, coded as 1 (Yes) and 0 (No) using GPA school scores. Linear model does not fit the data well in this case 1.2 Logistic regression Yanny or Laurel auditory illusion appeared online in May 2018. You could find lots of information about it, together with some plausible explanations why some people hear Yanny and some year Laurel One of the explanation is that with age we lose the ability to hear certain sounds To see if there is evidence for that, someone has already collected some data for 198 people including their age and gender # Read in and preview data yl &lt;- read.csv(&quot;data/lm/YannyLaurel.csv&quot;) head(yl) ## hear age gender ## 1 Yanny 4 F ## 2 Yanny 5 F ## 3 Yanny 7 M ## 4 Laurel 7 M ## 5 Yanny 8 F ## 6 Yanny 8 F # Recode Laurel to 0 and Yanny as 1 in new variable yl$word &lt;- 0 yl$word[yl$hear==&quot;Yanny&quot;] &lt;- 1 # Make some exploratory plots par(mfrow=c(1,2)) plot(yl$age, yl$word, pch=19, xlab=&quot;age&quot;, ylab=&quot;&quot;, las=1) boxplot(yl$age~yl$hear, xlab=&quot;&quot;, ylab=&quot;age&quot;, col=&quot;lightblue&quot;) Figure 1.2: Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0) Since the response variable takes only two values (Yanny or Laurel) we use GLM model to fit logistic regression model for the probability of hearing Yanny we let \\(p_i=P(Y_i=1)\\) denote the probability of hearing Yanny (success) and we assume that the response follows binomial distribution: \\(Y_i \\sim Bi(1, p_i)\\) distribution We can write the regression model now as: \\[log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i\\] and given the properties of logarithms this is also equivalent to: \\[p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}\\] In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function. Here, the link function \\(log(\\frac{p_i}{1-p_i})\\) provides the link between the binomial distribution of \\(Y_i\\) (hearing Yanny) the linear predictor (age) Thus GLM model can be written as \\[g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta\\] where g() is the link function. We use glm() function in R to fit GLM models # fit logistic regression model logmodel.1 &lt;- glm(word ~ age, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.1)) ## ## Call: ## glm(formula = word ~ age, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5319 -1.1656 0.8516 1.1376 1.5238 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.97429 0.42678 2.283 0.0224 * ## age -0.02444 0.01048 -2.332 0.0197 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 274.41 on 197 degrees of freedom ## Residual deviance: 268.73 on 196 degrees of freedom ## AIC: 272.73 ## ## Number of Fisher Scoring iterations: 4 # plot ggPredict(logmodel.1) Figure 1.3: Fitted logistic model to the Yanny and Laurel data # to get predictions use predict() functions # if no new observations is specified predictions are returned for the values of exploratory variables used # we specify response to return prediction on the probability scale predict(logmodel.1, type=&quot;response&quot;) ## 1 2 3 4 5 6 7 8 ## 0.7061023 0.7010050 0.6906602 0.6906602 0.6854144 0.6854144 0.6801208 0.6801208 ## 9 10 11 12 13 14 15 16 ## 0.6747804 0.6747804 0.6747804 0.6747804 0.6639632 0.6639632 0.6639632 0.6584885 ## 17 18 19 20 21 22 23 24 ## 0.6584885 0.6584885 0.6529712 0.6529712 0.6474126 0.6418137 0.6361759 0.6074493 ## 25 26 27 28 29 30 31 32 ## 0.6016063 0.6016063 0.6016063 0.5957342 0.5898346 0.5898346 0.5839091 0.5779592 ## 33 34 35 36 37 38 39 40 ## 0.5719866 0.5719866 0.5719866 0.5719866 0.5719866 0.5719866 0.5659929 0.5659929 ## 41 42 43 44 45 46 47 48 ## 0.5659929 0.5599798 0.5599798 0.5599798 0.5599798 0.5599798 0.5599798 0.5599798 ## 49 50 51 52 53 54 55 56 ## 0.5539491 0.5539491 0.5539491 0.5539491 0.5539491 0.5479025 0.5479025 0.5479025 ## 57 58 59 60 61 62 63 64 ## 0.5479025 0.5479025 0.5479025 0.5479025 0.5418417 0.5418417 0.5418417 0.5418417 ## 65 66 67 68 69 70 71 72 ## 0.5418417 0.5357685 0.5357685 0.5357685 0.5357685 0.5357685 0.5357685 0.5357685 ## 73 74 75 76 77 78 79 80 ## 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 ## 81 82 83 84 85 86 87 88 ## 0.5235921 0.5235921 0.5235921 0.5235921 0.5235921 0.5235921 0.5174924 0.5174924 ## 89 90 91 92 93 94 95 96 ## 0.5174924 0.5174924 0.5174924 0.5174924 0.5113875 0.5113875 0.5113875 0.5113875 ## 97 98 99 100 101 102 103 104 ## 0.5113875 0.5113875 0.5113875 0.5113875 0.5113875 0.5113875 0.5113875 0.5052791 ## 105 106 107 108 109 110 111 112 ## 0.5052791 0.5052791 0.5052791 0.5052791 0.4991693 0.4991693 0.4991693 0.4991693 ## 113 114 115 116 117 118 119 120 ## 0.4930596 0.4930596 0.4930596 0.4930596 0.4930596 0.4869521 0.4869521 0.4869521 ## 121 122 123 124 125 126 127 128 ## 0.4869521 0.4869521 0.4808484 0.4808484 0.4808484 0.4808484 0.4808484 0.4808484 ## 129 130 131 132 133 134 135 136 ## 0.4747504 0.4747504 0.4747504 0.4747504 0.4686600 0.4686600 0.4686600 0.4686600 ## 137 138 139 140 141 142 143 144 ## 0.4686600 0.4686600 0.4686600 0.4686600 0.4686600 0.4686600 0.4625789 0.4625789 ## 145 146 147 148 149 150 151 152 ## 0.4625789 0.4625789 0.4625789 0.4565089 0.4565089 0.4565089 0.4565089 0.4565089 ## 153 154 155 156 157 158 159 160 ## 0.4565089 0.4565089 0.4504518 0.4504518 0.4444093 0.4444093 0.4444093 0.4444093 ## 161 162 163 164 165 166 167 168 ## 0.4444093 0.4383833 0.4383833 0.4383833 0.4383833 0.4383833 0.4323753 0.4323753 ## 169 170 171 172 173 174 175 176 ## 0.4263872 0.4263872 0.4204206 0.4144771 0.4085585 0.4085585 0.4085585 0.4085585 ## 177 178 179 180 181 182 183 184 ## 0.4026662 0.4026662 0.3968019 0.3909671 0.3909671 0.3736547 0.3736547 0.3736547 ## 185 186 187 188 189 190 191 192 ## 0.3679527 0.3679527 0.3679527 0.3679527 0.3622873 0.3622873 0.3622873 0.3622873 ## 193 194 195 196 197 198 ## 0.3566600 0.3566600 0.3510719 0.3510719 0.3510719 0.3131544 The regression equation for the fitted model is: \\[log(\\frac{\\hat{p_i}}{1-\\hat{p_i}})=0.97 - 0.02x_i\\] we see from the output that \\(\\hat{\\beta_0} = 0.97\\) and \\(\\hat{\\beta_1} = -0.02\\) these estimates are arrived at via maximum likelihood estimation, something that is out of scope here but similarly to linear models, we can test the null hypothesis \\(H_0:\\beta_1=0\\) by comparing, \\(z = \\frac{\\hat{\\beta_1}}{e.s.e(\\hat{\\beta_1)}} = -2.33\\) with a standard normal distribution, and the associated value is small so there is enough evidence to reject the null, meaning that age is significantly associated with the probability with hearing Laurel and Yanny, Wald test the same conclusion can be reached if we compare the residual deviance Deviance deviance is the number that measures the goodness of fit of a logistic regression model we use saturated and residual deviance to assess model, instead of \\(R^2\\) or \\(R^2(adj)\\) for a GLM model that fits the data well the approximate deviance \\(D\\) is \\[\\chi^2(m-p)\\] where \\(m\\) is the number of parameters in the saturated model (full model) and \\(p\\) is the number of parameters in the model of interest for our above model we have \\(274.41 - 268.73 = 5.68\\) which is larger than 95th percentile of \\(\\chi^2(197-196)\\) qchisq(df=1, p=0.95) ## [1] 3.841459 i.e. \\(5.68 &gt; 3.84\\) and again we can conclude that age is a significant term in the model Odds ratios In logistic regression we often interpret the model coefficients by taking \\(e^{\\hat{\\beta}}\\) and we talk about odd ratios e.g. we can say, given our above model, \\(e^{-0.02444} = 0.9758562\\) that for each unit increase in age the odds of hearing Laurel get multiplied by 0.98 Other covariates Finally, we can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical E.g. we can test whether there is a gender effect when hearing Yanny or Laurel # fit logistic regression including age and gender logmodel.2 &lt;- glm(word ~ age + gender, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.2)) ## ## Call: ## glm(formula = word ~ age + gender, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6376 -1.1464 0.7595 1.1510 1.5592 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.24682 0.48166 2.589 0.00964 ** ## age -0.02325 0.01061 -2.191 0.02848 * ## genderM -0.43691 0.32798 -1.332 0.18282 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 274.41 on 197 degrees of freedom ## Residual deviance: 266.94 on 195 degrees of freedom ## AIC: 272.94 ## ## Number of Fisher Scoring iterations: 4 # plot model ggPredict(logmodel.2) Figure 1.4: Yanny Laurel data modelled with logistic regression given age and gender. Regression lines in males and females are very alike and the model suggest no gender effect Simulated data This is beyond the scope of this course but a more advanced model might be needed to better explain these specific data. As an exercise, let us simulate a dataset where the logistic regression would be a better fit (it would probably be the case if the age effect had been larger than the one observed in the Yanny/Laurel example above). # In a similar way as for the first Yanny/Laurel model above (logmodel.1) # where a binary variable (hearing Yanny/Laurel) was explained by one # continuous variable (age), let us simulate the data below: # - we will simulate a sample of 60 individuals where the binary variable # (e.g. hearing Yanny/Laurel) is equal to zero 30 times and to one 30 times set.seed(1) n &lt;- 30 binaryVar &lt;- c(rep(0, n), rep(1, n)) # - we would like to simulate a strong effect of the continuous variable # so we can simulate people with binaryVar 0 and binaryVar 1 from # different distributions. distr0 &lt;- rnorm(n, mean=65, sd=15) %&gt;% round() distr1 &lt;- rnorm(n, mean=25, sd=12) %&gt;% round() dat &lt;- data.frame(binaryVar=c(rep(0, n), rep(1, n)), continuousVar = c(distr0, distr1)) idx &lt;- sample(1:(2*n), 2*n) dat &lt;- dat[idx,] #reorder samples randomly head(dat) ## binaryVar continuousVar ## 42 1 22 ## 32 1 24 ## 39 1 38 ## 51 1 30 ## 29 0 58 ## 34 1 24 # Make some exploratory plots par(mfrow=c(1,2)) plot(dat$continuousVar, dat$binaryVar, pch=19, xlab=&quot;Continuous variable (for instance age)&quot;, ylab=&quot;Binary variable (for instance hearing Yanny/Laurel)&quot;, las=1) boxplot(dat$continuousVar~dat$binaryVar, xlab=&quot;Binary variable&quot;, ylab=&quot;Continuous variable&quot;, col=&quot;lightblue&quot;) # fit logistic regression model logmodel.3 &lt;- glm(binaryVar ~ continuousVar, family = binomial(link=&quot;logit&quot;), data = dat) # print model summary print(summary(logmodel.3)) ## ## Call: ## glm(formula = binaryVar ~ continuousVar, family = binomial(link = &quot;logit&quot;), ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.23294 -0.08988 0.01110 0.18155 1.70441 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 9.16941 2.55369 3.591 0.000330 *** ## continuousVar -0.21133 0.06112 -3.458 0.000544 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 83.178 on 59 degrees of freedom ## Residual deviance: 19.123 on 58 degrees of freedom ## AIC: 23.123 ## ## Number of Fisher Scoring iterations: 7 # plot ggPredict(logmodel.3) 1.3 Poisson regression GLMs can be also applied to count data e.g. hospital admissions due to respiratory disease or number of bird nests in a certain habitat here, we commonly assume that data follow the Poisson distribution \\(Y_i \\sim Pois(\\mu_i)\\) and the corresponding model is \\[E(Y_i)=\\mu_i = \\eta_ie^{\\mathbf{x_i}^T\\boldsymbol\\beta}\\] with a log link \\(\\ln\\mu_i = \\ln \\eta_i + \\mathbf{x_i}^T\\boldsymbol\\beta\\) Data set Suppose we wish to model \\(Y_i\\) the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 regions, a small areas that contain between 2500 and 6000 people. Together with cancer occurrence with have data: Y_all: number of cases of all types of cancer in the IG in 2013 E_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013 pm10: air pollution smoke: percentage of people in an area that smoke ethic: percentage of people who are non-white logpice: natural log of average house price easting and northing: co-ordinates of the central point of the IG divided by 10000 We can model the rate of occurrence of cancer using the very same glm function:¨ - now we use poisson family distribution to model counts - and we will include an offset term to model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions # Read in and preview data cancer &lt;- read.csv(&quot;data/lm/cancer.csv&quot;) head(cancer) ## IG Y_all E_all pm10 smoke ethnic log.price easting northing ## 1 S02000260 133 106.17907 17.8 21.9 5.58 11.59910 26.16245 66.96574 ## 2 S02000261 38 62.43131 18.6 21.8 7.91 11.84940 26.29271 67.00278 ## 3 S02000262 97 120.00694 18.6 20.8 9.58 11.74106 26.21429 67.04280 ## 4 S02000263 80 109.10245 17.0 14.0 10.39 12.30138 25.45705 67.05938 ## 5 S02000264 181 149.77821 18.6 15.2 5.67 11.88449 26.12484 67.09280 ## 6 S02000265 77 82.31156 17.0 14.6 5.61 11.82004 25.37644 67.09826 # fit Poisson regression epid1 &lt;- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), family = poisson, data = cancer) print(summary(epid1)) ## ## Call: ## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + ## northing + offset(log(E_all)), family = poisson, data = cancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.2011 -0.9338 -0.1763 0.8959 3.8416 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8592657 0.8029040 -1.070 0.284531 ## pm10 0.0500269 0.0066724 7.498 6.50e-14 *** ## smoke 0.0033516 0.0009463 3.542 0.000397 *** ## ethnic -0.0049388 0.0006354 -7.773 7.66e-15 *** ## log.price -0.1034461 0.0169943 -6.087 1.15e-09 *** ## easting -0.0331305 0.0103698 -3.195 0.001399 ** ## northing 0.0300213 0.0111013 2.704 0.006845 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 972.94 on 270 degrees of freedom ## Residual deviance: 565.18 on 264 degrees of freedom ## AIC: 2356.2 ## ## Number of Fisher Scoring iterations: 4 Hypothesis testing, model fit and predictions follows stay the same as for logistic regression Rate ratio similarly to logistic regression it common to look at the \\(e^\\beta\\) for instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air poluttion the rate ratio allows us to quantify by how much, here by a factor of \\(e^{0.0500269} = 1.05\\) 1.4 Exercises (GLMs) Exercise 1.1 Make sure you can run and understand the above code for logistic and Poisson regression Exercise 1.2 Additional practice with a bigger more realistic data set. What might affect the chance of getting a heart disease? One of the earliest studies addressing this issue started in 1960 in 3154 healthy men in the San Francisco area. At the start of the study all were free of heart disease. Eight years later the study recorded whether these men now suffered from heart disease (chd), along with many other variables that might be related. The data is available from the faraway package and includes variables: age: age in years height: height in inches weight: weight in pounds sdp: systolic blood pressure in mm Hg dbp: diastolic blood pressure in mm Hg chol: Fasting serum cholesterol in mm % behave: behavior type which is a factor with levels A1 A2 B3 B4 cigs: number of cigarettes smoked per day dibep: behavior type a factor with levels A (Agressive) B (Passive) chd: coronary heat disease developed is a factor with levels no yes typechd: type of coronary heart disease is a factor with levels angina infdeath none silent timechd: Time of CHD event or end of follow-up arcus: arcus senilis is a factor with levels absent present using logistic regression, can you discover anything interesting about the probability of developing heart disease (chd)? using Poisson regression, can you comment about the numbers of cigarettes smoked (cigs)? library(faraway) data(wcgs, package=&quot;faraway&quot;) head(wcgs) ## age height weight sdp dbp chol behave cigs dibep chd typechd timechd ## 2001 49 73 150 110 76 225 A2 25 B no none 1664 ## 2002 42 70 160 154 84 177 A2 20 B no none 3071 ## 2003 42 69 160 110 78 181 B3 0 A no none 3071 ## 2004 41 68 152 124 78 132 B4 20 A no none 3064 ## 2005 59 70 150 144 86 255 B3 20 A yes infdeath 1885 ## 2006 44 72 204 150 90 182 B4 0 A no none 3102 ## arcus ## 2001 absent ## 2002 present ## 2003 absent ## 2004 absent ## 2005 present ## 2006 absent Answers to selected exercises Exr. 1.2 possible solution probability of developing heart disease We first check the relationship between variables to gain more understanding of the data. We discover that a couple of variables are exactly collinear with other variables, including typechd, timechd and dibep. We do not include these in the model. # `chd` and `typechd` were correlated. with(wcgs, table(chd, typechd)) ## typechd ## chd angina infdeath none silent ## no 0 0 2897 0 ## yes 51 135 0 71 # `timechd` is an outcome variable affected by `chd`. by(wcgs$timechd, wcgs$chd, summary) ## wcgs$chd: no ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 238 2864 2952 2775 3048 3430 ## ------------------------------------------------------------ ## wcgs$chd: yes ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 18 934 1666 1655 2400 3229 # `behave` has more detailed info of `dibep` -&gt; exact collinearity with(wcgs, table(behave, dibep)) ## dibep ## behave A B ## A1 0 264 ## A2 0 1325 ## B3 1216 0 ## B4 349 0 We fit logistic regression model to explain the probability of developing cardiac disease (chd) given the remaining variables model1 &lt;- glm(chd ~ . - typechd - timechd - dibep, data = wcgs, family = binomial) summary(model1) ## ## Call: ## glm(formula = chd ~ . - typechd - timechd - dibep, family = binomial, ## data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3653 -0.4362 -0.3128 -0.2208 2.8603 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.331126 2.350347 -5.247 1.55e-07 *** ## age 0.061812 0.012421 4.977 6.47e-07 *** ## height 0.006903 0.033335 0.207 0.83594 ## weight 0.008637 0.003892 2.219 0.02647 * ## sdp 0.018146 0.006435 2.820 0.00481 ** ## dbp -0.000916 0.010903 -0.084 0.93305 ## chol 0.010726 0.001531 7.006 2.45e-12 *** ## behaveA2 0.082920 0.222909 0.372 0.70990 ## behaveB3 -0.618013 0.245032 -2.522 0.01166 * ## behaveB4 -0.487224 0.321325 -1.516 0.12944 ## cigs 0.021036 0.004298 4.895 9.84e-07 *** ## arcuspresent 0.212796 0.143915 1.479 0.13924 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1769.2 on 3139 degrees of freedom ## Residual deviance: 1569.1 on 3128 degrees of freedom ## (14 observations deleted due to missingness) ## AIC: 1593.1 ## ## Number of Fisher Scoring iterations: 6 And we notice that many variables including age, chol, and cigs, were significantly associated with heart disease development. For example, increment of one mm % of Fasting serum cholesterol (chol) elevated the odds of the disease by a factor of \\(e^{0.010726} = 1.010784\\) after adjustment for the effects of the other variables. numbers of cigarettes smoked Many variables were correlated with the number of cigarettes. For example, one mm Hg increase of systolic blood pressure was correlated with the increase of average number of cigarettes smoked by a factor of \\(e^{0.0024264} = 1.002429\\). # check distribution hist(wcgs$cigs, breaks = 25) # Poisson regression for age model2 &lt;- glm(cigs ~ age, data = wcgs, family = poisson) summary(model2) ## ## Call: ## glm(formula = cigs ~ age, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.837 -4.820 -4.787 2.254 15.839 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.5038936 0.0441558 56.706 &lt;2e-16 *** ## age -0.0011423 0.0009481 -1.205 0.228 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62696 on 3152 degrees of freedom ## AIC: 70053 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for weight model3 &lt;- glm(cigs ~ weight, data = wcgs, family = poisson) summary(model3) ## ## Call: ## glm(formula = cigs ~ weight, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.720 -4.803 -4.347 2.441 15.779 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.2939845 0.0430796 76.46 &lt;2e-16 *** ## weight -0.0049918 0.0002548 -19.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62307 on 3152 degrees of freedom ## AIC: 69664 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for systolic blood pressure model4 &lt;- glm(cigs ~ sdp, data = wcgs, family = poisson) summary(model4) ## ## Call: ## glm(formula = cigs ~ sdp, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.445 -4.800 -4.707 2.351 15.922 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1382494 0.0440018 48.595 &lt; 2e-16 *** ## sdp 0.0024264 0.0003382 7.175 7.21e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62647 on 3152 degrees of freedom ## AIC: 70004 ## ## Number of Fisher Scoring iterations: 6 ""],[""404.html"", ""Page not found"", "" Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. ""]]
+[[""index.html"", ""Introduction to GLM Preface"", "" Introduction to GLM Olga Dethlefsen, Julie Lorent 2022-09-14 Preface Generalized Linear Models, GLMs, extend linear model framework to outcome variables that do not follow normal distribution. They are most frequently used to model binary, categorical or count data. Do you see a mistake or a typo? I would be grateful if you let me know via edu.ml-biostats@nbis.se This repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/ ""],[""generalized-linear-models.html"", ""Chapter 1 Generalized linear models 1.1 Why Generalized Linear Models (GLMs) 1.2 Logistic regression 1.3 Poisson regression 1.4 Exercises (GLMs)"", "" Chapter 1 Generalized linear models Aims to briefly introduce GLMs via examples of modeling binary and count response Learning outcomes to understand the limits of linear regression and the application of GLMs to be able to use glm() function to fit and interpret logistic and Poisson regression 1.1 Why Generalized Linear Models (GLMs) GLMs extend linear model framework to outcome variables that do not follow normal distribution They are most frequently used to model binary, categorical or count data In the Galapagos Island example we have tried to model Species using linear model It kind of worked but the predicted counts were not counts (natural numbers) but rational numbers instead that make no sense when taking about count data Similarly, fitting a regression line to binary data yields predicted values that could take any value, including \\(&lt;0\\) not to mention that it is hard to argue that the values of 0 and 1s are normally distributed Figure 1.1: Example of fitting linear model to binary data, to model the acceptance to medical school, coded as 1 (Yes) and 0 (No) using GPA school scores. Linear model does not fit the data well in this case 1.2 Logistic regression Yanny or Laurel auditory illusion appeared online in May 2018. You could find lots of information about it, together with some plausible explanations why some people hear Yanny and some year Laurel One of the explanation is that with age we lose the ability to hear certain sounds To see if there is evidence for that, someone has already collected some data for 198 people including their age and gender # Read in and preview data yl &lt;- read.csv(&quot;data/lm/YannyLaurel.csv&quot;) head(yl) ## hear age gender ## 1 Yanny 4 F ## 2 Yanny 5 F ## 3 Yanny 7 M ## 4 Laurel 7 M ## 5 Yanny 8 F ## 6 Yanny 8 F # Recode Laurel to 0 and Yanny as 1 in new variable yl$word &lt;- 0 yl$word[yl$hear==&quot;Yanny&quot;] &lt;- 1 # Make some exploratory plots par(mfrow=c(1,2)) plot(yl$age, yl$word, pch=19, xlab=&quot;age&quot;, ylab=&quot;&quot;, las=1) boxplot(yl$age~yl$hear, xlab=&quot;&quot;, ylab=&quot;age&quot;, col=&quot;lightblue&quot;) Figure 1.2: Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0) Since the response variable takes only two values (Yanny or Laurel) we use GLM model to fit logistic regression model for the probability of hearing Yanny we let \\(p_i=P(Y_i=1)\\) denote the probability of hearing Yanny (success) and we assume that the response follows binomial distribution: \\(Y_i \\sim Bi(1, p_i)\\) distribution We can write the regression model now as: \\[log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i\\] and given the properties of logarithms this is also equivalent to: \\[p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}\\] In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function. Here, the link function \\(log(\\frac{p_i}{1-p_i})\\) provides the link between the binomial distribution of \\(Y_i\\) (hearing Yanny) and the linear predictor (age) Thus the GLM model can be written as \\[g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta\\] where g() is the link function. We use glm() function in R to fit GLM models # fit logistic regression model logmodel.1 &lt;- glm(word ~ age, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.1)) ## ## Call: ## glm(formula = word ~ age, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5319 -1.1656 0.8516 1.1376 1.5238 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.97429 0.42678 2.283 0.0224 * ## age -0.02444 0.01048 -2.332 0.0197 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 274.41 on 197 degrees of freedom ## Residual deviance: 268.73 on 196 degrees of freedom ## AIC: 272.73 ## ## Number of Fisher Scoring iterations: 4 # plot ggPredict(logmodel.1) Figure 1.3: Fitted logistic model to the Yanny and Laurel data # to get predictions use predict() functions # if no new observations is specified predictions are returned for the values of exploratory variables used # we specify response to return prediction on the probability scale predict(logmodel.1, type=&quot;response&quot;) ## 1 2 3 4 5 6 7 8 ## 0.7061023 0.7010050 0.6906602 0.6906602 0.6854144 0.6854144 0.6801208 0.6801208 ## 9 10 11 12 13 14 15 16 ## 0.6747804 0.6747804 0.6747804 0.6747804 0.6639632 0.6639632 0.6639632 0.6584885 ## 17 18 19 20 21 22 23 24 ## 0.6584885 0.6584885 0.6529712 0.6529712 0.6474126 0.6418137 0.6361759 0.6074493 ## 25 26 27 28 29 30 31 32 ## 0.6016063 0.6016063 0.6016063 0.5957342 0.5898346 0.5898346 0.5839091 0.5779592 ## 33 34 35 36 37 38 39 40 ## 0.5719866 0.5719866 0.5719866 0.5719866 0.5719866 0.5719866 0.5659929 0.5659929 ## 41 42 43 44 45 46 47 48 ## 0.5659929 0.5599798 0.5599798 0.5599798 0.5599798 0.5599798 0.5599798 0.5599798 ## 49 50 51 52 53 54 55 56 ## 0.5539491 0.5539491 0.5539491 0.5539491 0.5539491 0.5479025 0.5479025 0.5479025 ## 57 58 59 60 61 62 63 64 ## 0.5479025 0.5479025 0.5479025 0.5479025 0.5418417 0.5418417 0.5418417 0.5418417 ## 65 66 67 68 69 70 71 72 ## 0.5418417 0.5357685 0.5357685 0.5357685 0.5357685 0.5357685 0.5357685 0.5357685 ## 73 74 75 76 77 78 79 80 ## 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 0.5296847 ## 81 82 83 84 85 86 87 88 ## 0.5235921 0.5235921 0.5235921 0.5235921 0.5235921 0.5235921 0.5174924 0.5174924 ## 89 90 91 92 93 94 95 96 ## 0.5174924 0.5174924 0.5174924 0.5174924 0.5113875 0.5113875 0.5113875 0.5113875 ## 97 98 99 100 101 102 103 104 ## 0.5113875 0.5113875 0.5113875 0.5113875 0.5113875 0.5113875 0.5113875 0.5052791 ## 105 106 107 108 109 110 111 112 ## 0.5052791 0.5052791 0.5052791 0.5052791 0.4991693 0.4991693 0.4991693 0.4991693 ## 113 114 115 116 117 118 119 120 ## 0.4930596 0.4930596 0.4930596 0.4930596 0.4930596 0.4869521 0.4869521 0.4869521 ## 121 122 123 124 125 126 127 128 ## 0.4869521 0.4869521 0.4808484 0.4808484 0.4808484 0.4808484 0.4808484 0.4808484 ## 129 130 131 132 133 134 135 136 ## 0.4747504 0.4747504 0.4747504 0.4747504 0.4686600 0.4686600 0.4686600 0.4686600 ## 137 138 139 140 141 142 143 144 ## 0.4686600 0.4686600 0.4686600 0.4686600 0.4686600 0.4686600 0.4625789 0.4625789 ## 145 146 147 148 149 150 151 152 ## 0.4625789 0.4625789 0.4625789 0.4565089 0.4565089 0.4565089 0.4565089 0.4565089 ## 153 154 155 156 157 158 159 160 ## 0.4565089 0.4565089 0.4504518 0.4504518 0.4444093 0.4444093 0.4444093 0.4444093 ## 161 162 163 164 165 166 167 168 ## 0.4444093 0.4383833 0.4383833 0.4383833 0.4383833 0.4383833 0.4323753 0.4323753 ## 169 170 171 172 173 174 175 176 ## 0.4263872 0.4263872 0.4204206 0.4144771 0.4085585 0.4085585 0.4085585 0.4085585 ## 177 178 179 180 181 182 183 184 ## 0.4026662 0.4026662 0.3968019 0.3909671 0.3909671 0.3736547 0.3736547 0.3736547 ## 185 186 187 188 189 190 191 192 ## 0.3679527 0.3679527 0.3679527 0.3679527 0.3622873 0.3622873 0.3622873 0.3622873 ## 193 194 195 196 197 198 ## 0.3566600 0.3566600 0.3510719 0.3510719 0.3510719 0.3131544 The regression equation for the fitted model is: \\[log(\\frac{\\hat{p_i}}{1-\\hat{p_i}})=0.97 - 0.02x_i\\] we see from the output that \\(\\hat{\\beta_0} = 0.97\\) and \\(\\hat{\\beta_1} = -0.02\\) these estimates are arrived at via maximum likelihood estimation, something that is out of scope here but similarly to linear models, we can test the null hypothesis \\(H_0:\\beta_1=0\\) by comparing, \\(z = \\frac{\\hat{\\beta_1}}{e.s.e(\\hat{\\beta_1)}} = -2.33\\) with a standard normal distribution, and the associated value is small so there is enough evidence to reject the null, meaning that age is significantly associated with the probability with hearing Laurel and Yanny, Wald test the same conclusion can be reached if we compare the residual deviance Deviance deviance is the number that measures the goodness of fit of a logistic regression model we use saturated and residual deviance to assess model, instead of \\(R^2\\) or \\(R^2(adj)\\) for a GLM model that fits the data well the approximate deviance \\(D\\) is \\[\\chi^2(m-p)\\] where \\(m\\) is the number of parameters in the saturated model (full model) and \\(p\\) is the number of parameters in the model of interest for our above model we have \\(274.41 - 268.73 = 5.68\\) which is larger than 95th percentile of \\(\\chi^2(197-196)\\) qchisq(df=1, p=0.95) ## [1] 3.841459 i.e. \\(5.68 &gt; 3.84\\) and again we can conclude that age is a significant term in the model Odds ratios In logistic regression we often interpret the model coefficients by taking \\(e^{\\hat{\\beta}}\\) and we talk about odd ratios e.g. we can say, given our above model, \\(e^{-0.02444} = 0.9758562\\) that for each unit increase in age the odds of hearing Yanny get multiplied by 0.98 Other covariates Finally, we can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical E.g. we can test whether there is a gender effect when hearing Yanny or Laurel # fit logistic regression including age and gender logmodel.2 &lt;- glm(word ~ age + gender, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.2)) ## ## Call: ## glm(formula = word ~ age + gender, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6376 -1.1464 0.7595 1.1510 1.5592 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.24682 0.48166 2.589 0.00964 ** ## age -0.02325 0.01061 -2.191 0.02848 * ## genderM -0.43691 0.32798 -1.332 0.18282 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 274.41 on 197 degrees of freedom ## Residual deviance: 266.94 on 195 degrees of freedom ## AIC: 272.94 ## ## Number of Fisher Scoring iterations: 4 # plot model ggPredict(logmodel.2) Figure 1.4: Yanny Laurel data modelled with logistic regression given age and gender. Regression lines in males and females are very alike and the model suggest no gender effect Simulated data This is beyond the scope of this course but a more advanced model might be needed to better explain these specific data. As an exercise, let us simulate a dataset where the logistic regression would be a better fit (it would probably be the case if the age effect had been larger than the one observed in the Yanny/Laurel example above). # In a similar way as for the first Yanny/Laurel model above (logmodel.1) # where a binary variable (hearing Yanny/Laurel) was explained by one # continuous variable (age), let us simulate the data below: # - we will simulate a sample of 60 individuals where the binary variable # (e.g. hearing Yanny/Laurel) is equal to zero 30 times and to one 30 times set.seed(1) n &lt;- 30 binaryVar &lt;- c(rep(0, n), rep(1, n)) # - we would like to simulate a strong effect of the continuous variable # so we can simulate people with binaryVar 0 and binaryVar 1 from # different distributions. distr0 &lt;- rnorm(n, mean=65, sd=15) %&gt;% round() distr1 &lt;- rnorm(n, mean=25, sd=12) %&gt;% round() dat &lt;- data.frame(binaryVar=c(rep(0, n), rep(1, n)), continuousVar = c(distr0, distr1)) idx &lt;- sample(1:(2*n), 2*n) dat &lt;- dat[idx,] #reorder samples randomly head(dat) ## binaryVar continuousVar ## 42 1 22 ## 32 1 24 ## 39 1 38 ## 51 1 30 ## 29 0 58 ## 34 1 24 # Make some exploratory plots par(mfrow=c(1,2)) plot(dat$continuousVar, dat$binaryVar, pch=19, xlab=&quot;Continuous variable (for instance age)&quot;, ylab=&quot;Binary variable (for instance hearing Yanny/Laurel)&quot;, las=1) boxplot(dat$continuousVar~dat$binaryVar, xlab=&quot;Binary variable&quot;, ylab=&quot;Continuous variable&quot;, col=&quot;lightblue&quot;) # fit logistic regression model logmodel.3 &lt;- glm(binaryVar ~ continuousVar, family = binomial(link=&quot;logit&quot;), data = dat) # print model summary print(summary(logmodel.3)) ## ## Call: ## glm(formula = binaryVar ~ continuousVar, family = binomial(link = &quot;logit&quot;), ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.23294 -0.08988 0.01110 0.18155 1.70441 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 9.16941 2.55369 3.591 0.000330 *** ## continuousVar -0.21133 0.06112 -3.458 0.000544 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 83.178 on 59 degrees of freedom ## Residual deviance: 19.123 on 58 degrees of freedom ## AIC: 23.123 ## ## Number of Fisher Scoring iterations: 7 # plot ggPredict(logmodel.3) 1.3 Poisson regression GLMs can be also applied to count data e.g. hospital admissions due to respiratory disease or number of bird nests in a certain habitat here, we commonly assume that data follow the Poisson distribution \\(Y_i \\sim Pois(\\mu_i)\\) and the corresponding model is \\[E(Y_i)=\\mu_i = \\eta_ie^{\\mathbf{x_i}^T\\boldsymbol\\beta}\\] with a log link \\(\\ln\\mu_i = \\ln \\eta_i + \\mathbf{x_i}^T\\boldsymbol\\beta\\) Data set Suppose we wish to model \\(Y_i\\) the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 regions, a small areas that contain between 2500 and 6000 people. Together with cancer occurrence with have data: Y_all: number of cases of all types of cancer in the IG in 2013 E_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013 pm10: air pollution smoke: percentage of people in an area that smoke ethnic: percentage of people who are non-white log.price: natural log of average house price easting and northing: co-ordinates of the central point of the IG divided by 10000 We can model the rate of occurrence of cancer using the very same glm function:¨ - now we use poisson family distribution to model counts - and we will include an offset term to the model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions # Read in and preview data cancer &lt;- read.csv(&quot;data/lm/cancer.csv&quot;) head(cancer) ## IG Y_all E_all pm10 smoke ethnic log.price easting northing ## 1 S02000260 133 106.17907 17.8 21.9 5.58 11.59910 26.16245 66.96574 ## 2 S02000261 38 62.43131 18.6 21.8 7.91 11.84940 26.29271 67.00278 ## 3 S02000262 97 120.00694 18.6 20.8 9.58 11.74106 26.21429 67.04280 ## 4 S02000263 80 109.10245 17.0 14.0 10.39 12.30138 25.45705 67.05938 ## 5 S02000264 181 149.77821 18.6 15.2 5.67 11.88449 26.12484 67.09280 ## 6 S02000265 77 82.31156 17.0 14.6 5.61 11.82004 25.37644 67.09826 # fit Poisson regression epid1 &lt;- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), family = poisson, data = cancer) print(summary(epid1)) ## ## Call: ## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + ## northing + offset(log(E_all)), family = poisson, data = cancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.2011 -0.9338 -0.1763 0.8959 3.8416 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8592657 0.8029040 -1.070 0.284531 ## pm10 0.0500269 0.0066724 7.498 6.50e-14 *** ## smoke 0.0033516 0.0009463 3.542 0.000397 *** ## ethnic -0.0049388 0.0006354 -7.773 7.66e-15 *** ## log.price -0.1034461 0.0169943 -6.087 1.15e-09 *** ## easting -0.0331305 0.0103698 -3.195 0.001399 ** ## northing 0.0300213 0.0111013 2.704 0.006845 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 972.94 on 270 degrees of freedom ## Residual deviance: 565.18 on 264 degrees of freedom ## AIC: 2356.2 ## ## Number of Fisher Scoring iterations: 4 Hypothesis testing, model fit and predictions follows stay the same as for logistic regression Rate ratio similarly to logistic regression, it is common to look at the \\(e^\\beta\\) for instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air pollution the rate ratio allows us to quantify by how much, here by a factor of \\(e^{0.0500269} = 1.05\\) 1.4 Exercises (GLMs) Exercise 1.1 Make sure you can run and understand the above code for logistic and Poisson regression The data files can be downloaded in Canvas from Files/data-exercises/linear-models. An Rmd file with the code from the book chapter is provided in Files/exercises/GLM Exercise 1.2 Additional practice with a bigger more realistic data set. What might affect the chance of getting a heart disease? One of the earliest studies addressing this issue started in 1960 in 3154 healthy men in the San Francisco area. At the start of the study all were free of heart disease. Eight years later the study recorded whether these men now suffered from heart disease (chd), along with many other variables that might be related. The data is available from the faraway package and includes variables: age: age in years height: height in inches weight: weight in pounds sdp: systolic blood pressure in mm Hg dbp: diastolic blood pressure in mm Hg chol: Fasting serum cholesterol in mm % behave: behavior type which is a factor with levels A1 A2 B3 B4 cigs: number of cigarettes smoked per day dibep: behavior type a factor with levels A (Agressive) B (Passive) chd: coronary heat disease developed is a factor with levels no yes typechd: type of coronary heart disease is a factor with levels angina infdeath none silent timechd: Time of CHD event or end of follow-up arcus: arcus senilis is a factor with levels absent present using logistic regression, can you discover anything interesting about the probability of developing heart disease (chd)? using Poisson regression, can you comment about the numbers of cigarettes smoked (cigs)? library(faraway) data(wcgs, package=&quot;faraway&quot;) head(wcgs) ## age height weight sdp dbp chol behave cigs dibep chd typechd timechd ## 2001 49 73 150 110 76 225 A2 25 B no none 1664 ## 2002 42 70 160 154 84 177 A2 20 B no none 3071 ## 2003 42 69 160 110 78 181 B3 0 A no none 3071 ## 2004 41 68 152 124 78 132 B4 20 A no none 3064 ## 2005 59 70 150 144 86 255 B3 20 A yes infdeath 1885 ## 2006 44 72 204 150 90 182 B4 0 A no none 3102 ## arcus ## 2001 absent ## 2002 present ## 2003 absent ## 2004 absent ## 2005 present ## 2006 absent Answers to selected exercises Exr. 1.2 possible solution probability of developing heart disease We first check the relationship between variables to gain more understanding of the data. We discover that a couple of variables are exactly collinear with other variables, including typechd, timechd and dibep. We do not include these in the model. # `chd` and `typechd` were correlated. with(wcgs, table(chd, typechd)) ## typechd ## chd angina infdeath none silent ## no 0 0 2897 0 ## yes 51 135 0 71 # `timechd` is an outcome variable affected by `chd`. by(wcgs$timechd, wcgs$chd, summary) ## wcgs$chd: no ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 238 2864 2952 2775 3048 3430 ## ------------------------------------------------------------ ## wcgs$chd: yes ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 18 934 1666 1655 2400 3229 # `behave` has more detailed info of `dibep` -&gt; exact collinearity with(wcgs, table(behave, dibep)) ## dibep ## behave A B ## A1 0 264 ## A2 0 1325 ## B3 1216 0 ## B4 349 0 We fit logistic regression model to explain the probability of developing cardiac disease (chd) given the remaining variables model1 &lt;- glm(chd ~ . - typechd - timechd - dibep, data = wcgs, family = binomial) summary(model1) ## ## Call: ## glm(formula = chd ~ . - typechd - timechd - dibep, family = binomial, ## data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3653 -0.4362 -0.3128 -0.2208 2.8603 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.331126 2.350347 -5.247 1.55e-07 *** ## age 0.061812 0.012421 4.977 6.47e-07 *** ## height 0.006903 0.033335 0.207 0.83594 ## weight 0.008637 0.003892 2.219 0.02647 * ## sdp 0.018146 0.006435 2.820 0.00481 ** ## dbp -0.000916 0.010903 -0.084 0.93305 ## chol 0.010726 0.001531 7.006 2.45e-12 *** ## behaveA2 0.082920 0.222909 0.372 0.70990 ## behaveB3 -0.618013 0.245032 -2.522 0.01166 * ## behaveB4 -0.487224 0.321325 -1.516 0.12944 ## cigs 0.021036 0.004298 4.895 9.84e-07 *** ## arcuspresent 0.212796 0.143915 1.479 0.13924 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1769.2 on 3139 degrees of freedom ## Residual deviance: 1569.1 on 3128 degrees of freedom ## (14 observations deleted due to missingness) ## AIC: 1593.1 ## ## Number of Fisher Scoring iterations: 6 And we notice that many variables including age, chol, and cigs, were significantly associated with heart disease development. For example, increment of one mm % of Fasting serum cholesterol (chol) elevated the odds of the disease by a factor of \\(e^{0.010726} = 1.010784\\) after adjustment for the effects of the other variables. numbers of cigarettes smoked Many variables were correlated with the number of cigarettes. For example, one mm Hg increase of systolic blood pressure was correlated with the increase of average number of cigarettes smoked by a factor of \\(e^{0.0024264} = 1.002429\\). # check distribution hist(wcgs$cigs, breaks = 25) # Poisson regression for age model2 &lt;- glm(cigs ~ age, data = wcgs, family = poisson) summary(model2) ## ## Call: ## glm(formula = cigs ~ age, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.837 -4.820 -4.787 2.254 15.839 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.5038936 0.0441558 56.706 &lt;2e-16 *** ## age -0.0011423 0.0009481 -1.205 0.228 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62696 on 3152 degrees of freedom ## AIC: 70053 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for weight model3 &lt;- glm(cigs ~ weight, data = wcgs, family = poisson) summary(model3) ## ## Call: ## glm(formula = cigs ~ weight, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.720 -4.803 -4.347 2.441 15.779 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.2939845 0.0430796 76.46 &lt;2e-16 *** ## weight -0.0049918 0.0002548 -19.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62307 on 3152 degrees of freedom ## AIC: 69664 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for systolic blood pressure model4 &lt;- glm(cigs ~ sdp, data = wcgs, family = poisson) summary(model4) ## ## Call: ## glm(formula = cigs ~ sdp, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.445 -4.800 -4.707 2.351 15.922 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1382494 0.0440018 48.595 &lt; 2e-16 *** ## sdp 0.0024264 0.0003382 7.175 7.21e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62647 on 3152 degrees of freedom ## AIC: 70004 ## ## Number of Fisher Scoring iterations: 6 ""],[""404.html"", ""Page not found"", "" Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. ""]]

---FILE: session-glm/index.Rmd---
@@ -1,6 +1,6 @@
 ---
 title: ""Introduction to GLM""
-author: ""Olga Dethlefsen""
+author: ""Olga Dethlefsen, Julie Lorent""
 site: bookdown::bookdown_site
 documentclass: book
 date: ""`r Sys.Date()`""  

---FILE: session-glm/renv/.gitignore---
@@ -1,3 +1,4 @@
+cellar/
 library/
 local/
 lock/

---FILE: session-glm/renv/activate.R---
@@ -7,13 +7,45 @@ local({
   # the project directory
   project <- getwd()
 
+  # figure out whether the autoloader is enabled
+  enabled <- local({
+
+    # first, check config option
+    override <- getOption(""renv.config.autoloader.enabled"")
+    if (!is.null(override))
+      return(override)
+
+    # next, check environment variables
+    # TODO: prefer using the configuration one in the future
+    envvars <- c(
+      ""RENV_CONFIG_AUTOLOADER_ENABLED"",
+      ""RENV_AUTOLOADER_ENABLED"",
+      ""RENV_ACTIVATE_PROJECT""
+    )
+
+    for (envvar in envvars) {
+      envval <- Sys.getenv(envvar, unset = NA)
+      if (!is.na(envval))
+        return(tolower(envval) %in% c(""true"", ""t"", ""1""))
+    }
+
+    # enable by default
+    TRUE
+
+  })
+
+  if (!enabled)
+    return(FALSE)
+
   # avoid recursion
-  if (!is.na(Sys.getenv(""RENV_R_INITIALIZING"", unset = NA)))
+  if (identical(getOption(""renv.autoloader.running""), TRUE)) {
+    warning(""ignoring recursive attempt to run renv autoloader"")
     return(invisible(TRUE))
+  }
 
   # signal that we're loading renv during R startup
-  Sys.setenv(""RENV_R_INITIALIZING"" = ""true"")
-  on.exit(Sys.unsetenv(""RENV_R_INITIALIZING""), add = TRUE)
+  options(renv.autoloader.running = TRUE)
+  on.exit(options(renv.autoloader.running = NULL), add = TRUE)
 
   # signal that we've consented to use renv
   options(renv.consent = TRUE)
@@ -22,21 +54,15 @@ local({
   # mask 'utils' packages, will come first on the search path
   library(utils, lib.loc = .Library)
 
-  # check to see if renv has already been loaded
-  if (""renv"" %in% loadedNamespaces()) {
-
-    # if renv has already been loaded, and it's the requested version of renv,
-    # nothing to do
-    spec <- .getNamespaceInfo(.getNamespace(""renv""), ""spec"")
-    if (identical(spec[[""version""]], version))
-      return(invisible(TRUE))
-
-    # otherwise, unload and attempt to load the correct version of renv
+  # unload renv if it's already been loaded
+  if (""renv"" %in% loadedNamespaces())
     unloadNamespace(""renv"")
 
-  }
-
   # load bootstrap tools   
+  `%||%` <- function(x, y) {
+    if (is.environment(x) || length(x)) x else y
+  }
+  
   bootstrap <- function(version, library) {
   
     # attempt to download renv
@@ -62,6 +88,11 @@ local({
     if (!is.na(repos))
       return(repos)
   
+    # check for lockfile repositories
+    repos <- tryCatch(renv_bootstrap_repos_lockfile(), error = identity)
+    if (!inherits(repos, ""error"") && length(repos))
+      return(repos)
+  
     # if we're testing, re-use the test repositories
     if (renv_bootstrap_tests_running())
       return(getOption(""renv.tests.repos""))
@@ -70,10 +101,13 @@ local({
     repos <- getOption(""repos"")
   
     # ensure @CRAN@ entries are resolved
-    repos[repos == ""@CRAN@""] <- ""https://cloud.r-project.org""
+    repos[repos == ""@CRAN@""] <- getOption(
+      ""renv.repos.cran"",
+      ""https://cloud.r-project.org""
+    )
   
     # add in renv.bootstrap.repos if set
-    default <- c(CRAN = ""https://cloud.r-project.org"")
+    default <- c(FALLBACK = ""https://cloud.r-project.org"")
     extra <- getOption(""renv.bootstrap.repos"", default = default)
     repos <- c(repos, extra)
   
@@ -83,23 +117,51 @@ local({
   
   }
   
+  renv_bootstrap_repos_lockfile <- function() {
+  
+    lockpath <- Sys.getenv(""RENV_PATHS_LOCKFILE"", unset = ""renv.lock"")
+    if (!file.exists(lockpath))
+      return(NULL)
+  
+    lockfile <- tryCatch(renv_json_read(lockpath), error = identity)
+    if (inherits(lockfile, ""error"")) {
+      warning(lockfile)
+      return(NULL)
+    }
+  
+    repos <- lockfile$R$Repositories
+    if (length(repos) == 0)
+      return(NULL)
+  
+    keys <- vapply(repos, `[[`, ""Name"", FUN.VALUE = character(1))
+    vals <- vapply(repos, `[[`, ""URL"", FUN.VALUE = character(1))
+    names(vals) <- keys
+  
+    return(vals)
+  
+  }
+  
   renv_bootstrap_download <- function(version) {
   
     # if the renv version number has 4 components, assume it must
     # be retrieved via github
     nv <- numeric_version(version)
     components <- unclass(nv)[[1]]
   
-    methods <- if (length(components) == 4L) {
-      list(
+    # if this appears to be a development version of 'renv', we'll
+    # try to restore from github
+    dev <- length(components) == 4L
+  
+    # begin collecting different methods for finding renv
+    methods <- c(
+      renv_bootstrap_download_tarball,
+      if (dev)
         renv_bootstrap_download_github
-      )
-    } else {
-      list(
+      else c(
         renv_bootstrap_download_cran_latest,
         renv_bootstrap_download_cran_archive
       )
-    }
+    )
   
     for (method in methods) {
       path <- tryCatch(method(version), error = identity)
@@ -134,16 +196,20 @@ local({
   
   renv_bootstrap_download_cran_latest <- function(version) {
   
-    repos <- renv_bootstrap_download_cran_latest_find(version)
+    spec <- renv_bootstrap_download_cran_latest_find(version)
+  
+    message(""* Downloading renv "", version, "" ... "", appendLF = FALSE)
   
-    message(""* Downloading renv "", version, "" from CRAN ... "", appendLF = FALSE)
+    type  <- spec$type
+    repos <- spec$repos
   
     info <- tryCatch(
       utils::download.packages(
-        pkgs = ""renv"",
-        repos = repos,
+        pkgs    = ""renv"",
         destdir = tempdir(),
-        quiet = TRUE
+        repos   = repos,
+        type    = type,
+        quiet   = TRUE
       ),
       condition = identity
     )
@@ -153,36 +219,52 @@ local({
       return(FALSE)
     }
   
-    message(""OK"")
+    # report success and return
+    message(""OK (downloaded "", type, "")"")
     info[1, 2]
   
   }
   
   renv_bootstrap_download_cran_latest_find <- function(version) {
   
-    all <- renv_bootstrap_repos()
+    # check whether binaries are supported on this system
+    binary <-
+      getOption(""renv.bootstrap.binary"", default = TRUE) &&
+      !identical(.Platform$pkgType, ""source"") &&
+      !identical(getOption(""pkgType""), ""source"") &&
+      Sys.info()[[""sysname""]] %in% c(""Darwin"", ""Windows"")
   
-    for (repos in all) {
+    types <- c(if (binary) ""binary"", ""source"")
   
-      db <- tryCatch(
-        as.data.frame(
-          x = utils::available.packages(repos = repos),
-          stringsAsFactors = FALSE
-        ),
-        error = identity
-      )
+    # iterate over types + repositories
+    for (type in types) {
+      for (repos in renv_bootstrap_repos()) {
   
-      if (inherits(db, ""error""))
-        next
+        # retrieve package database
+        db <- tryCatch(
+          as.data.frame(
+            utils::available.packages(type = type, repos = repos),
+            stringsAsFactors = FALSE
+          ),
+          error = identity
+        )
   
-      entry <- db[db$Package %in% ""renv"" & db$Version %in% version, ]
-      if (nrow(entry) == 0)
-        next
+        if (inherits(db, ""error""))
+          next
   
-      return(repos)
+        # check for compatible entry
+        entry <- db[db$Package %in% ""renv"" & db$Version %in% version, ]
+        if (nrow(entry) == 0)
+          next
+  
+        # found it; return spec to caller
+        spec <- list(entry = entry, type = type, repos = repos)
+        return(spec)
   
+      }
     }
   
+    # if we got here, we failed to find renv
     fmt <- ""renv %s is not available from your declared package repositories""
     stop(sprintf(fmt, version))
   
@@ -195,7 +277,7 @@ local({
     urls <- file.path(repos, ""src/contrib/Archive/renv"", name)
     destfile <- file.path(tempdir(), name)
   
-    message(""* Downloading renv "", version, "" from CRAN archive ... "", appendLF = FALSE)
+    message(""* Downloading renv "", version, "" ... "", appendLF = FALSE)
   
     for (url in urls) {
   
@@ -216,6 +298,42 @@ local({
   
   }
   
+  renv_bootstrap_download_tarball <- function(version) {
+  
+    # if the user has provided the path to a tarball via
+    # an environment variable, then use it
+    tarball <- Sys.getenv(""RENV_BOOTSTRAP_TARBALL"", unset = NA)
+    if (is.na(tarball))
+      return()
+  
+    # allow directories
+    info <- file.info(tarball, extra_cols = FALSE)
+    if (identical(info$isdir, TRUE)) {
+      name <- sprintf(""renv_%s.tar.gz"", version)
+      tarball <- file.path(tarball, name)
+    }
+  
+    # bail if it doesn't exist
+    if (!file.exists(tarball)) {
+  
+      # let the user know we weren't able to honour their request
+      fmt <- ""* RENV_BOOTSTRAP_TARBALL is set (%s) but does not exist.""
+      msg <- sprintf(fmt, tarball)
+      warning(msg)
+  
+      # bail
+      return()
+  
+    }
+  
+    fmt <- ""* Bootstrapping with tarball at path '%s'.""
+    msg <- sprintf(fmt, tarball)
+    message(msg)
+  
+    tarball
+  
+  }
+  
   renv_bootstrap_download_github <- function(version) {
   
     enabled <- Sys.getenv(""RENV_BOOTSTRAP_FROM_GITHUB"", unset = ""TRUE"")
@@ -269,7 +387,13 @@ local({
     bin <- R.home(""bin"")
     exe <- if (Sys.info()[[""sysname""]] == ""Windows"") ""R.exe"" else ""R""
     r <- file.path(bin, exe)
-    args <- c(""--vanilla"", ""CMD"", ""INSTALL"", ""-l"", shQuote(library), shQuote(tarball))
+  
+    args <- c(
+      ""--vanilla"", ""CMD"", ""INSTALL"", ""--no-multiarch"",
+      ""-l"", shQuote(path.expand(library)),
+      shQuote(path.expand(tarball))
+    )
+  
     output <- system2(r, args, stdout = TRUE, stderr = TRUE)
     message(""Done!"")
   
@@ -286,7 +410,7 @@ local({
   
   }
   
-  renv_bootstrap_prefix <- function() {
+  renv_bootstrap_platform_prefix <- function() {
   
     # construct version prefix
     version <- paste(R.version$major, R.version$minor, sep = ""."")
@@ -305,15 +429,148 @@ local({
     components <- c(prefix, R.version$platform)
   
     # include prefix if provided by user
-    prefix <- Sys.getenv(""RENV_PATHS_PREFIX"")
-    if (nzchar(prefix))
+    prefix <- renv_bootstrap_platform_prefix_impl()
+    if (!is.na(prefix) && nzchar(prefix))
       components <- c(prefix, components)
   
     # build prefix
     paste(components, collapse = ""/"")
   
   }
   
+  renv_bootstrap_platform_prefix_impl <- function() {
+  
+    # if an explicit prefix has been supplied, use it
+    prefix <- Sys.getenv(""RENV_PATHS_PREFIX"", unset = NA)
+    if (!is.na(prefix))
+      return(prefix)
+  
+    # if the user has requested an automatic prefix, generate it
+    auto <- Sys.getenv(""RENV_PATHS_PREFIX_AUTO"", unset = NA)
+    if (auto %in% c(""TRUE"", ""True"", ""true"", ""1""))
+      return(renv_bootstrap_platform_prefix_auto())
+  
+    # empty string on failure
+    """"
+  
+  }
+  
+  renv_bootstrap_platform_prefix_auto <- function() {
+  
+    prefix <- tryCatch(renv_bootstrap_platform_os(), error = identity)
+    if (inherits(prefix, ""error"") || prefix %in% ""unknown"") {
+  
+      msg <- paste(
+        ""failed to infer current operating system"",
+        ""please file a bug report at https://github.com/rstudio/renv/issues"",
+        sep = ""; ""
+      )
+  
+      warning(msg)
+  
+    }
+  
+    prefix
+  
+  }
+  
+  renv_bootstrap_platform_os <- function() {
+  
+    sysinfo <- Sys.info()
+    sysname <- sysinfo[[""sysname""]]
+  
+    # handle Windows + macOS up front
+    if (sysname == ""Windows"")
+      return(""windows"")
+    else if (sysname == ""Darwin"")
+      return(""macos"")
+  
+    # check for os-release files
+    for (file in c(""/etc/os-release"", ""/usr/lib/os-release""))
+      if (file.exists(file))
+        return(renv_bootstrap_platform_os_via_os_release(file, sysinfo))
+  
+    # check for redhat-release files
+    if (file.exists(""/etc/redhat-release""))
+      return(renv_bootstrap_platform_os_via_redhat_release())
+  
+    ""unknown""
+  
+  }
+  
+  renv_bootstrap_platform_os_via_os_release <- function(file, sysinfo) {
+  
+    # read /etc/os-release
+    release <- utils::read.table(
+      file             = file,
+      sep              = ""="",
+      quote            = c(""\"""", ""'""),
+      col.names        = c(""Key"", ""Value""),
+      comment.char     = ""#"",
+      stringsAsFactors = FALSE
+    )
+  
+    vars <- as.list(release$Value)
+    names(vars) <- release$Key
+  
+    # get os name
+    os <- tolower(sysinfo[[""sysname""]])
+  
+    # read id
+    id <- ""unknown""
+    for (field in c(""ID"", ""ID_LIKE"")) {
+      if (field %in% names(vars) && nzchar(vars[[field]])) {
+        id <- vars[[field]]
+        break
+      }
+    }
+  
+    # read version
+    version <- ""unknown""
+    for (field in c(""UBUNTU_CODENAME"", ""VERSION_CODENAME"", ""VERSION_ID"", ""BUILD_ID"")) {
+      if (field %in% names(vars) && nzchar(vars[[field]])) {
+        version <- vars[[field]]
+        break
+      }
+    }
+  
+    # join together
+    paste(c(os, id, version), collapse = ""-"")
+  
+  }
+  
+  renv_bootstrap_platform_os_via_redhat_release <- function() {
+  
+    # read /etc/redhat-release
+    contents <- readLines(""/etc/redhat-release"", warn = FALSE)
+  
+    # infer id
+    id <- if (grepl(""centos"", contents, ignore.case = TRUE))
+      ""centos""
+    else if (grepl(""redhat"", contents, ignore.case = TRUE))
+      ""redhat""
+    else
+      ""unknown""
+  
+    # try to find a version component (very hacky)
+    version <- ""unknown""
+  
+    parts <- strsplit(contents, ""[[:space:]]"")[[1L]]
+    for (part in parts) {
+  
+      nv <- tryCatch(numeric_version(part), error = identity)
+      if (inherits(nv, ""error""))
+        next
+  
+      version <- nv[1, 1]
+      break
+  
+    }
+  
+    paste(c(""linux"", id, version), collapse = ""-"")
+  
+  }
+  
   renv_bootstrap_library_root_name <- function(project) {
   
     # use project name as-is if requested
@@ -329,17 +586,33 @@ local({
   
   renv_bootstrap_library_root <- function(project) {
   
+    prefix <- renv_bootstrap_profile_prefix()
+  
     path <- Sys.getenv(""RENV_PATHS_LIBRARY"", unset = NA)
     if (!is.na(path))
-      return(path)
+      return(paste(c(path, prefix), collapse = ""/""))
   
-    path <- Sys.getenv(""RENV_PATHS_LIBRARY_ROOT"", unset = NA)
-    if (!is.na(path)) {
+    path <- renv_bootstrap_library_root_impl(project)
+    if (!is.null(path)) {
       name <- renv_bootstrap_library_root_name(project)
-      return(file.path(path, name))
+      return(paste(c(path, prefix, name), collapse = ""/""))
     }
   
-    file.path(project, ""renv/library"")
+    renv_bootstrap_paths_renv(""library"", project = project)
+  
+  }
+  
+  renv_bootstrap_library_root_impl <- function(project) {
+  
+    root <- Sys.getenv(""RENV_PATHS_LIBRARY_ROOT"", unset = NA)
+    if (!is.na(root))
+      return(root)
+  
+    type <- renv_bootstrap_project_type(project)
+    if (identical(type, ""package"")) {
+      userdir <- renv_bootstrap_user_dir()
+      return(file.path(userdir, ""library""))
+    }
   
   }
   
@@ -396,12 +669,241 @@ local({
     TRUE
   
   }
+  
+  renv_bootstrap_profile_load <- function(project) {
+  
+    # if RENV_PROFILE is already set, just use that
+    profile <- Sys.getenv(""RENV_PROFILE"", unset = NA)
+    if (!is.na(profile) && nzchar(profile))
+      return(profile)
+  
+    # check for a profile file (nothing to do if it doesn't exist)
+    path <- renv_bootstrap_paths_renv(""profile"", profile = FALSE)
+    if (!file.exists(path))
+      return(NULL)
+  
+    # read the profile, and set it if it exists
+    contents <- readLines(path, warn = FALSE)
+    if (length(contents) == 0L)
+      return(NULL)
+  
+    # set RENV_PROFILE
+    profile <- contents[[1L]]
+    if (!profile %in% c("""", ""default""))
+      Sys.setenv(RENV_PROFILE = profile)
+  
+    profile
+  
+  }
+  
+  renv_bootstrap_profile_prefix <- function() {
+    profile <- renv_bootstrap_profile_get()
+    if (!is.null(profile))
+      return(file.path(""profiles"", profile, ""renv""))
+  }
+  
+  renv_bootstrap_profile_get <- function() {
+    profile <- Sys.getenv(""RENV_PROFILE"", unset = """")
+    renv_bootstrap_profile_normalize(profile)
+  }
+  
+  renv_bootstrap_profile_set <- function(profile) {
+    profile <- renv_bootstrap_profile_normalize(profile)
+    if (is.null(profile))
+      Sys.unsetenv(""RENV_PROFILE"")
+    else
+      Sys.setenv(RENV_PROFILE = profile)
+  }
+  
+  renv_bootstrap_profile_normalize <- function(profile) {
+  
+    if (is.null(profile) || profile %in% c("""", ""default""))
+      return(NULL)
+  
+    profile
+  
+  }
+  
+  renv_bootstrap_path_absolute <- function(path) {
+  
+    substr(path, 1L, 1L) %in% c(""~"", ""/"", ""\\"") || (
+      substr(path, 1L, 1L) %in% c(letters, LETTERS) &&
+      substr(path, 2L, 3L) %in% c("":/"", "":\\"")
+    )
+  
+  }
+  
+  renv_bootstrap_paths_renv <- function(..., profile = TRUE, project = NULL) {
+    renv <- Sys.getenv(""RENV_PATHS_RENV"", unset = ""renv"")
+    root <- if (renv_bootstrap_path_absolute(renv)) NULL else project
+    prefix <- if (profile) renv_bootstrap_profile_prefix()
+    components <- c(root, renv, prefix, ...)
+    paste(components, collapse = ""/"")
+  }
+  
+  renv_bootstrap_project_type <- function(path) {
+  
+    descpath <- file.path(path, ""DESCRIPTION"")
+    if (!file.exists(descpath))
+      return(""unknown"")
+  
+    desc <- tryCatch(
+      read.dcf(descpath, all = TRUE),
+      error = identity
+    )
+  
+    if (inherits(desc, ""error""))
+      return(""unknown"")
+  
+    type <- desc$Type
+    if (!is.null(type))
+      return(tolower(type))
+  
+    package <- desc$Package
+    if (!is.null(package))
+      return(""package"")
+  
+    ""unknown""
+  
+  }
+  
+  renv_bootstrap_user_dir <- function() {
+    dir <- renv_bootstrap_user_dir_impl()
+    path.expand(chartr(""\\"", ""/"", dir))
+  }
+  
+  renv_bootstrap_user_dir_impl <- function() {
+  
+    # use local override if set
+    override <- getOption(""renv.userdir.override"")
+    if (!is.null(override))
+      return(override)
+  
+    # use R_user_dir if available
+    tools <- asNamespace(""tools"")
+    if (is.function(tools$R_user_dir))
+      return(tools$R_user_dir(""renv"", ""cache""))
+  
+    # try using our own backfill for older versions of R
+    envvars <- c(""R_USER_CACHE_DIR"", ""XDG_CACHE_HOME"")
+    for (envvar in envvars) {
+      root <- Sys.getenv(envvar, unset = NA)
+      if (!is.na(root))
+        return(file.path(root, ""R/renv""))
+    }
+  
+    # use platform-specific default fallbacks
+    if (Sys.info()[[""sysname""]] == ""Windows"")
+      file.path(Sys.getenv(""LOCALAPPDATA""), ""R/cache/R/renv"")
+    else if (Sys.info()[[""sysname""]] == ""Darwin"")
+      ""~/Library/Caches/org.R-project.R/R/renv""
+    else
+      ""~/.cache/R/renv""
+  
+  }
+  
+  
+  renv_json_read <- function(file = NULL, text = NULL) {
+  
+    text <- paste(text %||% read(file), collapse = ""\n"")
+  
+    # find strings in the JSON
+    pattern <- '[""](?:(?:\\\\.)|(?:[^""\\\\]))*?[""]'
+    locs <- gregexpr(pattern, text, perl = TRUE)[[1]]
+  
+    # if any are found, replace them with placeholders
+    replaced <- text
+    strings <- character()
+    replacements <- character()
+  
+    if (!identical(c(locs), -1L)) {
+  
+      # get the string values
+      starts <- locs
+      ends <- locs + attr(locs, ""match.length"") - 1L
+      strings <- substring(text, starts, ends)
+  
+      # only keep those requiring escaping
+      strings <- grep(""[[\\]{}:]"", strings, perl = TRUE, value = TRUE)
+  
+      # compute replacements
+      replacements <- sprintf('""\032%i\032""', seq_along(strings))
+  
+      # replace the strings
+      mapply(function(string, replacement) {
+        replaced <<- sub(string, replacement, replaced, fixed = TRUE)
+      }, strings, replacements)
+  
+    }
+  
+    # transform the JSON into something the R parser understands
+    transformed <- replaced
+    transformed <- gsub(""[[{]"", ""list("", transformed)
+    transformed <- gsub(""[]}]"", "")"", transformed)
+    transformed <- gsub("":"", ""="", transformed, fixed = TRUE)
+    text <- paste(transformed, collapse = ""\n"")
+  
+    # parse it
+    json <- parse(text = text, keep.source = FALSE, srcfile = NULL)[[1L]]
+  
+    # construct map between source strings, replaced strings
+    map <- as.character(parse(text = strings))
+    names(map) <- as.character(parse(text = replacements))
+  
+    # convert to list
+    map <- as.list(map)
+  
+    # remap strings in object
+    remapped <- renv_json_remap(json, map)
+  
+    # evaluate
+    eval(remapped, envir = baseenv())
+  
+  }
+  
+  renv_json_remap <- function(json, map) {
+  
+    # fix names
+    if (!is.null(names(json))) {
+      lhs <- match(names(json), names(map), nomatch = 0L)
+      rhs <- match(names(map), names(json), nomatch = 0L)
+      names(json)[rhs] <- map[lhs]
+    }
+  
+    # fix values
+    if (is.character(json))
+      return(map[[json]] %||% json)
+  
+    # handle true, false, null
+    if (is.name(json)) {
+      text <- as.character(json)
+      if (text == ""true"")
+        return(TRUE)
+      else if (text == ""false"")
+        return(FALSE)
+      else if (text == ""null"")
+        return(NULL)
+    }
+  
+    # recurse
+    if (is.recursive(json)) {
+      for (i in seq_along(json)) {
+        json[i] <- list(renv_json_remap(json[[i]], map))
+      }
+    }
+  
+    json
+  
+  }
+
+  # load the renv profile, if any
+  renv_bootstrap_profile_load(project)
 
   # construct path to library root
   root <- renv_bootstrap_library_root(project)
 
   # construct library prefix for platform
-  prefix <- renv_bootstrap_prefix()
+  prefix <- renv_bootstrap_platform_prefix()
 
   # construct full libpath
   libpath <- file.path(root, prefix)"
NBISweden,workshop-mlbiostatistics,fa7fcf98dd33bf60520224909af8e36fa9a9addb,Olga,olga.dethlefsen@nbis.se,2022-09-13T14:40:47Z,GitHub,noreply@github.com,2022-09-13T14:40:47Z,"Ignore session-pca from rendering 

Caused Jekyll error",_config.yml,False,False,False,False,1,1,2,"---FILE: _config.yml---
@@ -7,7 +7,7 @@ includes_dir: ./workshop-common/includes
 sass:
   sass_dir: ./workshop-common/sass
 
-exclude: [""session-probability/*.Rmd"",""session-probability/lectures/""]
+exclude: [""session-probability/*.Rmd"",""session-probability/lectures/"", ""session-pca""]
 
 plugins:
   - jemoji"
NBISweden,workshop-mlbiostatistics,9985a26bbefe7e0113813edf21b49538d35d986e,Mun-Gwan Hong,mungwan.hong@nbis.se,2022-09-11T20:25:42Z,Mun-Gwan Hong,mungwan.hong@nbis.se,2022-09-11T20:25:42Z,fix adjust_html_for_canvas.sh to include images/,session-clustering/html/adjust_html_for_canvas.sh,False,False,False,False,2,1,3,"---FILE: session-clustering/html/adjust_html_for_canvas.sh---
@@ -4,5 +4,6 @@ COURSE_GITHUB_IO=""https://nbisweden.github.io/workshop-mlbiostatistics/session-c
 for html_out in chapters exercises exercises-solutions
 do
   html_out_files=$html_out""_files""
-  sed -e ""s,$html_out_files/figure-html,$COURSE_GITHUB_IO/$html_out_files/figure-html,g"" $html_out.html > to_canvas/$html_out.html
+  sed -e ""s,$html_out_files/figure-html,$COURSE_GITHUB_IO/$html_out_files/figure-html,g"" $html_out.html |
+    sed -e ""s,src=\""images/,src=\""$COURSE_GITHUB_IO/images/,g"" > to_canvas/$html_out.html
 done"
NBISweden,workshop-mlbiostatistics,53a20c930a557a1512afa86ae5a49150be905757,olgadet,,2022-09-10T10:13:36Z,olgadet,,2022-09-10T10:13:36Z,Fix name and link,schedule.md;session-rank-tests-presentation/session-rank-tests-presentatation_files/figure-revealjs/unnamed-chunk-11-1.png;session-rank-tests-presentation/session-rank-tests-presentatation_files/figure-revealjs/unnamed-chunk-13-1.png;session-rank-tests-presentation/session-rank-tests-presentatation_files/figure-revealjs/unnamed-chunk-14-1.png;session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/bootstrap/bootstrap-icons.css;session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/bootstrap/bootstrap-icons.woff;session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/bootstrap/bootstrap.min.css;session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/bootstrap/bootstrap.min.js;session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/quarto-html/anchor.min.js;session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/quarto-html/quarto.js;session-rank-tests-presentation/session-rank-tests-presentation.Rproj;session-rank-tests-presentation/session-rank-tests-presentation.html;session-rank-tests-presentation/session-rank-tests-presentation.qmd;session-rank-tests-presentation/session-rank-tests-presentation_files/figure-revealjs/unnamed-chunk-12-1.png;session-rank-tests-presentation/session-rank-tests-presentation_files/figure-revealjs/unnamed-chunk-6-1.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/clipboard/clipboard.min.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/kePrint-0.0.1/kePrint.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/lightable-0.0.1/lightable.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/quarto-html/popper.min.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/quarto-html/quarto-html.min.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/quarto-html/quarto-syntax-highlighting.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/quarto-html/tabby.min.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/quarto-html/tippy.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/quarto-html/tippy.umd.min.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/reset.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/reveal.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/reveal.esm.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/reveal.esm.js.map;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/reveal.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/reveal.js.map;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/league-gothic/LICENSE;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/league-gothic/league-gothic.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/league-gothic/league-gothic.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/league-gothic/league-gothic.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/league-gothic/league-gothic.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/LICENSE;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-italic.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-italic.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-italic.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-regular.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-regular.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-regular.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-semibold.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-semibold.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-semibold.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-semibolditalic.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-semibolditalic.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro-semibolditalic.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/fonts/source-sans-pro/source-sans-pro.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/dist/theme/quarto.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/highlight/highlight.esm.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/highlight/highlight.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/highlight/monokai.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/highlight/plugin.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/highlight/zenburn.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/markdown/markdown.esm.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/markdown/markdown.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/markdown/plugin.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/math/katex.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/math/math.esm.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/math/math.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/math/mathjax2.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/math/mathjax3.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/math/plugin.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/notes/notes.esm.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/notes/notes.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/notes/plugin.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/notes/speaker-view.html;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/pdf-export/plugin.yml;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-line-highlight/plugin.yml;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-support/footer.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-support/plugin.yml;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-support/support.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/README.md;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/LICENSE.txt;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/brands.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/fontawesome.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/regular.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/solid.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/svg-with-js.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/v4-shims.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/v4-shims.min.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-brands-400.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-brands-400.svg;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-brands-400.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-brands-400.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-brands-400.woff2;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-regular-400.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-regular-400.svg;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-regular-400.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-regular-400.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-regular-400.woff2;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-solid-900.eot;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-solid-900.svg;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-solid-900.ttf;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-solid-900.woff;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/webfonts/fa-solid-900.woff2;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/blackboard.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/boardmarker-black.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/boardmarker-blue.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/boardmarker-green.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/boardmarker-orange.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/boardmarker-purple.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/boardmarker-red.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/boardmarker-yellow.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/chalk-blue.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/chalk-green.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/chalk-orange.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/chalk-purple.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/chalk-red.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/chalk-white.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/chalk-yellow.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/sponge.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/img/whiteboard.png;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/plugin.yml;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/style.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/menu.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/menu.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/plugin.yml;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/search/plugin.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/search/search.esm.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/search/search.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/zoom/plugin.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/zoom/zoom.esm.js;session-rank-tests-presentation/session-rank-tests-presentation_files/libs/revealjs/plugin/zoom/zoom.js,True,False,True,False,159,2575,2734,"---FILE: schedule.md---
@@ -55,7 +55,7 @@ title:  'Schedule'
 
 **15.00 - 15.30** [Multiple testing correction](session-inference/infe-multiple.html)
 
-**15.30 - 16.30** [Non parametric tests](session-rank-tests/docs/index.html) ((.presentation)[session-rank-tests-presentation/session-rank-tests-presentatation.html])
+**15.30 - 16.30** [Non parametric tests](session-rank-tests/docs/index.html) ([.presentation](session-rank-tests-presentation/session-rank-tests-presentation.html))
 
 **16.30 - 17.00** Daily challenge
 

---FILE: session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/bootstrap/bootstrap-icons.css---
@@ -1,1704 +0,0 @@
-@font-face {
-  font-family: ""bootstrap-icons"";
-  src: 
-url(""./bootstrap-icons.woff?524846017b983fc8ded9325d94ed40f3"") format(""woff"");
-}
-
-.bi::before,
-[class^=""bi-""]::before,
-[class*="" bi-""]::before {
-  display: inline-block;
-  font-family: bootstrap-icons !important;
-  font-style: normal;
-  font-weight: normal !important;
-  font-variant: normal;
-  text-transform: none;
-  line-height: 1;
-  vertical-align: -.125em;
-  -webkit-font-smoothing: antialiased;
-  -moz-osx-font-smoothing: grayscale;
-}
-
-.bi-123::before { content: ""\f67f""; }
-.bi-alarm-fill::before { content: ""\f101""; }
-.bi-alarm::before { content: ""\f102""; }
-.bi-align-bottom::before { content: ""\f103""; }
-.bi-align-center::before { content: ""\f104""; }
-.bi-align-end::before { content: ""\f105""; }
-.bi-align-middle::before { content: ""\f106""; }
-.bi-align-start::before { content: ""\f107""; }
-.bi-align-top::before { content: ""\f108""; }
-.bi-alt::before { content: ""\f109""; }
-.bi-app-indicator::before { content: ""\f10a""; }
-.bi-app::before { content: ""\f10b""; }
-.bi-archive-fill::before { content: ""\f10c""; }
-.bi-archive::before { content: ""\f10d""; }
-.bi-arrow-90deg-down::before { content: ""\f10e""; }
-.bi-arrow-90deg-left::before { content: ""\f10f""; }
-.bi-arrow-90deg-right::before { content: ""\f110""; }
-.bi-arrow-90deg-up::before { content: ""\f111""; }
-.bi-arrow-bar-down::before { content: ""\f112""; }
-.bi-arrow-bar-left::before { content: ""\f113""; }
-.bi-arrow-bar-right::before { content: ""\f114""; }
-.bi-arrow-bar-up::before { content: ""\f115""; }
-.bi-arrow-clockwise::before { content: ""\f116""; }
-.bi-arrow-counterclockwise::before { content: ""\f117""; }
-.bi-arrow-down-circle-fill::before { content: ""\f118""; }
-.bi-arrow-down-circle::before { content: ""\f119""; }
-.bi-arrow-down-left-circle-fill::before { content: ""\f11a""; }
-.bi-arrow-down-left-circle::before { content: ""\f11b""; }
-.bi-arrow-down-left-square-fill::before { content: ""\f11c""; }
-.bi-arrow-down-left-square::before { content: ""\f11d""; }
-.bi-arrow-down-left::before { content: ""\f11e""; }
-.bi-arrow-down-right-circle-fill::before { content: ""\f11f""; }
-.bi-arrow-down-right-circle::before { content: ""\f120""; }
-.bi-arrow-down-right-square-fill::before { content: ""\f121""; }
-.bi-arrow-down-right-square::before { content: ""\f122""; }
-.bi-arrow-down-right::before { content: ""\f123""; }
-.bi-arrow-down-short::before { content: ""\f124""; }
-.bi-arrow-down-square-fill::before { content: ""\f125""; }
-.bi-arrow-down-square::before { content: ""\f126""; }
-.bi-arrow-down-up::before { content: ""\f127""; }
-.bi-arrow-down::before { content: ""\f128""; }
-.bi-arrow-left-circle-fill::before { content: ""\f129""; }
-.bi-arrow-left-circle::before { content: ""\f12a""; }
-.bi-arrow-left-right::before { content: ""\f12b""; }
-.bi-arrow-left-short::before { content: ""\f12c""; }
-.bi-arrow-left-square-fill::before { content: ""\f12d""; }
-.bi-arrow-left-square::before { content: ""\f12e""; }
-.bi-arrow-left::before { content: ""\f12f""; }
-.bi-arrow-repeat::before { content: ""\f130""; }
-.bi-arrow-return-left::before { content: ""\f131""; }
-.bi-arrow-return-right::before { content: ""\f132""; }
-.bi-arrow-right-circle-fill::before { content: ""\f133""; }
-.bi-arrow-right-circle::before { content: ""\f134""; }
-.bi-arrow-right-short::before { content: ""\f135""; }
-.bi-arrow-right-square-fill::before { content: ""\f136""; }
-.bi-arrow-right-square::before { content: ""\f137""; }
-.bi-arrow-right::before { content: ""\f138""; }
-.bi-arrow-up-circle-fill::before { content: ""\f139""; }
-.bi-arrow-up-circle::before { content: ""\f13a""; }
-.bi-arrow-up-left-circle-fill::before { content: ""\f13b""; }
-.bi-arrow-up-left-circle::before { content: ""\f13c""; }
-.bi-arrow-up-left-square-fill::before { content: ""\f13d""; }
-.bi-arrow-up-left-square::before { content: ""\f13e""; }
-.bi-arrow-up-left::before { content: ""\f13f""; }
-.bi-arrow-up-right-circle-fill::before { content: ""\f140""; }
-.bi-arrow-up-right-circle::before { content: ""\f141""; }
-.bi-arrow-up-right-square-fill::before { content: ""\f142""; }
-.bi-arrow-up-right-square::before { content: ""\f143""; }
-.bi-arrow-up-right::before { content: ""\f144""; }
-.bi-arrow-up-short::before { content: ""\f145""; }
-.bi-arrow-up-square-fill::before { content: ""\f146""; }
-.bi-arrow-up-square::before { content: ""\f147""; }
-.bi-arrow-up::before { content: ""\f148""; }
-.bi-arrows-angle-contract::before { content: ""\f149""; }
-.bi-arrows-angle-expand::before { content: ""\f14a""; }
-.bi-arrows-collapse::before { content: ""\f14b""; }
-.bi-arrows-expand::before { content: ""\f14c""; }
-.bi-arrows-fullscreen::before { content: ""\f14d""; }
-.bi-arrows-move::before { content: ""\f14e""; }
-.bi-aspect-ratio-fill::before { content: ""\f14f""; }
-.bi-aspect-ratio::before { content: ""\f150""; }
-.bi-asterisk::before { content: ""\f151""; }
-.bi-at::before { content: ""\f152""; }
-.bi-award-fill::before { content: ""\f153""; }
-.bi-award::before { content: ""\f154""; }
-.bi-back::before { content: ""\f155""; }
-.bi-backspace-fill::before { content: ""\f156""; }
-.bi-backspace-reverse-fill::before { content: ""\f157""; }
-.bi-backspace-reverse::before { content: ""\f158""; }
-.bi-backspace::before { content: ""\f159""; }
-.bi-badge-3d-fill::before { content: ""\f15a""; }
-.bi-badge-3d::before { content: ""\f15b""; }
-.bi-badge-4k-fill::before { content: ""\f15c""; }
-.bi-badge-4k::before { content: ""\f15d""; }
-.bi-badge-8k-fill::before { content: ""\f15e""; }
-.bi-badge-8k::before { content: ""\f15f""; }
-.bi-badge-ad-fill::before { content: ""\f160""; }
-.bi-badge-ad::before { content: ""\f161""; }
-.bi-badge-ar-fill::before { content: ""\f162""; }
-.bi-badge-ar::before { content: ""\f163""; }
-.bi-badge-cc-fill::before { content: ""\f164""; }
-.bi-badge-cc::before { content: ""\f165""; }
-.bi-badge-hd-fill::before { content: ""\f166""; }
-.bi-badge-hd::before { content: ""\f167""; }
-.bi-badge-tm-fill::before { content: ""\f168""; }
-.bi-badge-tm::before { content: ""\f169""; }
-.bi-badge-vo-fill::before { content: ""\f16a""; }
-.bi-badge-vo::before { content: ""\f16b""; }
-.bi-badge-vr-fill::before { content: ""\f16c""; }
-.bi-badge-vr::before { content: ""\f16d""; }
-.bi-badge-wc-fill::before { content: ""\f16e""; }
-.bi-badge-wc::before { content: ""\f16f""; }
-.bi-bag-check-fill::before { content: ""\f170""; }
-.bi-bag-check::before { content: ""\f171""; }
-.bi-bag-dash-fill::before { content: ""\f172""; }
-.bi-bag-dash::before { content: ""\f173""; }
-.bi-bag-fill::before { content: ""\f174""; }
-.bi-bag-plus-fill::before { content: ""\f175""; }
-.bi-bag-plus::before { content: ""\f176""; }
-.bi-bag-x-fill::before { content: ""\f177""; }
-.bi-bag-x::before { content: ""\f178""; }
-.bi-bag::before { content: ""\f179""; }
-.bi-bar-chart-fill::before { content: ""\f17a""; }
-.bi-bar-chart-line-fill::before { content: ""\f17b""; }
-.bi-bar-chart-line::before { content: ""\f17c""; }
-.bi-bar-chart-steps::before { content: ""\f17d""; }
-.bi-bar-chart::before { content: ""\f17e""; }
-.bi-basket-fill::before { content: ""\f17f""; }
-.bi-basket::before { content: ""\f180""; }
-.bi-basket2-fill::before { content: ""\f181""; }
-.bi-basket2::before { content: ""\f182""; }
-.bi-basket3-fill::before { content: ""\f183""; }
-.bi-basket3::before { content: ""\f184""; }
-.bi-battery-charging::before { content: ""\f185""; }
-.bi-battery-full::before { content: ""\f186""; }
-.bi-battery-half::before { content: ""\f187""; }
-.bi-battery::before { content: ""\f188""; }
-.bi-bell-fill::before { content: ""\f189""; }
-.bi-bell::before { content: ""\f18a""; }
-.bi-bezier::before { content: ""\f18b""; }
-.bi-bezier2::before { content: ""\f18c""; }
-.bi-bicycle::before { content: ""\f18d""; }
-.bi-binoculars-fill::before { content: ""\f18e""; }
-.bi-binoculars::before { content: ""\f18f""; }
-.bi-blockquote-left::before { content: ""\f190""; }
-.bi-blockquote-right::before { content: ""\f191""; }
-.bi-book-fill::before { content: ""\f192""; }
-.bi-book-half::before { content: ""\f193""; }
-.bi-book::before { content: ""\f194""; }
-.bi-bookmark-check-fill::before { content: ""\f195""; }
-.bi-bookmark-check::before { content: ""\f196""; }
-.bi-bookmark-dash-fill::before { content: ""\f197""; }
-.bi-bookmark-dash::before { content: ""\f198""; }
-.bi-bookmark-fill::before { content: ""\f199""; }
-.bi-bookmark-heart-fill::before { content: ""\f19a""; }
-.bi-bookmark-heart::before { content: ""\f19b""; }
-.bi-bookmark-plus-fill::before { content: ""\f19c""; }
-.bi-bookmark-plus::before { content: ""\f19d""; }
-.bi-bookmark-star-fill::before { content: ""\f19e""; }
-.bi-bookmark-star::before { content: ""\f19f""; }
-.bi-bookmark-x-fill::before { content: ""\f1a0""; }
-.bi-bookmark-x::before { content: ""\f1a1""; }
-.bi-bookmark::before { content: ""\f1a2""; }
-.bi-bookmarks-fill::before { content: ""\f1a3""; }
-.bi-bookmarks::before { content: ""\f1a4""; }
-.bi-bookshelf::before { content: ""\f1a5""; }
-.bi-bootstrap-fill::before { content: ""\f1a6""; }
-.bi-bootstrap-reboot::before { content: ""\f1a7""; }
-.bi-bootstrap::before { content: ""\f1a8""; }
-.bi-border-all::before { content: ""\f1a9""; }
-.bi-border-bottom::before { content: ""\f1aa""; }
-.bi-border-center::before { content: ""\f1ab""; }
-.bi-border-inner::before { content: ""\f1ac""; }
-.bi-border-left::before { content: ""\f1ad""; }
-.bi-border-middle::before { content: ""\f1ae""; }
-.bi-border-outer::before { content: ""\f1af""; }
-.bi-border-right::before { content: ""\f1b0""; }
-.bi-border-style::before { content: ""\f1b1""; }
-.bi-border-top::before { content: ""\f1b2""; }
-.bi-border-width::before { content: ""\f1b3""; }
-.bi-border::before { content: ""\f1b4""; }
-.bi-bounding-box-circles::before { content: ""\f1b5""; }
-.bi-bounding-box::before { content: ""\f1b6""; }
-.bi-box-arrow-down-left::before { content: ""\f1b7""; }
-.bi-box-arrow-down-right::before { content: ""\f1b8""; }
-.bi-box-arrow-down::before { content: ""\f1b9""; }
-.bi-box-arrow-in-down-left::before { content: ""\f1ba""; }
-.bi-box-arrow-in-down-right::before { content: ""\f1bb""; }
-.bi-box-arrow-in-down::before { content: ""\f1bc""; }
-.bi-box-arrow-in-left::before { content: ""\f1bd""; }
-.bi-box-arrow-in-right::before { content: ""\f1be""; }
-.bi-box-arrow-in-up-left::before { content: ""\f1bf""; }
-.bi-box-arrow-in-up-right::before { content: ""\f1c0""; }
-.bi-box-arrow-in-up::before { content: ""\f1c1""; }
-.bi-box-arrow-left::before { content: ""\f1c2""; }
-.bi-box-arrow-right::before { content: ""\f1c3""; }
-.bi-box-arrow-up-left::before { content: ""\f1c4""; }
-.bi-box-arrow-up-right::before { content: ""\f1c5""; }
-.bi-box-arrow-up::before { content: ""\f1c6""; }
-.bi-box-seam::before { content: ""\f1c7""; }
-.bi-box::before { content: ""\f1c8""; }
-.bi-braces::before { content: ""\f1c9""; }
-.bi-bricks::before { content: ""\f1ca""; }
-.bi-briefcase-fill::before { content: ""\f1cb""; }
-.bi-briefcase::before { content: ""\f1cc""; }
-.bi-brightness-alt-high-fill::before { content: ""\f1cd""; }
-.bi-brightness-alt-high::before { content: ""\f1ce""; }
-.bi-brightness-alt-low-fill::before { content: ""\f1cf""; }
-.bi-brightness-alt-low::before { content: ""\f1d0""; }
-.bi-brightness-high-fill::before { content: ""\f1d1""; }
-.bi-brightness-high::before { content: ""\f1d2""; }
-.bi-brightness-low-fill::before { content: ""\f1d3""; }
-.bi-brightness-low::before { content: ""\f1d4""; }
-.bi-broadcast-pin::before { content: ""\f1d5""; }
-.bi-broadcast::before { content: ""\f1d6""; }
-.bi-brush-fill::before { content: ""\f1d7""; }
-.bi-brush::before { content: ""\f1d8""; }
-.bi-bucket-fill::before { content: ""\f1d9""; }
-.bi-bucket::before { content: ""\f1da""; }
-.bi-bug-fill::before { content: ""\f1db""; }
-.bi-bug::before { content: ""\f1dc""; }
-.bi-building::before { content: ""\f1dd""; }
-.bi-bullseye::before { content: ""\f1de""; }
-.bi-calculator-fill::before { content: ""\f1df""; }
-.bi-calculator::before { content: ""\f1e0""; }
-.bi-calendar-check-fill::before { content: ""\f1e1""; }
-.bi-calendar-check::before { content: ""\f1e2""; }
-.bi-calendar-date-fill::before { content: ""\f1e3""; }
-.bi-calendar-date::before { content: ""\f1e4""; }
-.bi-calendar-day-fill::before { content: ""\f1e5""; }
-.bi-calendar-day::before { content: ""\f1e6""; }
-.bi-calendar-event-fill::before { content: ""\f1e7""; }
-.bi-calendar-event::before { content: ""\f1e8""; }
-.bi-calendar-fill::before { content: ""\f1e9""; }
-.bi-calendar-minus-fill::before { content: ""\f1ea""; }
-.bi-calendar-minus::before { content: ""\f1eb""; }
-.bi-calendar-month-fill::before { content: ""\f1ec""; }
-.bi-calendar-month::before { content: ""\f1ed""; }
-.bi-calendar-plus-fill::before { content: ""\f1ee""; }
-.bi-calendar-plus::before { content: ""\f1ef""; }
-.bi-calendar-range-fill::before { content: ""\f1f0""; }
-.bi-calendar-range::before { content: ""\f1f1""; }
-.bi-calendar-week-fill::before { content: ""\f1f2""; }
-.bi-calendar-week::before { content: ""\f1f3""; }
-.bi-calendar-x-fill::before { content: ""\f1f4""; }
-.bi-calendar-x::before { content: ""\f1f5""; }
-.bi-calendar::before { content: ""\f1f6""; }
-.bi-calendar2-check-fill::before { content: ""\f1f7""; }
-.bi-calendar2-check::before { content: ""\f1f8""; }
-.bi-calendar2-date-fill::before { content: ""\f1f9""; }
-.bi-calendar2-date::before { content: ""\f1fa""; }
-.bi-calendar2-day-fill::before { content: ""\f1fb""; }
-.bi-calendar2-day::before { content: ""\f1fc""; }
-.bi-calendar2-event-fill::before { content: ""\f1fd""; }
-.bi-calendar2-event::before { content: ""\f1fe""; }
-.bi-calendar2-fill::before { content: ""\f1ff""; }
-.bi-calendar2-minus-fill::before { content: ""\f200""; }
-.bi-calendar2-minus::before { content: ""\f201""; }
-.bi-calendar2-month-fill::before { content: ""\f202""; }
-.bi-calendar2-month::before { content: ""\f203""; }
-.bi-calendar2-plus-fill::before { content: ""\f204""; }
-.bi-calendar2-plus::before { content: ""\f205""; }
-.bi-calendar2-range-fill::before { content: ""\f206""; }
-.bi-calendar2-range::before { content: ""\f207""; }
-.bi-calendar2-week-fill::before { content: ""\f208""; }
-.bi-calendar2-week::before { content: ""\f209""; }
-.bi-calendar2-x-fill::before { content: ""\f20a""; }
-.bi-calendar2-x::before { content: ""\f20b""; }
-.bi-calendar2::before { content: ""\f20c""; }
-.bi-calendar3-event-fill::before { content: ""\f20d""; }
-.bi-calendar3-event::before { content: ""\f20e""; }
-.bi-calendar3-fill::before { content: ""\f20f""; }
-.bi-calendar3-range-fill::before { content: ""\f210""; }
-.bi-calendar3-range::before { content: ""\f211""; }
-.bi-calendar3-week-fill::before { content: ""\f212""; }
-.bi-calendar3-week::before { content: ""\f213""; }
-.bi-calendar3::before { content: ""\f214""; }
-.bi-calendar4-event::before { content: ""\f215""; }
-.bi-calendar4-range::before { content: ""\f216""; }
-.bi-calendar4-week::before { content: ""\f217""; }
-.bi-calendar4::before { content: ""\f218""; }
-.bi-camera-fill::before { content: ""\f219""; }
-.bi-camera-reels-fill::before { content: ""\f21a""; }
-.bi-camera-reels::before { content: ""\f21b""; }
-.bi-camera-video-fill::before { content: ""\f21c""; }
-.bi-camera-video-off-fill::before { content: ""\f21d""; }
-.bi-camera-video-off::before { content: ""\f21e""; }
-.bi-camera-video::before { content: ""\f21f""; }
-.bi-camera::before { content: ""\f220""; }
-.bi-camera2::before { content: ""\f221""; }
-.bi-capslock-fill::before { content: ""\f222""; }
-.bi-capslock::before { content: ""\f223""; }
-.bi-card-checklist::before { content: ""\f224""; }
-.bi-card-heading::before { content: ""\f225""; }
-.bi-card-image::before { content: ""\f226""; }
-.bi-card-list::before { content: ""\f227""; }
-.bi-card-text::before { content: ""\f228""; }
-.bi-caret-down-fill::before { content: ""\f229""; }
-.bi-caret-down-square-fill::before { content: ""\f22a""; }
-.bi-caret-down-square::before { content: ""\f22b""; }
-.bi-caret-down::before { content: ""\f22c""; }
-.bi-caret-left-fill::before { content: ""\f22d""; }
-.bi-caret-left-square-fill::before { content: ""\f22e""; }
-.bi-caret-left-square::before { content: ""\f22f""; }
-.bi-caret-left::before { content: ""\f230""; }
-.bi-caret-right-fill::before { content: ""\f231""; }
-.bi-caret-right-square-fill::before { content: ""\f232""; }
-.bi-caret-right-square::before { content: ""\f233""; }
-.bi-caret-right::before { content: ""\f234""; }
-.bi-caret-up-fill::before { content: ""\f235""; }
-.bi-caret-up-square-fill::before { content: ""\f236""; }
-.bi-caret-up-square::before { content: ""\f237""; }
-.bi-caret-up::before { content: ""\f238""; }
-.bi-cart-check-fill::before { content: ""\f239""; }
-.bi-cart-check::before { content: ""\f23a""; }
-.bi-cart-dash-fill::before { content: ""\f23b""; }
-.bi-cart-dash::before { content: ""\f23c""; }
-.bi-cart-fill::before { content: ""\f23d""; }
-.bi-cart-plus-fill::before { content: ""\f23e""; }
-.bi-cart-plus::before { content: ""\f23f""; }
-.bi-cart-x-fill::before { content: ""\f240""; }
-.bi-cart-x::before { content: ""\f241""; }
-.bi-cart::before { content: ""\f242""; }
-.bi-cart2::before { content: ""\f243""; }
-.bi-cart3::before { content: ""\f244""; }
-.bi-cart4::before { content: ""\f245""; }
-.bi-cash-stack::before { content: ""\f246""; }
-.bi-cash::before { content: ""\f247""; }
-.bi-cast::before { content: ""\f248""; }
-.bi-chat-dots-fill::before { content: ""\f249""; }
-.bi-chat-dots::before { content: ""\f24a""; }
-.bi-chat-fill::before { content: ""\f24b""; }
-.bi-chat-left-dots-fill::before { content: ""\f24c""; }
-.bi-chat-left-dots::before { content: ""\f24d""; }
-.bi-chat-left-fill::before { content: ""\f24e""; }
-.bi-chat-left-quote-fill::before { content: ""\f24f""; }
-.bi-chat-left-quote::before { content: ""\f250""; }
-.bi-chat-left-text-fill::before { content: ""\f251""; }
-.bi-chat-left-text::before { content: ""\f252""; }
-.bi-chat-left::before { content: ""\f253""; }
-.bi-chat-quote-fill::before { content: ""\f254""; }
-.bi-chat-quote::before { content: ""\f255""; }
-.bi-chat-right-dots-fill::before { content: ""\f256""; }
-.bi-chat-right-dots::before { content: ""\f257""; }
-.bi-chat-right-fill::before { content: ""\f258""; }
-.bi-chat-right-quote-fill::before { content: ""\f259""; }
-.bi-chat-right-quote::before { content: ""\f25a""; }
-.bi-chat-right-text-fill::before { content: ""\f25b""; }
-.bi-chat-right-text::before { content: ""\f25c""; }
-.bi-chat-right::before { content: ""\f25d""; }
-.bi-chat-square-dots-fill::before { content: ""\f25e""; }
-.bi-chat-square-dots::before { content: ""\f25f""; }
-.bi-chat-square-fill::before { content: ""\f260""; }
-.bi-chat-square-quote-fill::before { content: ""\f261""; }
-.bi-chat-square-quote::before { content: ""\f262""; }
-.bi-chat-square-text-fill::before { content: ""\f263""; }
-.bi-chat-square-text::before { content: ""\f264""; }
-.bi-chat-square::before { content: ""\f265""; }
-.bi-chat-text-fill::before { content: ""\f266""; }
-.bi-chat-text::before { content: ""\f267""; }
-.bi-chat::before { content: ""\f268""; }
-.bi-check-all::before { content: ""\f269""; }
-.bi-check-circle-fill::before { content: ""\f26a""; }
-.bi-check-circle::before { content: ""\f26b""; }
-.bi-check-square-fill::before { content: ""\f26c""; }
-.bi-check-square::before { content: ""\f26d""; }
-.bi-check::before { content: ""\f26e""; }
-.bi-check2-all::before { content: ""\f26f""; }
-.bi-check2-circle::before { content: ""\f270""; }
-.bi-check2-square::before { content: ""\f271""; }
-.bi-check2::before { content: ""\f272""; }
-.bi-chevron-bar-contract::before { content: ""\f273""; }
-.bi-chevron-bar-down::before { content: ""\f274""; }
-.bi-chevron-bar-expand::before { content: ""\f275""; }
-.bi-chevron-bar-left::before { content: ""\f276""; }
-.bi-chevron-bar-right::before { content: ""\f277""; }
-.bi-chevron-bar-up::before { content: ""\f278""; }
-.bi-chevron-compact-down::before { content: ""\f279""; }
-.bi-chevron-compact-left::before { content: ""\f27a""; }
-.bi-chevron-compact-right::before { content: ""\f27b""; }
-.bi-chevron-compact-up::before { content: ""\f27c""; }
-.bi-chevron-contract::before { content: ""\f27d""; }
-.bi-chevron-double-down::before { content: ""\f27e""; }
-.bi-chevron-double-left::before { content: ""\f27f""; }
-.bi-chevron-double-right::before { content: ""\f280""; }
-.bi-chevron-double-up::before { content: ""\f281""; }
-.bi-chevron-down::before { content: ""\f282""; }
-.bi-chevron-expand::before { content: ""\f283""; }
-.bi-chevron-left::before { content: ""\f284""; }
-.bi-chevron-right::before { content: ""\f285""; }
-.bi-chevron-up::before { content: ""\f286""; }
-.bi-circle-fill::before { content: ""\f287""; }
-.bi-circle-half::before { content: ""\f288""; }
-.bi-circle-square::before { content: ""\f289""; }
-.bi-circle::before { content: ""\f28a""; }
-.bi-clipboard-check::before { content: ""\f28b""; }
-.bi-clipboard-data::before { content: ""\f28c""; }
-.bi-clipboard-minus::before { content: ""\f28d""; }
-.bi-clipboard-plus::before { content: ""\f28e""; }
-.bi-clipboard-x::before { content: ""\f28f""; }
-.bi-clipboard::before { content: ""\f290""; }
-.bi-clock-fill::before { content: ""\f291""; }
-.bi-clock-history::before { content: ""\f292""; }
-.bi-clock::before { content: ""\f293""; }
-.bi-cloud-arrow-down-fill::before { content: ""\f294""; }
-.bi-cloud-arrow-down::before { content: ""\f295""; }
-.bi-cloud-arrow-up-fill::before { content: ""\f296""; }
-.bi-cloud-arrow-up::before { content: ""\f297""; }
-.bi-cloud-check-fill::before { content: ""\f298""; }
-.bi-cloud-check::before { content: ""\f299""; }
-.bi-cloud-download-fill::before { content: ""\f29a""; }
-.bi-cloud-download::before { content: ""\f29b""; }
-.bi-cloud-drizzle-fill::before { content: ""\f29c""; }
-.bi-cloud-drizzle::before { content: ""\f29d""; }
-.bi-cloud-fill::before { content: ""\f29e""; }
-.bi-cloud-fog-fill::before { content: ""\f29f""; }
-.bi-cloud-fog::before { content: ""\f2a0""; }
-.bi-cloud-fog2-fill::before { content: ""\f2a1""; }
-.bi-cloud-fog2::before { content: ""\f2a2""; }
-.bi-cloud-hail-fill::before { content: ""\f2a3""; }
-.bi-cloud-hail::before { content: ""\f2a4""; }
-.bi-cloud-haze-1::before { content: ""\f2a5""; }
-.bi-cloud-haze-fill::before { content: ""\f2a6""; }
-.bi-cloud-haze::before { content: ""\f2a7""; }
-.bi-cloud-haze2-fill::before { content: ""\f2a8""; }
-.bi-cloud-lightning-fill::before { content: ""\f2a9""; }
-.bi-cloud-lightning-rain-fill::before { content: ""\f2aa""; }
-.bi-cloud-lightning-rain::before { content: ""\f2ab""; }
-.bi-cloud-lightning::before { content: ""\f2ac""; }
-.bi-cloud-minus-fill::before { content: ""\f2ad""; }
-.bi-cloud-minus::before { content: ""\f2ae""; }
-.bi-cloud-moon-fill::before { content: ""\f2af""; }
-.bi-cloud-moon::before { content: ""\f2b0""; }
-.bi-cloud-plus-fill::before { content: ""\f2b1""; }
-.bi-cloud-plus::before { content: ""\f2b2""; }
-.bi-cloud-rain-fill::before { content: ""\f2b3""; }
-.bi-cloud-rain-heavy-fill::before { content: ""\f2b4""; }
-.bi-cloud-rain-heavy::before { content: ""\f2b5""; }
-.bi-cloud-rain::before { content: ""\f2b6""; }
-.bi-cloud-slash-fill::before { content: ""\f2b7""; }
-.bi-cloud-slash::before { content: ""\f2b8""; }
-.bi-cloud-sleet-fill::before { content: ""\f2b9""; }
-.bi-cloud-sleet::before { content: ""\f2ba""; }
-.bi-cloud-snow-fill::before { content: ""\f2bb""; }
-.bi-cloud-snow::before { content: ""\f2bc""; }
-.bi-cloud-sun-fill::before { content: ""\f2bd""; }
-.bi-cloud-sun::before { content: ""\f2be""; }
-.bi-cloud-upload-fill::before { content: ""\f2bf""; }
-.bi-cloud-upload::before { content: ""\f2c0""; }
-.bi-cloud::before { content: ""\f2c1""; }
-.bi-clouds-fill::before { content: ""\f2c2""; }
-.bi-clouds::before { content: ""\f2c3""; }
-.bi-cloudy-fill::before { content: ""\f2c4""; }
-.bi-cloudy::before { content: ""\f2c5""; }
-.bi-code-slash::before { content: ""\f2c6""; }
-.bi-code-square::before { content: ""\f2c7""; }
-.bi-code::before { content: ""\f2c8""; }
-.bi-collection-fill::before { content: ""\f2c9""; }
-.bi-collection-play-fill::before { content: ""\f2ca""; }
-.bi-collection-play::before { content: ""\f2cb""; }
-.bi-collection::before { content: ""\f2cc""; }
-.bi-columns-gap::before { content: ""\f2cd""; }
-.bi-columns::before { content: ""\f2ce""; }
-.bi-command::before { content: ""\f2cf""; }
-.bi-compass-fill::before { content: ""\f2d0""; }
-.bi-compass::before { content: ""\f2d1""; }
-.bi-cone-striped::before { content: ""\f2d2""; }
-.bi-cone::before { content: ""\f2d3""; }
-.bi-controller::before { content: ""\f2d4""; }
-.bi-cpu-fill::before { content: ""\f2d5""; }
-.bi-cpu::before { content: ""\f2d6""; }
-.bi-credit-card-2-back-fill::before { content: ""\f2d7""; }
-.bi-credit-card-2-back::before { content: ""\f2d8""; }
-.bi-credit-card-2-front-fill::before { content: ""\f2d9""; }
-.bi-credit-card-2-front::before { content: ""\f2da""; }
-.bi-credit-card-fill::before { content: ""\f2db""; }
-.bi-credit-card::before { content: ""\f2dc""; }
-.bi-crop::before { content: ""\f2dd""; }
-.bi-cup-fill::before { content: ""\f2de""; }
-.bi-cup-straw::before { content: ""\f2df""; }
-.bi-cup::before { content: ""\f2e0""; }
-.bi-cursor-fill::before { content: ""\f2e1""; }
-.bi-cursor-text::before { content: ""\f2e2""; }
-.bi-cursor::before { content: ""\f2e3""; }
-.bi-dash-circle-dotted::before { content: ""\f2e4""; }
-.bi-dash-circle-fill::before { content: ""\f2e5""; }
-.bi-dash-circle::before { content: ""\f2e6""; }
-.bi-dash-square-dotted::before { content: ""\f2e7""; }
-.bi-dash-square-fill::before { content: ""\f2e8""; }
-.bi-dash-square::before { content: ""\f2e9""; }
-.bi-dash::before { content: ""\f2ea""; }
-.bi-diagram-2-fill::before { content: ""\f2eb""; }
-.bi-diagram-2::before { content: ""\f2ec""; }
-.bi-diagram-3-fill::before { content: ""\f2ed""; }
-.bi-diagram-3::before { content: ""\f2ee""; }
-.bi-diamond-fill::before { content: ""\f2ef""; }
-.bi-diamond-half::before { content: ""\f2f0""; }
-.bi-diamond::before { content: ""\f2f1""; }
-.bi-dice-1-fill::before { content: ""\f2f2""; }
-.bi-dice-1::before { content: ""\f2f3""; }
-.bi-dice-2-fill::before { content: ""\f2f4""; }
-.bi-dice-2::before { content: ""\f2f5""; }
-.bi-dice-3-fill::before { content: ""\f2f6""; }
-.bi-dice-3::before { content: ""\f2f7""; }
-.bi-dice-4-fill::before { content: ""\f2f8""; }
-.bi-dice-4::before { content: ""\f2f9""; }
-.bi-dice-5-fill::before { content: ""\f2fa""; }
-.bi-dice-5::before { content: ""\f2fb""; }
-.bi-dice-6-fill::before { content: ""\f2fc""; }
-.bi-dice-6::before { content: ""\f2fd""; }
-.bi-disc-fill::before { content: ""\f2fe""; }
-.bi-disc::before { content: ""\f2ff""; }
-.bi-discord::before { content: ""\f300""; }
-.bi-display-fill::before { content: ""\f301""; }
-.bi-display::before { content: ""\f302""; }
-.bi-distribute-horizontal::before { content: ""\f303""; }
-.bi-distribute-vertical::before { content: ""\f304""; }
-.bi-door-closed-fill::before { content: ""\f305""; }
-.bi-door-closed::before { content: ""\f306""; }
-.bi-door-open-fill::before { content: ""\f307""; }
-.bi-door-open::before { content: ""\f308""; }
-.bi-dot::before { content: ""\f309""; }
-.bi-download::before { content: ""\f30a""; }
-.bi-droplet-fill::before { content: ""\f30b""; }
-.bi-droplet-half::before { content: ""\f30c""; }
-.bi-droplet::before { content: ""\f30d""; }
-.bi-earbuds::before { content: ""\f30e""; }
-.bi-easel-fill::before { content: ""\f30f""; }
-.bi-easel::before { content: ""\f310""; }
-.bi-egg-fill::before { content: ""\f311""; }
-.bi-egg-fried::before { content: ""\f312""; }
-.bi-egg::before { content: ""\f313""; }
-.bi-eject-fill::before { content: ""\f314""; }
-.bi-eject::before { content: ""\f315""; }
-.bi-emoji-angry-fill::before { content: ""\f316""; }
-.bi-emoji-angry::before { content: ""\f317""; }
-.bi-emoji-dizzy-fill::before { content: ""\f318""; }
-.bi-emoji-dizzy::before { content: ""\f319""; }
-.bi-emoji-expressionless-fill::before { content: ""\f31a""; }
-.bi-emoji-expressionless::before { content: ""\f31b""; }
-.bi-emoji-frown-fill::before { content: ""\f31c""; }
-.bi-emoji-frown::before { content: ""\f31d""; }
-.bi-emoji-heart-eyes-fill::before { content: ""\f31e""; }
-.bi-emoji-heart-eyes::before { content: ""\f31f""; }
-.bi-emoji-laughing-fill::before { content: ""\f320""; }
-.bi-emoji-laughing::before { content: ""\f321""; }
-.bi-emoji-neutral-fill::before { content: ""\f322""; }
-.bi-emoji-neutral::before { content: ""\f323""; }
-.bi-emoji-smile-fill::before { content: ""\f324""; }
-.bi-emoji-smile-upside-down-fill::before { content: ""\f325""; }
-.bi-emoji-smile-upside-down::before { content: ""\f326""; }
-.bi-emoji-smile::before { content: ""\f327""; }
-.bi-emoji-sunglasses-fill::before { content: ""\f328""; }
-.bi-emoji-sunglasses::before { content: ""\f329""; }
-.bi-emoji-wink-fill::before { content: ""\f32a""; }
-.bi-emoji-wink::before { content: ""\f32b""; }
-.bi-envelope-fill::before { content: ""\f32c""; }
-.bi-envelope-open-fill::before { content: ""\f32d""; }
-.bi-envelope-open::before { content: ""\f32e""; }
-.bi-envelope::before { content: ""\f32f""; }
-.bi-eraser-fill::before { content: ""\f330""; }
-.bi-eraser::before { content: ""\f331""; }
-.bi-exclamation-circle-fill::before { content: ""\f332""; }
-.bi-exclamation-circle::before { content: ""\f333""; }
-.bi-exclamation-diamond-fill::before { content: ""\f334""; }
-.bi-exclamation-diamond::before { content: ""\f335""; }
-.bi-exclamation-octagon-fill::before { content: ""\f336""; }
-.bi-exclamation-octagon::before { content: ""\f337""; }
-.bi-exclamation-square-fill::before { content: ""\f338""; }
-.bi-exclamation-square::before { content: ""\f339""; }
-.bi-exclamation-triangle-fill::before { content: ""\f33a""; }
-.bi-exclamation-triangle::before { content: ""\f33b""; }
-.bi-exclamation::before { content: ""\f33c""; }
-.bi-exclude::before { content: ""\f33d""; }
-.bi-eye-fill::before { content: ""\f33e""; }
-.bi-eye-slash-fill::before { content: ""\f33f""; }
-.bi-eye-slash::before { content: ""\f340""; }
-.bi-eye::before { content: ""\f341""; }
-.bi-eyedropper::before { content: ""\f342""; }
-.bi-eyeglasses::before { content: ""\f343""; }
-.bi-facebook::before { content: ""\f344""; }
-.bi-file-arrow-down-fill::before { content: ""\f345""; }
-.bi-file-arrow-down::before { content: ""\f346""; }
-.bi-file-arrow-up-fill::before { content: ""\f347""; }
-.bi-file-arrow-up::before { content: ""\f348""; }
-.bi-file-bar-graph-fill::before { content: ""\f349""; }
-.bi-file-bar-graph::before { content: ""\f34a""; }
-.bi-file-binary-fill::before { content: ""\f34b""; }
-.bi-file-binary::before { content: ""\f34c""; }
-.bi-file-break-fill::before { content: ""\f34d""; }
-.bi-file-break::before { content: ""\f34e""; }
-.bi-file-check-fill::before { content: ""\f34f""; }
-.bi-file-check::before { content: ""\f350""; }
-.bi-file-code-fill::before { content: ""\f351""; }
-.bi-file-code::before { content: ""\f352""; }
-.bi-file-diff-fill::before { content: ""\f353""; }
-.bi-file-diff::before { content: ""\f354""; }
-.bi-file-earmark-arrow-down-fill::before { content: ""\f355""; }
-.bi-file-earmark-arrow-down::before { content: ""\f356""; }
-.bi-file-earmark-arrow-up-fill::before { content: ""\f357""; }
-.bi-file-earmark-arrow-up::before { content: ""\f358""; }
-.bi-file-earmark-bar-graph-fill::before { content: ""\f359""; }
-.bi-file-earmark-bar-graph::before { content: ""\f35a""; }
-.bi-file-earmark-binary-fill::before { content: ""\f35b""; }
-.bi-file-earmark-binary::before { content: ""\f35c""; }
-.bi-file-earmark-break-fill::before { content: ""\f35d""; }
-.bi-file-earmark-break::before { content: ""\f35e""; }
-.bi-file-earmark-check-fill::before { content: ""\f35f""; }
-.bi-file-earmark-check::before { content: ""\f360""; }
-.bi-file-earmark-code-fill::before { content: ""\f361""; }
-.bi-file-earmark-code::before { content: ""\f362""; }
-.bi-file-earmark-diff-fill::before { content: ""\f363""; }
-.bi-file-earmark-diff::before { content: ""\f364""; }
-.bi-file-earmark-easel-fill::before { content: ""\f365""; }
-.bi-file-earmark-easel::before { content: ""\f366""; }
-.bi-file-earmark-excel-fill::before { content: ""\f367""; }
-.bi-file-earmark-excel::before { content: ""\f368""; }
-.bi-file-earmark-fill::before { content: ""\f369""; }
-.bi-file-earmark-font-fill::before { content: ""\f36a""; }
-.bi-file-earmark-font::before { content: ""\f36b""; }
-.bi-file-earmark-image-fill::before { content: ""\f36c""; }
-.bi-file-earmark-image::before { content: ""\f36d""; }
-.bi-file-earmark-lock-fill::before { content: ""\f36e""; }
-.bi-file-earmark-lock::before { content: ""\f36f""; }
-.bi-file-earmark-lock2-fill::before { content: ""\f370""; }
-.bi-file-earmark-lock2::before { content: ""\f371""; }
-.bi-file-earmark-medical-fill::before { content: ""\f372""; }
-.bi-file-earmark-medical::before { content: ""\f373""; }
-.bi-file-earmark-minus-fill::before { content: ""\f374""; }
-.bi-file-earmark-minus::before { content: ""\f375""; }
-.bi-file-earmark-music-fill::before { content: ""\f376""; }
-.bi-file-earmark-music::before { content: ""\f377""; }
-.bi-file-earmark-person-fill::before { content: ""\f378""; }
-.bi-file-earmark-person::before { content: ""\f379""; }
-.bi-file-earmark-play-fill::before { content: ""\f37a""; }
-.bi-file-earmark-play::before { content: ""\f37b""; }
-.bi-file-earmark-plus-fill::before { content: ""\f37c""; }
-.bi-file-earmark-plus::before { content: ""\f37d""; }
-.bi-file-earmark-post-fill::before { content: ""\f37e""; }
-.bi-file-earmark-post::before { content: ""\f37f""; }
-.bi-file-earmark-ppt-fill::before { content: ""\f380""; }
-.bi-file-earmark-ppt::before { content: ""\f381""; }
-.bi-file-earmark-richtext-fill::before { content: ""\f382""; }
-.bi-file-earmark-richtext::before { content: ""\f383""; }
-.bi-file-earmark-ruled-fill::before { content: ""\f384""; }
-.bi-file-earmark-ruled::before { content: ""\f385""; }
-.bi-file-earmark-slides-fill::before { content: ""\f386""; }
-.bi-file-earmark-slides::before { content: ""\f387""; }
-.bi-file-earmark-spreadsheet-fill::before { content: ""\f388""; }
-.bi-file-earmark-spreadsheet::before { content: ""\f389""; }
-.bi-file-earmark-text-fill::before { content: ""\f38a""; }
-.bi-file-earmark-text::before { content: ""\f38b""; }
-.bi-file-earmark-word-fill::before { content: ""\f38c""; }
-.bi-file-earmark-word::before { content: ""\f38d""; }
-.bi-file-earmark-x-fill::before { content: ""\f38e""; }
-.bi-file-earmark-x::before { content: ""\f38f""; }
-.bi-file-earmark-zip-fill::before { content: ""\f390""; }
-.bi-file-earmark-zip::before { content: ""\f391""; }
-.bi-file-earmark::before { content: ""\f392""; }
-.bi-file-easel-fill::before { content: ""\f393""; }
-.bi-file-easel::before { content: ""\f394""; }
-.bi-file-excel-fill::before { content: ""\f395""; }
-.bi-file-excel::before { content: ""\f396""; }
-.bi-file-fill::before { content: ""\f397""; }
-.bi-file-font-fill::before { content: ""\f398""; }
-.bi-file-font::before { content: ""\f399""; }
-.bi-file-image-fill::before { content: ""\f39a""; }
-.bi-file-image::before { content: ""\f39b""; }
-.bi-file-lock-fill::before { content: ""\f39c""; }
-.bi-file-lock::before { content: ""\f39d""; }
-.bi-file-lock2-fill::before { content: ""\f39e""; }
-.bi-file-lock2::before { content: ""\f39f""; }
-.bi-file-medical-fill::before { content: ""\f3a0""; }
-.bi-file-medical::before { content: ""\f3a1""; }
-.bi-file-minus-fill::before { content: ""\f3a2""; }
-.bi-file-minus::before { content: ""\f3a3""; }
-.bi-file-music-fill::before { content: ""\f3a4""; }
-.bi-file-music::before { content: ""\f3a5""; }
-.bi-file-person-fill::before { content: ""\f3a6""; }
-.bi-file-person::before { content: ""\f3a7""; }
-.bi-file-play-fill::before { content: ""\f3a8""; }
-.bi-file-play::before { content: ""\f3a9""; }
-.bi-file-plus-fill::before { content: ""\f3aa""; }
-.bi-file-plus::before { content: ""\f3ab""; }
-.bi-file-post-fill::before { content: ""\f3ac""; }
-.bi-file-post::before { content: ""\f3ad""; }
-.bi-file-ppt-fill::before { content: ""\f3ae""; }
-.bi-file-ppt::before { content: ""\f3af""; }
-.bi-file-richtext-fill::before { content: ""\f3b0""; }
-.bi-file-richtext::before { content: ""\f3b1""; }
-.bi-file-ruled-fill::before { content: ""\f3b2""; }
-.bi-file-ruled::before { content: ""\f3b3""; }
-.bi-file-slides-fill::before { content: ""\f3b4""; }
-.bi-file-slides::before { content: ""\f3b5""; }
-.bi-file-spreadsheet-fill::before { content: ""\f3b6""; }
-.bi-file-spreadsheet::before { content: ""\f3b7""; }
-.bi-file-text-fill::before { content: ""\f3b8""; }
-.bi-file-text::before { content: ""\f3b9""; }
-.bi-file-word-fill::before { content: ""\f3ba""; }
-.bi-file-word::before { content: ""\f3bb""; }
-.bi-file-x-fill::before { content: ""\f3bc""; }
-.bi-file-x::before { content: ""\f3bd""; }
-.bi-file-zip-fill::before { content: ""\f3be""; }
-.bi-file-zip::before { content: ""\f3bf""; }
-.bi-file::before { content: ""\f3c0""; }
-.bi-files-alt::before { content: ""\f3c1""; }
-.bi-files::before { content: ""\f3c2""; }
-.bi-film::before { content: ""\f3c3""; }
-.bi-filter-circle-fill::before { content: ""\f3c4""; }
-.bi-filter-circle::before { content: ""\f3c5""; }
-.bi-filter-left::before { content: ""\f3c6""; }
-.bi-filter-right::before { content: ""\f3c7""; }
-.bi-filter-square-fill::before { content: ""\f3c8""; }
-.bi-filter-square::before { content: ""\f3c9""; }
-.bi-filter::before { content: ""\f3ca""; }
-.bi-flag-fill::before { content: ""\f3cb""; }
-.bi-flag::before { content: ""\f3cc""; }
-.bi-flower1::before { content: ""\f3cd""; }
-.bi-flower2::before { content: ""\f3ce""; }
-.bi-flower3::before { content: ""\f3cf""; }
-.bi-folder-check::before { content: ""\f3d0""; }
-.bi-folder-fill::before { content: ""\f3d1""; }
-.bi-folder-minus::before { content: ""\f3d2""; }
-.bi-folder-plus::before { content: ""\f3d3""; }
-.bi-folder-symlink-fill::before { content: ""\f3d4""; }
-.bi-folder-symlink::before { content: ""\f3d5""; }
-.bi-folder-x::before { content: ""\f3d6""; }
-.bi-folder::before { content: ""\f3d7""; }
-.bi-folder2-open::before { content: ""\f3d8""; }
-.bi-folder2::before { content: ""\f3d9""; }
-.bi-fonts::before { content: ""\f3da""; }
-.bi-forward-fill::before { content: ""\f3db""; }
-.bi-forward::before { content: ""\f3dc""; }
-.bi-front::before { content: ""\f3dd""; }
-.bi-fullscreen-exit::before { content: ""\f3de""; }
-.bi-fullscreen::before { content: ""\f3df""; }
-.bi-funnel-fill::before { content: ""\f3e0""; }
-.bi-funnel::before { content: ""\f3e1""; }
-.bi-gear-fill::before { content: ""\f3e2""; }
-.bi-gear-wide-connected::before { content: ""\f3e3""; }
-.bi-gear-wide::before { content: ""\f3e4""; }
-.bi-gear::before { content: ""\f3e5""; }
-.bi-gem::before { content: ""\f3e6""; }
-.bi-geo-alt-fill::before { content: ""\f3e7""; }
-.bi-geo-alt::before { content: ""\f3e8""; }
-.bi-geo-fill::before { content: ""\f3e9""; }
-.bi-geo::before { content: ""\f3ea""; }
-.bi-gift-fill::before { content: ""\f3eb""; }
-.bi-gift::before { content: ""\f3ec""; }
-.bi-github::before { content: ""\f3ed""; }
-.bi-globe::before { content: ""\f3ee""; }
-.bi-globe2::before { content: ""\f3ef""; }
-.bi-google::before { content: ""\f3f0""; }
-.bi-graph-down::before { content: ""\f3f1""; }
-.bi-graph-up::before { content: ""\f3f2""; }
-.bi-grid-1x2-fill::before { content: ""\f3f3""; }
-.bi-grid-1x2::before { content: ""\f3f4""; }
-.bi-grid-3x2-gap-fill::before { content: ""\f3f5""; }
-.bi-grid-3x2-gap::before { content: ""\f3f6""; }
-.bi-grid-3x2::before { content: ""\f3f7""; }
-.bi-grid-3x3-gap-fill::before { content: ""\f3f8""; }
-.bi-grid-3x3-gap::before { content: ""\f3f9""; }
-.bi-grid-3x3::before { content: ""\f3fa""; }
-.bi-grid-fill::before { content: ""\f3fb""; }
-.bi-grid::before { content: ""\f3fc""; }
-.bi-grip-horizontal::before { content: ""\f3fd""; }
-.bi-grip-vertical::before { content: ""\f3fe""; }
-.bi-hammer::before { content: ""\f3ff""; }
-.bi-hand-index-fill::before { content: ""\f400""; }
-.bi-hand-index-thumb-fill::before { content: ""\f401""; }
-.bi-hand-index-thumb::before { content: ""\f402""; }
-.bi-hand-index::before { content: ""\f403""; }
-.bi-hand-thumbs-down-fill::before { content: ""\f404""; }
-.bi-hand-thumbs-down::before { content: ""\f405""; }
-.bi-hand-thumbs-up-fill::before { content: ""\f406""; }
-.bi-hand-thumbs-up::before { content: ""\f407""; }
-.bi-handbag-fill::before { content: ""\f408""; }
-.bi-handbag::before { content: ""\f409""; }
-.bi-hash::before { content: ""\f40a""; }
-.bi-hdd-fill::before { content: ""\f40b""; }
-.bi-hdd-network-fill::before { content: ""\f40c""; }
-.bi-hdd-network::before { content: ""\f40d""; }
-.bi-hdd-rack-fill::before { content: ""\f40e""; }
-.bi-hdd-rack::before { content: ""\f40f""; }
-.bi-hdd-stack-fill::before { content: ""\f410""; }
-.bi-hdd-stack::before { content: ""\f411""; }
-.bi-hdd::before { content: ""\f412""; }
-.bi-headphones::before { content: ""\f413""; }
-.bi-headset::before { content: ""\f414""; }
-.bi-heart-fill::before { content: ""\f415""; }
-.bi-heart-half::before { content: ""\f416""; }
-.bi-heart::before { content: ""\f417""; }
-.bi-heptagon-fill::before { content: ""\f418""; }
-.bi-heptagon-half::before { content: ""\f419""; }
-.bi-heptagon::before { content: ""\f41a""; }
-.bi-hexagon-fill::before { content: ""\f41b""; }
-.bi-hexagon-half::before { content: ""\f41c""; }
-.bi-hexagon::before { content: ""\f41d""; }
-.bi-hourglass-bottom::before { content: ""\f41e""; }
-.bi-hourglass-split::before { content: ""\f41f""; }
-.bi-hourglass-top::before { content: ""\f420""; }
-.bi-hourglass::before { content: ""\f421""; }
-.bi-house-door-fill::before { content: ""\f422""; }
-.bi-house-door::before { content: ""\f423""; }
-.bi-house-fill::before { content: ""\f424""; }
-.bi-house::before { content: ""\f425""; }
-.bi-hr::before { content: ""\f426""; }
-.bi-hurricane::before { content: ""\f427""; }
-.bi-image-alt::before { content: ""\f428""; }
-.bi-image-fill::before { content: ""\f429""; }
-.bi-image::before { content: ""\f42a""; }
-.bi-images::before { content: ""\f42b""; }
-.bi-inbox-fill::before { content: ""\f42c""; }
-.bi-inbox::before { content: ""\f42d""; }
-.bi-inboxes-fill::before { content: ""\f42e""; }
-.bi-inboxes::before { content: ""\f42f""; }
-.bi-info-circle-fill::before { content: ""\f430""; }
-.bi-info-circle::before { content: ""\f431""; }
-.bi-info-square-fill::before { content: ""\f432""; }
-.bi-info-square::before { content: ""\f433""; }
-.bi-info::before { content: ""\f434""; }
-.bi-input-cursor-text::before { content: ""\f435""; }
-.bi-input-cursor::before { content: ""\f436""; }
-.bi-instagram::before { content: ""\f437""; }
-.bi-intersect::before { content: ""\f438""; }
-.bi-journal-album::before { content: ""\f439""; }
-.bi-journal-arrow-down::before { content: ""\f43a""; }
-.bi-journal-arrow-up::before { content: ""\f43b""; }
-.bi-journal-bookmark-fill::before { content: ""\f43c""; }
-.bi-journal-bookmark::before { content: ""\f43d""; }
-.bi-journal-check::before { content: ""\f43e""; }
-.bi-journal-code::before { content: ""\f43f""; }
-.bi-journal-medical::before { content: ""\f440""; }
-.bi-journal-minus::before { content: ""\f441""; }
-.bi-journal-plus::before { content: ""\f442""; }
-.bi-journal-richtext::before { content: ""\f443""; }
-.bi-journal-text::before { content: ""\f444""; }
-.bi-journal-x::before { content: ""\f445""; }
-.bi-journal::before { content: ""\f446""; }
-.bi-journals::before { content: ""\f447""; }
-.bi-joystick::before { content: ""\f448""; }
-.bi-justify-left::before { content: ""\f449""; }
-.bi-justify-right::before { content: ""\f44a""; }
-.bi-justify::before { content: ""\f44b""; }
-.bi-kanban-fill::before { content: ""\f44c""; }
-.bi-kanban::before { content: ""\f44d""; }
-.bi-key-fill::before { content: ""\f44e""; }
-.bi-key::before { content: ""\f44f""; }
-.bi-keyboard-fill::before { content: ""\f450""; }
-.bi-keyboard::before { content: ""\f451""; }
-.bi-ladder::before { content: ""\f452""; }
-.bi-lamp-fill::before { content: ""\f453""; }
-.bi-lamp::before { content: ""\f454""; }
-.bi-laptop-fill::before { content: ""\f455""; }
-.bi-laptop::before { content: ""\f456""; }
-.bi-layer-backward::before { content: ""\f457""; }
-.bi-layer-forward::before { content: ""\f458""; }
-.bi-layers-fill::before { content: ""\f459""; }
-.bi-layers-half::before { content: ""\f45a""; }
-.bi-layers::before { content: ""\f45b""; }
-.bi-layout-sidebar-inset-reverse::before { content: ""\f45c""; }
-.bi-layout-sidebar-inset::before { content: ""\f45d""; }
-.bi-layout-sidebar-reverse::before { content: ""\f45e""; }
-.bi-layout-sidebar::before { content: ""\f45f""; }
-.bi-layout-split::before { content: ""\f460""; }
-.bi-layout-text-sidebar-reverse::before { content: ""\f461""; }
-.bi-layout-text-sidebar::before { content: ""\f462""; }
-.bi-layout-text-window-reverse::before { content: ""\f463""; }
-.bi-layout-text-window::before { content: ""\f464""; }
-.bi-layout-three-columns::before { content: ""\f465""; }
-.bi-layout-wtf::before { content: ""\f466""; }
-.bi-life-preserver::before { content: ""\f467""; }
-.bi-lightbulb-fill::before { content: ""\f468""; }
-.bi-lightbulb-off-fill::before { content: ""\f469""; }
-.bi-lightbulb-off::before { content: ""\f46a""; }
-.bi-lightbulb::before { content: ""\f46b""; }
-.bi-lightning-charge-fill::before { content: ""\f46c""; }
-.bi-lightning-charge::before { content: ""\f46d""; }
-.bi-lightning-fill::before { content: ""\f46e""; }
-.bi-lightning::before { content: ""\f46f""; }
-.bi-link-45deg::before { content: ""\f470""; }
-.bi-link::before { content: ""\f471""; }
-.bi-linkedin::before { content: ""\f472""; }
-.bi-list-check::before { content: ""\f473""; }
-.bi-list-nested::before { content: ""\f474""; }
-.bi-list-ol::before { content: ""\f475""; }
-.bi-list-stars::before { content: ""\f476""; }
-.bi-list-task::before { content: ""\f477""; }
-.bi-list-ul::before { content: ""\f478""; }
-.bi-list::before { content: ""\f479""; }
-.bi-lock-fill::before { content: ""\f47a""; }
-.bi-lock::before { content: ""\f47b""; }
-.bi-mailbox::before { content: ""\f47c""; }
-.bi-mailbox2::before { content: ""\f47d""; }
-.bi-map-fill::before { content: ""\f47e""; }
-.bi-map::before { content: ""\f47f""; }
-.bi-markdown-fill::before { content: ""\f480""; }
-.bi-markdown::before { content: ""\f481""; }
-.bi-mask::before { content: ""\f482""; }
-.bi-megaphone-fill::before { content: ""\f483""; }
-.bi-megaphone::before { content: ""\f484""; }
-.bi-menu-app-fill::before { content: ""\f485""; }
-.bi-menu-app::before { content: ""\f486""; }
-.bi-menu-button-fill::before { content: ""\f487""; }
-.bi-menu-button-wide-fill::before { content: ""\f488""; }
-.bi-menu-button-wide::before { content: ""\f489""; }
-.bi-menu-button::before { content: ""\f48a""; }
-.bi-menu-down::before { content: ""\f48b""; }
-.bi-menu-up::before { content: ""\f48c""; }
-.bi-mic-fill::before { content: ""\f48d""; }
-.bi-mic-mute-fill::before { content: ""\f48e""; }
-.bi-mic-mute::before { content: ""\f48f""; }
-.bi-mic::before { content: ""\f490""; }
-.bi-minecart-loaded::before { content: ""\f491""; }
-.bi-minecart::before { content: ""\f492""; }
-.bi-moisture::before { content: ""\f493""; }
-.bi-moon-fill::before { content: ""\f494""; }
-.bi-moon-stars-fill::before { content: ""\f495""; }
-.bi-moon-stars::before { content: ""\f496""; }
-.bi-moon::before { content: ""\f497""; }
-.bi-mouse-fill::before { content: ""\f498""; }
-.bi-mouse::before { content: ""\f499""; }
-.bi-mouse2-fill::before { content: ""\f49a""; }
-.bi-mouse2::before { content: ""\f49b""; }
-.bi-mouse3-fill::before { content: ""\f49c""; }
-.bi-mouse3::before { content: ""\f49d""; }
-.bi-music-note-beamed::before { content: ""\f49e""; }
-.bi-music-note-list::before { content: ""\f49f""; }
-.bi-music-note::before { content: ""\f4a0""; }
-.bi-music-player-fill::before { content: ""\f4a1""; }
-.bi-music-player::before { content: ""\f4a2""; }
-.bi-newspaper::before { content: ""\f4a3""; }
-.bi-node-minus-fill::before { content: ""\f4a4""; }
-.bi-node-minus::before { content: ""\f4a5""; }
-.bi-node-plus-fill::before { content: ""\f4a6""; }
-.bi-node-plus::before { content: ""\f4a7""; }
-.bi-nut-fill::before { content: ""\f4a8""; }
-.bi-nut::before { content: ""\f4a9""; }
-.bi-octagon-fill::before { content: ""\f4aa""; }
-.bi-octagon-half::before { content: ""\f4ab""; }
-.bi-octagon::before { content: ""\f4ac""; }
-.bi-option::before { content: ""\f4ad""; }
-.bi-outlet::before { content: ""\f4ae""; }
-.bi-paint-bucket::before { content: ""\f4af""; }
-.bi-palette-fill::before { content: ""\f4b0""; }
-.bi-palette::before { content: ""\f4b1""; }
-.bi-palette2::before { content: ""\f4b2""; }
-.bi-paperclip::before { content: ""\f4b3""; }
-.bi-paragraph::before { content: ""\f4b4""; }
-.bi-patch-check-fill::before { content: ""\f4b5""; }
-.bi-patch-check::before { content: ""\f4b6""; }
-.bi-patch-exclamation-fill::before { content: ""\f4b7""; }
-.bi-patch-exclamation::before { content: ""\f4b8""; }
-.bi-patch-minus-fill::before { content: ""\f4b9""; }
-.bi-patch-minus::before { content: ""\f4ba""; }
-.bi-patch-plus-fill::before { content: ""\f4bb""; }
-.bi-patch-plus::before { content: ""\f4bc""; }
-.bi-patch-question-fill::before { content: ""\f4bd""; }
-.bi-patch-question::before { content: ""\f4be""; }
-.bi-pause-btn-fill::before { content: ""\f4bf""; }
-.bi-pause-btn::before { content: ""\f4c0""; }
-.bi-pause-circle-fill::before { content: ""\f4c1""; }
-.bi-pause-circle::before { content: ""\f4c2""; }
-.bi-pause-fill::before { content: ""\f4c3""; }
-.bi-pause::before { content: ""\f4c4""; }
-.bi-peace-fill::before { content: ""\f4c5""; }
-.bi-peace::before { content: ""\f4c6""; }
-.bi-pen-fill::before { content: ""\f4c7""; }
-.bi-pen::before { content: ""\f4c8""; }
-.bi-pencil-fill::before { content: ""\f4c9""; }
-.bi-pencil-square::before { content: ""\f4ca""; }
-.bi-pencil::before { content: ""\f4cb""; }
-.bi-pentagon-fill::before { content: ""\f4cc""; }
-.bi-pentagon-half::before { content: ""\f4cd""; }
-.bi-pentagon::before { content: ""\f4ce""; }
-.bi-people-fill::before { content: ""\f4cf""; }
-.bi-people::before { content: ""\f4d0""; }
-.bi-percent::before { content: ""\f4d1""; }
-.bi-person-badge-fill::before { content: ""\f4d2""; }
-.bi-person-badge::before { content: ""\f4d3""; }
-.bi-person-bounding-box::before { content: ""\f4d4""; }
-.bi-person-check-fill::before { content: ""\f4d5""; }
-.bi-person-check::before { content: ""\f4d6""; }
-.bi-person-circle::before { content: ""\f4d7""; }
-.bi-person-dash-fill::before { content: ""\f4d8""; }
-.bi-person-dash::before { content: ""\f4d9""; }
-.bi-person-fill::before { content: ""\f4da""; }
-.bi-person-lines-fill::before { content: ""\f4db""; }
-.bi-person-plus-fill::before { content: ""\f4dc""; }
-.bi-person-plus::before { content: ""\f4dd""; }
-.bi-person-square::before { content: ""\f4de""; }
-.bi-person-x-fill::before { content: ""\f4df""; }
-.bi-person-x::before { content: ""\f4e0""; }
-.bi-person::before { content: ""\f4e1""; }
-.bi-phone-fill::before { content: ""\f4e2""; }
-.bi-phone-landscape-fill::before { content: ""\f4e3""; }
-.bi-phone-landscape::before { content: ""\f4e4""; }
-.bi-phone-vibrate-fill::before { content: ""\f4e5""; }
-.bi-phone-vibrate::before { content: ""\f4e6""; }
-.bi-phone::before { content: ""\f4e7""; }
-.bi-pie-chart-fill::before { content: ""\f4e8""; }
-.bi-pie-chart::before { content: ""\f4e9""; }
-.bi-pin-angle-fill::before { content: ""\f4ea""; }
-.bi-pin-angle::before { content: ""\f4eb""; }
-.bi-pin-fill::before { content: ""\f4ec""; }
-.bi-pin::before { content: ""\f4ed""; }
-.bi-pip-fill::before { content: ""\f4ee""; }
-.bi-pip::before { content: ""\f4ef""; }
-.bi-play-btn-fill::before { content: ""\f4f0""; }
-.bi-play-btn::before { content: ""\f4f1""; }
-.bi-play-circle-fill::before { content: ""\f4f2""; }
-.bi-play-circle::before { content: ""\f4f3""; }
-.bi-play-fill::before { content: ""\f4f4""; }
-.bi-play::before { content: ""\f4f5""; }
-.bi-plug-fill::before { content: ""\f4f6""; }
-.bi-plug::before { content: ""\f4f7""; }
-.bi-plus-circle-dotted::before { content: ""\f4f8""; }
-.bi-plus-circle-fill::before { content: ""\f4f9""; }
-.bi-plus-circle::before { content: ""\f4fa""; }
-.bi-plus-square-dotted::before { content: ""\f4fb""; }
-.bi-plus-square-fill::before { content: ""\f4fc""; }
-.bi-plus-square::before { content: ""\f4fd""; }
-.bi-plus::before { content: ""\f4fe""; }
-.bi-power::before { content: ""\f4ff""; }
-.bi-printer-fill::before { content: ""\f500""; }
-.bi-printer::before { content: ""\f501""; }
-.bi-puzzle-fill::before { content: ""\f502""; }
-.bi-puzzle::before { content: ""\f503""; }
-.bi-question-circle-fill::before { content: ""\f504""; }
-.bi-question-circle::before { content: ""\f505""; }
-.bi-question-diamond-fill::before { content: ""\f506""; }
-.bi-question-diamond::before { content: ""\f507""; }
-.bi-question-octagon-fill::before { content: ""\f508""; }
-.bi-question-octagon::before { content: ""\f509""; }
-.bi-question-square-fill::before { content: ""\f50a""; }
-.bi-question-square::before { content: ""\f50b""; }
-.bi-question::before { content: ""\f50c""; }
-.bi-rainbow::before { content: ""\f50d""; }
-.bi-receipt-cutoff::before { content: ""\f50e""; }
-.bi-receipt::before { content: ""\f50f""; }
-.bi-reception-0::before { content: ""\f510""; }
-.bi-reception-1::before { content: ""\f511""; }
-.bi-reception-2::before { content: ""\f512""; }
-.bi-reception-3::before { content: ""\f513""; }
-.bi-reception-4::before { content: ""\f514""; }
-.bi-record-btn-fill::before { content: ""\f515""; }
-.bi-record-btn::before { content: ""\f516""; }
-.bi-record-circle-fill::before { content: ""\f517""; }
-.bi-record-circle::before { content: ""\f518""; }
-.bi-record-fill::before { content: ""\f519""; }
-.bi-record::before { content: ""\f51a""; }
-.bi-record2-fill::before { content: ""\f51b""; }
-.bi-record2::before { content: ""\f51c""; }
-.bi-reply-all-fill::before { content: ""\f51d""; }
-.bi-reply-all::before { content: ""\f51e""; }
-.bi-reply-fill::before { content: ""\f51f""; }
-.bi-reply::before { content: ""\f520""; }
-.bi-rss-fill::before { content: ""\f521""; }
-.bi-rss::before { content: ""\f522""; }
-.bi-rulers::before { content: ""\f523""; }
-.bi-save-fill::before { content: ""\f524""; }
-.bi-save::before { content: ""\f525""; }
-.bi-save2-fill::before { content: ""\f526""; }
-.bi-save2::before { content: ""\f527""; }
-.bi-scissors::before { content: ""\f528""; }
-.bi-screwdriver::before { content: ""\f529""; }
-.bi-search::before { content: ""\f52a""; }
-.bi-segmented-nav::before { content: ""\f52b""; }
-.bi-server::before { content: ""\f52c""; }
-.bi-share-fill::before { content: ""\f52d""; }
-.bi-share::before { content: ""\f52e""; }
-.bi-shield-check::before { content: ""\f52f""; }
-.bi-shield-exclamation::before { content: ""\f530""; }
-.bi-shield-fill-check::before { content: ""\f531""; }
-.bi-shield-fill-exclamation::before { content: ""\f532""; }
-.bi-shield-fill-minus::before { content: ""\f533""; }
-.bi-shield-fill-plus::before { content: ""\f534""; }
-.bi-shield-fill-x::before { content: ""\f535""; }
-.bi-shield-fill::before { content: ""\f536""; }
-.bi-shield-lock-fill::before { content: ""\f537""; }
-.bi-shield-lock::before { content: ""\f538""; }
-.bi-shield-minus::before { content: ""\f539""; }
-.bi-shield-plus::before { content: ""\f53a""; }
-.bi-shield-shaded::before { content: ""\f53b""; }
-.bi-shield-slash-fill::before { content: ""\f53c""; }
-.bi-shield-slash::before { content: ""\f53d""; }
-.bi-shield-x::before { content: ""\f53e""; }
-.bi-shield::before { content: ""\f53f""; }
-.bi-shift-fill::before { content: ""\f540""; }
-.bi-shift::before { content: ""\f541""; }
-.bi-shop-window::before { content: ""\f542""; }
-.bi-shop::before { content: ""\f543""; }
-.bi-shuffle::before { content: ""\f544""; }
-.bi-signpost-2-fill::before { content: ""\f545""; }
-.bi-signpost-2::before { content: ""\f546""; }
-.bi-signpost-fill::before { content: ""\f547""; }
-.bi-signpost-split-fill::before { content: ""\f548""; }
-.bi-signpost-split::before { content: ""\f549""; }
-.bi-signpost::before { content: ""\f54a""; }
-.bi-sim-fill::before { content: ""\f54b""; }
-.bi-sim::before { content: ""\f54c""; }
-.bi-skip-backward-btn-fill::before { content: ""\f54d""; }
-.bi-skip-backward-btn::before { content: ""\f54e""; }
-.bi-skip-backward-circle-fill::before { content: ""\f54f""; }
-.bi-skip-backward-circle::before { content: ""\f550""; }
-.bi-skip-backward-fill::before { content: ""\f551""; }
-.bi-skip-backward::before { content: ""\f552""; }
-.bi-skip-end-btn-fill::before { content: ""\f553""; }
-.bi-skip-end-btn::before { content: ""\f554""; }
-.bi-skip-end-circle-fill::before { content: ""\f555""; }
-.bi-skip-end-circle::before { content: ""\f556""; }
-.bi-skip-end-fill::before { content: ""\f557""; }
-.bi-skip-end::before { content: ""\f558""; }
-.bi-skip-forward-btn-fill::before { content: ""\f559""; }
-.bi-skip-forward-btn::before { content: ""\f55a""; }
-.bi-skip-forward-circle-fill::before { content: ""\f55b""; }
-.bi-skip-forward-circle::before { content: ""\f55c""; }
-.bi-skip-forward-fill::before { content: ""\f55d""; }
-.bi-skip-forward::before { content: ""\f55e""; }
-.bi-skip-start-btn-fill::before { content: ""\f55f""; }
-.bi-skip-start-btn::before { content: ""\f560""; }
-.bi-skip-start-circle-fill::before { content: ""\f561""; }
-.bi-skip-start-circle::before { content: ""\f562""; }
-.bi-skip-start-fill::before { content: ""\f563""; }
-.bi-skip-start::before { content: ""\f564""; }
-.bi-slack::before { content: ""\f565""; }
-.bi-slash-circle-fill::before { content: ""\f566""; }
-.bi-slash-circle::before { content: ""\f567""; }
-.bi-slash-square-fill::before { content: ""\f568""; }
-.bi-slash-square::before { content: ""\f569""; }
-.bi-slash::before { content: ""\f56a""; }
-.bi-sliders::before { content: ""\f56b""; }
-.bi-smartwatch::before { content: ""\f56c""; }
-.bi-snow::before { content: ""\f56d""; }
-.bi-snow2::before { content: ""\f56e""; }
-.bi-snow3::before { content: ""\f56f""; }
-.bi-sort-alpha-down-alt::before { content: ""\f570""; }
-.bi-sort-alpha-down::before { content: ""\f571""; }
-.bi-sort-alpha-up-alt::before { content: ""\f572""; }
-.bi-sort-alpha-up::before { content: ""\f573""; }
-.bi-sort-down-alt::before { content: ""\f574""; }
-.bi-sort-down::before { content: ""\f575""; }
-.bi-sort-numeric-down-alt::before { content: ""\f576""; }
-.bi-sort-numeric-down::before { content: ""\f577""; }
-.bi-sort-numeric-up-alt::before { content: ""\f578""; }
-.bi-sort-numeric-up::before { content: ""\f579""; }
-.bi-sort-up-alt::before { content: ""\f57a""; }
-.bi-sort-up::before { content: ""\f57b""; }
-.bi-soundwave::before { content: ""\f57c""; }
-.bi-speaker-fill::before { content: ""\f57d""; }
-.bi-speaker::before { content: ""\f57e""; }
-.bi-speedometer::before { content: ""\f57f""; }
-.bi-speedometer2::before { content: ""\f580""; }
-.bi-spellcheck::before { content: ""\f581""; }
-.bi-square-fill::before { content: ""\f582""; }
-.bi-square-half::before { content: ""\f583""; }
-.bi-square::before { content: ""\f584""; }
-.bi-stack::before { content: ""\f585""; }
-.bi-star-fill::before { content: ""\f586""; }
-.bi-star-half::before { content: ""\f587""; }
-.bi-star::before { content: ""\f588""; }
-.bi-stars::before { content: ""\f589""; }
-.bi-stickies-fill::before { content: ""\f58a""; }
-.bi-stickies::before { content: ""\f58b""; }
-.bi-sticky-fill::before { content: ""\f58c""; }
-.bi-sticky::before { content: ""\f58d""; }
-.bi-stop-btn-fill::before { content: ""\f58e""; }
-.bi-stop-btn::before { content: ""\f58f""; }
-.bi-stop-circle-fill::before { content: ""\f590""; }
-.bi-stop-circle::before { content: ""\f591""; }
-.bi-stop-fill::before { content: ""\f592""; }
-.bi-stop::before { content: ""\f593""; }
-.bi-stoplights-fill::before { content: ""\f594""; }
-.bi-stoplights::before { content: ""\f595""; }
-.bi-stopwatch-fill::before { content: ""\f596""; }
-.bi-stopwatch::before { content: ""\f597""; }
-.bi-subtract::before { content: ""\f598""; }
-.bi-suit-club-fill::before { content: ""\f599""; }
-.bi-suit-club::before { content: ""\f59a""; }
-.bi-suit-diamond-fill::before { content: ""\f59b""; }
-.bi-suit-diamond::before { content: ""\f59c""; }
-.bi-suit-heart-fill::before { content: ""\f59d""; }
-.bi-suit-heart::before { content: ""\f59e""; }
-.bi-suit-spade-fill::before { content: ""\f59f""; }
-.bi-suit-spade::before { content: ""\f5a0""; }
-.bi-sun-fill::before { content: ""\f5a1""; }
-.bi-sun::before { content: ""\f5a2""; }
-.bi-sunglasses::before { content: ""\f5a3""; }
-.bi-sunrise-fill::before { content: ""\f5a4""; }
-.bi-sunrise::before { content: ""\f5a5""; }
-.bi-sunset-fill::before { content: ""\f5a6""; }
-.bi-sunset::before { content: ""\f5a7""; }
-.bi-symmetry-horizontal::before { content: ""\f5a8""; }
-.bi-symmetry-vertical::before { content: ""\f5a9""; }
-.bi-table::before { content: ""\f5aa""; }
-.bi-tablet-fill::before { content: ""\f5ab""; }
-.bi-tablet-landscape-fill::before { content: ""\f5ac""; }
-.bi-tablet-landscape::before { content: ""\f5ad""; }
-.bi-tablet::before { content: ""\f5ae""; }
-.bi-tag-fill::before { content: ""\f5af""; }
-.bi-tag::before { content: ""\f5b0""; }
-.bi-tags-fill::before { content: ""\f5b1""; }
-.bi-tags::before { content: ""\f5b2""; }
-.bi-telegram::before { content: ""\f5b3""; }
-.bi-telephone-fill::before { content: ""\f5b4""; }
-.bi-telephone-forward-fill::before { content: ""\f5b5""; }
-.bi-telephone-forward::before { content: ""\f5b6""; }
-.bi-telephone-inbound-fill::before { content: ""\f5b7""; }
-.bi-telephone-inbound::before { content: ""\f5b8""; }
-.bi-telephone-minus-fill::before { content: ""\f5b9""; }
-.bi-telephone-minus::before { content: ""\f5ba""; }
-.bi-telephone-outbound-fill::before { content: ""\f5bb""; }
-.bi-telephone-outbound::before { content: ""\f5bc""; }
-.bi-telephone-plus-fill::before { content: ""\f5bd""; }
-.bi-telephone-plus::before { content: ""\f5be""; }
-.bi-telephone-x-fill::before { content: ""\f5bf""; }
-.bi-telephone-x::before { content: ""\f5c0""; }
-.bi-telephone::before { content: ""\f5c1""; }
-.bi-terminal-fill::before { content: ""\f5c2""; }
-.bi-terminal::before { content: ""\f5c3""; }
-.bi-text-center::before { content: ""\f5c4""; }
-.bi-text-indent-left::before { content: ""\f5c5""; }
-.bi-text-indent-right::before { content: ""\f5c6""; }
-.bi-text-left::before { content: ""\f5c7""; }
-.bi-text-paragraph::before { content: ""\f5c8""; }
-.bi-text-right::before { content: ""\f5c9""; }
-.bi-textarea-resize::before { content: ""\f5ca""; }
-.bi-textarea-t::before { content: ""\f5cb""; }
-.bi-textarea::before { content: ""\f5cc""; }
-.bi-thermometer-half::before { content: ""\f5cd""; }
-.bi-thermometer-high::before { content: ""\f5ce""; }
-.bi-thermometer-low::before { content: ""\f5cf""; }
-.bi-thermometer-snow::before { content: ""\f5d0""; }
-.bi-thermometer-sun::before { content: ""\f5d1""; }
-.bi-thermometer::before { content: ""\f5d2""; }
-.bi-three-dots-vertical::before { content: ""\f5d3""; }
-.bi-three-dots::before { content: ""\f5d4""; }
-.bi-toggle-off::before { content: ""\f5d5""; }
-.bi-toggle-on::before { content: ""\f5d6""; }
-.bi-toggle2-off::before { content: ""\f5d7""; }
-.bi-toggle2-on::before { content: ""\f5d8""; }
-.bi-toggles::before { content: ""\f5d9""; }
-.bi-toggles2::before { content: ""\f5da""; }
-.bi-tools::before { content: ""\f5db""; }
-.bi-tornado::before { content: ""\f5dc""; }
-.bi-trash-fill::before { content: ""\f5dd""; }
-.bi-trash::before { content: ""\f5de""; }
-.bi-trash2-fill::before { content: ""\f5df""; }
-.bi-trash2::before { content: ""\f5e0""; }
-.bi-tree-fill::before { content: ""\f5e1""; }
-.bi-tree::before { content: ""\f5e2""; }
-.bi-triangle-fill::before { content: ""\f5e3""; }
-.bi-triangle-half::before { content: ""\f5e4""; }
-.bi-triangle::before { content: ""\f5e5""; }
-.bi-trophy-fill::before { content: ""\f5e6""; }
-.bi-trophy::before { content: ""\f5e7""; }
-.bi-tropical-storm::before { content: ""\f5e8""; }
-.bi-truck-flatbed::before { content: ""\f5e9""; }
-.bi-truck::before { content: ""\f5ea""; }
-.bi-tsunami::before { content: ""\f5eb""; }
-.bi-tv-fill::before { content: ""\f5ec""; }
-.bi-tv::before { content: ""\f5ed""; }
-.bi-twitch::before { content: ""\f5ee""; }
-.bi-twitter::before { content: ""\f5ef""; }
-.bi-type-bold::before { content: ""\f5f0""; }
-.bi-type-h1::before { content: ""\f5f1""; }
-.bi-type-h2::before { content: ""\f5f2""; }
-.bi-type-h3::before { content: ""\f5f3""; }
-.bi-type-italic::before { content: ""\f5f4""; }
-.bi-type-strikethrough::before { content: ""\f5f5""; }
-.bi-type-underline::before { content: ""\f5f6""; }
-.bi-type::before { content: ""\f5f7""; }
-.bi-ui-checks-grid::before { content: ""\f5f8""; }
-.bi-ui-checks::before { content: ""\f5f9""; }
-.bi-ui-radios-grid::before { content: ""\f5fa""; }
-.bi-ui-radios::before { content: ""\f5fb""; }
-.bi-umbrella-fill::before { content: ""\f5fc""; }
-.bi-umbrella::before { content: ""\f5fd""; }
-.bi-union::before { content: ""\f5fe""; }
-.bi-unlock-fill::before { content: ""\f5ff""; }
-.bi-unlock::before { content: ""\f600""; }
-.bi-upc-scan::before { content: ""\f601""; }
-.bi-upc::before { content: ""\f602""; }
-.bi-upload::before { content: ""\f603""; }
-.bi-vector-pen::before { content: ""\f604""; }
-.bi-view-list::before { content: ""\f605""; }
-.bi-view-stacked::before { content: ""\f606""; }
-.bi-vinyl-fill::before { content: ""\f607""; }
-.bi-vinyl::before { content: ""\f608""; }
-.bi-voicemail::before { content: ""\f609""; }
-.bi-volume-down-fill::before { content: ""\f60a""; }
-.bi-volume-down::before { content: ""\f60b""; }
-.bi-volume-mute-fill::before { content: ""\f60c""; }
-.bi-volume-mute::before { content: ""\f60d""; }
-.bi-volume-off-fill::before { content: ""\f60e""; }
-.bi-volume-off::before { content: ""\f60f""; }
-.bi-volume-up-fill::before { content: ""\f610""; }
-.bi-volume-up::before { content: ""\f611""; }
-.bi-vr::before { content: ""\f612""; }
-.bi-wallet-fill::before { content: ""\f613""; }
-.bi-wallet::before { content: ""\f614""; }
-.bi-wallet2::before { content: ""\f615""; }
-.bi-watch::before { content: ""\f616""; }
-.bi-water::before { content: ""\f617""; }
-.bi-whatsapp::before { content: ""\f618""; }
-.bi-wifi-1::before { content: ""\f619""; }
-.bi-wifi-2::before { content: ""\f61a""; }
-.bi-wifi-off::before { content: ""\f61b""; }
-.bi-wifi::before { content: ""\f61c""; }
-.bi-wind::before { content: ""\f61d""; }
-.bi-window-dock::before { content: ""\f61e""; }
-.bi-window-sidebar::before { content: ""\f61f""; }
-.bi-window::before { content: ""\f620""; }
-.bi-wrench::before { content: ""\f621""; }
-.bi-x-circle-fill::before { content: ""\f622""; }
-.bi-x-circle::before { content: ""\f623""; }
-.bi-x-diamond-fill::before { content: ""\f624""; }
-.bi-x-diamond::before { content: ""\f625""; }
-.bi-x-octagon-fill::before { content: ""\f626""; }
-.bi-x-octagon::before { content: ""\f627""; }
-.bi-x-square-fill::before { content: ""\f628""; }
-.bi-x-square::before { content: ""\f629""; }
-.bi-x::before { content: ""\f62a""; }
-.bi-youtube::before { content: ""\f62b""; }
-.bi-zoom-in::before { content: ""\f62c""; }
-.bi-zoom-out::before { content: ""\f62d""; }
-.bi-bank::before { content: ""\f62e""; }
-.bi-bank2::before { content: ""\f62f""; }
-.bi-bell-slash-fill::before { content: ""\f630""; }
-.bi-bell-slash::before { content: ""\f631""; }
-.bi-cash-coin::before { content: ""\f632""; }
-.bi-check-lg::before { content: ""\f633""; }
-.bi-coin::before { content: ""\f634""; }
-.bi-currency-bitcoin::before { content: ""\f635""; }
-.bi-currency-dollar::before { content: ""\f636""; }
-.bi-currency-euro::before { content: ""\f637""; }
-.bi-currency-exchange::before { content: ""\f638""; }
-.bi-currency-pound::before { content: ""\f639""; }
-.bi-currency-yen::before { content: ""\f63a""; }
-.bi-dash-lg::before { content: ""\f63b""; }
-.bi-exclamation-lg::before { content: ""\f63c""; }
-.bi-file-earmark-pdf-fill::before { content: ""\f63d""; }
-.bi-file-earmark-pdf::before { content: ""\f63e""; }
-.bi-file-pdf-fill::before { content: ""\f63f""; }
-.bi-file-pdf::before { content: ""\f640""; }
-.bi-gender-ambiguous::before { content: ""\f641""; }
-.bi-gender-female::before { content: ""\f642""; }
-.bi-gender-male::before { content: ""\f643""; }
-.bi-gender-trans::before { content: ""\f644""; }
-.bi-headset-vr::before { content: ""\f645""; }
-.bi-info-lg::before { content: ""\f646""; }
-.bi-mastodon::before { content: ""\f647""; }
-.bi-messenger::before { content: ""\f648""; }
-.bi-piggy-bank-fill::before { content: ""\f649""; }
-.bi-piggy-bank::before { content: ""\f64a""; }
-.bi-pin-map-fill::before { content: ""\f64b""; }
-.bi-pin-map::before { content: ""\f64c""; }
-.bi-plus-lg::before { content: ""\f64d""; }
-.bi-question-lg::before { content: ""\f64e""; }
-.bi-recycle::before { content: ""\f64f""; }
-.bi-reddit::before { content: ""\f650""; }
-.bi-safe-fill::before { content: ""\f651""; }
-.bi-safe2-fill::before { content: ""\f652""; }
-.bi-safe2::before { content: ""\f653""; }
-.bi-sd-card-fill::before { content: ""\f654""; }
-.bi-sd-card::before { content: ""\f655""; }
-.bi-skype::before { content: ""\f656""; }
-.bi-slash-lg::before { content: ""\f657""; }
-.bi-translate::before { content: ""\f658""; }
-.bi-x-lg::before { content: ""\f659""; }
-.bi-safe::before { content: ""\f65a""; }
-.bi-apple::before { content: ""\f65b""; }
-.bi-microsoft::before { content: ""\f65d""; }
-.bi-windows::before { content: ""\f65e""; }
-.bi-behance::before { content: ""\f65c""; }
-.bi-dribbble::before { content: ""\f65f""; }
-.bi-line::before { content: ""\f660""; }
-.bi-medium::before { content: ""\f661""; }
-.bi-paypal::before { content: ""\f662""; }
-.bi-pinterest::before { content: ""\f663""; }
-.bi-signal::before { content: ""\f664""; }
-.bi-snapchat::before { content: ""\f665""; }
-.bi-spotify::before { content: ""\f666""; }
-.bi-stack-overflow::before { content: ""\f667""; }
-.bi-strava::before { content: ""\f668""; }
-.bi-wordpress::before { content: ""\f669""; }
-.bi-vimeo::before { content: ""\f66a""; }
-.bi-activity::before { content: ""\f66b""; }
-.bi-easel2-fill::before { content: ""\f66c""; }
-.bi-easel2::before { content: ""\f66d""; }
-.bi-easel3-fill::before { content: ""\f66e""; }
-.bi-easel3::before { content: ""\f66f""; }
-.bi-fan::before { content: ""\f670""; }
-.bi-fingerprint::before { content: ""\f671""; }
-.bi-graph-down-arrow::before { content: ""\f672""; }
-.bi-graph-up-arrow::before { content: ""\f673""; }
-.bi-hypnotize::before { content: ""\f674""; }
-.bi-magic::before { content: ""\f675""; }
-.bi-person-rolodex::before { content: ""\f676""; }
-.bi-person-video::before { content: ""\f677""; }
-.bi-person-video2::before { content: ""\f678""; }
-.bi-person-video3::before { content: ""\f679""; }
-.bi-person-workspace::before { content: ""\f67a""; }
-.bi-radioactive::before { content: ""\f67b""; }
-.bi-webcam-fill::before { content: ""\f67c""; }
-.bi-webcam::before { content: ""\f67d""; }
-.bi-yin-yang::before { content: ""\f67e""; }
-.bi-bandaid-fill::before { content: ""\f680""; }
-.bi-bandaid::before { content: ""\f681""; }
-.bi-bluetooth::before { content: ""\f682""; }
-.bi-body-text::before { content: ""\f683""; }
-.bi-boombox::before { content: ""\f684""; }
-.bi-boxes::before { content: ""\f685""; }
-.bi-dpad-fill::before { content: ""\f686""; }
-.bi-dpad::before { content: ""\f687""; }
-.bi-ear-fill::before { content: ""\f688""; }
-.bi-ear::before { content: ""\f689""; }
-.bi-envelope-check-1::before { content: ""\f68a""; }
-.bi-envelope-check-fill::before { content: ""\f68b""; }
-.bi-envelope-check::before { content: ""\f68c""; }
-.bi-envelope-dash-1::before { content: ""\f68d""; }
-.bi-envelope-dash-fill::before { content: ""\f68e""; }
-.bi-envelope-dash::before { content: ""\f68f""; }
-.bi-envelope-exclamation-1::before { content: ""\f690""; }
-.bi-envelope-exclamation-fill::before { content: ""\f691""; }
-.bi-envelope-exclamation::before { content: ""\f692""; }
-.bi-envelope-plus-fill::before { content: ""\f693""; }
-.bi-envelope-plus::before { content: ""\f694""; }
-.bi-envelope-slash-1::before { content: ""\f695""; }
-.bi-envelope-slash-fill::before { content: ""\f696""; }
-.bi-envelope-slash::before { content: ""\f697""; }
-.bi-envelope-x-1::before { content: ""\f698""; }
-.bi-envelope-x-fill::before { content: ""\f699""; }
-.bi-envelope-x::before { content: ""\f69a""; }
-.bi-explicit-fill::before { content: ""\f69b""; }
-.bi-explicit::before { content: ""\f69c""; }
-.bi-git::before { content: ""\f69d""; }
-.bi-infinity::before { content: ""\f69e""; }
-.bi-list-columns-reverse::before { content: ""\f69f""; }
-.bi-list-columns::before { content: ""\f6a0""; }
-.bi-meta::before { content: ""\f6a1""; }
-.bi-mortorboard-fill::before { content: ""\f6a2""; }
-.bi-mortorboard::before { content: ""\f6a3""; }
-.bi-nintendo-switch::before { content: ""\f6a4""; }
-.bi-pc-display-horizontal::before { content: ""\f6a5""; }
-.bi-pc-display::before { content: ""\f6a6""; }
-.bi-pc-horizontal::before { content: ""\f6a7""; }
-.bi-pc::before { content: ""\f6a8""; }
-.bi-playstation::before { content: ""\f6a9""; }
-.bi-plus-slash-minus::before { content: ""\f6aa""; }
-.bi-projector-fill::before { content: ""\f6ab""; }
-.bi-projector::before { content: ""\f6ac""; }
-.bi-qr-code-scan::before { content: ""\f6ad""; }
-.bi-qr-code::before { content: ""\f6ae""; }
-.bi-quora::before { content: ""\f6af""; }
-.bi-quote::before { content: ""\f6b0""; }
-.bi-robot::before { content: ""\f6b1""; }
-.bi-send-check-fill::before { content: ""\f6b2""; }
-.bi-send-check::before { content: ""\f6b3""; }
-.bi-send-dash-fill::before { content: ""\f6b4""; }
-.bi-send-dash::before { content: ""\f6b5""; }
-.bi-send-exclamation-1::before { content: ""\f6b6""; }
-.bi-send-exclamation-fill::before { content: ""\f6b7""; }
-.bi-send-exclamation::before { content: ""\f6b8""; }
-.bi-send-fill::before { content: ""\f6b9""; }
-.bi-send-plus-fill::before { content: ""\f6ba""; }
-.bi-send-plus::before { content: ""\f6bb""; }
-.bi-send-slash-fill::before { content: ""\f6bc""; }
-.bi-send-slash::before { content: ""\f6bd""; }
-.bi-send-x-fill::before { content: ""\f6be""; }
-.bi-send-x::before { content: ""\f6bf""; }
-.bi-send::before { content: ""\f6c0""; }
-.bi-steam::before { content: ""\f6c1""; }
-.bi-terminal-dash-1::before { content: ""\f6c2""; }
-.bi-terminal-dash::before { content: ""\f6c3""; }
-.bi-terminal-plus::before { content: ""\f6c4""; }
-.bi-terminal-split::before { content: ""\f6c5""; }
-.bi-ticket-detailed-fill::before { content: ""\f6c6""; }
-.bi-ticket-detailed::before { content: ""\f6c7""; }
-.bi-ticket-fill::before { content: ""\f6c8""; }
-.bi-ticket-perforated-fill::before { content: ""\f6c9""; }
-.bi-ticket-perforated::before { content: ""\f6ca""; }
-.bi-ticket::before { content: ""\f6cb""; }
-.bi-tiktok::before { content: ""\f6cc""; }
-.bi-window-dash::before { content: ""\f6cd""; }
-.bi-window-desktop::before { content: ""\f6ce""; }
-.bi-window-fullscreen::before { content: ""\f6cf""; }
-.bi-window-plus::before { content: ""\f6d0""; }
-.bi-window-split::before { content: ""\f6d1""; }
-.bi-window-stack::before { content: ""\f6d2""; }
-.bi-window-x::before { content: ""\f6d3""; }
-.bi-xbox::before { content: ""\f6d4""; }
-.bi-ethernet::before { content: ""\f6d5""; }
-.bi-hdmi-fill::before { content: ""\f6d6""; }
-.bi-hdmi::before { content: ""\f6d7""; }
-.bi-usb-c-fill::before { content: ""\f6d8""; }
-.bi-usb-c::before { content: ""\f6d9""; }
-.bi-usb-fill::before { content: ""\f6da""; }
-.bi-usb-plug-fill::before { content: ""\f6db""; }
-.bi-usb-plug::before { content: ""\f6dc""; }
-.bi-usb-symbol::before { content: ""\f6dd""; }
-.bi-usb::before { content: ""\f6de""; }
-.bi-boombox-fill::before { content: ""\f6df""; }
-.bi-displayport-1::before { content: ""\f6e0""; }
-.bi-displayport::before { content: ""\f6e1""; }
-.bi-gpu-card::before { content: ""\f6e2""; }
-.bi-memory::before { content: ""\f6e3""; }
-.bi-modem-fill::before { content: ""\f6e4""; }
-.bi-modem::before { content: ""\f6e5""; }
-.bi-motherboard-fill::before { content: ""\f6e6""; }
-.bi-motherboard::before { content: ""\f6e7""; }
-.bi-optical-audio-fill::before { content: ""\f6e8""; }
-.bi-optical-audio::before { content: ""\f6e9""; }
-.bi-pci-card::before { content: ""\f6ea""; }
-.bi-router-fill::before { content: ""\f6eb""; }
-.bi-router::before { content: ""\f6ec""; }
-.bi-ssd-fill::before { content: ""\f6ed""; }
-.bi-ssd::before { content: ""\f6ee""; }
-.bi-thunderbolt-fill::before { content: ""\f6ef""; }
-.bi-thunderbolt::before { content: ""\f6f0""; }
-.bi-usb-drive-fill::before { content: ""\f6f1""; }
-.bi-usb-drive::before { content: ""\f6f2""; }
-.bi-usb-micro-fill::before { content: ""\f6f3""; }
-.bi-usb-micro::before { content: ""\f6f4""; }
-.bi-usb-mini-fill::before { content: ""\f6f5""; }
-.bi-usb-mini::before { content: ""\f6f6""; }
-.bi-cloud-haze2::before { content: ""\f6f7""; }
-.bi-device-hdd-fill::before { content: ""\f6f8""; }
-.bi-device-hdd::before { content: ""\f6f9""; }
-.bi-device-ssd-fill::before { content: ""\f6fa""; }
-.bi-device-ssd::before { content: ""\f6fb""; }
-.bi-displayport-fill::before { content: ""\f6fc""; }
-.bi-mortarboard-fill::before { content: ""\f6fd""; }
-.bi-mortarboard::before { content: ""\f6fe""; }
-.bi-terminal-x::before { content: ""\f6ff""; }
-.bi-arrow-through-heart-fill::before { content: ""\f700""; }
-.bi-arrow-through-heart::before { content: ""\f701""; }
-.bi-badge-sd-fill::before { content: ""\f702""; }
-.bi-badge-sd::before { content: ""\f703""; }
-.bi-bag-heart-fill::before { content: ""\f704""; }
-.bi-bag-heart::before { content: ""\f705""; }
-.bi-balloon-fill::before { content: ""\f706""; }
-.bi-balloon-heart-fill::before { content: ""\f707""; }
-.bi-balloon-heart::before { content: ""\f708""; }
-.bi-balloon::before { content: ""\f709""; }
-.bi-box2-fill::before { content: ""\f70a""; }
-.bi-box2-heart-fill::before { content: ""\f70b""; }
-.bi-box2-heart::before { content: ""\f70c""; }
-.bi-box2::before { content: ""\f70d""; }
-.bi-braces-asterisk::before { content: ""\f70e""; }
-.bi-calendar-heart-fill::before { content: ""\f70f""; }
-.bi-calendar-heart::before { content: ""\f710""; }
-.bi-calendar2-heart-fill::before { content: ""\f711""; }
-.bi-calendar2-heart::before { content: ""\f712""; }
-.bi-chat-heart-fill::before { content: ""\f713""; }
-.bi-chat-heart::before { content: ""\f714""; }
-.bi-chat-left-heart-fill::before { content: ""\f715""; }
-.bi-chat-left-heart::before { content: ""\f716""; }
-.bi-chat-right-heart-fill::before { content: ""\f717""; }
-.bi-chat-right-heart::before { content: ""\f718""; }
-.bi-chat-square-heart-fill::before { content: ""\f719""; }
-.bi-chat-square-heart::before { content: ""\f71a""; }
-.bi-clipboard-check-fill::before { content: ""\f71b""; }
-.bi-clipboard-data-fill::before { content: ""\f71c""; }
-.bi-clipboard-fill::before { content: ""\f71d""; }
-.bi-clipboard-heart-fill::before { content: ""\f71e""; }
-.bi-clipboard-heart::before { content: ""\f71f""; }
-.bi-clipboard-minus-fill::before { content: ""\f720""; }
-.bi-clipboard-plus-fill::before { content: ""\f721""; }
-.bi-clipboard-pulse::before { content: ""\f722""; }
-.bi-clipboard-x-fill::before { content: ""\f723""; }
-.bi-clipboard2-check-fill::before { content: ""\f724""; }
-.bi-clipboard2-check::before { content: ""\f725""; }
-.bi-clipboard2-data-fill::before { content: ""\f726""; }
-.bi-clipboard2-data::before { content: ""\f727""; }
-.bi-clipboard2-fill::before { content: ""\f728""; }
-.bi-clipboard2-heart-fill::before { content: ""\f729""; }
-.bi-clipboard2-heart::before { content: ""\f72a""; }
-.bi-clipboard2-minus-fill::before { content: ""\f72b""; }
-.bi-clipboard2-minus::before { content: ""\f72c""; }
-.bi-clipboard2-plus-fill::before { content: ""\f72d""; }
-.bi-clipboard2-plus::before { content: ""\f72e""; }
-.bi-clipboard2-pulse-fill::before { content: ""\f72f""; }
-.bi-clipboard2-pulse::before { content: ""\f730""; }
-.bi-clipboard2-x-fill::before { content: ""\f731""; }
-.bi-clipboard2-x::before { content: ""\f732""; }
-.bi-clipboard2::before { content: ""\f733""; }
-.bi-emoji-kiss-fill::before { content: ""\f734""; }
-.bi-emoji-kiss::before { content: ""\f735""; }
-.bi-envelope-heart-fill::before { content: ""\f736""; }
-.bi-envelope-heart::before { content: ""\f737""; }
-.bi-envelope-open-heart-fill::before { content: ""\f738""; }
-.bi-envelope-open-heart::before { content: ""\f739""; }
-.bi-envelope-paper-fill::before { content: ""\f73a""; }
-.bi-envelope-paper-heart-fill::before { content: ""\f73b""; }
-.bi-envelope-paper-heart::before { content: ""\f73c""; }
-.bi-envelope-paper::before { content: ""\f73d""; }
-.bi-filetype-aac::before { content: ""\f73e""; }
-.bi-filetype-ai::before { content: ""\f73f""; }
-.bi-filetype-bmp::before { content: ""\f740""; }
-.bi-filetype-cs::before { content: ""\f741""; }
-.bi-filetype-css::before { content: ""\f742""; }
-.bi-filetype-csv::before { content: ""\f743""; }
-.bi-filetype-doc::before { content: ""\f744""; }
-.bi-filetype-docx::before { content: ""\f745""; }
-.bi-filetype-exe::before { content: ""\f746""; }
-.bi-filetype-gif::before { content: ""\f747""; }
-.bi-filetype-heic::before { content: ""\f748""; }
-.bi-filetype-html::before { content: ""\f749""; }
-.bi-filetype-java::before { content: ""\f74a""; }
-.bi-filetype-jpg::before { content: ""\f74b""; }
-.bi-filetype-js::before { content: ""\f74c""; }
-.bi-filetype-jsx::before { content: ""\f74d""; }
-.bi-filetype-key::before { content: ""\f74e""; }
-.bi-filetype-m4p::before { content: ""\f74f""; }
-.bi-filetype-md::before { content: ""\f750""; }
-.bi-filetype-mdx::before { content: ""\f751""; }
-.bi-filetype-mov::before { content: ""\f752""; }
-.bi-filetype-mp3::before { content: ""\f753""; }
-.bi-filetype-mp4::before { content: ""\f754""; }
-.bi-filetype-otf::before { content: ""\f755""; }
-.bi-filetype-pdf::before { content: ""\f756""; }
-.bi-filetype-php::before { content: ""\f757""; }
-.bi-filetype-png::before { content: ""\f758""; }
-.bi-filetype-ppt-1::before { content: ""\f759""; }
-.bi-filetype-ppt::before { content: ""\f75a""; }
-.bi-filetype-psd::before { content: ""\f75b""; }
-.bi-filetype-py::before { content: ""\f75c""; }
-.bi-filetype-raw::before { content: ""\f75d""; }
-.bi-filetype-rb::before { content: ""\f75e""; }
-.bi-filetype-sass::before { content: ""\f75f""; }
-.bi-filetype-scss::before { content: ""\f760""; }
-.bi-filetype-sh::before { content: ""\f761""; }
-.bi-filetype-svg::before { content: ""\f762""; }
-.bi-filetype-tiff::before { content: ""\f763""; }
-.bi-filetype-tsx::before { content: ""\f764""; }
-.bi-filetype-ttf::before { content: ""\f765""; }
-.bi-filetype-txt::before { content: ""\f766""; }
-.bi-filetype-wav::before { content: ""\f767""; }
-.bi-filetype-woff::before { content: ""\f768""; }
-.bi-filetype-xls-1::before { content: ""\f769""; }
-.bi-filetype-xls::before { content: ""\f76a""; }
-.bi-filetype-xml::before { content: ""\f76b""; }
-.bi-filetype-yml::before { content: ""\f76c""; }
-.bi-heart-arrow::before { content: ""\f76d""; }
-.bi-heart-pulse-fill::before { content: ""\f76e""; }
-.bi-heart-pulse::before { content: ""\f76f""; }
-.bi-heartbreak-fill::before { content: ""\f770""; }
-.bi-heartbreak::before { content: ""\f771""; }
-.bi-hearts::before { content: ""\f772""; }
-.bi-hospital-fill::before { content: ""\f773""; }
-.bi-hospital::before { content: ""\f774""; }
-.bi-house-heart-fill::before { content: ""\f775""; }
-.bi-house-heart::before { content: ""\f776""; }
-.bi-incognito::before { content: ""\f777""; }
-.bi-magnet-fill::before { content: ""\f778""; }
-.bi-magnet::before { content: ""\f779""; }
-.bi-person-heart::before { content: ""\f77a""; }
-.bi-person-hearts::before { content: ""\f77b""; }
-.bi-phone-flip::before { content: ""\f77c""; }
-.bi-plugin::before { content: ""\f77d""; }
-.bi-postage-fill::before { content: ""\f77e""; }
-.bi-postage-heart-fill::before { content: ""\f77f""; }
-.bi-postage-heart::before { content: ""\f780""; }
-.bi-postage::before { content: ""\f781""; }
-.bi-postcard-fill::before { content: ""\f782""; }
-.bi-postcard-heart-fill::before { content: ""\f783""; }
-.bi-postcard-heart::before { content: ""\f784""; }
-.bi-postcard::before { content: ""\f785""; }
-.bi-search-heart-fill::before { content: ""\f786""; }
-.bi-search-heart::before { content: ""\f787""; }
-.bi-sliders2-vertical::before { content: ""\f788""; }
-.bi-sliders2::before { content: ""\f789""; }
-.bi-trash3-fill::before { content: ""\f78a""; }
-.bi-trash3::before { content: ""\f78b""; }
-.bi-valentine::before { content: ""\f78c""; }
-.bi-valentine2::before { content: ""\f78d""; }
-.bi-wrench-adjustable-circle-fill::before { content: ""\f78e""; }
-.bi-wrench-adjustable-circle::before { content: ""\f78f""; }
-.bi-wrench-adjustable::before { content: ""\f790""; }
-.bi-filetype-json::before { content: ""\f791""; }
-.bi-filetype-pptx::before { content: ""\f792""; }
-.bi-filetype-xlsx::before { content: ""\f793""; }

---FILE: session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/bootstrap/bootstrap.min.js---
@@ -1,7 +0,0 @@
-/*!
-  * Bootstrap v5.1.3 (https://getbootstrap.com/)
-  * Copyright 2011-2021 The Bootstrap Authors (https://github.com/twbs/bootstrap/graphs/contributors)
-  * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE)
-  */
-!function(t,e){""object""==typeof exports&&""undefined""!=typeof module?module.exports=e():""function""==typeof define&&define.amd?define(e):(t=""undefined""!=typeof globalThis?globalThis:t||self).bootstrap=e()}(this,(function(){""use strict"";const t=""transitionend"",e=t=>{let e=t.getAttribute(""data-bs-target"");if(!e||""#""===e){let i=t.getAttribute(""href"");if(!i||!i.includes(""#"")&&!i.startsWith("".""))return null;i.includes(""#"")&&!i.startsWith(""#"")&&(i=`#${i.split(""#"")[1]}`),e=i&&""#""!==i?i.trim():null}return e},i=t=>{const i=e(t);return i&&document.querySelector(i)?i:null},n=t=>{const i=e(t);return i?document.querySelector(i):null},s=e=>{e.dispatchEvent(new Event(t))},o=t=>!(!t||""object""!=typeof t)&&(void 0!==t.jquery&&(t=t[0]),void 0!==t.nodeType),r=t=>o(t)?t.jquery?t[0]:t:""string""==typeof t&&t.length>0?document.querySelector(t):null,a=(t,e,i)=>{Object.keys(i).forEach((n=>{const s=i[n],r=e[n],a=r&&o(r)?""element"":null==(l=r)?`${l}`:{}.toString.call(l).match(/\s([a-z]+)/i)[1].toLowerCase();var l;if(!new RegExp(s).test(a))throw new TypeError(`${t.toUpperCase()}: Option ""${n}"" provided type ""${a}"" but expected type ""${s}"".`)}))},l=t=>!(!o(t)||0===t.getClientRects().length)&&""visible""===getComputedStyle(t).getPropertyValue(""visibility""),c=t=>!t||t.nodeType!==Node.ELEMENT_NODE||!!t.classList.contains(""disabled"")||(void 0!==t.disabled?t.disabled:t.hasAttribute(""disabled"")&&""false""!==t.getAttribute(""disabled"")),h=t=>{if(!document.documentElement.attachShadow)return null;if(""function""==typeof t.getRootNode){const e=t.getRootNode();return e instanceof ShadowRoot?e:null}return t instanceof ShadowRoot?t:t.parentNode?h(t.parentNode):null},d=()=>{},u=t=>{t.offsetHeight},f=()=>{const{jQuery:t}=window;return t&&!document.body.hasAttribute(""data-bs-no-jquery"")?t:null},p=[],m=()=>""rtl""===document.documentElement.dir,g=t=>{var e;e=()=>{const e=f();if(e){const i=t.NAME,n=e.fn[i];e.fn[i]=t.jQueryInterface,e.fn[i].Constructor=t,e.fn[i].noConflict=()=>(e.fn[i]=n,t.jQueryInterface)}},""loading""===document.readyState?(p.length||document.addEventListener(""DOMContentLoaded"",(()=>{p.forEach((t=>t()))})),p.push(e)):e()},_=t=>{""function""==typeof t&&t()},b=(e,i,n=!0)=>{if(!n)return void _(e);const o=(t=>{if(!t)return 0;let{transitionDuration:e,transitionDelay:i}=window.getComputedStyle(t);const n=Number.parseFloat(e),s=Number.parseFloat(i);return n||s?(e=e.split("","")[0],i=i.split("","")[0],1e3*(Number.parseFloat(e)+Number.parseFloat(i))):0})(i)+5;let r=!1;const a=({target:n})=>{n===i&&(r=!0,i.removeEventListener(t,a),_(e))};i.addEventListener(t,a),setTimeout((()=>{r||s(i)}),o)},v=(t,e,i,n)=>{let s=t.indexOf(e);if(-1===s)return t[!i&&n?t.length-1:0];const o=t.length;return s+=i?1:-1,n&&(s=(s+o)%o),t[Math.max(0,Math.min(s,o-1))]},y=/[^.]*(?=\..*)\.|.*/,w=/\..*/,E=/::\d+$/,A={};let T=1;const O={mouseenter:""mouseover"",mouseleave:""mouseout""},C=/^(mouseenter|mouseleave)/i,k=new Set([""click"",""dblclick"",""mouseup"",""mousedown"",""contextmenu"",""mousewheel"",""DOMMouseScroll"",""mouseover"",""mouseout"",""mousemove"",""selectstart"",""selectend"",""keydown"",""keypress"",""keyup"",""orientationchange"",""touchstart"",""touchmove"",""touchend"",""touchcancel"",""pointerdown"",""pointermove"",""pointerup"",""pointerleave"",""pointercancel"",""gesturestart"",""gesturechange"",""gestureend"",""focus"",""blur"",""change"",""reset"",""select"",""submit"",""focusin"",""focusout"",""load"",""unload"",""beforeunload"",""resize"",""move"",""DOMContentLoaded"",""readystatechange"",""error"",""abort"",""scroll""]);function L(t,e){return e&&`${e}::${T++}`||t.uidEvent||T++}function x(t){const e=L(t);return t.uidEvent=e,A[e]=A[e]||{},A[e]}function D(t,e,i=null){const n=Object.keys(t);for(let s=0,o=n.length;s<o;s++){const o=t[n[s]];if(o.originalHandler===e&&o.delegationSelector===i)return o}return null}function S(t,e,i){const n=""string""==typeof e,s=n?i:e;let o=P(t);return k.has(o)||(o=t),[n,s,o]}function N(t,e,i,n,s){if(""string""!=typeof e||!t)return;if(i||(i=n,n=null),C.test(e)){const t=t=>function(e){if(!e.relatedTarget||e.relatedTarget!==e.delegateTarget&&!e.delegateTarget.contains(e.relatedTarget))return t.call(this,e)};n?n=t(n):i=t(i)}const[o,r,a]=S(e,i,n),l=x(t),c=l[a]||(l[a]={}),h=D(c,r,o?i:null);if(h)return void(h.oneOff=h.oneOff&&s);const d=L(r,e.replace(y,"""")),u=o?function(t,e,i){return function n(s){const o=t.querySelectorAll(e);for(let{target:r}=s;r&&r!==this;r=r.parentNode)for(let a=o.length;a--;)if(o[a]===r)return s.delegateTarget=r,n.oneOff&&j.off(t,s.type,e,i),i.apply(r,[s]);return null}}(t,i,n):function(t,e){return function i(n){return n.delegateTarget=t,i.oneOff&&j.off(t,n.type,e),e.apply(t,[n])}}(t,i);u.delegationSelector=o?i:null,u.originalHandler=r,u.oneOff=s,u.uidEvent=d,c[d]=u,t.addEventListener(a,u,o)}function I(t,e,i,n,s){const o=D(e[i],n,s);o&&(t.removeEventListener(i,o,Boolean(s)),delete e[i][o.uidEvent])}function P(t){return t=t.replace(w,""""),O[t]||t}const j={on(t,e,i,n){N(t,e,i,n,!1)},one(t,e,i,n){N(t,e,i,n,!0)},off(t,e,i,n){if(""string""!=typeof e||!t)return;const[s,o,r]=S(e,i,n),a=r!==e,l=x(t),c=e.startsWith(""."");if(void 0!==o){if(!l||!l[r])return;return void I(t,l,r,o,s?i:null)}c&&Object.keys(l).forEach((i=>{!function(t,e,i,n){const s=e[i]||{};Object.keys(s).forEach((o=>{if(o.includes(n)){const n=s[o];I(t,e,i,n.originalHandler,n.delegationSelector)}}))}(t,l,i,e.slice(1))}));const h=l[r]||{};Object.keys(h).forEach((i=>{const n=i.replace(E,"""");if(!a||e.includes(n)){const e=h[i];I(t,l,r,e.originalHandler,e.delegationSelector)}}))},trigger(t,e,i){if(""string""!=typeof e||!t)return null;const n=f(),s=P(e),o=e!==s,r=k.has(s);let a,l=!0,c=!0,h=!1,d=null;return o&&n&&(a=n.Event(e,i),n(t).trigger(a),l=!a.isPropagationStopped(),c=!a.isImmediatePropagationStopped(),h=a.isDefaultPrevented()),r?(d=document.createEvent(""HTMLEvents""),d.initEvent(s,l,!0)):d=new CustomEvent(e,{bubbles:l,cancelable:!0}),void 0!==i&&Object.keys(i).forEach((t=>{Object.defineProperty(d,t,{get:()=>i[t]})})),h&&d.preventDefault(),c&&t.dispatchEvent(d),d.defaultPrevented&&void 0!==a&&a.preventDefault(),d}},M=new Map,H={set(t,e,i){M.has(t)||M.set(t,new Map);const n=M.get(t);n.has(e)||0===n.size?n.set(e,i):console.error(`Bootstrap doesn't allow more than one instance per element. Bound instance: ${Array.from(n.keys())[0]}.`)},get:(t,e)=>M.has(t)&&M.get(t).get(e)||null,remove(t,e){if(!M.has(t))return;const i=M.get(t);i.delete(e),0===i.size&&M.delete(t)}};class B{constructor(t){(t=r(t))&&(this._element=t,H.set(this._element,this.constructor.DATA_KEY,this))}dispose(){H.remove(this._element,this.constructor.DATA_KEY),j.off(this._element,this.constructor.EVENT_KEY),Object.getOwnPropertyNames(this).forEach((t=>{this[t]=null}))}_queueCallback(t,e,i=!0){b(t,e,i)}static getInstance(t){return H.get(r(t),this.DATA_KEY)}static getOrCreateInstance(t,e={}){return this.getInstance(t)||new this(t,""object""==typeof e?e:null)}static get VERSION(){return""5.1.3""}static get NAME(){throw new Error('You have to implement the static method ""NAME"", for each component!')}static get DATA_KEY(){return`bs.${this.NAME}`}static get EVENT_KEY(){return`.${this.DATA_KEY}`}}const R=(t,e=""hide"")=>{const i=`click.dismiss${t.EVENT_KEY}`,s=t.NAME;j.on(document,i,`[data-bs-dismiss=""${s}""]`,(function(i){if([""A"",""AREA""].includes(this.tagName)&&i.preventDefault(),c(this))return;const o=n(this)||this.closest(`.${s}`);t.getOrCreateInstance(o)[e]()}))};class W extends B{static get NAME(){return""alert""}close(){if(j.trigger(this._element,""close.bs.alert"").defaultPrevented)return;this._element.classList.remove(""show"");const t=this._element.classList.contains(""fade"");this._queueCallback((()=>this._destroyElement()),this._element,t)}_destroyElement(){this._element.remove(),j.trigger(this._element,""closed.bs.alert""),this.dispose()}static jQueryInterface(t){return this.each((function(){const e=W.getOrCreateInstance(this);if(""string""==typeof t){if(void 0===e[t]||t.startsWith(""_"")||""constructor""===t)throw new TypeError(`No method named ""${t}""`);e[t](this)}}))}}R(W,""close""),g(W);const $='[data-bs-toggle=""button""]';class z extends B{static get NAME(){return""button""}toggle(){this._element.setAttribute(""aria-pressed"",this._element.classList.toggle(""active""))}static jQueryInterface(t){return this.each((function(){const e=z.getOrCreateInstance(this);""toggle""===t&&e[t]()}))}}function q(t){return""true""===t||""false""!==t&&(t===Number(t).toString()?Number(t):""""===t||""null""===t?null:t)}function F(t){return t.replace(/[A-Z]/g,(t=>`-${t.toLowerCase()}`))}j.on(document,""click.bs.button.data-api"",$,(t=>{t.preventDefault();const e=t.target.closest($);z.getOrCreateInstance(e).toggle()})),g(z);const U={setDataAttribute(t,e,i){t.setAttribute(`data-bs-${F(e)}`,i)},removeDataAttribute(t,e){t.removeAttribute(`data-bs-${F(e)}`)},getDataAttributes(t){if(!t)return{};const e={};return Object.keys(t.dataset).filter((t=>t.startsWith(""bs""))).forEach((i=>{let n=i.replace(/^bs/,"""");n=n.charAt(0).toLowerCase()+n.slice(1,n.length),e[n]=q(t.dataset[i])})),e},getDataAttribute:(t,e)=>q(t.getAttribute(`data-bs-${F(e)}`)),offset(t){const e=t.getBoundingClientRect();return{top:e.top+window.pageYOffset,left:e.left+window.pageXOffset}},position:t=>({top:t.offsetTop,left:t.offsetLeft})},V={find:(t,e=document.documentElement)=>[].concat(...Element.prototype.querySelectorAll.call(e,t)),findOne:(t,e=document.documentElement)=>Element.prototype.querySelector.call(e,t),children:(t,e)=>[].concat(...t.children).filter((t=>t.matches(e))),parents(t,e){const i=[];let n=t.parentNode;for(;n&&n.nodeType===Node.ELEMENT_NODE&&3!==n.nodeType;)n.matches(e)&&i.push(n),n=n.parentNode;return i},prev(t,e){let i=t.previousElementSibling;for(;i;){if(i.matches(e))return[i];i=i.previousElementSibling}return[]},next(t,e){let i=t.nextElementSibling;for(;i;){if(i.matches(e))return[i];i=i.nextElementSibling}return[]},focusableChildren(t){const e=[""a"",""button"",""input"",""textarea"",""select"",""details"",""[tabindex]"",'[contenteditable=""true""]'].map((t=>`${t}:not([tabindex^=""-""])`)).join("", "");return this.find(e,t).filter((t=>!c(t)&&l(t)))}},K=""carousel"",X={interval:5e3,keyboard:!0,slide:!1,pause:""hover"",wrap:!0,touch:!0},Y={interval:""(number|boolean)"",keyboard:""boolean"",slide:""(boolean|string)"",pause:""(string|boolean)"",wrap:""boolean"",touch:""boolean""},Q=""next"",G=""prev"",Z=""left"",J=""right"",tt={ArrowLeft:J,ArrowRight:Z},et=""slid.bs.carousel"",it=""active"",nt="".active.carousel-item"";class st extends B{constructor(t,e){super(t),this._items=null,this._interval=null,this._activeElement=null,this._isPaused=!1,this._isSliding=!1,this.touchTimeout=null,this.touchStartX=0,this.touchDeltaX=0,this._config=this._getConfig(e),this._indicatorsElement=V.findOne("".carousel-indicators"",this._element),this._touchSupported=""ontouchstart""in document.documentElement||navigator.maxTouchPoints>0,this._pointerEvent=Boolean(window.PointerEvent),this._addEventListeners()}static get Default(){return X}static get NAME(){return K}next(){this._slide(Q)}nextWhenVisible(){!document.hidden&&l(this._element)&&this.next()}prev(){this._slide(G)}pause(t){t||(this._isPaused=!0),V.findOne("".carousel-item-next, .carousel-item-prev"",this._element)&&(s(this._element),this.cycle(!0)),clearInterval(this._interval),this._interval=null}cycle(t){t||(this._isPaused=!1),this._interval&&(clearInterval(this._interval),this._interval=null),this._config&&this._config.interval&&!this._isPaused&&(this._updateInterval(),this._interval=setInterval((document.visibilityState?this.nextWhenVisible:this.next).bind(this),this._config.interval))}to(t){this._activeElement=V.findOne(nt,this._element);const e=this._getItemIndex(this._activeElement);if(t>this._items.length-1||t<0)return;if(this._isSliding)return void j.one(this._element,et,(()=>this.to(t)));if(e===t)return this.pause(),void this.cycle();const i=t>e?Q:G;this._slide(i,this._items[t])}_getConfig(t){return t={...X,...U.getDataAttributes(this._element),...""object""==typeof t?t:{}},a(K,t,Y),t}_handleSwipe(){const t=Math.abs(this.touchDeltaX);if(t<=40)return;const e=t/this.touchDeltaX;this.touchDeltaX=0,e&&this._slide(e>0?J:Z)}_addEventListeners(){this._config.keyboard&&j.on(this._element,""keydown.bs.carousel"",(t=>this._keydown(t))),""hover""===this._config.pause&&(j.on(this._element,""mouseenter.bs.carousel"",(t=>this.pause(t))),j.on(this._element,""mouseleave.bs.carousel"",(t=>this.cycle(t)))),this._config.touch&&this._touchSupported&&this._addTouchEventListeners()}_addTouchEventListeners(){const t=t=>this._pointerEvent&&(""pen""===t.pointerType||""touch""===t.pointerType),e=e=>{t(e)?this.touchStartX=e.clientX:this._pointerEvent||(this.touchStartX=e.touches[0].clientX)},i=t=>{this.touchDeltaX=t.touches&&t.touches.length>1?0:t.touches[0].clientX-this.touchStartX},n=e=>{t(e)&&(this.touchDeltaX=e.clientX-this.touchStartX),this._handleSwipe(),""hover""===this._config.pause&&(this.pause(),this.touchTimeout&&clearTimeout(this.touchTimeout),this.touchTimeout=setTimeout((t=>this.cycle(t)),500+this._config.interval))};V.find("".carousel-item img"",this._element).forEach((t=>{j.on(t,""dragstart.bs.carousel"",(t=>t.preventDefault()))})),this._pointerEvent?(j.on(this._element,""pointerdown.bs.carousel"",(t=>e(t))),j.on(this._element,""pointerup.bs.carousel"",(t=>n(t))),this._element.classList.add(""pointer-event"")):(j.on(this._element,""touchstart.bs.carousel"",(t=>e(t))),j.on(this._element,""touchmove.bs.carousel"",(t=>i(t))),j.on(this._element,""touchend.bs.carousel"",(t=>n(t))))}_keydown(t){if(/input|textarea/i.test(t.target.tagName))return;const e=tt[t.key];e&&(t.preventDefault(),this._slide(e))}_getItemIndex(t){return this._items=t&&t.parentNode?V.find("".carousel-item"",t.parentNode):[],this._items.indexOf(t)}_getItemByOrder(t,e){const i=t===Q;return v(this._items,e,i,this._config.wrap)}_triggerSlideEvent(t,e){const i=this._getItemIndex(t),n=this._getItemIndex(V.findOne(nt,this._element));return j.trigger(this._element,""slide.bs.carousel"",{relatedTarget:t,direction:e,from:n,to:i})}_setActiveIndicatorElement(t){if(this._indicatorsElement){const e=V.findOne("".active"",this._indicatorsElement);e.classList.remove(it),e.removeAttribute(""aria-current"");const i=V.find(""[data-bs-target]"",this._indicatorsElement);for(let e=0;e<i.length;e++)if(Number.parseInt(i[e].getAttribute(""data-bs-slide-to""),10)===this._getItemIndex(t)){i[e].classList.add(it),i[e].setAttribute(""aria-current"",""true"");break}}}_updateInterval(){const t=this._activeElement||V.findOne(nt,this._element);if(!t)return;const e=Number.parseInt(t.getAttribute(""data-bs-interval""),10);e?(this._config.defaultInterval=this._config.defaultInterval||this._config.interval,this._config.interval=e):this._config.interval=this._config.defaultInterval||this._config.interval}_slide(t,e){const i=this._directionToOrder(t),n=V.findOne(nt,this._element),s=this._getItemIndex(n),o=e||this._getItemByOrder(i,n),r=this._getItemIndex(o),a=Boolean(this._interval),l=i===Q,c=l?""carousel-item-start"":""carousel-item-end"",h=l?""carousel-item-next"":""carousel-item-prev"",d=this._orderToDirection(i);if(o&&o.classList.contains(it))return void(this._isSliding=!1);if(this._isSliding)return;if(this._triggerSlideEvent(o,d).defaultPrevented)return;if(!n||!o)return;this._isSliding=!0,a&&this.pause(),this._setActiveIndicatorElement(o),this._activeElement=o;const f=()=>{j.trigger(this._element,et,{relatedTarget:o,direction:d,from:s,to:r})};if(this._element.classList.contains(""slide"")){o.classList.add(h),u(o),n.classList.add(c),o.classList.add(c);const t=()=>{o.classList.remove(c,h),o.classList.add(it),n.classList.remove(it,h,c),this._isSliding=!1,setTimeout(f,0)};this._queueCallback(t,n,!0)}else n.classList.remove(it),o.classList.add(it),this._isSliding=!1,f();a&&this.cycle()}_directionToOrder(t){return[J,Z].includes(t)?m()?t===Z?G:Q:t===Z?Q:G:t}_orderToDirection(t){return[Q,G].includes(t)?m()?t===G?Z:J:t===G?J:Z:t}static carouselInterface(t,e){const i=st.getOrCreateInstance(t,e);let{_config:n}=i;""object""==typeof e&&(n={...n,...e});const s=""string""==typeof e?e:n.slide;if(""number""==typeof e)i.to(e);else if(""string""==typeof s){if(void 0===i[s])throw new TypeError(`No method named ""${s}""`);i[s]()}else n.interval&&n.ride&&(i.pause(),i.cycle())}static jQueryInterface(t){return this.each((function(){st.carouselInterface(this,t)}))}static dataApiClickHandler(t){const e=n(this);if(!e||!e.classList.contains(""carousel""))return;const i={...U.getDataAttributes(e),...U.getDataAttributes(this)},s=this.getAttribute(""data-bs-slide-to"");s&&(i.interval=!1),st.carouselInterface(e,i),s&&st.getInstance(e).to(s),t.preventDefault()}}j.on(document,""click.bs.carousel.data-api"",""[data-bs-slide], [data-bs-slide-to]"",st.dataApiClickHandler),j.on(window,""load.bs.carousel.data-api"",(()=>{const t=V.find('[data-bs-ride=""carousel""]');for(let e=0,i=t.length;e<i;e++)st.carouselInterface(t[e],st.getInstance(t[e]))})),g(st);const ot=""collapse"",rt={toggle:!0,parent:null},at={toggle:""boolean"",parent:""(null|element)""},lt=""show"",ct=""collapse"",ht=""collapsing"",dt=""collapsed"",ut="":scope .collapse .collapse"",ft='[data-bs-toggle=""collapse""]';class pt extends B{constructor(t,e){super(t),this._isTransitioning=!1,this._config=this._getConfig(e),this._triggerArray=[];const n=V.find(ft);for(let t=0,e=n.length;t<e;t++){const e=n[t],s=i(e),o=V.find(s).filter((t=>t===this._element));null!==s&&o.length&&(this._selector=s,this._triggerArray.push(e))}this._initializeChildren(),this._config.parent||this._addAriaAndCollapsedClass(this._triggerArray,this._isShown()),this._config.toggle&&this.toggle()}static get Default(){return rt}static get NAME(){return ot}toggle(){this._isShown()?this.hide():this.show()}show(){if(this._isTransitioning||this._isShown())return;let t,e=[];if(this._config.parent){const t=V.find(ut,this._config.parent);e=V.find("".collapse.show, .collapse.collapsing"",this._config.parent).filter((e=>!t.includes(e)))}const i=V.findOne(this._selector);if(e.length){const n=e.find((t=>i!==t));if(t=n?pt.getInstance(n):null,t&&t._isTransitioning)return}if(j.trigger(this._element,""show.bs.collapse"").defaultPrevented)return;e.forEach((e=>{i!==e&&pt.getOrCreateInstance(e,{toggle:!1}).hide(),t||H.set(e,""bs.collapse"",null)}));const n=this._getDimension();this._element.classList.remove(ct),this._element.classList.add(ht),this._element.style[n]=0,this._addAriaAndCollapsedClass(this._triggerArray,!0),this._isTransitioning=!0;const s=`scroll${n[0].toUpperCase()+n.slice(1)}`;this._queueCallback((()=>{this._isTransitioning=!1,this._element.classList.remove(ht),this._element.classList.add(ct,lt),this._element.style[n]="""",j.trigger(this._element,""shown.bs.collapse"")}),this._element,!0),this._element.style[n]=`${this._element[s]}px`}hide(){if(this._isTransitioning||!this._isShown())return;if(j.trigger(this._element,""hide.bs.collapse"").defaultPrevented)return;const t=this._getDimension();this._element.style[t]=`${this._element.getBoundingClientRect()[t]}px`,u(this._element),this._element.classList.add(ht),this._element.classList.remove(ct,lt);const e=this._triggerArray.length;for(let t=0;t<e;t++){const e=this._triggerArray[t],i=n(e);i&&!this._isShown(i)&&this._addAriaAndCollapsedClass([e],!1)}this._isTransitioning=!0,this._element.style[t]="""",this._queueCallback((()=>{this._isTransitioning=!1,this._element.classList.remove(ht),this._element.classList.add(ct),j.trigger(this._element,""hidden.bs.collapse"")}),this._element,!0)}_isShown(t=this._element){return t.classList.contains(lt)}_getConfig(t){return(t={...rt,...U.getDataAttributes(this._element),...t}).toggle=Boolean(t.toggle),t.parent=r(t.parent),a(ot,t,at),t}_getDimension(){return this._element.classList.contains(""collapse-horizontal"")?""width"":""height""}_initializeChildren(){if(!this._config.parent)return;const t=V.find(ut,this._config.parent);V.find(ft,this._config.parent).filter((e=>!t.includes(e))).forEach((t=>{const e=n(t);e&&this._addAriaAndCollapsedClass([t],this._isShown(e))}))}_addAriaAndCollapsedClass(t,e){t.length&&t.forEach((t=>{e?t.classList.remove(dt):t.classList.add(dt),t.setAttribute(""aria-expanded"",e)}))}static jQueryInterface(t){return this.each((function(){const e={};""string""==typeof t&&/show|hide/.test(t)&&(e.toggle=!1);const i=pt.getOrCreateInstance(this,e);if(""string""==typeof t){if(void 0===i[t])throw new TypeError(`No method named ""${t}""`);i[t]()}}))}}j.on(document,""click.bs.collapse.data-api"",ft,(function(t){(""A""===t.target.tagName||t.delegateTarget&&""A""===t.delegateTarget.tagName)&&t.preventDefault();const e=i(this);V.find(e).forEach((t=>{pt.getOrCreateInstance(t,{toggle:!1}).toggle()}))})),g(pt);var mt=""top"",gt=""bottom"",_t=""right"",bt=""left"",vt=""auto"",yt=[mt,gt,_t,bt],wt=""start"",Et=""end"",At=""clippingParents"",Tt=""viewport"",Ot=""popper"",Ct=""reference"",kt=yt.reduce((function(t,e){return t.concat([e+""-""+wt,e+""-""+Et])}),[]),Lt=[].concat(yt,[vt]).reduce((function(t,e){return t.concat([e,e+""-""+wt,e+""-""+Et])}),[]),xt=""beforeRead"",Dt=""read"",St=""afterRead"",Nt=""beforeMain"",It=""main"",Pt=""afterMain"",jt=""beforeWrite"",Mt=""write"",Ht=""afterWrite"",Bt=[xt,Dt,St,Nt,It,Pt,jt,Mt,Ht];function Rt(t){return t?(t.nodeName||"""").toLowerCase():null}function Wt(t){if(null==t)return window;if(""[object Window]""!==t.toString()){var e=t.ownerDocument;return e&&e.defaultView||window}return t}function $t(t){return t instanceof Wt(t).Element||t instanceof Element}function zt(t){return t instanceof Wt(t).HTMLElement||t instanceof HTMLElement}function qt(t){return""undefined""!=typeof ShadowRoot&&(t instanceof Wt(t).ShadowRoot||t instanceof ShadowRoot)}const Ft={name:""applyStyles"",enabled:!0,phase:""write"",fn:function(t){var e=t.state;Object.keys(e.elements).forEach((function(t){var i=e.styles[t]||{},n=e.attributes[t]||{},s=e.elements[t];zt(s)&&Rt(s)&&(Object.assign(s.style,i),Object.keys(n).forEach((function(t){var e=n[t];!1===e?s.removeAttribute(t):s.setAttribute(t,!0===e?"""":e)})))}))},effect:function(t){var e=t.state,i={popper:{position:e.options.strategy,left:""0"",top:""0"",margin:""0""},arrow:{position:""absolute""},reference:{}};return Object.assign(e.elements.popper.style,i.popper),e.styles=i,e.elements.arrow&&Object.assign(e.elements.arrow.style,i.arrow),function(){Object.keys(e.elements).forEach((function(t){var n=e.elements[t],s=e.attributes[t]||{},o=Object.keys(e.styles.hasOwnProperty(t)?e.styles[t]:i[t]).reduce((function(t,e){return t[e]="""",t}),{});zt(n)&&Rt(n)&&(Object.assign(n.style,o),Object.keys(s).forEach((function(t){n.removeAttribute(t)})))}))}},requires:[""computeStyles""]};function Ut(t){return t.split(""-"")[0]}function Vt(t,e){var i=t.getBoundingClientRect();return{width:i.width/1,height:i.height/1,top:i.top/1,right:i.right/1,bottom:i.bottom/1,left:i.left/1,x:i.left/1,y:i.top/1}}function Kt(t){var e=Vt(t),i=t.offsetWidth,n=t.offsetHeight;return Math.abs(e.width-i)<=1&&(i=e.width),Math.abs(e.height-n)<=1&&(n=e.height),{x:t.offsetLeft,y:t.offsetTop,width:i,height:n}}function Xt(t,e){var i=e.getRootNode&&e.getRootNode();if(t.contains(e))return!0;if(i&&qt(i)){var n=e;do{if(n&&t.isSameNode(n))return!0;n=n.parentNode||n.host}while(n)}return!1}function Yt(t){return Wt(t).getComputedStyle(t)}function Qt(t){return[""table"",""td"",""th""].indexOf(Rt(t))>=0}function Gt(t){return(($t(t)?t.ownerDocument:t.document)||window.document).documentElement}function Zt(t){return""html""===Rt(t)?t:t.assignedSlot||t.parentNode||(qt(t)?t.host:null)||Gt(t)}function Jt(t){return zt(t)&&""fixed""!==Yt(t).position?t.offsetParent:null}function te(t){for(var e=Wt(t),i=Jt(t);i&&Qt(i)&&""static""===Yt(i).position;)i=Jt(i);return i&&(""html""===Rt(i)||""body""===Rt(i)&&""static""===Yt(i).position)?e:i||function(t){var e=-1!==navigator.userAgent.toLowerCase().indexOf(""firefox"");if(-1!==navigator.userAgent.indexOf(""Trident"")&&zt(t)&&""fixed""===Yt(t).position)return null;for(var i=Zt(t);zt(i)&&[""html"",""body""].indexOf(Rt(i))<0;){var n=Yt(i);if(""none""!==n.transform||""none""!==n.perspective||""paint""===n.contain||-1!==[""transform"",""perspective""].indexOf(n.willChange)||e&&""filter""===n.willChange||e&&n.filter&&""none""!==n.filter)return i;i=i.parentNode}return null}(t)||e}function ee(t){return[""top"",""bottom""].indexOf(t)>=0?""x"":""y""}var ie=Math.max,ne=Math.min,se=Math.round;function oe(t,e,i){return ie(t,ne(e,i))}function re(t){return Object.assign({},{top:0,right:0,bottom:0,left:0},t)}function ae(t,e){return e.reduce((function(e,i){return e[i]=t,e}),{})}const le={name:""arrow"",enabled:!0,phase:""main"",fn:function(t){var e,i=t.state,n=t.name,s=t.options,o=i.elements.arrow,r=i.modifiersData.popperOffsets,a=Ut(i.placement),l=ee(a),c=[bt,_t].indexOf(a)>=0?""height"":""width"";if(o&&r){var h=function(t,e){return re(""number""!=typeof(t=""function""==typeof t?t(Object.assign({},e.rects,{placement:e.placement})):t)?t:ae(t,yt))}(s.padding,i),d=Kt(o),u=""y""===l?mt:bt,f=""y""===l?gt:_t,p=i.rects.reference[c]+i.rects.reference[l]-r[l]-i.rects.popper[c],m=r[l]-i.rects.reference[l],g=te(o),_=g?""y""===l?g.clientHeight||0:g.clientWidth||0:0,b=p/2-m/2,v=h[u],y=_-d[c]-h[f],w=_/2-d[c]/2+b,E=oe(v,w,y),A=l;i.modifiersData[n]=((e={})[A]=E,e.centerOffset=E-w,e)}},effect:function(t){var e=t.state,i=t.options.element,n=void 0===i?""[data-popper-arrow]"":i;null!=n&&(""string""!=typeof n||(n=e.elements.popper.querySelector(n)))&&Xt(e.elements.popper,n)&&(e.elements.arrow=n)},requires:[""popperOffsets""],requiresIfExists:[""preventOverflow""]};function ce(t){return t.split(""-"")[1]}var he={top:""auto"",right:""auto"",bottom:""auto"",left:""auto""};function de(t){var e,i=t.popper,n=t.popperRect,s=t.placement,o=t.variation,r=t.offsets,a=t.position,l=t.gpuAcceleration,c=t.adaptive,h=t.roundOffsets,d=!0===h?function(t){var e=t.x,i=t.y,n=window.devicePixelRatio||1;return{x:se(se(e*n)/n)||0,y:se(se(i*n)/n)||0}}(r):""function""==typeof h?h(r):r,u=d.x,f=void 0===u?0:u,p=d.y,m=void 0===p?0:p,g=r.hasOwnProperty(""x""),_=r.hasOwnProperty(""y""),b=bt,v=mt,y=window;if(c){var w=te(i),E=""clientHeight"",A=""clientWidth"";w===Wt(i)&&""static""!==Yt(w=Gt(i)).position&&""absolute""===a&&(E=""scrollHeight"",A=""scrollWidth""),w=w,s!==mt&&(s!==bt&&s!==_t||o!==Et)||(v=gt,m-=w[E]-n.height,m*=l?1:-1),s!==bt&&(s!==mt&&s!==gt||o!==Et)||(b=_t,f-=w[A]-n.width,f*=l?1:-1)}var T,O=Object.assign({position:a},c&&he);return l?Object.assign({},O,((T={})[v]=_?""0"":"""",T[b]=g?""0"":"""",T.transform=(y.devicePixelRatio||1)<=1?""translate(""+f+""px, ""+m+""px)"":""translate3d(""+f+""px, ""+m+""px, 0)"",T)):Object.assign({},O,((e={})[v]=_?m+""px"":"""",e[b]=g?f+""px"":"""",e.transform="""",e))}const ue={name:""computeStyles"",enabled:!0,phase:""beforeWrite"",fn:function(t){var e=t.state,i=t.options,n=i.gpuAcceleration,s=void 0===n||n,o=i.adaptive,r=void 0===o||o,a=i.roundOffsets,l=void 0===a||a,c={placement:Ut(e.placement),variation:ce(e.placement),popper:e.elements.popper,popperRect:e.rects.popper,gpuAcceleration:s};null!=e.modifiersData.popperOffsets&&(e.styles.popper=Object.assign({},e.styles.popper,de(Object.assign({},c,{offsets:e.modifiersData.popperOffsets,position:e.options.strategy,adaptive:r,roundOffsets:l})))),null!=e.modifiersData.arrow&&(e.styles.arrow=Object.assign({},e.styles.arrow,de(Object.assign({},c,{offsets:e.modifiersData.arrow,position:""absolute"",adaptive:!1,roundOffsets:l})))),e.attributes.popper=Object.assign({},e.attributes.popper,{""data-popper-placement"":e.placement})},data:{}};var fe={passive:!0};const pe={name:""eventListeners"",enabled:!0,phase:""write"",fn:function(){},effect:function(t){var e=t.state,i=t.instance,n=t.options,s=n.scroll,o=void 0===s||s,r=n.resize,a=void 0===r||r,l=Wt(e.elements.popper),c=[].concat(e.scrollParents.reference,e.scrollParents.popper);return o&&c.forEach((function(t){t.addEventListener(""scroll"",i.update,fe)})),a&&l.addEventListener(""resize"",i.update,fe),function(){o&&c.forEach((function(t){t.removeEventListener(""scroll"",i.update,fe)})),a&&l.removeEventListener(""resize"",i.update,fe)}},data:{}};var me={left:""right"",right:""left"",bottom:""top"",top:""bottom""};function ge(t){return t.replace(/left|right|bottom|top/g,(function(t){return me[t]}))}var _e={start:""end"",end:""start""};function be(t){return t.replace(/start|end/g,(function(t){return _e[t]}))}function ve(t){var e=Wt(t);return{scrollLeft:e.pageXOffset,scrollTop:e.pageYOffset}}function ye(t){return Vt(Gt(t)).left+ve(t).scrollLeft}function we(t){var e=Yt(t),i=e.overflow,n=e.overflowX,s=e.overflowY;return/auto|scroll|overlay|hidden/.test(i+s+n)}function Ee(t){return[""html"",""body"",""#document""].indexOf(Rt(t))>=0?t.ownerDocument.body:zt(t)&&we(t)?t:Ee(Zt(t))}function Ae(t,e){var i;void 0===e&&(e=[]);var n=Ee(t),s=n===(null==(i=t.ownerDocument)?void 0:i.body),o=Wt(n),r=s?[o].concat(o.visualViewport||[],we(n)?n:[]):n,a=e.concat(r);return s?a:a.concat(Ae(Zt(r)))}function Te(t){return Object.assign({},t,{left:t.x,top:t.y,right:t.x+t.width,bottom:t.y+t.height})}function Oe(t,e){return e===Tt?Te(function(t){var e=Wt(t),i=Gt(t),n=e.visualViewport,s=i.clientWidth,o=i.clientHeight,r=0,a=0;return n&&(s=n.width,o=n.height,/^((?!chrome|android).)*safari/i.test(navigator.userAgent)||(r=n.offsetLeft,a=n.offsetTop)),{width:s,height:o,x:r+ye(t),y:a}}(t)):zt(e)?function(t){var e=Vt(t);return e.top=e.top+t.clientTop,e.left=e.left+t.clientLeft,e.bottom=e.top+t.clientHeight,e.right=e.left+t.clientWidth,e.width=t.clientWidth,e.height=t.clientHeight,e.x=e.left,e.y=e.top,e}(e):Te(function(t){var e,i=Gt(t),n=ve(t),s=null==(e=t.ownerDocument)?void 0:e.body,o=ie(i.scrollWidth,i.clientWidth,s?s.scrollWidth:0,s?s.clientWidth:0),r=ie(i.scrollHeight,i.clientHeight,s?s.scrollHeight:0,s?s.clientHeight:0),a=-n.scrollLeft+ye(t),l=-n.scrollTop;return""rtl""===Yt(s||i).direction&&(a+=ie(i.clientWidth,s?s.clientWidth:0)-o),{width:o,height:r,x:a,y:l}}(Gt(t)))}function Ce(t){var e,i=t.reference,n=t.element,s=t.placement,o=s?Ut(s):null,r=s?ce(s):null,a=i.x+i.width/2-n.width/2,l=i.y+i.height/2-n.height/2;switch(o){case mt:e={x:a,y:i.y-n.height};break;case gt:e={x:a,y:i.y+i.height};break;case _t:e={x:i.x+i.width,y:l};break;case bt:e={x:i.x-n.width,y:l};break;default:e={x:i.x,y:i.y}}var c=o?ee(o):null;if(null!=c){var h=""y""===c?""height"":""width"";switch(r){case wt:e[c]=e[c]-(i[h]/2-n[h]/2);break;case Et:e[c]=e[c]+(i[h]/2-n[h]/2)}}return e}function ke(t,e){void 0===e&&(e={});var i=e,n=i.placement,s=void 0===n?t.placement:n,o=i.boundary,r=void 0===o?At:o,a=i.rootBoundary,l=void 0===a?Tt:a,c=i.elementContext,h=void 0===c?Ot:c,d=i.altBoundary,u=void 0!==d&&d,f=i.padding,p=void 0===f?0:f,m=re(""number""!=typeof p?p:ae(p,yt)),g=h===Ot?Ct:Ot,_=t.rects.popper,b=t.elements[u?g:h],v=function(t,e,i){var n=""clippingParents""===e?function(t){var e=Ae(Zt(t)),i=[""absolute"",""fixed""].indexOf(Yt(t).position)>=0&&zt(t)?te(t):t;return $t(i)?e.filter((function(t){return $t(t)&&Xt(t,i)&&""body""!==Rt(t)})):[]}(t):[].concat(e),s=[].concat(n,[i]),o=s[0],r=s.reduce((function(e,i){var n=Oe(t,i);return e.top=ie(n.top,e.top),e.right=ne(n.right,e.right),e.bottom=ne(n.bottom,e.bottom),e.left=ie(n.left,e.left),e}),Oe(t,o));return r.width=r.right-r.left,r.height=r.bottom-r.top,r.x=r.left,r.y=r.top,r}($t(b)?b:b.contextElement||Gt(t.elements.popper),r,l),y=Vt(t.elements.reference),w=Ce({reference:y,element:_,strategy:""absolute"",placement:s}),E=Te(Object.assign({},_,w)),A=h===Ot?E:y,T={top:v.top-A.top+m.top,bottom:A.bottom-v.bottom+m.bottom,left:v.left-A.left+m.left,right:A.right-v.right+m.right},O=t.modifiersData.offset;if(h===Ot&&O){var C=O[s];Object.keys(T).forEach((function(t){var e=[_t,gt].indexOf(t)>=0?1:-1,i=[mt,gt].indexOf(t)>=0?""y"":""x"";T[t]+=C[i]*e}))}return T}function Le(t,e){void 0===e&&(e={});var i=e,n=i.placement,s=i.boundary,o=i.rootBoundary,r=i.padding,a=i.flipVariations,l=i.allowedAutoPlacements,c=void 0===l?Lt:l,h=ce(n),d=h?a?kt:kt.filter((function(t){return ce(t)===h})):yt,u=d.filter((function(t){return c.indexOf(t)>=0}));0===u.length&&(u=d);var f=u.reduce((function(e,i){return e[i]=ke(t,{placement:i,boundary:s,rootBoundary:o,padding:r})[Ut(i)],e}),{});return Object.keys(f).sort((function(t,e){return f[t]-f[e]}))}const xe={name:""flip"",enabled:!0,phase:""main"",fn:function(t){var e=t.state,i=t.options,n=t.name;if(!e.modifiersData[n]._skip){for(var s=i.mainAxis,o=void 0===s||s,r=i.altAxis,a=void 0===r||r,l=i.fallbackPlacements,c=i.padding,h=i.boundary,d=i.rootBoundary,u=i.altBoundary,f=i.flipVariations,p=void 0===f||f,m=i.allowedAutoPlacements,g=e.options.placement,_=Ut(g),b=l||(_!==g&&p?function(t){if(Ut(t)===vt)return[];var e=ge(t);return[be(t),e,be(e)]}(g):[ge(g)]),v=[g].concat(b).reduce((function(t,i){return t.concat(Ut(i)===vt?Le(e,{placement:i,boundary:h,rootBoundary:d,padding:c,flipVariations:p,allowedAutoPlacements:m}):i)}),[]),y=e.rects.reference,w=e.rects.popper,E=new Map,A=!0,T=v[0],O=0;O<v.length;O++){var C=v[O],k=Ut(C),L=ce(C)===wt,x=[mt,gt].indexOf(k)>=0,D=x?""width"":""height"",S=ke(e,{placement:C,boundary:h,rootBoundary:d,altBoundary:u,padding:c}),N=x?L?_t:bt:L?gt:mt;y[D]>w[D]&&(N=ge(N));var I=ge(N),P=[];if(o&&P.push(S[k]<=0),a&&P.push(S[N]<=0,S[I]<=0),P.every((function(t){return t}))){T=C,A=!1;break}E.set(C,P)}if(A)for(var j=function(t){var e=v.find((function(e){var i=E.get(e);if(i)return i.slice(0,t).every((function(t){return t}))}));if(e)return T=e,""break""},M=p?3:1;M>0&&""break""!==j(M);M--);e.placement!==T&&(e.modifiersData[n]._skip=!0,e.placement=T,e.reset=!0)}},requiresIfExists:[""offset""],data:{_skip:!1}};function De(t,e,i){return void 0===i&&(i={x:0,y:0}),{top:t.top-e.height-i.y,right:t.right-e.width+i.x,bottom:t.bottom-e.height+i.y,left:t.left-e.width-i.x}}function Se(t){return[mt,_t,gt,bt].some((function(e){return t[e]>=0}))}const Ne={name:""hide"",enabled:!0,phase:""main"",requiresIfExists:[""preventOverflow""],fn:function(t){var e=t.state,i=t.name,n=e.rects.reference,s=e.rects.popper,o=e.modifiersData.preventOverflow,r=ke(e,{elementContext:""reference""}),a=ke(e,{altBoundary:!0}),l=De(r,n),c=De(a,s,o),h=Se(l),d=Se(c);e.modifiersData[i]={referenceClippingOffsets:l,popperEscapeOffsets:c,isReferenceHidden:h,hasPopperEscaped:d},e.attributes.popper=Object.assign({},e.attributes.popper,{""data-popper-reference-hidden"":h,""data-popper-escaped"":d})}},Ie={name:""offset"",enabled:!0,phase:""main"",requires:[""popperOffsets""],fn:function(t){var e=t.state,i=t.options,n=t.name,s=i.offset,o=void 0===s?[0,0]:s,r=Lt.reduce((function(t,i){return t[i]=function(t,e,i){var n=Ut(t),s=[bt,mt].indexOf(n)>=0?-1:1,o=""function""==typeof i?i(Object.assign({},e,{placement:t})):i,r=o[0],a=o[1];return r=r||0,a=(a||0)*s,[bt,_t].indexOf(n)>=0?{x:a,y:r}:{x:r,y:a}}(i,e.rects,o),t}),{}),a=r[e.placement],l=a.x,c=a.y;null!=e.modifiersData.popperOffsets&&(e.modifiersData.popperOffsets.x+=l,e.modifiersData.popperOffsets.y+=c),e.modifiersData[n]=r}},Pe={name:""popperOffsets"",enabled:!0,phase:""read"",fn:function(t){var e=t.state,i=t.name;e.modifiersData[i]=Ce({reference:e.rects.reference,element:e.rects.popper,strategy:""absolute"",placement:e.placement})},data:{}},je={name:""preventOverflow"",enabled:!0,phase:""main"",fn:function(t){var e=t.state,i=t.options,n=t.name,s=i.mainAxis,o=void 0===s||s,r=i.altAxis,a=void 0!==r&&r,l=i.boundary,c=i.rootBoundary,h=i.altBoundary,d=i.padding,u=i.tether,f=void 0===u||u,p=i.tetherOffset,m=void 0===p?0:p,g=ke(e,{boundary:l,rootBoundary:c,padding:d,altBoundary:h}),_=Ut(e.placement),b=ce(e.placement),v=!b,y=ee(_),w=""x""===y?""y"":""x"",E=e.modifiersData.popperOffsets,A=e.rects.reference,T=e.rects.popper,O=""function""==typeof m?m(Object.assign({},e.rects,{placement:e.placement})):m,C={x:0,y:0};if(E){if(o||a){var k=""y""===y?mt:bt,L=""y""===y?gt:_t,x=""y""===y?""height"":""width"",D=E[y],S=E[y]+g[k],N=E[y]-g[L],I=f?-T[x]/2:0,P=b===wt?A[x]:T[x],j=b===wt?-T[x]:-A[x],M=e.elements.arrow,H=f&&M?Kt(M):{width:0,height:0},B=e.modifiersData[""arrow#persistent""]?e.modifiersData[""arrow#persistent""].padding:{top:0,right:0,bottom:0,left:0},R=B[k],W=B[L],$=oe(0,A[x],H[x]),z=v?A[x]/2-I-$-R-O:P-$-R-O,q=v?-A[x]/2+I+$+W+O:j+$+W+O,F=e.elements.arrow&&te(e.elements.arrow),U=F?""y""===y?F.clientTop||0:F.clientLeft||0:0,V=e.modifiersData.offset?e.modifiersData.offset[e.placement][y]:0,K=E[y]+z-V-U,X=E[y]+q-V;if(o){var Y=oe(f?ne(S,K):S,D,f?ie(N,X):N);E[y]=Y,C[y]=Y-D}if(a){var Q=""x""===y?mt:bt,G=""x""===y?gt:_t,Z=E[w],J=Z+g[Q],tt=Z-g[G],et=oe(f?ne(J,K):J,Z,f?ie(tt,X):tt);E[w]=et,C[w]=et-Z}}e.modifiersData[n]=C}},requiresIfExists:[""offset""]};function Me(t,e,i){void 0===i&&(i=!1);var n=zt(e);zt(e)&&function(t){var e=t.getBoundingClientRect();e.width,t.offsetWidth,e.height,t.offsetHeight}(e);var s,o,r=Gt(e),a=Vt(t),l={scrollLeft:0,scrollTop:0},c={x:0,y:0};return(n||!n&&!i)&&((""body""!==Rt(e)||we(r))&&(l=(s=e)!==Wt(s)&&zt(s)?{scrollLeft:(o=s).scrollLeft,scrollTop:o.scrollTop}:ve(s)),zt(e)?((c=Vt(e)).x+=e.clientLeft,c.y+=e.clientTop):r&&(c.x=ye(r))),{x:a.left+l.scrollLeft-c.x,y:a.top+l.scrollTop-c.y,width:a.width,height:a.height}}function He(t){var e=new Map,i=new Set,n=[];function s(t){i.add(t.name),[].concat(t.requires||[],t.requiresIfExists||[]).forEach((function(t){if(!i.has(t)){var n=e.get(t);n&&s(n)}})),n.push(t)}return t.forEach((function(t){e.set(t.name,t)})),t.forEach((function(t){i.has(t.name)||s(t)})),n}var Be={placement:""bottom"",modifiers:[],strategy:""absolute""};function Re(){for(var t=arguments.length,e=new Array(t),i=0;i<t;i++)e[i]=arguments[i];return!e.some((function(t){return!(t&&""function""==typeof t.getBoundingClientRect)}))}function We(t){void 0===t&&(t={});var e=t,i=e.defaultModifiers,n=void 0===i?[]:i,s=e.defaultOptions,o=void 0===s?Be:s;return function(t,e,i){void 0===i&&(i=o);var s,r,a={placement:""bottom"",orderedModifiers:[],options:Object.assign({},Be,o),modifiersData:{},elements:{reference:t,popper:e},attributes:{},styles:{}},l=[],c=!1,h={state:a,setOptions:function(i){var s=""function""==typeof i?i(a.options):i;d(),a.options=Object.assign({},o,a.options,s),a.scrollParents={reference:$t(t)?Ae(t):t.contextElement?Ae(t.contextElement):[],popper:Ae(e)};var r,c,u=function(t){var e=He(t);return Bt.reduce((function(t,i){return t.concat(e.filter((function(t){return t.phase===i})))}),[])}((r=[].concat(n,a.options.modifiers),c=r.reduce((function(t,e){var i=t[e.name];return t[e.name]=i?Object.assign({},i,e,{options:Object.assign({},i.options,e.options),data:Object.assign({},i.data,e.data)}):e,t}),{}),Object.keys(c).map((function(t){return c[t]}))));return a.orderedModifiers=u.filter((function(t){return t.enabled})),a.orderedModifiers.forEach((function(t){var e=t.name,i=t.options,n=void 0===i?{}:i,s=t.effect;if(""function""==typeof s){var o=s({state:a,name:e,instance:h,options:n});l.push(o||function(){})}})),h.update()},forceUpdate:function(){if(!c){var t=a.elements,e=t.reference,i=t.popper;if(Re(e,i)){a.rects={reference:Me(e,te(i),""fixed""===a.options.strategy),popper:Kt(i)},a.reset=!1,a.placement=a.options.placement,a.orderedModifiers.forEach((function(t){return a.modifiersData[t.name]=Object.assign({},t.data)}));for(var n=0;n<a.orderedModifiers.length;n++)if(!0!==a.reset){var s=a.orderedModifiers[n],o=s.fn,r=s.options,l=void 0===r?{}:r,d=s.name;""function""==typeof o&&(a=o({state:a,options:l,name:d,instance:h})||a)}else a.reset=!1,n=-1}}},update:(s=function(){return new Promise((function(t){h.forceUpdate(),t(a)}))},function(){return r||(r=new Promise((function(t){Promise.resolve().then((function(){r=void 0,t(s())}))}))),r}),destroy:function(){d(),c=!0}};if(!Re(t,e))return h;function d(){l.forEach((function(t){return t()})),l=[]}return h.setOptions(i).then((function(t){!c&&i.onFirstUpdate&&i.onFirstUpdate(t)})),h}}var $e=We(),ze=We({defaultModifiers:[pe,Pe,ue,Ft]}),qe=We({defaultModifiers:[pe,Pe,ue,Ft,Ie,xe,je,le,Ne]});const Fe=Object.freeze({__proto__:null,popperGenerator:We,detectOverflow:ke,createPopperBase:$e,createPopper:qe,createPopperLite:ze,top:mt,bottom:gt,right:_t,left:bt,auto:vt,basePlacements:yt,start:wt,end:Et,clippingParents:At,viewport:Tt,popper:Ot,reference:Ct,variationPlacements:kt,placements:Lt,beforeRead:xt,read:Dt,afterRead:St,beforeMain:Nt,main:It,afterMain:Pt,beforeWrite:jt,write:Mt,afterWrite:Ht,modifierPhases:Bt,applyStyles:Ft,arrow:le,computeStyles:ue,eventListeners:pe,flip:xe,hide:Ne,offset:Ie,popperOffsets:Pe,preventOverflow:je}),Ue=""dropdown"",Ve=""Escape"",Ke=""Space"",Xe=""ArrowUp"",Ye=""ArrowDown"",Qe=new RegExp(""ArrowUp|ArrowDown|Escape""),Ge=""click.bs.dropdown.data-api"",Ze=""keydown.bs.dropdown.data-api"",Je=""show"",ti='[data-bs-toggle=""dropdown""]',ei="".dropdown-menu"",ii=m()?""top-end"":""top-start"",ni=m()?""top-start"":""top-end"",si=m()?""bottom-end"":""bottom-start"",oi=m()?""bottom-start"":""bottom-end"",ri=m()?""left-start"":""right-start"",ai=m()?""right-start"":""left-start"",li={offset:[0,2],boundary:""clippingParents"",reference:""toggle"",display:""dynamic"",popperConfig:null,autoClose:!0},ci={offset:""(array|string|function)"",boundary:""(string|element)"",reference:""(string|element|object)"",display:""string"",popperConfig:""(null|object|function)"",autoClose:""(boolean|string)""};class hi extends B{constructor(t,e){super(t),this._popper=null,this._config=this._getConfig(e),this._menu=this._getMenuElement(),this._inNavbar=this._detectNavbar()}static get Default(){return li}static get DefaultType(){return ci}static get NAME(){return Ue}toggle(){return this._isShown()?this.hide():this.show()}show(){if(c(this._element)||this._isShown(this._menu))return;const t={relatedTarget:this._element};if(j.trigger(this._element,""show.bs.dropdown"",t).defaultPrevented)return;const e=hi.getParentFromElement(this._element);this._inNavbar?U.setDataAttribute(this._menu,""popper"",""none""):this._createPopper(e),""ontouchstart""in document.documentElement&&!e.closest("".navbar-nav"")&&[].concat(...document.body.children).forEach((t=>j.on(t,""mouseover"",d))),this._element.focus(),this._element.setAttribute(""aria-expanded"",!0),this._menu.classList.add(Je),this._element.classList.add(Je),j.trigger(this._element,""shown.bs.dropdown"",t)}hide(){if(c(this._element)||!this._isShown(this._menu))return;const t={relatedTarget:this._element};this._completeHide(t)}dispose(){this._popper&&this._popper.destroy(),super.dispose()}update(){this._inNavbar=this._detectNavbar(),this._popper&&this._popper.update()}_completeHide(t){j.trigger(this._element,""hide.bs.dropdown"",t).defaultPrevented||(""ontouchstart""in document.documentElement&&[].concat(...document.body.children).forEach((t=>j.off(t,""mouseover"",d))),this._popper&&this._popper.destroy(),this._menu.classList.remove(Je),this._element.classList.remove(Je),this._element.setAttribute(""aria-expanded"",""false""),U.removeDataAttribute(this._menu,""popper""),j.trigger(this._element,""hidden.bs.dropdown"",t))}_getConfig(t){if(t={...this.constructor.Default,...U.getDataAttributes(this._element),...t},a(Ue,t,this.constructor.DefaultType),""object""==typeof t.reference&&!o(t.reference)&&""function""!=typeof t.reference.getBoundingClientRect)throw new TypeError(`${Ue.toUpperCase()}: Option ""reference"" provided type ""object"" without a required ""getBoundingClientRect"" method.`);return t}_createPopper(t){if(void 0===Fe)throw new TypeError(""Bootstrap's dropdowns require Popper (https://popper.js.org)"");let e=this._element;""parent""===this._config.reference?e=t:o(this._config.reference)?e=r(this._config.reference):""object""==typeof this._config.reference&&(e=this._config.reference);const i=this._getPopperConfig(),n=i.modifiers.find((t=>""applyStyles""===t.name&&!1===t.enabled));this._popper=qe(e,this._menu,i),n&&U.setDataAttribute(this._menu,""popper"",""static"")}_isShown(t=this._element){return t.classList.contains(Je)}_getMenuElement(){return V.next(this._element,ei)[0]}_getPlacement(){const t=this._element.parentNode;if(t.classList.contains(""dropend""))return ri;if(t.classList.contains(""dropstart""))return ai;const e=""end""===getComputedStyle(this._menu).getPropertyValue(""--bs-position"").trim();return t.classList.contains(""dropup"")?e?ni:ii:e?oi:si}_detectNavbar(){return null!==this._element.closest("".navbar"")}_getOffset(){const{offset:t}=this._config;return""string""==typeof t?t.split("","").map((t=>Number.parseInt(t,10))):""function""==typeof t?e=>t(e,this._element):t}_getPopperConfig(){const t={placement:this._getPlacement(),modifiers:[{name:""preventOverflow"",options:{boundary:this._config.boundary}},{name:""offset"",options:{offset:this._getOffset()}}]};return""static""===this._config.display&&(t.modifiers=[{name:""applyStyles"",enabled:!1}]),{...t,...""function""==typeof this._config.popperConfig?this._config.popperConfig(t):this._config.popperConfig}}_selectMenuItem({key:t,target:e}){const i=V.find("".dropdown-menu .dropdown-item:not(.disabled):not(:disabled)"",this._menu).filter(l);i.length&&v(i,e,t===Ye,!i.includes(e)).focus()}static jQueryInterface(t){return this.each((function(){const e=hi.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}static clearMenus(t){if(t&&(2===t.button||""keyup""===t.type&&""Tab""!==t.key))return;const e=V.find(ti);for(let i=0,n=e.length;i<n;i++){const n=hi.getInstance(e[i]);if(!n||!1===n._config.autoClose)continue;if(!n._isShown())continue;const s={relatedTarget:n._element};if(t){const e=t.composedPath(),i=e.includes(n._menu);if(e.includes(n._element)||""inside""===n._config.autoClose&&!i||""outside""===n._config.autoClose&&i)continue;if(n._menu.contains(t.target)&&(""keyup""===t.type&&""Tab""===t.key||/input|select|option|textarea|form/i.test(t.target.tagName)))continue;""click""===t.type&&(s.clickEvent=t)}n._completeHide(s)}}static getParentFromElement(t){return n(t)||t.parentNode}static dataApiKeydownHandler(t){if(/input|textarea/i.test(t.target.tagName)?t.key===Ke||t.key!==Ve&&(t.key!==Ye&&t.key!==Xe||t.target.closest(ei)):!Qe.test(t.key))return;const e=this.classList.contains(Je);if(!e&&t.key===Ve)return;if(t.preventDefault(),t.stopPropagation(),c(this))return;const i=this.matches(ti)?this:V.prev(this,ti)[0],n=hi.getOrCreateInstance(i);if(t.key!==Ve)return t.key===Xe||t.key===Ye?(e||n.show(),void n._selectMenuItem(t)):void(e&&t.key!==Ke||hi.clearMenus());n.hide()}}j.on(document,Ze,ti,hi.dataApiKeydownHandler),j.on(document,Ze,ei,hi.dataApiKeydownHandler),j.on(document,Ge,hi.clearMenus),j.on(document,""keyup.bs.dropdown.data-api"",hi.clearMenus),j.on(document,Ge,ti,(function(t){t.preventDefault(),hi.getOrCreateInstance(this).toggle()})),g(hi);const di="".fixed-top, .fixed-bottom, .is-fixed, .sticky-top"",ui="".sticky-top"";class fi{constructor(){this._element=document.body}getWidth(){const t=document.documentElement.clientWidth;return Math.abs(window.innerWidth-t)}hide(){const t=this.getWidth();this._disableOverFlow(),this._setElementAttributes(this._element,""paddingRight"",(e=>e+t)),this._setElementAttributes(di,""paddingRight"",(e=>e+t)),this._setElementAttributes(ui,""marginRight"",(e=>e-t))}_disableOverFlow(){this._saveInitialAttribute(this._element,""overflow""),this._element.style.overflow=""hidden""}_setElementAttributes(t,e,i){const n=this.getWidth();this._applyManipulationCallback(t,(t=>{if(t!==this._element&&window.innerWidth>t.clientWidth+n)return;this._saveInitialAttribute(t,e);const s=window.getComputedStyle(t)[e];t.style[e]=`${i(Number.parseFloat(s))}px`}))}reset(){this._resetElementAttributes(this._element,""overflow""),this._resetElementAttributes(this._element,""paddingRight""),this._resetElementAttributes(di,""paddingRight""),this._resetElementAttributes(ui,""marginRight"")}_saveInitialAttribute(t,e){const i=t.style[e];i&&U.setDataAttribute(t,e,i)}_resetElementAttributes(t,e){this._applyManipulationCallback(t,(t=>{const i=U.getDataAttribute(t,e);void 0===i?t.style.removeProperty(e):(U.removeDataAttribute(t,e),t.style[e]=i)}))}_applyManipulationCallback(t,e){o(t)?e(t):V.find(t,this._element).forEach(e)}isOverflowing(){return this.getWidth()>0}}const pi={className:""modal-backdrop"",isVisible:!0,isAnimated:!1,rootElement:""body"",clickCallback:null},mi={className:""string"",isVisible:""boolean"",isAnimated:""boolean"",rootElement:""(element|string)"",clickCallback:""(function|null)""},gi=""show"",_i=""mousedown.bs.backdrop"";class bi{constructor(t){this._config=this._getConfig(t),this._isAppended=!1,this._element=null}show(t){this._config.isVisible?(this._append(),this._config.isAnimated&&u(this._getElement()),this._getElement().classList.add(gi),this._emulateAnimation((()=>{_(t)}))):_(t)}hide(t){this._config.isVisible?(this._getElement().classList.remove(gi),this._emulateAnimation((()=>{this.dispose(),_(t)}))):_(t)}_getElement(){if(!this._element){const t=document.createElement(""div"");t.className=this._config.className,this._config.isAnimated&&t.classList.add(""fade""),this._element=t}return this._element}_getConfig(t){return(t={...pi,...""object""==typeof t?t:{}}).rootElement=r(t.rootElement),a(""backdrop"",t,mi),t}_append(){this._isAppended||(this._config.rootElement.append(this._getElement()),j.on(this._getElement(),_i,(()=>{_(this._config.clickCallback)})),this._isAppended=!0)}dispose(){this._isAppended&&(j.off(this._element,_i),this._element.remove(),this._isAppended=!1)}_emulateAnimation(t){b(t,this._getElement(),this._config.isAnimated)}}const vi={trapElement:null,autofocus:!0},yi={trapElement:""element"",autofocus:""boolean""},wi="".bs.focustrap"",Ei=""backward"";class Ai{constructor(t){this._config=this._getConfig(t),this._isActive=!1,this._lastTabNavDirection=null}activate(){const{trapElement:t,autofocus:e}=this._config;this._isActive||(e&&t.focus(),j.off(document,wi),j.on(document,""focusin.bs.focustrap"",(t=>this._handleFocusin(t))),j.on(document,""keydown.tab.bs.focustrap"",(t=>this._handleKeydown(t))),this._isActive=!0)}deactivate(){this._isActive&&(this._isActive=!1,j.off(document,wi))}_handleFocusin(t){const{target:e}=t,{trapElement:i}=this._config;if(e===document||e===i||i.contains(e))return;const n=V.focusableChildren(i);0===n.length?i.focus():this._lastTabNavDirection===Ei?n[n.length-1].focus():n[0].focus()}_handleKeydown(t){""Tab""===t.key&&(this._lastTabNavDirection=t.shiftKey?Ei:""forward"")}_getConfig(t){return t={...vi,...""object""==typeof t?t:{}},a(""focustrap"",t,yi),t}}const Ti=""modal"",Oi=""Escape"",Ci={backdrop:!0,keyboard:!0,focus:!0},ki={backdrop:""(boolean|string)"",keyboard:""boolean"",focus:""boolean""},Li=""hidden.bs.modal"",xi=""show.bs.modal"",Di=""resize.bs.modal"",Si=""click.dismiss.bs.modal"",Ni=""keydown.dismiss.bs.modal"",Ii=""mousedown.dismiss.bs.modal"",Pi=""modal-open"",ji=""show"",Mi=""modal-static"";class Hi extends B{constructor(t,e){super(t),this._config=this._getConfig(e),this._dialog=V.findOne("".modal-dialog"",this._element),this._backdrop=this._initializeBackDrop(),this._focustrap=this._initializeFocusTrap(),this._isShown=!1,this._ignoreBackdropClick=!1,this._isTransitioning=!1,this._scrollBar=new fi}static get Default(){return Ci}static get NAME(){return Ti}toggle(t){return this._isShown?this.hide():this.show(t)}show(t){this._isShown||this._isTransitioning||j.trigger(this._element,xi,{relatedTarget:t}).defaultPrevented||(this._isShown=!0,this._isAnimated()&&(this._isTransitioning=!0),this._scrollBar.hide(),document.body.classList.add(Pi),this._adjustDialog(),this._setEscapeEvent(),this._setResizeEvent(),j.on(this._dialog,Ii,(()=>{j.one(this._element,""mouseup.dismiss.bs.modal"",(t=>{t.target===this._element&&(this._ignoreBackdropClick=!0)}))})),this._showBackdrop((()=>this._showElement(t))))}hide(){if(!this._isShown||this._isTransitioning)return;if(j.trigger(this._element,""hide.bs.modal"").defaultPrevented)return;this._isShown=!1;const t=this._isAnimated();t&&(this._isTransitioning=!0),this._setEscapeEvent(),this._setResizeEvent(),this._focustrap.deactivate(),this._element.classList.remove(ji),j.off(this._element,Si),j.off(this._dialog,Ii),this._queueCallback((()=>this._hideModal()),this._element,t)}dispose(){[window,this._dialog].forEach((t=>j.off(t,"".bs.modal""))),this._backdrop.dispose(),this._focustrap.deactivate(),super.dispose()}handleUpdate(){this._adjustDialog()}_initializeBackDrop(){return new bi({isVisible:Boolean(this._config.backdrop),isAnimated:this._isAnimated()})}_initializeFocusTrap(){return new Ai({trapElement:this._element})}_getConfig(t){return t={...Ci,...U.getDataAttributes(this._element),...""object""==typeof t?t:{}},a(Ti,t,ki),t}_showElement(t){const e=this._isAnimated(),i=V.findOne("".modal-body"",this._dialog);this._element.parentNode&&this._element.parentNode.nodeType===Node.ELEMENT_NODE||document.body.append(this._element),this._element.style.display=""block"",this._element.removeAttribute(""aria-hidden""),this._element.setAttribute(""aria-modal"",!0),this._element.setAttribute(""role"",""dialog""),this._element.scrollTop=0,i&&(i.scrollTop=0),e&&u(this._element),this._element.classList.add(ji),this._queueCallback((()=>{this._config.focus&&this._focustrap.activate(),this._isTransitioning=!1,j.trigger(this._element,""shown.bs.modal"",{relatedTarget:t})}),this._dialog,e)}_setEscapeEvent(){this._isShown?j.on(this._element,Ni,(t=>{this._config.keyboard&&t.key===Oi?(t.preventDefault(),this.hide()):this._config.keyboard||t.key!==Oi||this._triggerBackdropTransition()})):j.off(this._element,Ni)}_setResizeEvent(){this._isShown?j.on(window,Di,(()=>this._adjustDialog())):j.off(window,Di)}_hideModal(){this._element.style.display=""none"",this._element.setAttribute(""aria-hidden"",!0),this._element.removeAttribute(""aria-modal""),this._element.removeAttribute(""role""),this._isTransitioning=!1,this._backdrop.hide((()=>{document.body.classList.remove(Pi),this._resetAdjustments(),this._scrollBar.reset(),j.trigger(this._element,Li)}))}_showBackdrop(t){j.on(this._element,Si,(t=>{this._ignoreBackdropClick?this._ignoreBackdropClick=!1:t.target===t.currentTarget&&(!0===this._config.backdrop?this.hide():""static""===this._config.backdrop&&this._triggerBackdropTransition())})),this._backdrop.show(t)}_isAnimated(){return this._element.classList.contains(""fade"")}_triggerBackdropTransition(){if(j.trigger(this._element,""hidePrevented.bs.modal"").defaultPrevented)return;const{classList:t,scrollHeight:e,style:i}=this._element,n=e>document.documentElement.clientHeight;!n&&""hidden""===i.overflowY||t.contains(Mi)||(n||(i.overflowY=""hidden""),t.add(Mi),this._queueCallback((()=>{t.remove(Mi),n||this._queueCallback((()=>{i.overflowY=""""}),this._dialog)}),this._dialog),this._element.focus())}_adjustDialog(){const t=this._element.scrollHeight>document.documentElement.clientHeight,e=this._scrollBar.getWidth(),i=e>0;(!i&&t&&!m()||i&&!t&&m())&&(this._element.style.paddingLeft=`${e}px`),(i&&!t&&!m()||!i&&t&&m())&&(this._element.style.paddingRight=`${e}px`)}_resetAdjustments(){this._element.style.paddingLeft="""",this._element.style.paddingRight=""""}static jQueryInterface(t,e){return this.each((function(){const i=Hi.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===i[t])throw new TypeError(`No method named ""${t}""`);i[t](e)}}))}}j.on(document,""click.bs.modal.data-api"",'[data-bs-toggle=""modal""]',(function(t){const e=n(this);[""A"",""AREA""].includes(this.tagName)&&t.preventDefault(),j.one(e,xi,(t=>{t.defaultPrevented||j.one(e,Li,(()=>{l(this)&&this.focus()}))}));const i=V.findOne("".modal.show"");i&&Hi.getInstance(i).hide(),Hi.getOrCreateInstance(e).toggle(this)})),R(Hi),g(Hi);const Bi=""offcanvas"",Ri={backdrop:!0,keyboard:!0,scroll:!1},Wi={backdrop:""boolean"",keyboard:""boolean"",scroll:""boolean""},$i=""show"",zi="".offcanvas.show"",qi=""hidden.bs.offcanvas"";class Fi extends B{constructor(t,e){super(t),this._config=this._getConfig(e),this._isShown=!1,this._backdrop=this._initializeBackDrop(),this._focustrap=this._initializeFocusTrap(),this._addEventListeners()}static get NAME(){return Bi}static get Default(){return Ri}toggle(t){return this._isShown?this.hide():this.show(t)}show(t){this._isShown||j.trigger(this._element,""show.bs.offcanvas"",{relatedTarget:t}).defaultPrevented||(this._isShown=!0,this._element.style.visibility=""visible"",this._backdrop.show(),this._config.scroll||(new fi).hide(),this._element.removeAttribute(""aria-hidden""),this._element.setAttribute(""aria-modal"",!0),this._element.setAttribute(""role"",""dialog""),this._element.classList.add($i),this._queueCallback((()=>{this._config.scroll||this._focustrap.activate(),j.trigger(this._element,""shown.bs.offcanvas"",{relatedTarget:t})}),this._element,!0))}hide(){this._isShown&&(j.trigger(this._element,""hide.bs.offcanvas"").defaultPrevented||(this._focustrap.deactivate(),this._element.blur(),this._isShown=!1,this._element.classList.remove($i),this._backdrop.hide(),this._queueCallback((()=>{this._element.setAttribute(""aria-hidden"",!0),this._element.removeAttribute(""aria-modal""),this._element.removeAttribute(""role""),this._element.style.visibility=""hidden"",this._config.scroll||(new fi).reset(),j.trigger(this._element,qi)}),this._element,!0)))}dispose(){this._backdrop.dispose(),this._focustrap.deactivate(),super.dispose()}_getConfig(t){return t={...Ri,...U.getDataAttributes(this._element),...""object""==typeof t?t:{}},a(Bi,t,Wi),t}_initializeBackDrop(){return new bi({className:""offcanvas-backdrop"",isVisible:this._config.backdrop,isAnimated:!0,rootElement:this._element.parentNode,clickCallback:()=>this.hide()})}_initializeFocusTrap(){return new Ai({trapElement:this._element})}_addEventListeners(){j.on(this._element,""keydown.dismiss.bs.offcanvas"",(t=>{this._config.keyboard&&""Escape""===t.key&&this.hide()}))}static jQueryInterface(t){return this.each((function(){const e=Fi.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t]||t.startsWith(""_"")||""constructor""===t)throw new TypeError(`No method named ""${t}""`);e[t](this)}}))}}j.on(document,""click.bs.offcanvas.data-api"",'[data-bs-toggle=""offcanvas""]',(function(t){const e=n(this);if([""A"",""AREA""].includes(this.tagName)&&t.preventDefault(),c(this))return;j.one(e,qi,(()=>{l(this)&&this.focus()}));const i=V.findOne(zi);i&&i!==e&&Fi.getInstance(i).hide(),Fi.getOrCreateInstance(e).toggle(this)})),j.on(window,""load.bs.offcanvas.data-api"",(()=>V.find(zi).forEach((t=>Fi.getOrCreateInstance(t).show())))),R(Fi),g(Fi);const Ui=new Set([""background"",""cite"",""href"",""itemtype"",""longdesc"",""poster"",""src"",""xlink:href""]),Vi=/^(?:(?:https?|mailto|ftp|tel|file|sms):|[^#&/:?]*(?:[#/?]|$))/i,Ki=/^data:(?:image\/(?:bmp|gif|jpeg|jpg|png|tiff|webp)|video\/(?:mpeg|mp4|ogg|webm)|audio\/(?:mp3|oga|ogg|opus));base64,[\d+/a-z]+=*$/i,Xi=(t,e)=>{const i=t.nodeName.toLowerCase();if(e.includes(i))return!Ui.has(i)||Boolean(Vi.test(t.nodeValue)||Ki.test(t.nodeValue));const n=e.filter((t=>t instanceof RegExp));for(let t=0,e=n.length;t<e;t++)if(n[t].test(i))return!0;return!1};function Yi(t,e,i){if(!t.length)return t;if(i&&""function""==typeof i)return i(t);const n=(new window.DOMParser).parseFromString(t,""text/html""),s=[].concat(...n.body.querySelectorAll(""*""));for(let t=0,i=s.length;t<i;t++){const i=s[t],n=i.nodeName.toLowerCase();if(!Object.keys(e).includes(n)){i.remove();continue}const o=[].concat(...i.attributes),r=[].concat(e[""*""]||[],e[n]||[]);o.forEach((t=>{Xi(t,r)||i.removeAttribute(t.nodeName)}))}return n.body.innerHTML}const Qi=""tooltip"",Gi=new Set([""sanitize"",""allowList"",""sanitizeFn""]),Zi={animation:""boolean"",template:""string"",title:""(string|element|function)"",trigger:""string"",delay:""(number|object)"",html:""boolean"",selector:""(string|boolean)"",placement:""(string|function)"",offset:""(array|string|function)"",container:""(string|element|boolean)"",fallbackPlacements:""array"",boundary:""(string|element)"",customClass:""(string|function)"",sanitize:""boolean"",sanitizeFn:""(null|function)"",allowList:""object"",popperConfig:""(null|object|function)""},Ji={AUTO:""auto"",TOP:""top"",RIGHT:m()?""left"":""right"",BOTTOM:""bottom"",LEFT:m()?""right"":""left""},tn={animation:!0,template:'<div class=""tooltip"" role=""tooltip""><div class=""tooltip-arrow""></div><div class=""tooltip-inner""></div></div>',trigger:""hover focus"",title:"""",delay:0,html:!1,selector:!1,placement:""top"",offset:[0,0],container:!1,fallbackPlacements:[""top"",""right"",""bottom"",""left""],boundary:""clippingParents"",customClass:"""",sanitize:!0,sanitizeFn:null,allowList:{""*"":[""class"",""dir"",""id"",""lang"",""role"",/^aria-[\w-]*$/i],a:[""target"",""href"",""title"",""rel""],area:[],b:[],br:[],col:[],code:[],div:[],em:[],hr:[],h1:[],h2:[],h3:[],h4:[],h5:[],h6:[],i:[],img:[""src"",""srcset"",""alt"",""title"",""width"",""height""],li:[],ol:[],p:[],pre:[],s:[],small:[],span:[],sub:[],sup:[],strong:[],u:[],ul:[]},popperConfig:null},en={HIDE:""hide.bs.tooltip"",HIDDEN:""hidden.bs.tooltip"",SHOW:""show.bs.tooltip"",SHOWN:""shown.bs.tooltip"",INSERTED:""inserted.bs.tooltip"",CLICK:""click.bs.tooltip"",FOCUSIN:""focusin.bs.tooltip"",FOCUSOUT:""focusout.bs.tooltip"",MOUSEENTER:""mouseenter.bs.tooltip"",MOUSELEAVE:""mouseleave.bs.tooltip""},nn=""fade"",sn=""show"",on=""show"",rn=""out"",an="".tooltip-inner"",ln="".modal"",cn=""hide.bs.modal"",hn=""hover"",dn=""focus"";class un extends B{constructor(t,e){if(void 0===Fe)throw new TypeError(""Bootstrap's tooltips require Popper (https://popper.js.org)"");super(t),this._isEnabled=!0,this._timeout=0,this._hoverState="""",this._activeTrigger={},this._popper=null,this._config=this._getConfig(e),this.tip=null,this._setListeners()}static get Default(){return tn}static get NAME(){return Qi}static get Event(){return en}static get DefaultType(){return Zi}enable(){this._isEnabled=!0}disable(){this._isEnabled=!1}toggleEnabled(){this._isEnabled=!this._isEnabled}toggle(t){if(this._isEnabled)if(t){const e=this._initializeOnDelegatedTarget(t);e._activeTrigger.click=!e._activeTrigger.click,e._isWithActiveTrigger()?e._enter(null,e):e._leave(null,e)}else{if(this.getTipElement().classList.contains(sn))return void this._leave(null,this);this._enter(null,this)}}dispose(){clearTimeout(this._timeout),j.off(this._element.closest(ln),cn,this._hideModalHandler),this.tip&&this.tip.remove(),this._disposePopper(),super.dispose()}show(){if(""none""===this._element.style.display)throw new Error(""Please use show on visible elements"");if(!this.isWithContent()||!this._isEnabled)return;const t=j.trigger(this._element,this.constructor.Event.SHOW),e=h(this._element),i=null===e?this._element.ownerDocument.documentElement.contains(this._element):e.contains(this._element);if(t.defaultPrevented||!i)return;""tooltip""===this.constructor.NAME&&this.tip&&this.getTitle()!==this.tip.querySelector(an).innerHTML&&(this._disposePopper(),this.tip.remove(),this.tip=null);const n=this.getTipElement(),s=(t=>{do{t+=Math.floor(1e6*Math.random())}while(document.getElementById(t));return t})(this.constructor.NAME);n.setAttribute(""id"",s),this._element.setAttribute(""aria-describedby"",s),this._config.animation&&n.classList.add(nn);const o=""function""==typeof this._config.placement?this._config.placement.call(this,n,this._element):this._config.placement,r=this._getAttachment(o);this._addAttachmentClass(r);const{container:a}=this._config;H.set(n,this.constructor.DATA_KEY,this),this._element.ownerDocument.documentElement.contains(this.tip)||(a.append(n),j.trigger(this._element,this.constructor.Event.INSERTED)),this._popper?this._popper.update():this._popper=qe(this._element,n,this._getPopperConfig(r)),n.classList.add(sn);const l=this._resolvePossibleFunction(this._config.customClass);l&&n.classList.add(...l.split("" "")),""ontouchstart""in document.documentElement&&[].concat(...document.body.children).forEach((t=>{j.on(t,""mouseover"",d)}));const c=this.tip.classList.contains(nn);this._queueCallback((()=>{const t=this._hoverState;this._hoverState=null,j.trigger(this._element,this.constructor.Event.SHOWN),t===rn&&this._leave(null,this)}),this.tip,c)}hide(){if(!this._popper)return;const t=this.getTipElement();if(j.trigger(this._element,this.constructor.Event.HIDE).defaultPrevented)return;t.classList.remove(sn),""ontouchstart""in document.documentElement&&[].concat(...document.body.children).forEach((t=>j.off(t,""mouseover"",d))),this._activeTrigger.click=!1,this._activeTrigger.focus=!1,this._activeTrigger.hover=!1;const e=this.tip.classList.contains(nn);this._queueCallback((()=>{this._isWithActiveTrigger()||(this._hoverState!==on&&t.remove(),this._cleanTipClass(),this._element.removeAttribute(""aria-describedby""),j.trigger(this._element,this.constructor.Event.HIDDEN),this._disposePopper())}),this.tip,e),this._hoverState=""""}update(){null!==this._popper&&this._popper.update()}isWithContent(){return Boolean(this.getTitle())}getTipElement(){if(this.tip)return this.tip;const t=document.createElement(""div"");t.innerHTML=this._config.template;const e=t.children[0];return this.setContent(e),e.classList.remove(nn,sn),this.tip=e,this.tip}setContent(t){this._sanitizeAndSetContent(t,this.getTitle(),an)}_sanitizeAndSetContent(t,e,i){const n=V.findOne(i,t);e||!n?this.setElementContent(n,e):n.remove()}setElementContent(t,e){if(null!==t)return o(e)?(e=r(e),void(this._config.html?e.parentNode!==t&&(t.innerHTML="""",t.append(e)):t.textContent=e.textContent)):void(this._config.html?(this._config.sanitize&&(e=Yi(e,this._config.allowList,this._config.sanitizeFn)),t.innerHTML=e):t.textContent=e)}getTitle(){const t=this._element.getAttribute(""data-bs-original-title"")||this._config.title;return this._resolvePossibleFunction(t)}updateAttachment(t){return""right""===t?""end"":""left""===t?""start"":t}_initializeOnDelegatedTarget(t,e){return e||this.constructor.getOrCreateInstance(t.delegateTarget,this._getDelegateConfig())}_getOffset(){const{offset:t}=this._config;return""string""==typeof t?t.split("","").map((t=>Number.parseInt(t,10))):""function""==typeof t?e=>t(e,this._element):t}_resolvePossibleFunction(t){return""function""==typeof t?t.call(this._element):t}_getPopperConfig(t){const e={placement:t,modifiers:[{name:""flip"",options:{fallbackPlacements:this._config.fallbackPlacements}},{name:""offset"",options:{offset:this._getOffset()}},{name:""preventOverflow"",options:{boundary:this._config.boundary}},{name:""arrow"",options:{element:`.${this.constructor.NAME}-arrow`}},{name:""onChange"",enabled:!0,phase:""afterWrite"",fn:t=>this._handlePopperPlacementChange(t)}],onFirstUpdate:t=>{t.options.placement!==t.placement&&this._handlePopperPlacementChange(t)}};return{...e,...""function""==typeof this._config.popperConfig?this._config.popperConfig(e):this._config.popperConfig}}_addAttachmentClass(t){this.getTipElement().classList.add(`${this._getBasicClassPrefix()}-${this.updateAttachment(t)}`)}_getAttachment(t){return Ji[t.toUpperCase()]}_setListeners(){this._config.trigger.split("" "").forEach((t=>{if(""click""===t)j.on(this._element,this.constructor.Event.CLICK,this._config.selector,(t=>this.toggle(t)));else if(""manual""!==t){const e=t===hn?this.constructor.Event.MOUSEENTER:this.constructor.Event.FOCUSIN,i=t===hn?this.constructor.Event.MOUSELEAVE:this.constructor.Event.FOCUSOUT;j.on(this._element,e,this._config.selector,(t=>this._enter(t))),j.on(this._element,i,this._config.selector,(t=>this._leave(t)))}})),this._hideModalHandler=()=>{this._element&&this.hide()},j.on(this._element.closest(ln),cn,this._hideModalHandler),this._config.selector?this._config={...this._config,trigger:""manual"",selector:""""}:this._fixTitle()}_fixTitle(){const t=this._element.getAttribute(""title""),e=typeof this._element.getAttribute(""data-bs-original-title"");(t||""string""!==e)&&(this._element.setAttribute(""data-bs-original-title"",t||""""),!t||this._element.getAttribute(""aria-label"")||this._element.textContent||this._element.setAttribute(""aria-label"",t),this._element.setAttribute(""title"",""""))}_enter(t,e){e=this._initializeOnDelegatedTarget(t,e),t&&(e._activeTrigger[""focusin""===t.type?dn:hn]=!0),e.getTipElement().classList.contains(sn)||e._hoverState===on?e._hoverState=on:(clearTimeout(e._timeout),e._hoverState=on,e._config.delay&&e._config.delay.show?e._timeout=setTimeout((()=>{e._hoverState===on&&e.show()}),e._config.delay.show):e.show())}_leave(t,e){e=this._initializeOnDelegatedTarget(t,e),t&&(e._activeTrigger[""focusout""===t.type?dn:hn]=e._element.contains(t.relatedTarget)),e._isWithActiveTrigger()||(clearTimeout(e._timeout),e._hoverState=rn,e._config.delay&&e._config.delay.hide?e._timeout=setTimeout((()=>{e._hoverState===rn&&e.hide()}),e._config.delay.hide):e.hide())}_isWithActiveTrigger(){for(const t in this._activeTrigger)if(this._activeTrigger[t])return!0;return!1}_getConfig(t){const e=U.getDataAttributes(this._element);return Object.keys(e).forEach((t=>{Gi.has(t)&&delete e[t]})),(t={...this.constructor.Default,...e,...""object""==typeof t&&t?t:{}}).container=!1===t.container?document.body:r(t.container),""number""==typeof t.delay&&(t.delay={show:t.delay,hide:t.delay}),""number""==typeof t.title&&(t.title=t.title.toString()),""number""==typeof t.content&&(t.content=t.content.toString()),a(Qi,t,this.constructor.DefaultType),t.sanitize&&(t.template=Yi(t.template,t.allowList,t.sanitizeFn)),t}_getDelegateConfig(){const t={};for(const e in this._config)this.constructor.Default[e]!==this._config[e]&&(t[e]=this._config[e]);return t}_cleanTipClass(){const t=this.getTipElement(),e=new RegExp(`(^|\\s)${this._getBasicClassPrefix()}\\S+`,""g""),i=t.getAttribute(""class"").match(e);null!==i&&i.length>0&&i.map((t=>t.trim())).forEach((e=>t.classList.remove(e)))}_getBasicClassPrefix(){return""bs-tooltip""}_handlePopperPlacementChange(t){const{state:e}=t;e&&(this.tip=e.elements.popper,this._cleanTipClass(),this._addAttachmentClass(this._getAttachment(e.placement)))}_disposePopper(){this._popper&&(this._popper.destroy(),this._popper=null)}static jQueryInterface(t){return this.each((function(){const e=un.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}g(un);const fn={...un.Default,placement:""right"",offset:[0,8],trigger:""click"",content:"""",template:'<div class=""popover"" role=""tooltip""><div class=""popover-arrow""></div><h3 class=""popover-header""></h3><div class=""popover-body""></div></div>'},pn={...un.DefaultType,content:""(string|element|function)""},mn={HIDE:""hide.bs.popover"",HIDDEN:""hidden.bs.popover"",SHOW:""show.bs.popover"",SHOWN:""shown.bs.popover"",INSERTED:""inserted.bs.popover"",CLICK:""click.bs.popover"",FOCUSIN:""focusin.bs.popover"",FOCUSOUT:""focusout.bs.popover"",MOUSEENTER:""mouseenter.bs.popover"",MOUSELEAVE:""mouseleave.bs.popover""};class gn extends un{static get Default(){return fn}static get NAME(){return""popover""}static get Event(){return mn}static get DefaultType(){return pn}isWithContent(){return this.getTitle()||this._getContent()}setContent(t){this._sanitizeAndSetContent(t,this.getTitle(),"".popover-header""),this._sanitizeAndSetContent(t,this._getContent(),"".popover-body"")}_getContent(){return this._resolvePossibleFunction(this._config.content)}_getBasicClassPrefix(){return""bs-popover""}static jQueryInterface(t){return this.each((function(){const e=gn.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}g(gn);const _n=""scrollspy"",bn={offset:10,method:""auto"",target:""""},vn={offset:""number"",method:""string"",target:""(string|element)""},yn=""active"",wn="".nav-link, .list-group-item, .dropdown-item"",En=""position"";class An extends B{constructor(t,e){super(t),this._scrollElement=""BODY""===this._element.tagName?window:this._element,this._config=this._getConfig(e),this._offsets=[],this._targets=[],this._activeTarget=null,this._scrollHeight=0,j.on(this._scrollElement,""scroll.bs.scrollspy"",(()=>this._process())),this.refresh(),this._process()}static get Default(){return bn}static get NAME(){return _n}refresh(){const t=this._scrollElement===this._scrollElement.window?""offset"":En,e=""auto""===this._config.method?t:this._config.method,n=e===En?this._getScrollTop():0;this._offsets=[],this._targets=[],this._scrollHeight=this._getScrollHeight(),V.find(wn,this._config.target).map((t=>{const s=i(t),o=s?V.findOne(s):null;if(o){const t=o.getBoundingClientRect();if(t.width||t.height)return[U[e](o).top+n,s]}return null})).filter((t=>t)).sort(((t,e)=>t[0]-e[0])).forEach((t=>{this._offsets.push(t[0]),this._targets.push(t[1])}))}dispose(){j.off(this._scrollElement,"".bs.scrollspy""),super.dispose()}_getConfig(t){return(t={...bn,...U.getDataAttributes(this._element),...""object""==typeof t&&t?t:{}}).target=r(t.target)||document.documentElement,a(_n,t,vn),t}_getScrollTop(){return this._scrollElement===window?this._scrollElement.pageYOffset:this._scrollElement.scrollTop}_getScrollHeight(){return this._scrollElement.scrollHeight||Math.max(document.body.scrollHeight,document.documentElement.scrollHeight)}_getOffsetHeight(){return this._scrollElement===window?window.innerHeight:this._scrollElement.getBoundingClientRect().height}_process(){const t=this._getScrollTop()+this._config.offset,e=this._getScrollHeight(),i=this._config.offset+e-this._getOffsetHeight();if(this._scrollHeight!==e&&this.refresh(),t>=i){const t=this._targets[this._targets.length-1];this._activeTarget!==t&&this._activate(t)}else{if(this._activeTarget&&t<this._offsets[0]&&this._offsets[0]>0)return this._activeTarget=null,void this._clear();for(let e=this._offsets.length;e--;)this._activeTarget!==this._targets[e]&&t>=this._offsets[e]&&(void 0===this._offsets[e+1]||t<this._offsets[e+1])&&this._activate(this._targets[e])}}_activate(t){this._activeTarget=t,this._clear();const e=wn.split("","").map((e=>`${e}[data-bs-target=""${t}""],${e}[href=""${t}""]`)),i=V.findOne(e.join("",""),this._config.target);i.classList.add(yn),i.classList.contains(""dropdown-item"")?V.findOne("".dropdown-toggle"",i.closest("".dropdown"")).classList.add(yn):V.parents(i,"".nav, .list-group"").forEach((t=>{V.prev(t,"".nav-link, .list-group-item"").forEach((t=>t.classList.add(yn))),V.prev(t,"".nav-item"").forEach((t=>{V.children(t,"".nav-link"").forEach((t=>t.classList.add(yn)))}))})),j.trigger(this._scrollElement,""activate.bs.scrollspy"",{relatedTarget:t})}_clear(){V.find(wn,this._config.target).filter((t=>t.classList.contains(yn))).forEach((t=>t.classList.remove(yn)))}static jQueryInterface(t){return this.each((function(){const e=An.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}j.on(window,""load.bs.scrollspy.data-api"",(()=>{V.find('[data-bs-spy=""scroll""]').forEach((t=>new An(t)))})),g(An);const Tn=""active"",On=""fade"",Cn=""show"",kn="".active"",Ln="":scope > li > .active"";class xn extends B{static get NAME(){return""tab""}show(){if(this._element.parentNode&&this._element.parentNode.nodeType===Node.ELEMENT_NODE&&this._element.classList.contains(Tn))return;let t;const e=n(this._element),i=this._element.closest("".nav, .list-group"");if(i){const e=""UL""===i.nodeName||""OL""===i.nodeName?Ln:kn;t=V.find(e,i),t=t[t.length-1]}const s=t?j.trigger(t,""hide.bs.tab"",{relatedTarget:this._element}):null;if(j.trigger(this._element,""show.bs.tab"",{relatedTarget:t}).defaultPrevented||null!==s&&s.defaultPrevented)return;this._activate(this._element,i);const o=()=>{j.trigger(t,""hidden.bs.tab"",{relatedTarget:this._element}),j.trigger(this._element,""shown.bs.tab"",{relatedTarget:t})};e?this._activate(e,e.parentNode,o):o()}_activate(t,e,i){const n=(!e||""UL""!==e.nodeName&&""OL""!==e.nodeName?V.children(e,kn):V.find(Ln,e))[0],s=i&&n&&n.classList.contains(On),o=()=>this._transitionComplete(t,n,i);n&&s?(n.classList.remove(Cn),this._queueCallback(o,t,!0)):o()}_transitionComplete(t,e,i){if(e){e.classList.remove(Tn);const t=V.findOne("":scope > .dropdown-menu .active"",e.parentNode);t&&t.classList.remove(Tn),""tab""===e.getAttribute(""role"")&&e.setAttribute(""aria-selected"",!1)}t.classList.add(Tn),""tab""===t.getAttribute(""role"")&&t.setAttribute(""aria-selected"",!0),u(t),t.classList.contains(On)&&t.classList.add(Cn);let n=t.parentNode;if(n&&""LI""===n.nodeName&&(n=n.parentNode),n&&n.classList.contains(""dropdown-menu"")){const e=t.closest("".dropdown"");e&&V.find("".dropdown-toggle"",e).forEach((t=>t.classList.add(Tn))),t.setAttribute(""aria-expanded"",!0)}i&&i()}static jQueryInterface(t){return this.each((function(){const e=xn.getOrCreateInstance(this);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t]()}}))}}j.on(document,""click.bs.tab.data-api"",'[data-bs-toggle=""tab""], [data-bs-toggle=""pill""], [data-bs-toggle=""list""]',(function(t){[""A"",""AREA""].includes(this.tagName)&&t.preventDefault(),c(this)||xn.getOrCreateInstance(this).show()})),g(xn);const Dn=""toast"",Sn=""hide"",Nn=""show"",In=""showing"",Pn={animation:""boolean"",autohide:""boolean"",delay:""number""},jn={animation:!0,autohide:!0,delay:5e3};class Mn extends B{constructor(t,e){super(t),this._config=this._getConfig(e),this._timeout=null,this._hasMouseInteraction=!1,this._hasKeyboardInteraction=!1,this._setListeners()}static get DefaultType(){return Pn}static get Default(){return jn}static get NAME(){return Dn}show(){j.trigger(this._element,""show.bs.toast"").defaultPrevented||(this._clearTimeout(),this._config.animation&&this._element.classList.add(""fade""),this._element.classList.remove(Sn),u(this._element),this._element.classList.add(Nn),this._element.classList.add(In),this._queueCallback((()=>{this._element.classList.remove(In),j.trigger(this._element,""shown.bs.toast""),this._maybeScheduleHide()}),this._element,this._config.animation))}hide(){this._element.classList.contains(Nn)&&(j.trigger(this._element,""hide.bs.toast"").defaultPrevented||(this._element.classList.add(In),this._queueCallback((()=>{this._element.classList.add(Sn),this._element.classList.remove(In),this._element.classList.remove(Nn),j.trigger(this._element,""hidden.bs.toast"")}),this._element,this._config.animation)))}dispose(){this._clearTimeout(),this._element.classList.contains(Nn)&&this._element.classList.remove(Nn),super.dispose()}_getConfig(t){return t={...jn,...U.getDataAttributes(this._element),...""object""==typeof t&&t?t:{}},a(Dn,t,this.constructor.DefaultType),t}_maybeScheduleHide(){this._config.autohide&&(this._hasMouseInteraction||this._hasKeyboardInteraction||(this._timeout=setTimeout((()=>{this.hide()}),this._config.delay)))}_onInteraction(t,e){switch(t.type){case""mouseover"":case""mouseout"":this._hasMouseInteraction=e;break;case""focusin"":case""focusout"":this._hasKeyboardInteraction=e}if(e)return void this._clearTimeout();const i=t.relatedTarget;this._element===i||this._element.contains(i)||this._maybeScheduleHide()}_setListeners(){j.on(this._element,""mouseover.bs.toast"",(t=>this._onInteraction(t,!0))),j.on(this._element,""mouseout.bs.toast"",(t=>this._onInteraction(t,!1))),j.on(this._element,""focusin.bs.toast"",(t=>this._onInteraction(t,!0))),j.on(this._element,""focusout.bs.toast"",(t=>this._onInteraction(t,!1)))}_clearTimeout(){clearTimeout(this._timeout),this._timeout=null}static jQueryInterface(t){return this.each((function(){const e=Mn.getOrCreateInstance(this,t);if(""string""==typeof t){if(void 0===e[t])throw new TypeError(`No method named ""${t}""`);e[t](this)}}))}}return R(Mn),g(Mn),{Alert:W,Button:z,Carousel:st,Collapse:pt,Dropdown:hi,Modal:Hi,Offcanvas:Fi,Popover:gn,ScrollSpy:An,Tab:xn,Toast:Mn,Tooltip:un}}));
-//# sourceMappingURL=bootstrap.bundle.min.js.map
\ No newline at end of file

---FILE: session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/quarto-html/anchor.min.js---
@@ -1,9 +0,0 @@
-// @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&dn=expat.txt Expat
-//
-// AnchorJS - v4.3.1 - 2021-04-17
-// https://www.bryanbraun.com/anchorjs/
-// Copyright (c) 2021 Bryan Braun; Licensed MIT
-//
-// @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&dn=expat.txt Expat
-!function(A,e){""use strict"";""function""==typeof define&&define.amd?define([],e):""object""==typeof module&&module.exports?module.exports=e():(A.AnchorJS=e(),A.anchors=new A.AnchorJS)}(this,function(){""use strict"";return function(A){function d(A){A.icon=Object.prototype.hasOwnProperty.call(A,""icon"")?A.icon:"""",A.visible=Object.prototype.hasOwnProperty.call(A,""visible"")?A.visible:""hover"",A.placement=Object.prototype.hasOwnProperty.call(A,""placement"")?A.placement:""right"",A.ariaLabel=Object.prototype.hasOwnProperty.call(A,""ariaLabel"")?A.ariaLabel:""Anchor"",A.class=Object.prototype.hasOwnProperty.call(A,""class"")?A.class:"""",A.base=Object.prototype.hasOwnProperty.call(A,""base"")?A.base:"""",A.truncate=Object.prototype.hasOwnProperty.call(A,""truncate"")?Math.floor(A.truncate):64,A.titleText=Object.prototype.hasOwnProperty.call(A,""titleText"")?A.titleText:""""}function w(A){var e;if(""string""==typeof A||A instanceof String)e=[].slice.call(document.querySelectorAll(A));else{if(!(Array.isArray(A)||A instanceof NodeList))throw new TypeError(""The selector provided to AnchorJS was invalid."");e=[].slice.call(A)}return e}this.options=A||{},this.elements=[],d(this.options),this.isTouchDevice=function(){return Boolean(""ontouchstart""in window||window.TouchEvent||window.DocumentTouch&&document instanceof DocumentTouch)},this.add=function(A){var e,t,o,i,n,s,a,c,r,l,h,u,p=[];if(d(this.options),""touch""===(l=this.options.visible)&&(l=this.isTouchDevice()?""always"":""hover""),0===(e=w(A=A||""h2, h3, h4, h5, h6"")).length)return this;for(null===document.head.querySelector(""style.anchorjs"")&&((u=document.createElement(""style"")).className=""anchorjs"",u.appendChild(document.createTextNode("""")),void 0===(A=document.head.querySelector('[rel=""stylesheet""],style'))?document.head.appendChild(u):document.head.insertBefore(u,A),u.sheet.insertRule("".anchorjs-link{opacity:0;text-decoration:none;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}"",u.sheet.cssRules.length),u.sheet.insertRule("":hover>.anchorjs-link,.anchorjs-link:focus{opacity:1}"",u.sheet.cssRules.length),u.sheet.insertRule(""[data-anchorjs-icon]::after{content:attr(data-anchorjs-icon)}"",u.sheet.cssRules.length),u.sheet.insertRule('@font-face{font-family:anchorjs-icons;src:url(data:n/a;base64,AAEAAAALAIAAAwAwT1MvMg8yG2cAAAE4AAAAYGNtYXDp3gC3AAABpAAAAExnYXNwAAAAEAAAA9wAAAAIZ2x5ZlQCcfwAAAH4AAABCGhlYWQHFvHyAAAAvAAAADZoaGVhBnACFwAAAPQAAAAkaG10eASAADEAAAGYAAAADGxvY2EACACEAAAB8AAAAAhtYXhwAAYAVwAAARgAAAAgbmFtZQGOH9cAAAMAAAAAunBvc3QAAwAAAAADvAAAACAAAQAAAAEAAHzE2p9fDzz1AAkEAAAAAADRecUWAAAAANQA6R8AAAAAAoACwAAAAAgAAgAAAAAAAAABAAADwP/AAAACgAAA/9MCrQABAAAAAAAAAAAAAAAAAAAAAwABAAAAAwBVAAIAAAAAAAIAAAAAAAAAAAAAAAAAAAAAAAMCQAGQAAUAAAKZAswAAACPApkCzAAAAesAMwEJAAAAAAAAAAAAAAAAAAAAARAAAAAAAAAAAAAAAAAAAAAAQAAg//0DwP/AAEADwABAAAAAAQAAAAAAAAAAAAAAIAAAAAAAAAIAAAACgAAxAAAAAwAAAAMAAAAcAAEAAwAAABwAAwABAAAAHAAEADAAAAAIAAgAAgAAACDpy//9//8AAAAg6cv//f///+EWNwADAAEAAAAAAAAAAAAAAAAACACEAAEAAAAAAAAAAAAAAAAxAAACAAQARAKAAsAAKwBUAAABIiYnJjQ3NzY2MzIWFxYUBwcGIicmNDc3NjQnJiYjIgYHBwYUFxYUBwYGIwciJicmNDc3NjIXFhQHBwYUFxYWMzI2Nzc2NCcmNDc2MhcWFAcHBgYjARQGDAUtLXoWOR8fORYtLTgKGwoKCjgaGg0gEhIgDXoaGgkJBQwHdR85Fi0tOAobCgoKOBoaDSASEiANehoaCQkKGwotLXoWOR8BMwUFLYEuehYXFxYugC44CQkKGwo4GkoaDQ0NDXoaShoKGwoFBe8XFi6ALjgJCQobCjgaShoNDQ0NehpKGgobCgoKLYEuehYXAAAADACWAAEAAAAAAAEACAAAAAEAAAAAAAIAAwAIAAEAAAAAAAMACAAAAAEAAAAAAAQACAAAAAEAAAAAAAUAAQALAAEAAAAAAAYACAAAAAMAAQQJAAEAEAAMAAMAAQQJAAIABgAcAAMAAQQJAAMAEAAMAAMAAQQJAAQAEAAMAAMAAQQJAAUAAgAiAAMAAQQJAAYAEAAMYW5jaG9yanM0MDBAAGEAbgBjAGgAbwByAGoAcwA0ADAAMABAAAAAAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAH//wAP) format(""truetype"")}',u.sheet.cssRules.length)),u=document.querySelectorAll(""[id]""),t=[].map.call(u,function(A){return A.id}),i=0;i<e.length;i++)if(this.hasAnchorJSLink(e[i]))p.push(i);else{if(e[i].hasAttribute(""id""))o=e[i].getAttribute(""id"");else if(e[i].hasAttribute(""data-anchor-id""))o=e[i].getAttribute(""data-anchor-id"");else{for(c=a=this.urlify(e[i].textContent),s=0;n=t.indexOf(c=void 0!==n?a+""-""+s:c),s+=1,-1!==n;);n=void 0,t.push(c),e[i].setAttribute(""id"",c),o=c}(r=document.createElement(""a"")).className=""anchorjs-link ""+this.options.class,r.setAttribute(""aria-label"",this.options.ariaLabel),r.setAttribute(""data-anchorjs-icon"",this.options.icon),this.options.titleText&&(r.title=this.options.titleText),h=document.querySelector(""base"")?window.location.pathname+window.location.search:"""",h=this.options.base||h,r.href=h+""#""+o,""always""===l&&(r.style.opacity=""1""),""""===this.options.icon&&(r.style.font=""1em/1 anchorjs-icons"",""left""===this.options.placement&&(r.style.lineHeight=""inherit"")),""left""===this.options.placement?(r.style.position=""absolute"",r.style.marginLeft=""-1em"",r.style.paddingRight="".5em"",e[i].insertBefore(r,e[i].firstChild)):(r.style.paddingLeft="".375em"",e[i].appendChild(r))}for(i=0;i<p.length;i++)e.splice(p[i]-i,1);return this.elements=this.elements.concat(e),this},this.remove=function(A){for(var e,t,o=w(A),i=0;i<o.length;i++)(t=o[i].querySelector("".anchorjs-link""))&&(-1!==(e=this.elements.indexOf(o[i]))&&this.elements.splice(e,1),o[i].removeChild(t));return this},this.removeAll=function(){this.remove(this.elements)},this.urlify=function(A){var e=document.createElement(""textarea"");return e.innerHTML=A,A=e.value,this.options.truncate||d(this.options),A.trim().replace(/'/gi,"""").replace(/[& +$,:;=?@""#{}|^~[`%!'<>\]./()*\\\n\t\b\v\u00A0]/g,""-"").replace(/-{2,}/g,""-"").substring(0,this.options.truncate).replace(/^-+|-+$/gm,"""").toLowerCase()},this.hasAnchorJSLink=function(A){var e=A.firstChild&&-1<("" ""+A.firstChild.className+"" "").indexOf("" anchorjs-link ""),A=A.lastChild&&-1<("" ""+A.lastChild.className+"" "").indexOf("" anchorjs-link "");return e||A||!1}}});
-// @license-end
\ No newline at end of file

---FILE: session-rank-tests-presentation/session-rank-tests-presentatation_files/libs/quarto-html/quarto.js---
@@ -1,667 +0,0 @@
-const sectionChanged = new CustomEvent(""quarto-sectionChanged"", {
-  detail: {},
-  bubbles: true,
-  cancelable: false,
-  composed: false,
-});
-
-window.document.addEventListener(""DOMContentLoaded"", function (_event) {
-  const tocEl = window.document.querySelector('nav[role=""doc-toc""]');
-  const sidebarEl = window.document.getElementById(""quarto-sidebar"");
-  const leftTocEl = window.document.getElementById(""quarto-sidebar-toc-left"");
-  const marginSidebarEl = window.document.getElementById(
-    ""quarto-margin-sidebar""
-  );
-  // function to determine whether the element has a previous sibling that is active
-  const prevSiblingIsActiveLink = (el) => {
-    const sibling = el.previousElementSibling;
-    if (sibling && sibling.tagName === ""A"") {
-      return sibling.classList.contains(""active"");
-    } else {
-      return false;
-    }
-  };
-
-  // fire slideEnter for bootstrap tab activations (for htmlwidget resize behavior)
-  function fireSlideEnter(e) {
-    const event = window.document.createEvent(""Event"");
-    event.initEvent(""slideenter"", true, true);
-    window.document.dispatchEvent(event);
-  }
-  const tabs = window.document.querySelectorAll('a[data-bs-toggle=""tab""]');
-  tabs.forEach((tab) => {
-    tab.addEventListener(""shown.bs.tab"", fireSlideEnter);
-  });
-
-  // Track scrolling and mark TOC links as active
-  // get table of contents and sidebar (bail if we don't have at least one)
-  const tocLinks = tocEl
-    ? [...tocEl.querySelectorAll(""a[data-scroll-target]"")]
-    : [];
-  const makeActive = (link) => tocLinks[link].classList.add(""active"");
-  const removeActive = (link) => tocLinks[link].classList.remove(""active"");
-  const removeAllActive = () =>
-    [...Array(tocLinks.length).keys()].forEach((link) => removeActive(link));
-
-  // activate the anchor for a section associated with this TOC entry
-  tocLinks.forEach((link) => {
-    link.addEventListener(""click"", () => {
-      if (link.href.indexOf(""#"") !== -1) {
-        const anchor = link.href.split(""#"")[1];
-        const heading = window.document.querySelector(
-          `[data-anchor-id=${anchor}]`
-        );
-        if (heading) {
-          // Add the class
-          heading.classList.add(""reveal-anchorjs-link"");
-
-          // function to show the anchor
-          const handleMouseout = () => {
-            heading.classList.remove(""reveal-anchorjs-link"");
-            heading.removeEventListener(""mouseout"", handleMouseout);
-          };
-
-          // add a function to clear the anchor when the user mouses out of it
-          heading.addEventListener(""mouseout"", handleMouseout);
-        }
-      }
-    });
-  });
-
-  const sections = tocLinks.map((link) => {
-    const target = link.getAttribute(""data-scroll-target"");
-    if (target.startsWith(""#"")) {
-      return window.document.getElementById(decodeURI(`${target.slice(1)}`));
-    } else {
-      return window.document.querySelector(decodeURI(`${target}`));
-    }
-  });
-
-  const sectionMargin = 200;
-  let currentActive = 0;
-  // track whether we've initialized state the first time
-  let init = false;
-
-  const updateActiveLink = () => {
-    // The index from bottom to top (e.g. reversed list)
-    let sectionIndex = -1;
-    if (
-      window.innerHeight + window.pageYOffset >=
-      window.document.body.offsetHeight
-    ) {
-      sectionIndex = 0;
-    } else {
-      sectionIndex = [...sections].reverse().findIndex((section) => {
-        if (section) {
-          return window.pageYOffset >= section.offsetTop - sectionMargin;
-        } else {
-          return false;
-        }
-      });
-    }
-    if (sectionIndex > -1) {
-      const current = sections.length - sectionIndex - 1;
-      if (current !== currentActive) {
-        removeAllActive();
-        currentActive = current;
-        makeActive(current);
-        if (init) {
-          window.dispatchEvent(sectionChanged);
-        }
-        init = true;
-      }
-    }
-  };
-
-  const inHiddenRegion = (top, bottom, hiddenRegions) => {
-    for (const region of hiddenRegions) {
-      if (top <= region.bottom && bottom >= region.top) {
-        return true;
-      }
-    }
-    return false;
-  };
-
-  const categorySelector = ""header.quarto-title-block .quarto-category"";
-  const activateCategories = (href) => {
-    // Find any categories
-    // Surround them with a link pointing back to:
-    // #category=Authoring
-    try {
-      const categoryEls = window.document.querySelectorAll(categorySelector);
-      for (const categoryEl of categoryEls) {
-        const categoryText = categoryEl.textContent;
-        if (categoryText) {
-          const link = `${href}#category=${encodeURIComponent(categoryText)}`;
-          const linkEl = window.document.createElement(""a"");
-          linkEl.setAttribute(""href"", link);
-          for (const child of categoryEl.childNodes) {
-            linkEl.append(child);
-          }
-          categoryEl.appendChild(linkEl);
-        }
-      }
-    } catch {
-      // Ignore errors
-    }
-  };
-  function hasTitleCategories() {
-    return window.document.querySelector(categorySelector) !== null;
-  }
-
-  function offsetRelativeUrl(url) {
-    const offset = getMeta(""quarto:offset"");
-    return offset ? offset + url : url;
-  }
-
-  function offsetAbsoluteUrl(url) {
-    const offset = getMeta(""quarto:offset"");
-    const baseUrl = new URL(offset, window.location);
-
-    const projRelativeUrl = url.replace(baseUrl, """");
-    if (projRelativeUrl.startsWith(""/"")) {
-      return projRelativeUrl;
-    } else {
-      return ""/"" + projRelativeUrl;
-    }
-  }
-
-  // read a meta tag value
-  function getMeta(metaName) {
-    const metas = window.document.getElementsByTagName(""meta"");
-    for (let i = 0; i < metas.length; i++) {
-      if (metas[i].getAttribute(""name"") === metaName) {
-        return metas[i].getAttribute(""content"");
-      }
-    }
-    return """";
-  }
-
-  async function findAndActivateCategories() {
-    const currentPagePath = offsetAbsoluteUrl(window.location.href);
-    const response = await fetch(offsetRelativeUrl(""listings.json""));
-    if (response.status == 200) {
-      return response.json().then(function (listingPaths) {
-        const listingHrefs = [];
-        for (const listingPath of listingPaths) {
-          const pathWithoutLeadingSlash = listingPath.listing.substring(1);
-          for (const item of listingPath.items) {
-            if (
-              item === currentPagePath ||
-              item === currentPagePath + ""index.html""
-            ) {
-              // Resolve this path against the offset to be sure
-              // we already are using the correct path to the listing
-              // (this adjusts the listing urls to be rooted against
-              // whatever root the page is actually running against)
-              const relative = offsetRelativeUrl(pathWithoutLeadingSlash);
-              const baseUrl = window.location;
-              const resolvedPath = new URL(relative, baseUrl);
-              listingHrefs.push(resolvedPath.pathname);
-              break;
-            }
-          }
-        }
-
-        // Look up the tree for a nearby linting and use that if we find one
-        const nearestListing = findNearestParentListing(
-          offsetAbsoluteUrl(window.location.pathname),
-          listingHrefs
-        );
-        if (nearestListing) {
-          activateCategories(nearestListing);
-        } else {
-          // See if the referrer is a listing page for this item
-          const referredRelativePath = offsetAbsoluteUrl(document.referrer);
-          const referrerListing = listingHrefs.find((listingHref) => {
-            const isListingReferrer =
-              listingHref === referredRelativePath ||
-              listingHref === referredRelativePath + ""index.html"";
-            return isListingReferrer;
-          });
-
-          if (referrerListing) {
-            // Try to use the referrer if possible
-            activateCategories(referrerListing);
-          } else if (listingHrefs.length > 0) {
-            // Otherwise, just fall back to the first listing
-            activateCategories(listingHrefs[0]);
-          }
-        }
-      });
-    }
-  }
-  if (hasTitleCategories()) {
-    findAndActivateCategories();
-  }
-
-  const findNearestParentListing = (href, listingHrefs) => {
-    if (!href || !listingHrefs) {
-      return undefined;
-    }
-    // Look up the tree for a nearby linting and use that if we find one
-    const relativeParts = href.substring(1).split(""/"");
-    while (relativeParts.length > 0) {
-      const path = relativeParts.join(""/"");
-      for (const listingHref of listingHrefs) {
-        if (listingHref.startsWith(path)) {
-          return listingHref;
-        }
-      }
-      relativeParts.pop();
-    }
-
-    return undefined;
-  };
-
-  const manageSidebarVisiblity = (el, placeholderDescriptor) => {
-    let isVisible = true;
-
-    return (hiddenRegions) => {
-      if (el === null) {
-        return;
-      }
-
-      // Find the last element of the TOC
-      const lastChildEl = el.lastElementChild;
-
-      if (lastChildEl) {
-        // Find the top and bottom o the element that is being managed
-        const elTop = el.offsetTop;
-        const elBottom =
-          elTop + lastChildEl.offsetTop + lastChildEl.offsetHeight;
-
-        // Converts the sidebar to a menu
-        const convertToMenu = () => {
-          for (const child of el.children) {
-            child.style.opacity = 0;
-            child.style.display = ""none"";
-          }
-
-          const toggleContainer = window.document.createElement(""div"");
-          toggleContainer.style.width = ""100%"";
-          toggleContainer.classList.add(""zindex-over-content"");
-          toggleContainer.classList.add(""quarto-sidebar-toggle"");
-          toggleContainer.classList.add(""headroom-target""); // Marks this to be managed by headeroom
-          toggleContainer.id = placeholderDescriptor.id;
-          toggleContainer.style.position = ""fixed"";
-
-          const toggleIcon = window.document.createElement(""i"");
-          toggleIcon.classList.add(""quarto-sidebar-toggle-icon"");
-          toggleIcon.classList.add(""bi"");
-          toggleIcon.classList.add(""bi-caret-down-fill"");
-
-          const toggleTitle = window.document.createElement(""div"");
-          const titleEl = window.document.body.querySelector(
-            placeholderDescriptor.titleSelector
-          );
-          if (titleEl) {
-            toggleTitle.append(titleEl.innerText, toggleIcon);
-          }
-          toggleTitle.classList.add(""zindex-over-content"");
-          toggleTitle.classList.add(""quarto-sidebar-toggle-title"");
-          toggleContainer.append(toggleTitle);
-
-          const toggleContents = window.document.createElement(""div"");
-          toggleContents.classList = el.classList;
-          toggleContents.classList.add(""zindex-over-content"");
-          toggleContents.classList.add(""quarto-sidebar-toggle-contents"");
-          for (const child of el.children) {
-            if (child.id === ""toc-title"") {
-              continue;
-            }
-
-            const clone = child.cloneNode(true);
-            clone.style.opacity = 1;
-            clone.style.display = null;
-            toggleContents.append(clone);
-          }
-          toggleContents.style.height = ""0px"";
-          toggleContainer.append(toggleContents);
-          el.parentElement.prepend(toggleContainer);
-
-          // Process clicks
-          let tocShowing = false;
-          // Allow the caller to control whether this is dismissed
-          // when it is clicked (e.g. sidebar navigation supports
-          // opening and closing the nav tree, so don't dismiss on click)
-          const clickEl = placeholderDescriptor.dismissOnClick
-            ? toggleContainer
-            : toggleTitle;
-
-          const closeToggle = () => {
-            if (tocShowing) {
-              toggleContainer.classList.remove(""expanded"");
-              toggleContents.style.height = ""0px"";
-              tocShowing = false;
-            }
-          };
-
-          const positionToggle = () => {
-            // position the element (top left of parent, same width as parent)
-            const elRect = el.getBoundingClientRect();
-            toggleContainer.style.left = `${elRect.left}px`;
-            toggleContainer.style.top = `${elRect.top}px`;
-            toggleContainer.style.width = `${elRect.width}px`;
-          };
-
-          // Get rid of any expanded toggle if the user scrolls
-          window.document.addEventListener(
-            ""scroll"",
-            throttle(() => {
-              closeToggle();
-            }, 50)
-          );
-
-          // Handle positioning of the toggle
-          window.addEventListener(
-            ""resize"",
-            throttle(() => {
-              positionToggle();
-            }, 50)
-          );
-          positionToggle();
-
-          // Process the click
-          clickEl.onclick = () => {
-            if (!tocShowing) {
-              toggleContainer.classList.add(""expanded"");
-              toggleContents.style.height = null;
-              tocShowing = true;
-            } else {
-              closeToggle();
-            }
-          };
-        };
-
-        // Converts a sidebar from a menu back to a sidebar
-        const convertToSidebar = () => {
-          for (const child of el.children) {
-            child.style.opacity = 1;
-            clone.style.display = null;
-          }
-
-          const placeholderEl = window.document.getElementById(
-            placeholderDescriptor.id
-          );
-          if (placeholderEl) {
-            placeholderEl.remove();
-          }
-
-          el.classList.remove(""rollup"");
-        };
-
-        if (isReaderMode()) {
-          convertToMenu();
-          isVisible = false;
-        } else {
-          if (!isVisible) {
-            // If the element is current not visible reveal if there are
-            // no conflicts with overlay regions
-            if (!inHiddenRegion(elTop, elBottom, hiddenRegions)) {
-              convertToSidebar();
-              isVisible = true;
-            }
-          } else {
-            // If the element is visible, hide it if it conflicts with overlay regions
-            // and insert a placeholder toggle (or if we're in reader mode)
-            if (inHiddenRegion(elTop, elBottom, hiddenRegions)) {
-              convertToMenu();
-              isVisible = false;
-            }
-          }
-        }
-      }
-    };
-  };
-
-  // Find any conflicting margin elements and add margins to the
-  // top to prevent overlap
-  const marginChildren = window.document.querySelectorAll(
-    "".column-margin.column-container > * ""
-  );
-  let lastBottom = 0;
-  for (const marginChild of marginChildren) {
-    const top = marginChild.getBoundingClientRect().top;
-    if (top < lastBottom) {
-      const margin = lastBottom - top;
-      marginChild.style.marginTop = `${margin}px`;
-    }
-    const styles = window.getComputedStyle(marginChild);
-    const marginTop = parseFloat(styles[""marginTop""]);
-
-    lastBottom = top + marginChild.getBoundingClientRect().height + marginTop;
-  }
-
-  // Manage the visibility of the toc and the sidebar
-  const marginScrollVisibility = manageSidebarVisiblity(marginSidebarEl, {
-    id: ""quarto-toc-toggle"",
-    titleSelector: ""#toc-title"",
-    dismissOnClick: true,
-  });
-  const sidebarScrollVisiblity = manageSidebarVisiblity(sidebarEl, {
-    id: ""quarto-sidebarnav-toggle"",
-    titleSelector: "".title"",
-    dismissOnClick: false,
-  });
-  let tocLeftScrollVisibility;
-  if (leftTocEl) {
-    tocLeftScrollVisibility = manageSidebarVisiblity(leftTocEl, {
-      id: ""quarto-lefttoc-toggle"",
-      titleSelector: ""#toc-title"",
-      dismissOnClick: true,
-    });
-  }
-
-  // Find the first element that uses formatting in special columns
-  const conflictingEls = window.document.body.querySelectorAll(
-    '[class^=""column-""], [class*="" column-""], aside, [class*=""margin-caption""], [class*="" margin-caption""], [class*=""margin-ref""], [class*="" margin-ref""]'
-  );
-
-  // Filter all the possibly conflicting elements into ones
-  // the do conflict on the left or ride side
-  const arrConflictingEls = Array.from(conflictingEls);
-  const leftSideConflictEls = arrConflictingEls.filter((el) => {
-    if (el.tagName === ""ASIDE"") {
-      return false;
-    }
-    return Array.from(el.classList).find((className) => {
-      return (
-        className !== ""column-body"" &&
-        className.startsWith(""column-"") &&
-        !className.endsWith(""right"") &&
-        !className.endsWith(""container"") &&
-        className !== ""column-margin""
-      );
-    });
-  });
-  const rightSideConflictEls = arrConflictingEls.filter((el) => {
-    if (el.tagName === ""ASIDE"") {
-      return true;
-    }
-
-    const hasMarginCaption = Array.from(el.classList).find((className) => {
-      return className == ""margin-caption"";
-    });
-    if (hasMarginCaption) {
-      return true;
-    }
-
-    return Array.from(el.classList).find((className) => {
-      return (
-        className !== ""column-body"" &&
-        !className.endsWith(""container"") &&
-        className.startsWith(""column-"") &&
-        !className.endsWith(""left"")
-      );
-    });
-  });
-
-  const kOverlapPaddingSize = 10;
-  function toRegions(els) {
-    return els.map((el) => {
-      const top =
-        el.getBoundingClientRect().top +
-        document.documentElement.scrollTop -
-        kOverlapPaddingSize;
-      return {
-        top,
-        bottom: top + el.scrollHeight + 2 * kOverlapPaddingSize,
-      };
-    });
-  }
-
-  const hideOverlappedSidebars = () => {
-    marginScrollVisibility(toRegions(rightSideConflictEls));
-    sidebarScrollVisiblity(toRegions(leftSideConflictEls));
-    if (tocLeftScrollVisibility) {
-      tocLeftScrollVisibility(toRegions(leftSideConflictEls));
-    }
-  };
-
-  window.quartoToggleReader = () => {
-    // Applies a slow class (or removes it)
-    // to update the transition speed
-    const slowTransition = (slow) => {
-      const manageTransition = (id, slow) => {
-        const el = document.getElementById(id);
-        if (el) {
-          if (slow) {
-            el.classList.add(""slow"");
-          } else {
-            el.classList.remove(""slow"");
-          }
-        }
-      };
-
-      manageTransition(""TOC"", slow);
-      manageTransition(""quarto-sidebar"", slow);
-    };
-
-    const readerMode = !isReaderMode();
-    setReaderModeValue(readerMode);
-
-    // If we're entering reader mode, slow the transition
-    if (readerMode) {
-      slowTransition(readerMode);
-    }
-    highlightReaderToggle(readerMode);
-    hideOverlappedSidebars();
-
-    // If we're exiting reader mode, restore the non-slow transition
-    if (!readerMode) {
-      slowTransition(!readerMode);
-    }
-  };
-
-  const highlightReaderToggle = (readerMode) => {
-    const els = document.querySelectorAll("".quarto-reader-toggle"");
-    if (els) {
-      els.forEach((el) => {
-        if (readerMode) {
-          el.classList.add(""reader"");
-        } else {
-          el.classList.remove(""reader"");
-        }
-      });
-    }
-  };
-
-  const setReaderModeValue = (val) => {
-    if (window.location.protocol !== ""file:"") {
-      window.localStorage.setItem(""quarto-reader-mode"", val);
-    } else {
-      localReaderMode = val;
-    }
-  };
-
-  const isReaderMode = () => {
-    if (window.location.protocol !== ""file:"") {
-      return window.localStorage.getItem(""quarto-reader-mode"") === ""true"";
-    } else {
-      return localReaderMode;
-    }
-  };
-  let localReaderMode = null;
-
-  // Walk the TOC and collapse/expand nodes
-  // Nodes are expanded if:
-  // - they are top level
-  // - they have children that are 'active' links
-  // - they are directly below an link that is 'active'
-  const walk = (el, depth) => {
-    // Tick depth when we enter a UL
-    if (el.tagName === ""UL"") {
-      depth = depth + 1;
-    }
-
-    // It this is active link
-    let isActiveNode = false;
-    if (el.tagName === ""A"" && el.classList.contains(""active"")) {
-      isActiveNode = true;
-    }
-
-    // See if there is an active child to this element
-    let hasActiveChild = false;
-    for (child of el.children) {
-      hasActiveChild = walk(child, depth) || hasActiveChild;
-    }
-
-    // Process the collapse state if this is an UL
-    if (el.tagName === ""UL"") {
-      if (depth === 1 || hasActiveChild || prevSiblingIsActiveLink(el)) {
-        el.classList.remove(""collapse"");
-      } else {
-        el.classList.add(""collapse"");
-      }
-
-      // untick depth when we leave a UL
-      depth = depth - 1;
-    }
-    return hasActiveChild || isActiveNode;
-  };
-
-  // walk the TOC and expand / collapse any items that should be shown
-
-  if (tocEl) {
-    walk(tocEl, 0);
-    updateActiveLink();
-  }
-
-  // Throttle the scroll event and walk peridiocally
-  window.document.addEventListener(
-    ""scroll"",
-    throttle(() => {
-      if (tocEl) {
-        updateActiveLink();
-        walk(tocEl, 0);
-      }
-      if (!isReaderMode()) {
-        hideOverlappedSidebars();
-      }
-    }, 5)
-  );
-  window.addEventListener(
-    ""resize"",
-    throttle(() => {
-      if (!isReaderMode()) {
-        hideOverlappedSidebars();
-      }
-    }, 10)
-  );
-  hideOverlappedSidebars();
-  highlightReaderToggle(isReaderMode());
-});
-
-function throttle(func, wait) {
-  let waiting = false;
-  return function () {
-    if (!waiting) {
-      func.apply(this, arguments);
-      waiting = true;
-      setTimeout(function () {
-        waiting = false;
-      }, wait);
-    }
-  };
-}

---FILE: session-rank-tests-presentation/session-rank-tests-presentation.html---
@@ -1,20 +1,20 @@
 <!DOCTYPE html>
 <html lang=""en""><head>
-<script src=""session-rank-tests-presentatation_files/libs/clipboard/clipboard.min.js""></script>
-<script src=""session-rank-tests-presentatation_files/libs/quarto-html/tabby.min.js""></script>
-<script src=""session-rank-tests-presentatation_files/libs/quarto-html/popper.min.js""></script>
-<script src=""session-rank-tests-presentatation_files/libs/quarto-html/tippy.umd.min.js""></script>
-<link href=""session-rank-tests-presentatation_files/libs/quarto-html/tippy.css"" rel=""stylesheet"">
-<link href=""session-rank-tests-presentatation_files/libs/quarto-html/quarto-html.min.css"" rel=""stylesheet"" data-mode=""light"">
-<link href=""session-rank-tests-presentatation_files/libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles""><meta charset=""utf-8"">
+<script src=""session-rank-tests-presentation_files/libs/clipboard/clipboard.min.js""></script>
+<script src=""session-rank-tests-presentation_files/libs/quarto-html/tabby.min.js""></script>
+<script src=""session-rank-tests-presentation_files/libs/quarto-html/popper.min.js""></script>
+<script src=""session-rank-tests-presentation_files/libs/quarto-html/tippy.umd.min.js""></script>
+<link href=""session-rank-tests-presentation_files/libs/quarto-html/tippy.css"" rel=""stylesheet"">
+<link href=""session-rank-tests-presentation_files/libs/quarto-html/quarto-html.min.css"" rel=""stylesheet"" data-mode=""light"">
+<link href=""session-rank-tests-presentation_files/libs/quarto-html/quarto-syntax-highlighting.css"" rel=""stylesheet"" id=""quarto-text-highlighting-styles""><meta charset=""utf-8"">
   <meta name=""generator"" content=""quarto-1.0.36"">
 
   <title>Non-parametric rank based tests</title>
   <meta name=""apple-mobile-web-app-capable"" content=""yes"">
   <meta name=""apple-mobile-web-app-status-bar-style"" content=""black-translucent"">
   <meta name=""viewport"" content=""width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"">
-  <link rel=""stylesheet"" href=""session-rank-tests-presentatation_files/libs/revealjs/dist/reset.css"">
-  <link rel=""stylesheet"" href=""session-rank-tests-presentatation_files/libs/revealjs/dist/reveal.css"">
+  <link rel=""stylesheet"" href=""session-rank-tests-presentation_files/libs/revealjs/dist/reset.css"">
+  <link rel=""stylesheet"" href=""session-rank-tests-presentation_files/libs/revealjs/dist/reveal.css"">
   <style>
     code{white-space: pre-wrap;}
     span.smallcaps{font-variant: small-caps;}
@@ -106,13 +106,13 @@
       margin-left: 2em;
     }
   </style>
-  <link rel=""stylesheet"" href=""session-rank-tests-presentatation_files/libs/revealjs/dist/theme/quarto.css"" id=""theme"">
-  <link href=""session-rank-tests-presentatation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css"" rel=""stylesheet"">
-  <link href=""session-rank-tests-presentatation_files/libs/revealjs/plugin/reveal-menu/menu.css"" rel=""stylesheet"">
-  <link href=""session-rank-tests-presentatation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css"" rel=""stylesheet"">
-  <link href=""session-rank-tests-presentatation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css"" rel=""stylesheet"">
-  <link href=""session-rank-tests-presentatation_files/libs/revealjs/plugin/reveal-chalkboard/style.css"" rel=""stylesheet"">
-  <link href=""session-rank-tests-presentatation_files/libs/revealjs/plugin/quarto-support/footer.css"" rel=""stylesheet"">
+  <link rel=""stylesheet"" href=""session-rank-tests-presentation_files/libs/revealjs/dist/theme/quarto.css"" id=""theme"">
+  <link href=""session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css"" rel=""stylesheet"">
+  <link href=""session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/menu.css"" rel=""stylesheet"">
+  <link href=""session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css"" rel=""stylesheet"">
+  <link href=""session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css"" rel=""stylesheet"">
+  <link href=""session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/style.css"" rel=""stylesheet"">
+  <link href=""session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-support/footer.css"" rel=""stylesheet"">
   <style type=""text/css"">
 
   .callout {
@@ -398,8 +398,8 @@
     }
   </style>
   
-  <script src=""session-rank-tests-presentatation_files/libs/kePrint-0.0.1/kePrint.js""></script>
-  <link href=""session-rank-tests-presentatation_files/libs/lightable-0.0.1/lightable.css"" rel=""stylesheet"">
+  <script src=""session-rank-tests-presentation_files/libs/kePrint-0.0.1/kePrint.js""></script>
+  <link href=""session-rank-tests-presentation_files/libs/lightable-0.0.1/lightable.css"" rel=""stylesheet"">
 </head>
 <body class=""quarto-light"">
   <div class=""reveal"">
@@ -604,15 +604,15 @@ <h2>Wilcoxon signed rank test</h2>
 <li>we rank the absolute value of the difference</li>
 <li>we find the value of <span class=""math inline"">\(W\)</span>, the Wilcoxon signed-rank test statistics as <span class=""math display"">\[W =\displaystyle \sum_{i=1}^{n}Z_iR_i\]</span> where <span class=""math inline"">\(Z_i\)</span> is an indicator variable such as:</li>
 </ol>
-<p><span class=""math display"">\[\begin{equation}
+<span class=""math display"">\[\begin{equation}
     Z_i =
     \left\{
         \begin{array}{cc}
                 0  &amp; \mathrm{if\ } X_i - m_0 &lt; 0 \\
                 1  &amp;  otherwise \\
         \end{array}
     \right.
-\end{equation}\]</span></p>
+\end{equation}\]</span>
 </div><div class=""column"" style=""width:60%;"">
 <div class=""cell"">
 <div class=""cell-output-display"">
@@ -952,7 +952,7 @@ <h2>Wilcoxon signed rank test</h2>
 <div class=""cell-output-display"">
 <div class=""quarto-figure quarto-figure-center"">
 <figure>
-<p><img data-src=""session-rank-tests-presentatation_files/figure-revealjs/unnamed-chunk-6-1.png"" width=""576""></p>
+<p><img data-src=""session-rank-tests-presentation_files/figure-revealjs/unnamed-chunk-6-1.png"" width=""576""></p>
 </figure>
 </div>
 </div>
@@ -1189,7 +1189,7 @@ <h2>Wilcoxon signed rank test</h2>
 <div>
 <ul>
 <li class=""fragment""><p>The “Critical values for the Wilcoxon matched pairs signed rank test” table can be found online or <a href=""statstable/Wilcoxon-signed-rank-test-pairs-T-distribution.pdf"">here</a></p></li>
-<li class=""fragment""><p><strong>The Wilcoxon signed rank test is based on assessing whether <span class=""math inline"">\(T\)</span>, the smaller of <span class=""math inline"">\(T_{-}\)</span> and <span class=""math inline"">\(T_{+}\)</span>, is smaller than would be expected by chance, under the null hypothesis that the median of the paired differences is zero.</strong></p></li>
+<li class=""fragment""><p><strong>The Wilcoxon signed rank test is based on assessing whether</strong> <span class=""math inline"">\(T\)</span>, the smaller of <span class=""math inline"">\(T_{-}\)</span> and <span class=""math inline"">\(T_{+}\)</span>, is smaller than would be expected by chance, under the null hypothesis that the median of the paired differences is zero.</p></li>
 <li class=""fragment""><p>The hypothesis is that <span class=""math inline"">\(T\)</span> is equal to the sum of the ranks divided by 2, so that the smaller <span class=""math inline"">\(T\)</span> the more evidence there is against the null hypothesis.</p></li>
 <li class=""fragment""><p>Having our <span class=""math inline"">\(T\)</span> value we can check what is the probability of observing the value of <span class=""math inline"">\(T\)</span> under the null hypothesis, by checking the statistical table of “Critical values for the Wilcoxon matched pairs signed rank test”.</p></li>
 <li class=""fragment""><p>In our example, the sample size <span class=""math inline"">\(n=10\)</span>, where <span class=""math inline"">\(n\)</span> is the number of non-zero differences (we had none) and 5% percentage point is 8. Since <span class=""math inline"">\(T=15 &gt; 8\)</span> our <span class=""math inline"">\(P-value &gt; 0.05\)</span> and we do not have enough evidence to reject the null hypothesis. There is no evidence of the sleeping drug working.</p></li>
@@ -1347,7 +1347,7 @@ <h2>Pearson correleation coefficient</h2>
 <p>Pearson correlation coefficient, or rather more correctly Pearson product moment correlation coefficient, gives us an idea about the strength of association between two numerical variables. Its true value in the population, <span class=""math inline"">\(\rho\)</span>, is estimated in the sample by <span class=""math inline"">\(r\)</span>, where:</p>
 <p><span id=""eq-pearson""><span class=""math display"">\[r=\frac{\sum(x-\bar{x})(x-\bar{y})}{\sqrt{\sum(x-\bar{x})^2\sum(x-\bar{y})^2}} \qquad(1)\]</span></span></p>
 
-<img data-src=""session-rank-tests-presentatation_files/figure-revealjs/unnamed-chunk-12-1.png"" width=""960"" class=""r-stretch""></section>
+<img data-src=""session-rank-tests-presentation_files/figure-revealjs/unnamed-chunk-12-1.png"" width=""960"" class=""r-stretch""></section>
 <section id=""spearman-and-kendal-tau"" class=""slide level2 smaller"">
 <h2>Spearman and Kendal tau</h2>
 <p><strong>Spearman’s rank correlation</strong></p>
@@ -1417,20 +1417,20 @@ <h2>Thank you for listening</h2>
   </div>
 
   <script>window.backupDefine = window.define; window.define = undefined;</script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/dist/reveal.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/dist/reveal.js""></script>
   <!-- reveal.js plugins -->
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/pdf-export/pdfexport.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/reveal-menu/menu.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/quarto-support/support.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/menu.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/quarto-support/support.js""></script>
   
 
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/notes/notes.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/search/search.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/zoom/zoom.js""></script>
-  <script src=""session-rank-tests-presentatation_files/libs/revealjs/plugin/math/math.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/notes/notes.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/search/search.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/zoom/zoom.js""></script>
+  <script src=""session-rank-tests-presentation_files/libs/revealjs/plugin/math/math.js""></script>
   <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>
 
   <script>

---FILE: session-rank-tests-presentation/session-rank-tests-presentation.qmd---
@@ -28,73 +28,69 @@ library(rmarkdown)
 library(gridExtra)
 ```
 
-
-
 ## Introduction
 
-Hypothesis tests can be done via: 
+Hypothesis tests can be done via:
 
 ::: incremental
-- resampling (to obtain the null distribution)
-- using parametric tests (when the null distribution is known)
-- non-parametric rank based tests (derived before computers empowered statistics)
+-   resampling (to obtain the null distribution)
+-   using parametric tests (when the null distribution is known)
+-   non-parametric rank based tests (derived before computers empowered statistics)
 :::
 
 . . .
 
 Non-parametric rank based test are useful when:
 
 ::: incremental
-- we do not know the underlying probability distribution and/or our data does not meet parametric test requirements
-- sample size is too small to properly assess the distribution of the data
-- transforming our data to meet the parametric test requirements would make interpretation of the results harder
+-   we do not know the underlying probability distribution and/or our data does not meet parametric test requirements
+-   sample size is too small to properly assess the distribution of the data
+-   transforming our data to meet the parametric test requirements would make interpretation of the results harder
 :::
 
-
 ## Limitations
 
 Some limitations of the non-parametric rank based tests include the facts that:
 
 ::: incremental
-- they are primary significance tests that often do not provide estimates of the effects of interest
-- they lead to waste of information and in consequence they have less power
-- when sample size are extremely small (e.g. $n=3$) rank tests cannot produce small *P*-values, even when the outcomes in the two groups are very different
-- non-parametric tests are less easily extended to situations where we wish to take into account the effect of more than one exposure on the outcome
+-   they are primary significance tests that often do not provide estimates of the effects of interest
+-   they lead to waste of information and in consequence they have less power
+-   when sample size are extremely small (e.g. $n=3$) rank tests cannot produce small *P*-values, even when the outcomes in the two groups are very different
+-   non-parametric tests are less easily extended to situations where we wish to take into account the effect of more than one exposure on the outcome
 :::
 
 ## Main non-parametric rank tests
 
 . . .
 
-- **Wilcoxon signed rank test**
-  - compares the sample median against a hypothetical median (equivalent to one sample *t*-test)
-  - or examine the difference between paired observations (equivalent to paired *t*-test)
-- **Wilcoxon rank sum test**
-  - examines the difference between two unrelated groups
-  - equivalent to two sample *t*-test
-- **Kruskal-Wallis one-way analysis of variance**
-  - examines the difference between two or more unrelated groups
-  - equivalent to ANOVA
-  
-. . .   
+-   **Wilcoxon signed rank test**
+    -   compares the sample median against a hypothetical median (equivalent to one sample *t*-test)
+    -   or examine the difference between paired observations (equivalent to paired *t*-test)
+-   **Wilcoxon rank sum test**
+    -   examines the difference between two unrelated groups
+    -   equivalent to two sample *t*-test
+-   **Kruskal-Wallis one-way analysis of variance**
+    -   examines the difference between two or more unrelated groups
+    -   equivalent to ANOVA
+
+. . .
 
 **Rank based correlation**
 
-- **Spearman's rank correlation**
-  - Pearson's correlation coefficient calculated on ranks
-  
-. . .
+-   **Spearman's rank correlation**
+    -   Pearson's correlation coefficient calculated on ranks
 
-- **Kendall's rank correlation**
-  - based on number of concordant/discordant pairs
-  - alternative to Pearson correlation coefficient
+. . .
 
+-   **Kendall's rank correlation**
+    -   based on number of concordant/discordant pairs
+    -   alternative to Pearson correlation coefficient
 
-# Wilcoxon signed rank test  
+# Wilcoxon signed rank test
 
-## Wilcoxon signed rank test 
+## Wilcoxon signed rank test
 
-Named after Frank Wilcoxon (1892–1945), Wilcoxon signed rank test was one of the first ""non-parametric"" methods developed.
+Named after Frank Wilcoxon (1892--1945), Wilcoxon signed rank test was one of the first ""non-parametric"" methods developed.
 
 . . .
 
@@ -103,28 +99,33 @@ Named after Frank Wilcoxon (1892–1945), Wilcoxon signed rank test was one of t
 It can be used to:
 
 ::: incremental
-  1. compare the sample median against a hypothetical median (equivalent to one sample *t*-test) 
-  2. examine the difference between paired observations (equivalent to paired *t*-test). 
+1.  compare the sample median against a hypothetical median (equivalent to one sample *t*-test)
+2.  examine the difference between paired observations (equivalent to paired *t*-test).
 :::
 
 . . .
 
 <!-- It does make some assumptions about the data, namely:  -->
 
 <!-- ::: incremental   -->
+
 <!-- - the random variable X is continuous -->
+
 <!-- - the probability density function of X is symmetric -->
+
 <!-- ::: -->
 
 ## Wilcoxon signed rank test {.smaller}
+
 *for a median*
 
 ::: {#exm-wilcoxon-signed}
 ## Wilcoxon signed rank test (for a median)
 
 Let's imagine we are a part of team analyzing results of a placebo-controlled clinical trial to test the effectiveness of a sleeping drug. We have collected data on 10 patients when they took a sleeping drug and when they took a placebo.
 
-The hours of sleep recorded for each study participant: 
+The hours of sleep recorded for each study participant:
+
 ```{r}
 # input data
 data.sleep <- data.frame(id = 1:10, 
@@ -138,35 +139,33 @@ data.sleep %>%
   kable_styling(full_width = F) 
   
 ```
-
 :::
 
 . . .
 
-<br>
-Before we investigate the effect of drug, a senior statistician ask us: 
+<br> Before we investigate the effect of drug, a senior statistician ask us:
 
 **""Is the median sleeping time without taking the drug significantly less than the recommended 7 h of sleep?""**
 
 ## Wilcoxon signed rank test {.smaller}
-*for a median (cont.)*
 
+*for a median (cont.)*
 
-:::: {.columns}
-
+::: columns
 ::: {.column width=""40%""}
 **Define the null and alternative hypothesis under the study**
 
-- $H_0: m = m_0$  the median sleeping time is equal to $m_0$, $m_0 = 7$ h
-- $H_1 < m_0$  the median sleeping time is less than $m_0$, $m_0 = 7$ h
+-   $H_0: m = m_0$ the median sleeping time is equal to $m_0$, $m_0 = 7$ h
+-   $H_1 < m_0$ the median sleeping time is less than $m_0$, $m_0 = 7$ h
 
 **Calculate the value of the test statistics**
 
-1. we subtract the median from each measurement, $X_i - m_0$
-2. we find absolute value of the difference, $|X_i - m_0|$
-3. we rank the absolute value of the difference
-4. we find the value of $W$, the Wilcoxon signed-rank test statistics as $$W =\displaystyle \sum_{i=1}^{n}Z_iR_i$$ where $Z_i$ is an indicator variable such as: 
+1.  we subtract the median from each measurement, $X_i - m_0$
+2.  we find absolute value of the difference, $|X_i - m_0|$
+3.  we rank the absolute value of the difference
+4.  we find the value of $W$, the Wilcoxon signed-rank test statistics as $$W =\displaystyle \sum_{i=1}^{n}Z_iR_i$$ where $Z_i$ is an indicator variable such as:
 
+```{=tex}
 \begin{equation}
     Z_i =
     \left\{
@@ -176,7 +175,7 @@ Before we investigate the effect of drug, a senior statistician ask us:
         \end{array}
     \right.
 \end{equation}
-
+```
 :::
 
 ::: {.column width=""60%""}
@@ -211,35 +210,35 @@ data.wilcoxon %>%
 W <- data.wilcoxon$RZ %>% sum()
 
 ```
-
+:::
 :::
 
-::::
-
+## Wilcoxon signed rank test
 
-## Wilcoxon signed rank test 
 *for a median (cont.)*
 
 **Compare the value to the test statistics to values from known probability distribution**
 
 ::: incremental
-- we got $W = 6.5$ and now we need to calculate the *P*-value associated with $W$ to be able to make decision about rejecting the null hypothesis. 
-- we refer to a statistical table ""Upper and Lower Percentiles of the Wilcoxon Signed Rank Test, W"" that can be found online or [here](statstable/Wilcoxon-signed-rank-test.pdf).
-- we can see, at sample size $n=10$, that observing a *P*-value associated with observing $W=6.5$ is just under $0.019$
-- assuming 5% significance level, we have enough evidence to reject the null hypothesis and conclude that the median is significantly less than 7 hours. 
+-   we got $W = 6.5$ and now we need to calculate the *P*-value associated with $W$ to be able to make decision about rejecting the null hypothesis.
+-   we refer to a statistical table ""Upper and Lower Percentiles of the Wilcoxon Signed Rank Test, W"" that can be found online or [here](statstable/Wilcoxon-signed-rank-test.pdf).
+-   we can see, at sample size $n=10$, that observing a *P*-value associated with observing $W=6.5$ is just under $0.019$
+-   assuming 5% significance level, we have enough evidence to reject the null hypothesis and conclude that the median is significantly less than 7 hours.
 :::
 
-## Wilcoxon signed rank test 
+## Wilcoxon signed rank test
+
 *for a median (cont.)*
 
 Where did that known distribution come from?
 
 ::: incremental
-- Wilcoxon described and showed examples how to calculate both the test statistics $W$ for an example data as well as the distribution of $W$ under the null hypothesis @Wilcoxon1945
-- Let's try to find the distribution of W assuming we only have four observation ($n=4$)
+-   Wilcoxon described and showed examples how to calculate both the test statistics $W$ for an example data as well as the distribution of $W$ under the null hypothesis @Wilcoxon1945
+-   Let's try to find the distribution of W assuming we only have four observation ($n=4$)
 :::
 
 ## Wilcoxon signed rank test {.smaller .scrollable}
+
 *for a median (cont.)...Where did that known distribution come from?*
 
 ```{r}
@@ -267,15 +266,15 @@ data.w %>% kable() %>% kable_styling(full_width = TRUE) %>%
 
 <br>
 
-- Given 4 observations, we could get ranks $R_i$ of 1, 2, 3 or 4 only. Further, depending where the observation would be with respect to $m_0$, the rank $R_i$ could be positive or negative. For example, the first column $c1$ corresponds to all 4 observations having positive ranks, so all $x_i - m_0 > 0$, whereas column $c16$ corresponds to all observations having negative ranks, so $x_i - m_0 < 0$. 
+-   Given 4 observations, we could get ranks $R_i$ of 1, 2, 3 or 4 only. Further, depending where the observation would be with respect to $m_0$, the rank $R_i$ could be positive or negative. For example, the first column $c1$ corresponds to all 4 observations having positive ranks, so all $x_i - m_0 > 0$, whereas column $c16$ corresponds to all observations having negative ranks, so $x_i - m_0 < 0$.
 
 . . .
 
-- As $W$ test statistics is derived by summing up the positive ranks, we can see by listing all the combinations in the table, that $0 \le W \le10$. 
+-   As $W$ test statistics is derived by summing up the positive ranks, we can see by listing all the combinations in the table, that $0 \le W \le10$.
 
-. . . 
+. . .
 
-- We can also write down the probability mass function
+-   We can also write down the probability mass function
 
 . . .
 
@@ -299,7 +298,7 @@ dist.W %>%
 
 . . .
 
-- And now we can use our knowledge from the Probability session on discrete distributions to calculate the probability of observed test statistics $W$ given the known probability mass function 
+-   And now we can use our knowledge from the Probability session on discrete distributions to calculate the probability of observed test statistics $W$ given the known probability mass function
 
 ```{r}
 #| fig-width: 6
@@ -310,12 +309,13 @@ dist.W %>%
 barplot(df.w$per, names.arg = 0:10, ylab = ""p(W)"", xlab=""W"")
 ```
 
-## Wilcoxon signed rank test 
+## Wilcoxon signed rank test
+
 *for a median (cont.)*
 
 In `R` we use `wilcox.test()` function:
 
-```{.r}
+``` r
 # run Wilcoxon signed rank test for a median
 wilcox.test(x = data.sleep$placebo, 
             y = NULL,
@@ -325,7 +325,6 @@ wilcox.test(x = data.sleep$placebo,
             
 ```
 
-
 ```{r}
 # run Wilcoxon signed rank test for a median
 wilcox.test(x = data.sleep$placebo, 
@@ -337,14 +336,16 @@ wilcox.test(x = data.sleep$placebo,
 ```
 
 ## Wilcoxon signed rank test {.smaller}
+
 *paired observations*
 
 ::: {#exm-wilcoxon-signed}
 ## Wilcoxon signed rank test (paired observations)
 
-Let's return to our placebo-controlled clinical trial to test the effectiveness of a sleeping drug. Again, the hours of sleep we recorded for each participants are: 
+Let's return to our placebo-controlled clinical trial to test the effectiveness of a sleeping drug. Again, the hours of sleep we recorded for each participants are:
+
+The hours of sleep recorded for each study participant:
 
-The hours of sleep recorded for each study participant: 
 ```{r}
 # input data
 data.sleep <- data.frame(id = 1:10, 
@@ -357,38 +358,37 @@ data.sleep %>%
   kable_styling(full_width = F) 
 
 ```
-
 :::
 
 <br>
 
-. . . 
+. . .
 
 **Is there enough evidence to reject a null hypothesis of the median of the differences between the paired observations being equal to 0? I.e. is the drug having an effect?**
 
 ## Wilcoxon signed rank test
+
 *paired observations*
 
 Define the null and alternative hypothesis under the study
 
-- $H_0:$  the median difference in the population equals to zero
-- $H_1:$  the median difference in the population does not equals to zero
+-   $H_0:$ the median difference in the population equals to zero
+-   $H_1:$ the median difference in the population does not equals to zero
 
 ## Wilcoxon signed rank test {.smaller}
-*paired observations*
 
-:::: {.columns}
+*paired observations*
 
+::: columns
 ::: {.column width=""40%""}
 To calculate test statistics:
 
-- calculate difference and exclude differences that equal to 0
-- rank difference in ascending order, ignoring the sign, e.g. the smallest difference value, here 0.6 is ranked 1. 
-- sum up the ranks of the negative differences and of positive differences and denote these sums by $T_{-}$ and $T_{+}$ respectively
-- Why? If there were no differences in effectiveness between the sleeping drug and the placebo then the sums $T_{-}$ and $T_{+}$ would be similar. **If there were a difference then one sum would be much smaller and one sum would be much larger than expected.**
-- we get $T_{-} = 40$ and $T_{+} = 15$
-- denote the smaller sum by T and interpret the *P*-value, here $T = 15$
-
+-   calculate difference and exclude differences that equal to 0
+-   rank difference in ascending order, ignoring the sign, e.g. the smallest difference value, here 0.6 is ranked 1.
+-   sum up the ranks of the negative differences and of positive differences and denote these sums by $T_{-}$ and $T_{+}$ respectively
+-   Why? If there were no differences in effectiveness between the sleeping drug and the placebo then the sums $T_{-}$ and $T_{+}$ would be similar. **If there were a difference then one sum would be much smaller and one sum would be much larger than expected.**
+-   we get $T_{-} = 40$ and $T_{+} = 15$
+-   denote the smaller sum by T and interpret the *P*-value, here $T = 15$
 :::
 
 ::: {.column width=""60%""}
@@ -411,35 +411,33 @@ df.wilcox.signed%>%
 
 ```
 :::
+:::
 
-::::
-
-
+## Wilcoxon signed rank test
 
-## Wilcoxon signed rank test 
 *paired observations*
 
 ::: incremental
+-   The ""Critical values for the Wilcoxon matched pairs signed rank test"" table can be found online or [here](statstable/Wilcoxon-signed-rank-test-pairs-T-distribution.pdf)
 
-- The ""Critical values for the Wilcoxon matched pairs signed rank test"" table can be found online or [here](statstable/Wilcoxon-signed-rank-test-pairs-T-distribution.pdf)
+-   **The Wilcoxon signed rank test is based on assessing whether** $T$, the smaller of $T_{-}$ and $T_{+}$, is smaller than would be expected by chance, under the null hypothesis that the median of the paired differences is zero.
 
-- **The Wilcoxon signed rank test is based on assessing whether $T$, the smaller of $T_{-}$ and $T_{+}$, is smaller than would be expected by chance, under the null hypothesis that the median of the paired differences is zero.** 
+-   The hypothesis is that $T$ is equal to the sum of the ranks divided by 2, so that the smaller $T$ the more evidence there is against the null hypothesis.
 
-- The hypothesis is that $T$ is equal to the sum of the ranks divided by 2, so that the smaller $T$ the more evidence there is against the null hypothesis. 
-
-- Having our $T$ value we can check what is the probability of observing the value of $T$ under the null hypothesis, by checking the statistical table of ""Critical values for the Wilcoxon matched pairs signed rank test"".
-
-- In our example, the sample size $n=10$, where $n$ is the number of non-zero differences (we had none) and 5% percentage point is 8. Since $T=15 > 8$ our $P-value > 0.05$ and we do not have enough evidence to reject the null hypothesis. There is no evidence of the sleeping drug working. 
+-   Having our $T$ value we can check what is the probability of observing the value of $T$ under the null hypothesis, by checking the statistical table of ""Critical values for the Wilcoxon matched pairs signed rank test"".
 
+-   In our example, the sample size $n=10$, where $n$ is the number of non-zero differences (we had none) and 5% percentage point is 8. Since $T=15 > 8$ our $P-value > 0.05$ and we do not have enough evidence to reject the null hypothesis. There is no evidence of the sleeping drug working.
 :::
 
-## Wilcoxon signed rank test 
+## Wilcoxon signed rank test
+
 *paired observations*
 
 In `R` we use `wilcox.test()` function adjusting `paired` argument.
 
 Before, Wilcoxon signed rank test for a median
-```{.r}
+
+``` r
 # run Wilcoxon signed rank test for a median
 wilcox.test(x = data.sleep$placebo, 
             y = NULL,
@@ -450,9 +448,9 @@ wilcox.test(x = data.sleep$placebo,
 
 . . .
 
+Now, Wilcoxon signed rank test for paired observations
 
-Now, Wilcoxon signed rank test for paired observations 
-```{.r code-line-numbers=""3|4|5|6""}
+``` {.r code-line-numbers=""3|4|5|6""}
 # run Wilcoxon signed rank test for paired observations 
 wilcox.test(x = data.sleep$placebo, 
             y = data.sleep$drug,
@@ -471,34 +469,28 @@ wilcox.test(x = data.sleep$placebo,
             exact = F)
 ```
 
-
 # 2 or more unrelated groups
 
 ## Wilcoxon rank sum test & Krusall-Wallis
 
-
-:::: {.columns}
-
+::: columns
 ::: {.column width=""40%""}
 **Wilcoxon rank sum test**
 
-- test statistics, $T$, is the sum or ranks in the smaller group
-- refer to table “Critical range for the Wilcoxon rank sum test” found online or [here](statstable/Wilcoxon-rank-sum-test.pdf)
+-   test statistics, $T$, is the sum or ranks in the smaller group
+-   refer to table ""Critical range for the Wilcoxon rank sum test"" found online or [here](statstable/Wilcoxon-rank-sum-test.pdf)
 
 **Kruskall-Wallis**
 
-- can be seen as extension of Wilcoxon rank sum test to $k≥2$ groups
-- for $k=2$ gives the same results as Wilcoxon rank sum test
-- the sums of the ranks in each of the $k$ groups should be comparable after allowing for any differences in sample size.
-
+-   can be seen as extension of Wilcoxon rank sum test to $k≥2$ groups
+-   for $k=2$ gives the same results as Wilcoxon rank sum test
+-   the sums of the ranks in each of the $k$ groups should be comparable after allowing for any differences in sample size.
 
 More details and examples in the chapter.
 :::
 
 ::: {.column width=""60%""}
-
-<br>
-<br>
+<br> <br>
 
 ```{r}
 
@@ -526,19 +518,16 @@ data.babies %>%
   kable_styling(full_width = F)
 
 ```
-
 :::
-
-::::
-
+:::
 
 # Correlation
 
 ## Pearson correleation coefficient {.smaller .scrollable}
 
 Pearson correlation coefficient, or rather more correctly Pearson product moment correlation coefficient, gives us an idea about the strength of association between two numerical variables. Its true value in the population, $\rho$, is estimated in the sample by $r$, where:
 
-$$r=\frac{\sum(x-\bar{x})(x-\bar{y})}{\sqrt{\sum(x-\bar{x})^2\sum(x-\bar{y})^2}}$${#eq-pearson}
+$$r=\frac{\sum(x-\bar{x})(x-\bar{y})}{\sqrt{\sum(x-\bar{x})^2\sum(x-\bar{y})^2}}$$ {#eq-pearson}
 
 ```{r}
 
@@ -592,60 +581,52 @@ plot(x6, y6, xlab=""x"", ylab=""y"", main = paste(""r = NA"", sep=""""), pch=19)
 
 ## Spearman and Kendal tau {.smaller}
 
-**Spearman's rank correlation** 
+**Spearman's rank correlation**
 
-. . . 
+. . .
 
-To calculate Spearman's rank correlation between two variables $X$ and $Y$ we: 
+To calculate Spearman's rank correlation between two variables $X$ and $Y$ we:
 
-- rank the values of $X$ and $Y$ independently
+-   rank the values of $X$ and $Y$ independently
 
 . . .
 
-- follow the formula to calculate the Pearson correlation coefficient using ranks 
+-   follow the formula to calculate the Pearson correlation coefficient using ranks
 
 . . .
 
 **Kendall's tau**
 
-To calculate Kendall's tau, $\tau$, we compare ranks of $X$ and $Y$ between every pair of observation. (There are n(n-1)/2 possible pairs). The pairs of ranks for observation $i$ and $j$ are said to be: 
+To calculate Kendall's tau, $\tau$, we compare ranks of $X$ and $Y$ between every pair of observation. (There are n(n-1)/2 possible pairs). The pairs of ranks for observation $i$ and $j$ are said to be:
 
-- concordant: if they differ in the same direction, i.e. if both the $X$ and $Y$ ranks of subject $i$ are lower than the corresponding ranks of subject $j$, or both are higher
-- discordant: otherwise
+-   concordant: if they differ in the same direction, i.e. if both the $X$ and $Y$ ranks of subject $i$ are lower than the corresponding ranks of subject $j$, or both are higher
+-   discordant: otherwise
 
-$$\tau = \frac{n_C-n_D}{n(n-1)/2}$$ where 
+$$\tau = \frac{n_C-n_D}{n(n-1)/2}$$ where
 
-$n_C$, number of concordant pairs
-$n_D$, number of discordant pairs
+$n_C$, number of concordant pairs $n_D$, number of discordant pairs
 
 . . .
 
-:::{.callout-tip}
+::: callout-tip
 ## Kendall $\tau$
 
-Although Spearman correlation coefficient is commonly used it may be easier to build intuitive understanding of Kendall $\tau$. A positive correlation indicates 
-that the ranks of both variables increase together whilst a negative correlation indicates that as the rank of one variable increases the other one decreases
-
+Although Spearman correlation coefficient is commonly used it may be easier to build intuitive understanding of Kendall $\tau$. A positive correlation indicates that the ranks of both variables increase together whilst a negative correlation indicates that as the rank of one variable increases the other one decreases
 :::
 
 ## Summary
 
 ::: incremental
-
-- Non-parametric rank based tests still have their place in modern data analysis
-- They are based on a neat idea of turning data into ranks that is useful when sample is small or when parametric based test assumptions cannot be met
-- Spearman correlation is perhaps used as a first choice when Pearson correlation coefficient should not be calculated. However Kendall tau's offers much easier interpretation, with positive correlation indicating that the ranks of both variables increase together
+-   Non-parametric rank based tests still have their place in modern data analysis
+-   They are based on a neat idea of turning data into ranks that is useful when sample is small or when parametric based test assumptions cannot be met
+-   Spearman correlation is perhaps used as a first choice when Pearson correlation coefficient should not be calculated. However Kendall tau's offers much easier interpretation, with positive correlation indicating that the ranks of both variables increase together
 :::
 
-
 ## References
+
 ::: {#refs}
 :::
 
 ## Thank you for listening
-*Any questions?*
-
-
-
-
 
+*Any questions?*"
NBISweden,workshop-mlbiostatistics,95ab216e63541b428757d33e8ec236605584ab38,Julie Lorent,5382593+jlorent@users.noreply.github.com,2022-09-02T13:13:05Z,Julie Lorent,5382593+jlorent@users.noreply.github.com,2022-09-02T13:13:05Z,fix typo logitsic and code Yanny as 1,session-glm/304-linear-GLM.Rmd;session-glm/Introduction-to-biostatistics-and-machine-learning.rds;session-glm/_bookdown_files/304-linear-GLM_files/figure-html/unnamed-chunk-12-1.png;session-glm/_bookdown_files/304-linear-GLM_files/figure-html/unnamed-chunk-2-1.png;session-glm/_bookdown_files/304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png;session-glm/_bookdown_files/304-linear-GLM_files/figure-html/unnamed-chunk-5-1.png;session-glm/_bookdown_files/304-linear-GLM_files/figure-html/unnamed-chunk-7-1.png;session-glm/docs/304-linear-GLM.md;session-glm/docs/304-linear-GLM_files/figure-html/unnamed-chunk-12-1.png;session-glm/docs/304-linear-GLM_files/figure-html/unnamed-chunk-2-1.png;session-glm/docs/304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png;session-glm/docs/304-linear-GLM_files/figure-html/unnamed-chunk-5-1.png;session-glm/docs/304-linear-GLM_files/figure-html/unnamed-chunk-7-1.png;session-glm/docs/404.html;session-glm/docs/generalized-linear-models.html;session-glm/docs/index.html;session-glm/docs/index.md;session-glm/docs/libs/gitbook-2.6.7/css/plugin-bookdown.css;session-glm/docs/libs/gitbook-2.6.7/css/plugin-fontsettings.css;session-glm/docs/libs/gitbook-2.6.7/css/style.css;session-glm/docs/libs/gitbook-2.6.7/js/plugin-search.js;session-glm/docs/libs/gitbook-2.6.7/js/plugin-sharing.js;session-glm/docs/reference-keys.txt;session-glm/docs/search_index.json,True,False,True,False,275,198,473,"---FILE: session-glm/304-linear-GLM.Rmd---
@@ -9,7 +9,6 @@ editor_options:
 **Aims**
 
 - to briefly introduce GLMs via examples of modeling binary and count response
-- to test if repo works
 
 **Learning outcomes**
 
@@ -42,7 +41,7 @@ ggplot(data = medgpa, aes(y=Acceptance, x=GPA)) +
 
 ```
 
-## Logisitc regression
+## Logistic regression
 - [Yanny or Laurel auditory illusion](https://www.theguardian.com/global/video/2018/may/16/what-do-you-hear-in-this-audio-clip-yanny-or-laurel-takes-internet-by-storm-video) appeared online in May 2018. You could find lots of information about it, together with some plausible explanations why some people hear Yanny and some year Laurel
 - One of the explanation is that with age we lose the ability to hear certain sounds
 - To see if there is evidence for that, someone has already collected some data for 53 people including their age and gender
@@ -74,14 +73,14 @@ range(yl$age)
 ```
 
 
-```{r, echo=T, collapse=TRUE, fig.align=""center"", fig.width=8, fig.height=4, fig.cap=""Yanny and Laurel auditory illusion data, Yanny (1), Luarel (0)""}
+```{r, echo=T, collapse=TRUE, fig.align=""center"", fig.width=8, fig.height=4, fig.cap=""Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0)""}
 # Read in and preview data
 yl <- read.csv(""data/lm/yanny-laurel.csv"")
 head(yl)
 
-# Recode Laurel to 0 and Yanny as 1 in new variable (what)
+# Recode Laurel to 0 and Yanny as 1 in new variable
 yl$word <- 0
-yl$word[yl$hear==""Laurel""] <- 1
+yl$word[yl$hear==""Yanny""] <- 1
 
 # Make some exploratory plots
 par(mfrow=c(1,2))

---FILE: session-glm/docs/304-linear-GLM.md---
@@ -9,7 +9,6 @@ editor_options:
 **Aims**
 
 - to briefly introduce GLMs via examples of modeling binary and count response
-- to test if repo works
 
 **Learning outcomes**
 
@@ -31,7 +30,7 @@ editor_options:
 <p class=""caption"">(\#fig:unnamed-chunk-2)Example of fitting linear model to binary data, to model the acceptance to medical school, coded as 1 (Yes) and 0 (No) using GPA school scores. Linear model does not fit the data well in this case</p>
 </div>
 
-## Logisitc regression
+## Logistic regression
 - [Yanny or Laurel auditory illusion](https://www.theguardian.com/global/video/2018/may/16/what-do-you-hear-in-this-audio-clip-yanny-or-laurel-takes-internet-by-storm-video) appeared online in May 2018. You could find lots of information about it, together with some plausible explanations why some people hear Yanny and some year Laurel
 - One of the explanation is that with age we lose the ability to hear certain sounds
 - To see if there is evidence for that, someone has already collected some data for 53 people including their age and gender
@@ -51,10 +50,9 @@ head(yl)
 ## 4 Laurel  47 Female
 ## 5 Laurel  60   Male
 ## 6  Yanny  11 Female
-
-# Recode Laurel to 0 and Yanny as 1 in new variable (what)
+# Recode Laurel to 0 and Yanny as 1 in new variable
 yl$word <- 0
-yl$word[yl$hear==""Laurel""] <- 1
+yl$word[yl$hear==""Yanny""] <- 1
 
 # Make some exploratory plots
 par(mfrow=c(1,2))
@@ -63,8 +61,8 @@ boxplot(yl$age~yl$hear, xlab="""", ylab=""age"", col=""lightblue"")
 ```
 
 <div class=""figure"" style=""text-align: center"">
-<img src=""304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png"" alt=""Yanny and Laurel auditory illusion data, Yanny (1), Luarel (0)"" width=""768"" />
-<p class=""caption"">(\#fig:unnamed-chunk-4)Yanny and Laurel auditory illusion data, Yanny (1), Luarel (0)</p>
+<img src=""304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png"" alt=""Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0)"" width=""768"" />
+<p class=""caption"">(\#fig:unnamed-chunk-4)Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0)</p>
 </div>
 
 - Since the response variable takes only two values (Yanny or Laurel) we use GLM model 
@@ -97,12 +95,12 @@ print(summary(logmodel.1))
 ## 
 ## Deviance Residuals: 
 ##      Min        1Q    Median        3Q       Max  
-## -1.86068  -0.71414  -0.04733   0.64434   2.47887  
+## -2.47887  -0.64434   0.04733   0.71414   1.86068  
 ## 
 ## Coefficients:
 ##             Estimate Std. Error z value Pr(>|z|)    
-## (Intercept) -3.56159    0.95790  -3.718 0.000201 ***
-## age          0.08943    0.02297   3.893 9.89e-05 ***
+## (Intercept)  3.56159    0.95790   3.718 0.000201 ***
+## age         -0.08943    0.02297  -3.893 9.89e-05 ***
 ## ---
 ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 ## 
@@ -134,23 +132,23 @@ predict(logmodel.1, type=""response"")
 
 ```
 ##          1          2          3          4          5          6          7 
-## 0.50394192 0.67507724 0.33187793 0.65516152 0.85868941 0.07057982 0.87903410 
+## 0.49605808 0.32492276 0.66812207 0.34483848 0.14131059 0.92942018 0.12096590 
 ##          8          9         10         11         12         13         14 
-## 0.06493370 0.35199768 0.69437930 0.91222027 0.15663558 0.78037334 0.43719992 
+## 0.93506630 0.64800232 0.30562070 0.08777973 0.84336442 0.21962666 0.56280008 
 ##         15         16         17         18         19         20         21 
-## 0.39379165 0.59230535 0.20986467 0.45931517 0.69437930 0.22507939 0.48159183 
+## 0.60620835 0.40769465 0.79013533 0.54068483 0.30562070 0.77492061 0.51840817 
 ##         22         23         24         25         26         27         28 
-## 0.20986467 0.71302217 0.10615359 0.97322447 0.65516152 0.11494328 0.20986467 
+## 0.79013533 0.28697783 0.89384641 0.02677553 0.34483848 0.88505672 0.79013533 
 ##         29         30         31         32         33         34         35 
-## 0.12435950 0.90478991 0.87903410 0.93146108 0.15663558 0.18173908 0.33187793 
+## 0.87564050 0.09521009 0.12096590 0.06853892 0.84336442 0.81826092 0.66812207 
 ##         36         37         38         39         40         41         42 
-## 0.59230535 0.94673070 0.06493370 0.82290366 0.91222027 0.82290366 0.92552645 
+## 0.40769465 0.05326930 0.93506630 0.17709634 0.08777973 0.17709634 0.07447355 
 ##         43         44         45         46         47         48         49 
-## 0.83556313 0.04630975 0.50394192 0.37265683 0.97751124 0.45931517 0.41533155 
+## 0.16443687 0.95369025 0.49605808 0.62734317 0.02248876 0.54068483 0.58466845 
 ##         50         51         52         53         54         55         56 
-## 0.35199768 0.04630975 0.83556313 0.15663558 0.20986467 0.22507939 0.15663558 
+## 0.64800232 0.95369025 0.16443687 0.84336442 0.79013533 0.77492061 0.84336442 
 ##         57         58         59         60 
-## 0.73096842 0.35199768 0.43719992 0.87903410
+## 0.26903158 0.64800232 0.56280008 0.12096590
 ```
 
 - The regression equation for the fitted model is:
@@ -199,13 +197,13 @@ print(summary(logmodel.2))
 ## 
 ## Deviance Residuals: 
 ##      Min        1Q    Median        3Q       Max  
-## -1.81723  -0.72585  -0.06218   0.67360   2.44755  
+## -2.44755  -0.67360   0.06218   0.72585   1.81723  
 ## 
 ## Coefficients:
 ##             Estimate Std. Error z value Pr(>|z|)    
-## (Intercept) -3.72679    1.07333  -3.472 0.000516 ***
-## age          0.09061    0.02337   3.877 0.000106 ***
-## genderMale   0.23919    0.65938   0.363 0.716789    
+## (Intercept)  3.72679    1.07333   3.472 0.000516 ***
+## age         -0.09061    0.02337  -3.877 0.000106 ***
+## genderMale  -0.23919    0.65938  -0.363 0.716789    
 ## ---
 ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 ## 
@@ -216,7 +214,6 @@ print(summary(logmodel.2))
 ## AIC: 63.835
 ## 
 ## Number of Fisher Scoring iterations: 5
-
 # plot model
 ggPredict(logmodel.2)
 ```
@@ -260,7 +257,6 @@ head(cancer)
 ## 4 S02000263    80 109.10245 17.0  14.0  10.39  12.30138 25.45705 67.05938
 ## 5 S02000264   181 149.77821 18.6  15.2   5.67  11.88449 26.12484 67.09280
 ## 6 S02000265    77  82.31156 17.0  14.6   5.61  11.82004 25.37644 67.09826
-
 # fit Poisson regression
 epid1 <- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), 
              family = poisson, 
@@ -345,7 +341,6 @@ b) using Poisson regression, can you comment about the numbers of cigarettes smo
 
 
 ```r
-
 library(faraway)
 data(wcgs, package=""faraway"")
 
@@ -377,14 +372,12 @@ a) probability of developing heart disease
 We first check the relationship between variables to gain more understanding of the data. We discover that a couple of variables are exactly collinear with other variables, including `typechd`, `timechd` and `dibep`. We do not include these in the model. 
 
 ```r
-
 # `chd` and `typechd` were correlated.
 with(wcgs, table(chd, typechd))
 ##      typechd
 ## chd   angina infdeath none silent
 ##   no       0        0 2897      0
 ##   yes     51      135    0     71
-
 # `timechd` is an outcome variable affected by `chd`.
 by(wcgs$timechd, wcgs$chd, summary)
 ## wcgs$chd: no
@@ -394,7 +387,6 @@ by(wcgs$timechd, wcgs$chd, summary)
 ## wcgs$chd: yes
 ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 ##      18     934    1666    1655    2400    3229
-
 # `behave` has more detailed info of `dibep` -> exact collinearity
 with(wcgs, table(behave, dibep))
 ##       dibep

---FILE: session-glm/docs/404.html---
@@ -24,7 +24,7 @@
 <meta name=""author"" content=""Olga Dethlefsen"" />
 
 
-<meta name=""date"" content=""2022-08-30"" />
+<meta name=""date"" content=""2022-09-02"" />
 
   <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
   <meta name=""apple-mobile-web-app-capable"" content=""yes"" />
@@ -141,7 +141,7 @@
 <li><a href=""generalized-linear-models.html#generalized-linear-models"" id=""toc-generalized-linear-models""><span class=""toc-section-number"">1</span> Generalized linear models</a>
 <ul>
 <li><a href=""generalized-linear-models.html#why-generalized-linear-models-glms"" id=""toc-why-generalized-linear-models-glms""><span class=""toc-section-number"">1.1</span> Why Generalized Linear Models (GLMs)</a></li>
-<li><a href=""generalized-linear-models.html#logisitc-regression"" id=""toc-logisitc-regression""><span class=""toc-section-number"">1.2</span> Logisitc regression</a></li>
+<li><a href=""generalized-linear-models.html#logistic-regression"" id=""toc-logistic-regression""><span class=""toc-section-number"">1.2</span> Logistic regression</a></li>
 <li><a href=""generalized-linear-models.html#poisson-regression"" id=""toc-poisson-regression""><span class=""toc-section-number"">1.3</span> Poisson regression</a></li>
 <li><a href=""generalized-linear-models.html#exercises-glms"" id=""toc-exercises-glms""><span class=""toc-section-number"">1.4</span> Exercises (GLMs)</a></li>
 </ul></li>

---FILE: session-glm/docs/generalized-linear-models.html---
@@ -6,7 +6,7 @@
   <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
   <title>Chapter 1 Generalized linear models | Introduction to GLM</title>
   <meta name=""description"" content=""Chapter 1 Generalized linear models | Introduction to GLM"" />
-  <meta name=""generator"" content=""bookdown 0.21 and GitBook 2.6.7"" />
+  <meta name=""generator"" content=""bookdown 0.24 and GitBook 2.6.7"" />
 
   <meta property=""og:title"" content=""Chapter 1 Generalized linear models | Introduction to GLM"" />
   <meta property=""og:type"" content=""book"" />
@@ -24,7 +24,7 @@
 <meta name=""author"" content=""Olga Dethlefsen"" />
 
 
-<meta name=""date"" content=""2022-09-01"" />
+<meta name=""date"" content=""2022-09-02"" />
 
   <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
   <meta name=""apple-mobile-web-app-capable"" content=""yes"" />
@@ -33,8 +33,9 @@
   
 <link rel=""prev"" href=""index.html""/>
 
-<script src=""libs/header-attrs-2.14/header-attrs.js""></script>
-<script src=""libs/jquery-2.2.3/jquery.min.js""></script>
+<script src=""libs/header-attrs-2.10/header-attrs.js""></script>
+<script src=""libs/jquery-3.6.0/jquery-3.6.0.min.js""></script>
+<script src=""https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js""></script>
 <link href=""libs/gitbook-2.6.7/css/style.css"" rel=""stylesheet"" />
 <link href=""libs/gitbook-2.6.7/css/plugin-table.css"" rel=""stylesheet"" />
 <link href=""libs/gitbook-2.6.7/css/plugin-bookdown.css"" rel=""stylesheet"" />
@@ -50,7 +51,8 @@
 
 
 
-
+<link href=""libs/anchor-sections-1.0.1/anchor-sections.css"" rel=""stylesheet"" />
+<script src=""libs/anchor-sections-1.0.1/anchor-sections.js""></script>
 
 
 <style type=""text/css"">
@@ -118,6 +120,7 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <link rel=""stylesheet"" href=""style.css"" type=""text/css"" />
 </head>
 
@@ -138,7 +141,7 @@
 <li><a href=""generalized-linear-models.html#generalized-linear-models"" id=""toc-generalized-linear-models""><span class=""toc-section-number"">1</span> Generalized linear models</a>
 <ul>
 <li><a href=""generalized-linear-models.html#why-generalized-linear-models-glms"" id=""toc-why-generalized-linear-models-glms""><span class=""toc-section-number"">1.1</span> Why Generalized Linear Models (GLMs)</a></li>
-<li><a href=""generalized-linear-models.html#logisitc-regression"" id=""toc-logisitc-regression""><span class=""toc-section-number"">1.2</span> Logisitc regression</a></li>
+<li><a href=""generalized-linear-models.html#logistic-regression"" id=""toc-logistic-regression""><span class=""toc-section-number"">1.2</span> Logistic regression</a></li>
 <li><a href=""generalized-linear-models.html#poisson-regression"" id=""toc-poisson-regression""><span class=""toc-section-number"">1.3</span> Poisson regression</a></li>
 <li><a href=""generalized-linear-models.html#exercises-glms"" id=""toc-exercises-glms""><span class=""toc-section-number"">1.4</span> Exercises (GLMs)</a></li>
 </ul></li>
@@ -164,7 +167,6 @@ <h1><span class=""header-section-number"">Chapter 1</span> Generalized linear mode
 <p><strong>Aims</strong></p>
 <ul>
 <li>to briefly introduce GLMs via examples of modeling binary and count response</li>
-<li>to test if repo works</li>
 </ul>
 <p><strong>Learning outcomes</strong></p>
 <ul>
@@ -181,15 +183,15 @@ <h2><span class=""header-section-number"">1.1</span> Why Generalized Linear Models
 <li>Similarly, fitting a regression line to binary data yields predicted values that could take any value, including <span class=""math inline"">\(&lt;0\)</span></li>
 <li>not to mention that it is hard to argue that the values of 0 and 1s are normally distributed</li>
 </ul>
-<div class=""figure"" style=""text-align: center""><span id=""fig:unnamed-chunk-2""></span>
+<div class=""figure"" style=""text-align: center""><span style=""display:block;"" id=""fig:unnamed-chunk-2""></span>
 <img src=""304-linear-GLM_files/figure-html/unnamed-chunk-2-1.png"" alt=""Example of fitting linear model to binary data, to model the acceptance to medical school, coded as 1 (Yes) and 0 (No) using GPA school scores. Linear model does not fit the data well in this case"" width=""384"" />
 <p class=""caption"">
 Figure 1.1: Example of fitting linear model to binary data, to model the acceptance to medical school, coded as 1 (Yes) and 0 (No) using GPA school scores. Linear model does not fit the data well in this case
 </p>
 </div>
 </div>
-<div id=""logisitc-regression"" class=""section level2"" number=""1.2"">
-<h2><span class=""header-section-number"">1.2</span> Logisitc regression</h2>
+<div id=""logistic-regression"" class=""section level2"" number=""1.2"">
+<h2><span class=""header-section-number"">1.2</span> Logistic regression</h2>
 <ul>
 <li><a href=""https://www.theguardian.com/global/video/2018/may/16/what-do-you-hear-in-this-audio-clip-yanny-or-laurel-takes-internet-by-storm-video"">Yanny or Laurel auditory illusion</a> appeared online in May 2018. You could find lots of information about it, together with some plausible explanations why some people hear Yanny and some year Laurel</li>
 <li>One of the explanation is that with age we lose the ability to hear certain sounds</li>
@@ -205,19 +207,18 @@ <h2><span class=""header-section-number"">1.2</span> Logisitc regression</h2>
 <span id=""cb1-8""><a href=""generalized-linear-models.html#cb1-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 4 Laurel  47 Female</span></span>
 <span id=""cb1-9""><a href=""generalized-linear-models.html#cb1-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 5 Laurel  60   Male</span></span>
 <span id=""cb1-10""><a href=""generalized-linear-models.html#cb1-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 6  Yanny  11 Female</span></span>
-<span id=""cb1-11""><a href=""generalized-linear-models.html#cb1-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb1-12""><a href=""generalized-linear-models.html#cb1-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Recode Laurel to 0 and Yanny as 1 in new variable (what)</span></span>
-<span id=""cb1-13""><a href=""generalized-linear-models.html#cb1-13"" aria-hidden=""true"" tabindex=""-1""></a>yl<span class=""sc"">$</span>word <span class=""ot"">&lt;-</span> <span class=""dv"">0</span></span>
-<span id=""cb1-14""><a href=""generalized-linear-models.html#cb1-14"" aria-hidden=""true"" tabindex=""-1""></a>yl<span class=""sc"">$</span>word[yl<span class=""sc"">$</span>hear<span class=""sc"">==</span><span class=""st"">&quot;Laurel&quot;</span>] <span class=""ot"">&lt;-</span> <span class=""dv"">1</span></span>
-<span id=""cb1-15""><a href=""generalized-linear-models.html#cb1-15"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb1-16""><a href=""generalized-linear-models.html#cb1-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Make some exploratory plots</span></span>
-<span id=""cb1-17""><a href=""generalized-linear-models.html#cb1-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">par</span>(<span class=""at"">mfrow=</span><span class=""fu"">c</span>(<span class=""dv"">1</span>,<span class=""dv"">2</span>))</span>
-<span id=""cb1-18""><a href=""generalized-linear-models.html#cb1-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">plot</span>(yl<span class=""sc"">$</span>age, yl<span class=""sc"">$</span>word, <span class=""at"">pch=</span><span class=""dv"">19</span>, <span class=""at"">xlab=</span><span class=""st"">&quot;age&quot;</span>, <span class=""at"">ylab=</span><span class=""st"">&quot;&quot;</span>, <span class=""at"">las=</span><span class=""dv"">1</span>)</span>
-<span id=""cb1-19""><a href=""generalized-linear-models.html#cb1-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">boxplot</span>(yl<span class=""sc"">$</span>age<span class=""sc"">~</span>yl<span class=""sc"">$</span>hear, <span class=""at"">xlab=</span><span class=""st"">&quot;&quot;</span>, <span class=""at"">ylab=</span><span class=""st"">&quot;age&quot;</span>, <span class=""at"">col=</span><span class=""st"">&quot;lightblue&quot;</span>)</span></code></pre></div>
-<div class=""figure"" style=""text-align: center""><span id=""fig:unnamed-chunk-4""></span>
-<img src=""304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png"" alt=""Yanny and Laurel auditory illusion data, Yanny (1), Luarel (0)"" width=""768"" />
+<span id=""cb1-11""><a href=""generalized-linear-models.html#cb1-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Recode Laurel to 0 and Yanny as 1 in new variable</span></span>
+<span id=""cb1-12""><a href=""generalized-linear-models.html#cb1-12"" aria-hidden=""true"" tabindex=""-1""></a>yl<span class=""sc"">$</span>word <span class=""ot"">&lt;-</span> <span class=""dv"">0</span></span>
+<span id=""cb1-13""><a href=""generalized-linear-models.html#cb1-13"" aria-hidden=""true"" tabindex=""-1""></a>yl<span class=""sc"">$</span>word[yl<span class=""sc"">$</span>hear<span class=""sc"">==</span><span class=""st"">&quot;Yanny&quot;</span>] <span class=""ot"">&lt;-</span> <span class=""dv"">1</span></span>
+<span id=""cb1-14""><a href=""generalized-linear-models.html#cb1-14"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb1-15""><a href=""generalized-linear-models.html#cb1-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># Make some exploratory plots</span></span>
+<span id=""cb1-16""><a href=""generalized-linear-models.html#cb1-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">par</span>(<span class=""at"">mfrow=</span><span class=""fu"">c</span>(<span class=""dv"">1</span>,<span class=""dv"">2</span>))</span>
+<span id=""cb1-17""><a href=""generalized-linear-models.html#cb1-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">plot</span>(yl<span class=""sc"">$</span>age, yl<span class=""sc"">$</span>word, <span class=""at"">pch=</span><span class=""dv"">19</span>, <span class=""at"">xlab=</span><span class=""st"">&quot;age&quot;</span>, <span class=""at"">ylab=</span><span class=""st"">&quot;&quot;</span>, <span class=""at"">las=</span><span class=""dv"">1</span>)</span>
+<span id=""cb1-18""><a href=""generalized-linear-models.html#cb1-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">boxplot</span>(yl<span class=""sc"">$</span>age<span class=""sc"">~</span>yl<span class=""sc"">$</span>hear, <span class=""at"">xlab=</span><span class=""st"">&quot;&quot;</span>, <span class=""at"">ylab=</span><span class=""st"">&quot;age&quot;</span>, <span class=""at"">col=</span><span class=""st"">&quot;lightblue&quot;</span>)</span></code></pre></div>
+<div class=""figure"" style=""text-align: center""><span style=""display:block;"" id=""fig:unnamed-chunk-4""></span>
+<img src=""304-linear-GLM_files/figure-html/unnamed-chunk-4-1.png"" alt=""Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0)"" width=""768"" />
 <p class=""caption"">
-Figure 1.2: Yanny and Laurel auditory illusion data, Yanny (1), Luarel (0)
+Figure 1.2: Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0)
 </p>
 </div>
 <ul>
@@ -246,12 +247,12 @@ <h2><span class=""header-section-number"">1.2</span> Logisitc regression</h2>
 ## 
 ## Deviance Residuals: 
 ##      Min        1Q    Median        3Q       Max  
-## -1.86068  -0.71414  -0.04733   0.64434   2.47887  
+## -2.47887  -0.64434   0.04733   0.71414   1.86068  
 ## 
 ## Coefficients:
 ##             Estimate Std. Error z value Pr(&gt;|z|)    
-## (Intercept) -3.56159    0.95790  -3.718 0.000201 ***
-## age          0.08943    0.02297   3.893 9.89e-05 ***
+## (Intercept)  3.56159    0.95790   3.718 0.000201 ***
+## age         -0.08943    0.02297  -3.893 9.89e-05 ***
 ## ---
 ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
 ## 
@@ -264,7 +265,7 @@ <h2><span class=""header-section-number"">1.2</span> Logisitc regression</h2>
 ## Number of Fisher Scoring iterations: 4</code></pre>
 <div class=""sourceCode"" id=""cb4""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb4-1""><a href=""generalized-linear-models.html#cb4-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># plot</span></span>
 <span id=""cb4-2""><a href=""generalized-linear-models.html#cb4-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">ggPredict</span>(logmodel<span class=""fl"">.1</span>)</span></code></pre></div>
-<div class=""figure"" style=""text-align: center""><span id=""fig:unnamed-chunk-5""></span>
+<div class=""figure"" style=""text-align: center""><span style=""display:block;"" id=""fig:unnamed-chunk-5""></span>
 <img src=""304-linear-GLM_files/figure-html/unnamed-chunk-5-1.png"" alt=""Fitted logistic model to the Yanny and Laurel data"" width=""384"" />
 <p class=""caption"">
 Figure 1.3: Fitted logistic model to the Yanny and Laurel data
@@ -275,23 +276,23 @@ <h2><span class=""header-section-number"">1.2</span> Logisitc regression</h2>
 <span id=""cb5-3""><a href=""generalized-linear-models.html#cb5-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># we specify response to return prediction on the probability scale</span></span>
 <span id=""cb5-4""><a href=""generalized-linear-models.html#cb5-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">predict</span>(logmodel<span class=""fl"">.1</span>, <span class=""at"">type=</span><span class=""st"">&quot;response&quot;</span>)</span></code></pre></div>
 <pre><code>##          1          2          3          4          5          6          7 
-## 0.50394192 0.67507724 0.33187793 0.65516152 0.85868941 0.07057982 0.87903410 
+## 0.49605808 0.32492276 0.66812207 0.34483848 0.14131059 0.92942018 0.12096590 
 ##          8          9         10         11         12         13         14 
-## 0.06493370 0.35199768 0.69437930 0.91222027 0.15663558 0.78037334 0.43719992 
+## 0.93506630 0.64800232 0.30562070 0.08777973 0.84336442 0.21962666 0.56280008 
 ##         15         16         17         18         19         20         21 
-## 0.39379165 0.59230535 0.20986467 0.45931517 0.69437930 0.22507939 0.48159183 
+## 0.60620835 0.40769465 0.79013533 0.54068483 0.30562070 0.77492061 0.51840817 
 ##         22         23         24         25         26         27         28 
-## 0.20986467 0.71302217 0.10615359 0.97322447 0.65516152 0.11494328 0.20986467 
+## 0.79013533 0.28697783 0.89384641 0.02677553 0.34483848 0.88505672 0.79013533 
 ##         29         30         31         32         33         34         35 
-## 0.12435950 0.90478991 0.87903410 0.93146108 0.15663558 0.18173908 0.33187793 
+## 0.87564050 0.09521009 0.12096590 0.06853892 0.84336442 0.81826092 0.66812207 
 ##         36         37         38         39         40         41         42 
-## 0.59230535 0.94673070 0.06493370 0.82290366 0.91222027 0.82290366 0.92552645 
+## 0.40769465 0.05326930 0.93506630 0.17709634 0.08777973 0.17709634 0.07447355 
 ##         43         44         45         46         47         48         49 
-## 0.83556313 0.04630975 0.50394192 0.37265683 0.97751124 0.45931517 0.41533155 
+## 0.16443687 0.95369025 0.49605808 0.62734317 0.02248876 0.54068483 0.58466845 
 ##         50         51         52         53         54         55         56 
-## 0.35199768 0.04630975 0.83556313 0.15663558 0.20986467 0.22507939 0.15663558 
+## 0.64800232 0.95369025 0.16443687 0.84336442 0.79013533 0.77492061 0.84336442 
 ##         57         58         59         60 
-## 0.73096842 0.35199768 0.43719992 0.87903410</code></pre>
+## 0.26903158 0.64800232 0.56280008 0.12096590</code></pre>
 <ul>
 <li>The regression equation for the fitted model is:
 <span class=""math display"">\[log(\frac{\hat{p_i}}{1-\hat{p_i}})=-3.56  +  0.09x_i\]</span></li>
@@ -336,13 +337,13 @@ <h2><span class=""header-section-number"">1.2</span> Logisitc regression</h2>
 <span id=""cb8-10""><a href=""generalized-linear-models.html#cb8-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
 <span id=""cb8-11""><a href=""generalized-linear-models.html#cb8-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Deviance Residuals: </span></span>
 <span id=""cb8-12""><a href=""generalized-linear-models.html#cb8-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##      Min        1Q    Median        3Q       Max  </span></span>
-<span id=""cb8-13""><a href=""generalized-linear-models.html#cb8-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## -1.81723  -0.72585  -0.06218   0.67360   2.44755  </span></span>
+<span id=""cb8-13""><a href=""generalized-linear-models.html#cb8-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## -2.44755  -0.67360   0.06218   0.72585   1.81723  </span></span>
 <span id=""cb8-14""><a href=""generalized-linear-models.html#cb8-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
 <span id=""cb8-15""><a href=""generalized-linear-models.html#cb8-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Coefficients:</span></span>
 <span id=""cb8-16""><a href=""generalized-linear-models.html#cb8-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
-<span id=""cb8-17""><a href=""generalized-linear-models.html#cb8-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## (Intercept) -3.72679    1.07333  -3.472 0.000516 ***</span></span>
-<span id=""cb8-18""><a href=""generalized-linear-models.html#cb8-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## age          0.09061    0.02337   3.877 0.000106 ***</span></span>
-<span id=""cb8-19""><a href=""generalized-linear-models.html#cb8-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## genderMale   0.23919    0.65938   0.363 0.716789    </span></span>
+<span id=""cb8-17""><a href=""generalized-linear-models.html#cb8-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## (Intercept)  3.72679    1.07333   3.472 0.000516 ***</span></span>
+<span id=""cb8-18""><a href=""generalized-linear-models.html#cb8-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## age         -0.09061    0.02337  -3.877 0.000106 ***</span></span>
+<span id=""cb8-19""><a href=""generalized-linear-models.html#cb8-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## genderMale  -0.23919    0.65938  -0.363 0.716789    </span></span>
 <span id=""cb8-20""><a href=""generalized-linear-models.html#cb8-20"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## ---</span></span>
 <span id=""cb8-21""><a href=""generalized-linear-models.html#cb8-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
 <span id=""cb8-22""><a href=""generalized-linear-models.html#cb8-22"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
@@ -353,10 +354,9 @@ <h2><span class=""header-section-number"">1.2</span> Logisitc regression</h2>
 <span id=""cb8-27""><a href=""generalized-linear-models.html#cb8-27"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## AIC: 63.835</span></span>
 <span id=""cb8-28""><a href=""generalized-linear-models.html#cb8-28"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
 <span id=""cb8-29""><a href=""generalized-linear-models.html#cb8-29"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Number of Fisher Scoring iterations: 5</span></span>
-<span id=""cb8-30""><a href=""generalized-linear-models.html#cb8-30"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb8-31""><a href=""generalized-linear-models.html#cb8-31"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># plot model</span></span>
-<span id=""cb8-32""><a href=""generalized-linear-models.html#cb8-32"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">ggPredict</span>(logmodel<span class=""fl"">.2</span>)</span></code></pre></div>
-<div class=""figure"" style=""text-align: center""><span id=""fig:unnamed-chunk-7""></span>
+<span id=""cb8-30""><a href=""generalized-linear-models.html#cb8-30"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># plot model</span></span>
+<span id=""cb8-31""><a href=""generalized-linear-models.html#cb8-31"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">ggPredict</span>(logmodel<span class=""fl"">.2</span>)</span></code></pre></div>
+<div class=""figure"" style=""text-align: center""><span style=""display:block;"" id=""fig:unnamed-chunk-7""></span>
 <img src=""304-linear-GLM_files/figure-html/unnamed-chunk-7-1.png"" alt=""Yanny Laurel data modelled with logistic regression given age and gender. Regression lines in males and femals are very alike and the model suggest no gender effect"" width=""384"" />
 <p class=""caption"">
 Figure 1.4: Yanny Laurel data modelled with logistic regression given age and gender. Regression lines in males and femals are very alike and the model suggest no gender effect
@@ -396,41 +396,40 @@ <h2><span class=""header-section-number"">1.3</span> Poisson regression</h2>
 <span id=""cb9-8""><a href=""generalized-linear-models.html#cb9-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 4 S02000263    80 109.10245 17.0  14.0  10.39  12.30138 25.45705 67.05938</span></span>
 <span id=""cb9-9""><a href=""generalized-linear-models.html#cb9-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 5 S02000264   181 149.77821 18.6  15.2   5.67  11.88449 26.12484 67.09280</span></span>
 <span id=""cb9-10""><a href=""generalized-linear-models.html#cb9-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 6 S02000265    77  82.31156 17.0  14.6   5.61  11.82004 25.37644 67.09826</span></span>
-<span id=""cb9-11""><a href=""generalized-linear-models.html#cb9-11"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb9-12""><a href=""generalized-linear-models.html#cb9-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># fit Poisson regression</span></span>
-<span id=""cb9-13""><a href=""generalized-linear-models.html#cb9-13"" aria-hidden=""true"" tabindex=""-1""></a>epid1 <span class=""ot"">&lt;-</span> <span class=""fu"">glm</span>(Y_all <span class=""sc"">~</span> pm10 <span class=""sc"">+</span> smoke <span class=""sc"">+</span> ethnic <span class=""sc"">+</span> log.price <span class=""sc"">+</span> easting <span class=""sc"">+</span> northing <span class=""sc"">+</span> <span class=""fu"">offset</span>(<span class=""fu"">log</span>(E_all)), </span>
-<span id=""cb9-14""><a href=""generalized-linear-models.html#cb9-14"" aria-hidden=""true"" tabindex=""-1""></a>             <span class=""at"">family =</span> poisson, </span>
-<span id=""cb9-15""><a href=""generalized-linear-models.html#cb9-15"" aria-hidden=""true"" tabindex=""-1""></a>             <span class=""at"">data =</span> cancer)</span>
-<span id=""cb9-16""><a href=""generalized-linear-models.html#cb9-16"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb9-17""><a href=""generalized-linear-models.html#cb9-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(<span class=""fu"">summary</span>(epid1))</span>
-<span id=""cb9-18""><a href=""generalized-linear-models.html#cb9-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
-<span id=""cb9-19""><a href=""generalized-linear-models.html#cb9-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Call:</span></span>
-<span id=""cb9-20""><a href=""generalized-linear-models.html#cb9-20"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + </span></span>
-<span id=""cb9-21""><a href=""generalized-linear-models.html#cb9-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     northing + offset(log(E_all)), family = poisson, data = cancer)</span></span>
-<span id=""cb9-22""><a href=""generalized-linear-models.html#cb9-22"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
-<span id=""cb9-23""><a href=""generalized-linear-models.html#cb9-23"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Deviance Residuals: </span></span>
-<span id=""cb9-24""><a href=""generalized-linear-models.html#cb9-24"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     Min       1Q   Median       3Q      Max  </span></span>
-<span id=""cb9-25""><a href=""generalized-linear-models.html#cb9-25"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## -4.2011  -0.9338  -0.1763   0.8959   3.8416  </span></span>
-<span id=""cb9-26""><a href=""generalized-linear-models.html#cb9-26"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
-<span id=""cb9-27""><a href=""generalized-linear-models.html#cb9-27"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Coefficients:</span></span>
-<span id=""cb9-28""><a href=""generalized-linear-models.html#cb9-28"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##               Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
-<span id=""cb9-29""><a href=""generalized-linear-models.html#cb9-29"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## (Intercept) -0.8592657  0.8029040  -1.070 0.284531    </span></span>
-<span id=""cb9-30""><a href=""generalized-linear-models.html#cb9-30"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## pm10         0.0500269  0.0066724   7.498 6.50e-14 ***</span></span>
-<span id=""cb9-31""><a href=""generalized-linear-models.html#cb9-31"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## smoke        0.0033516  0.0009463   3.542 0.000397 ***</span></span>
-<span id=""cb9-32""><a href=""generalized-linear-models.html#cb9-32"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## ethnic      -0.0049388  0.0006354  -7.773 7.66e-15 ***</span></span>
-<span id=""cb9-33""><a href=""generalized-linear-models.html#cb9-33"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## log.price   -0.1034461  0.0169943  -6.087 1.15e-09 ***</span></span>
-<span id=""cb9-34""><a href=""generalized-linear-models.html#cb9-34"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## easting     -0.0331305  0.0103698  -3.195 0.001399 ** </span></span>
-<span id=""cb9-35""><a href=""generalized-linear-models.html#cb9-35"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## northing     0.0300213  0.0111013   2.704 0.006845 ** </span></span>
-<span id=""cb9-36""><a href=""generalized-linear-models.html#cb9-36"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## ---</span></span>
-<span id=""cb9-37""><a href=""generalized-linear-models.html#cb9-37"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
-<span id=""cb9-38""><a href=""generalized-linear-models.html#cb9-38"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
-<span id=""cb9-39""><a href=""generalized-linear-models.html#cb9-39"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## (Dispersion parameter for poisson family taken to be 1)</span></span>
-<span id=""cb9-40""><a href=""generalized-linear-models.html#cb9-40"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
-<span id=""cb9-41""><a href=""generalized-linear-models.html#cb9-41"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     Null deviance: 972.94  on 270  degrees of freedom</span></span>
-<span id=""cb9-42""><a href=""generalized-linear-models.html#cb9-42"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Residual deviance: 565.18  on 264  degrees of freedom</span></span>
-<span id=""cb9-43""><a href=""generalized-linear-models.html#cb9-43"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## AIC: 2356.2</span></span>
-<span id=""cb9-44""><a href=""generalized-linear-models.html#cb9-44"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
-<span id=""cb9-45""><a href=""generalized-linear-models.html#cb9-45"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Number of Fisher Scoring iterations: 4</span></span></code></pre></div>
+<span id=""cb9-11""><a href=""generalized-linear-models.html#cb9-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># fit Poisson regression</span></span>
+<span id=""cb9-12""><a href=""generalized-linear-models.html#cb9-12"" aria-hidden=""true"" tabindex=""-1""></a>epid1 <span class=""ot"">&lt;-</span> <span class=""fu"">glm</span>(Y_all <span class=""sc"">~</span> pm10 <span class=""sc"">+</span> smoke <span class=""sc"">+</span> ethnic <span class=""sc"">+</span> log.price <span class=""sc"">+</span> easting <span class=""sc"">+</span> northing <span class=""sc"">+</span> <span class=""fu"">offset</span>(<span class=""fu"">log</span>(E_all)), </span>
+<span id=""cb9-13""><a href=""generalized-linear-models.html#cb9-13"" aria-hidden=""true"" tabindex=""-1""></a>             <span class=""at"">family =</span> poisson, </span>
+<span id=""cb9-14""><a href=""generalized-linear-models.html#cb9-14"" aria-hidden=""true"" tabindex=""-1""></a>             <span class=""at"">data =</span> cancer)</span>
+<span id=""cb9-15""><a href=""generalized-linear-models.html#cb9-15"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb9-16""><a href=""generalized-linear-models.html#cb9-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">print</span>(<span class=""fu"">summary</span>(epid1))</span>
+<span id=""cb9-17""><a href=""generalized-linear-models.html#cb9-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
+<span id=""cb9-18""><a href=""generalized-linear-models.html#cb9-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Call:</span></span>
+<span id=""cb9-19""><a href=""generalized-linear-models.html#cb9-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + </span></span>
+<span id=""cb9-20""><a href=""generalized-linear-models.html#cb9-20"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     northing + offset(log(E_all)), family = poisson, data = cancer)</span></span>
+<span id=""cb9-21""><a href=""generalized-linear-models.html#cb9-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
+<span id=""cb9-22""><a href=""generalized-linear-models.html#cb9-22"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Deviance Residuals: </span></span>
+<span id=""cb9-23""><a href=""generalized-linear-models.html#cb9-23"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     Min       1Q   Median       3Q      Max  </span></span>
+<span id=""cb9-24""><a href=""generalized-linear-models.html#cb9-24"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## -4.2011  -0.9338  -0.1763   0.8959   3.8416  </span></span>
+<span id=""cb9-25""><a href=""generalized-linear-models.html#cb9-25"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
+<span id=""cb9-26""><a href=""generalized-linear-models.html#cb9-26"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Coefficients:</span></span>
+<span id=""cb9-27""><a href=""generalized-linear-models.html#cb9-27"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##               Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
+<span id=""cb9-28""><a href=""generalized-linear-models.html#cb9-28"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## (Intercept) -0.8592657  0.8029040  -1.070 0.284531    </span></span>
+<span id=""cb9-29""><a href=""generalized-linear-models.html#cb9-29"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## pm10         0.0500269  0.0066724   7.498 6.50e-14 ***</span></span>
+<span id=""cb9-30""><a href=""generalized-linear-models.html#cb9-30"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## smoke        0.0033516  0.0009463   3.542 0.000397 ***</span></span>
+<span id=""cb9-31""><a href=""generalized-linear-models.html#cb9-31"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## ethnic      -0.0049388  0.0006354  -7.773 7.66e-15 ***</span></span>
+<span id=""cb9-32""><a href=""generalized-linear-models.html#cb9-32"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## log.price   -0.1034461  0.0169943  -6.087 1.15e-09 ***</span></span>
+<span id=""cb9-33""><a href=""generalized-linear-models.html#cb9-33"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## easting     -0.0331305  0.0103698  -3.195 0.001399 ** </span></span>
+<span id=""cb9-34""><a href=""generalized-linear-models.html#cb9-34"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## northing     0.0300213  0.0111013   2.704 0.006845 ** </span></span>
+<span id=""cb9-35""><a href=""generalized-linear-models.html#cb9-35"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## ---</span></span>
+<span id=""cb9-36""><a href=""generalized-linear-models.html#cb9-36"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
+<span id=""cb9-37""><a href=""generalized-linear-models.html#cb9-37"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
+<span id=""cb9-38""><a href=""generalized-linear-models.html#cb9-38"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## (Dispersion parameter for poisson family taken to be 1)</span></span>
+<span id=""cb9-39""><a href=""generalized-linear-models.html#cb9-39"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
+<span id=""cb9-40""><a href=""generalized-linear-models.html#cb9-40"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     Null deviance: 972.94  on 270  degrees of freedom</span></span>
+<span id=""cb9-41""><a href=""generalized-linear-models.html#cb9-41"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Residual deviance: 565.18  on 264  degrees of freedom</span></span>
+<span id=""cb9-42""><a href=""generalized-linear-models.html#cb9-42"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## AIC: 2356.2</span></span>
+<span id=""cb9-43""><a href=""generalized-linear-models.html#cb9-43"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## </span></span>
+<span id=""cb9-44""><a href=""generalized-linear-models.html#cb9-44"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## Number of Fisher Scoring iterations: 4</span></span></code></pre></div>
 <p><strong>Hypothesis testing, model fit and predictions</strong></p>
 <ul>
 <li>follows stay the same as for logistic regression</li>
@@ -477,58 +476,54 @@ <h2><span class=""header-section-number"">1.4</span> Exercises (GLMs)</h2>
 <li>using Poisson regression, can you comment about the numbers of cigarettes smoked (cigs)?</li>
 </ol>
 </div>
-<div class=""sourceCode"" id=""cb10""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb10-1""><a href=""generalized-linear-models.html#cb10-1"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb10-2""><a href=""generalized-linear-models.html#cb10-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(faraway)</span>
-<span id=""cb10-3""><a href=""generalized-linear-models.html#cb10-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">data</span>(wcgs, <span class=""at"">package=</span><span class=""st"">&quot;faraway&quot;</span>)</span>
-<span id=""cb10-4""><a href=""generalized-linear-models.html#cb10-4"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb10-5""><a href=""generalized-linear-models.html#cb10-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">head</span>(wcgs)</span>
-<span id=""cb10-6""><a href=""generalized-linear-models.html#cb10-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##      age height weight sdp dbp chol behave cigs dibep chd  typechd timechd</span></span>
-<span id=""cb10-7""><a href=""generalized-linear-models.html#cb10-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2001  49     73    150 110  76  225     A2   25     B  no     none    1664</span></span>
-<span id=""cb10-8""><a href=""generalized-linear-models.html#cb10-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2002  42     70    160 154  84  177     A2   20     B  no     none    3071</span></span>
-<span id=""cb10-9""><a href=""generalized-linear-models.html#cb10-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2003  42     69    160 110  78  181     B3    0     A  no     none    3071</span></span>
-<span id=""cb10-10""><a href=""generalized-linear-models.html#cb10-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2004  41     68    152 124  78  132     B4   20     A  no     none    3064</span></span>
-<span id=""cb10-11""><a href=""generalized-linear-models.html#cb10-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2005  59     70    150 144  86  255     B3   20     A yes infdeath    1885</span></span>
-<span id=""cb10-12""><a href=""generalized-linear-models.html#cb10-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2006  44     72    204 150  90  182     B4    0     A  no     none    3102</span></span>
-<span id=""cb10-13""><a href=""generalized-linear-models.html#cb10-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##        arcus</span></span>
-<span id=""cb10-14""><a href=""generalized-linear-models.html#cb10-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2001  absent</span></span>
-<span id=""cb10-15""><a href=""generalized-linear-models.html#cb10-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2002 present</span></span>
-<span id=""cb10-16""><a href=""generalized-linear-models.html#cb10-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2003  absent</span></span>
-<span id=""cb10-17""><a href=""generalized-linear-models.html#cb10-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2004  absent</span></span>
-<span id=""cb10-18""><a href=""generalized-linear-models.html#cb10-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2005 present</span></span>
-<span id=""cb10-19""><a href=""generalized-linear-models.html#cb10-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2006  absent</span></span></code></pre></div>
+<div class=""sourceCode"" id=""cb10""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb10-1""><a href=""generalized-linear-models.html#cb10-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">library</span>(faraway)</span>
+<span id=""cb10-2""><a href=""generalized-linear-models.html#cb10-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">data</span>(wcgs, <span class=""at"">package=</span><span class=""st"">&quot;faraway&quot;</span>)</span>
+<span id=""cb10-3""><a href=""generalized-linear-models.html#cb10-3"" aria-hidden=""true"" tabindex=""-1""></a></span>
+<span id=""cb10-4""><a href=""generalized-linear-models.html#cb10-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">head</span>(wcgs)</span>
+<span id=""cb10-5""><a href=""generalized-linear-models.html#cb10-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##      age height weight sdp dbp chol behave cigs dibep chd  typechd timechd</span></span>
+<span id=""cb10-6""><a href=""generalized-linear-models.html#cb10-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2001  49     73    150 110  76  225     A2   25     B  no     none    1664</span></span>
+<span id=""cb10-7""><a href=""generalized-linear-models.html#cb10-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2002  42     70    160 154  84  177     A2   20     B  no     none    3071</span></span>
+<span id=""cb10-8""><a href=""generalized-linear-models.html#cb10-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2003  42     69    160 110  78  181     B3    0     A  no     none    3071</span></span>
+<span id=""cb10-9""><a href=""generalized-linear-models.html#cb10-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2004  41     68    152 124  78  132     B4   20     A  no     none    3064</span></span>
+<span id=""cb10-10""><a href=""generalized-linear-models.html#cb10-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2005  59     70    150 144  86  255     B3   20     A yes infdeath    1885</span></span>
+<span id=""cb10-11""><a href=""generalized-linear-models.html#cb10-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2006  44     72    204 150  90  182     B4    0     A  no     none    3102</span></span>
+<span id=""cb10-12""><a href=""generalized-linear-models.html#cb10-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##        arcus</span></span>
+<span id=""cb10-13""><a href=""generalized-linear-models.html#cb10-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2001  absent</span></span>
+<span id=""cb10-14""><a href=""generalized-linear-models.html#cb10-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2002 present</span></span>
+<span id=""cb10-15""><a href=""generalized-linear-models.html#cb10-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2003  absent</span></span>
+<span id=""cb10-16""><a href=""generalized-linear-models.html#cb10-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2004  absent</span></span>
+<span id=""cb10-17""><a href=""generalized-linear-models.html#cb10-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2005 present</span></span>
+<span id=""cb10-18""><a href=""generalized-linear-models.html#cb10-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## 2006  absent</span></span></code></pre></div>
 <hr />
 <p>Answers to selected exercises</p>
 <p>Exr. <a href=""generalized-linear-models.html#exr:glm-wcgs"">1.2</a> possible solution</p>
 <ol style=""list-style-type: lower-alpha"">
 <li>probability of developing heart disease</li>
 </ol>
 <p>We first check the relationship between variables to gain more understanding of the data. We discover that a couple of variables are exactly collinear with other variables, including <code>typechd</code>, <code>timechd</code> and <code>dibep</code>. We do not include these in the model.</p>
-<div class=""sourceCode"" id=""cb11""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb11-1""><a href=""generalized-linear-models.html#cb11-1"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-2""><a href=""generalized-linear-models.html#cb11-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># `chd` and `typechd` were correlated.</span></span>
-<span id=""cb11-3""><a href=""generalized-linear-models.html#cb11-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">with</span>(wcgs, <span class=""fu"">table</span>(chd, typechd))</span>
-<span id=""cb11-4""><a href=""generalized-linear-models.html#cb11-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##      typechd</span></span>
-<span id=""cb11-5""><a href=""generalized-linear-models.html#cb11-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## chd   angina infdeath none silent</span></span>
-<span id=""cb11-6""><a href=""generalized-linear-models.html#cb11-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##   no       0        0 2897      0</span></span>
-<span id=""cb11-7""><a href=""generalized-linear-models.html#cb11-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##   yes     51      135    0     71</span></span>
-<span id=""cb11-8""><a href=""generalized-linear-models.html#cb11-8"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-9""><a href=""generalized-linear-models.html#cb11-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># `timechd` is an outcome variable affected by `chd`.</span></span>
-<span id=""cb11-10""><a href=""generalized-linear-models.html#cb11-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">by</span>(wcgs<span class=""sc"">$</span>timechd, wcgs<span class=""sc"">$</span>chd, summary)</span>
-<span id=""cb11-11""><a href=""generalized-linear-models.html#cb11-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## wcgs$chd: no</span></span>
-<span id=""cb11-12""><a href=""generalized-linear-models.html#cb11-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
-<span id=""cb11-13""><a href=""generalized-linear-models.html#cb11-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     238    2864    2952    2775    3048    3430 </span></span>
-<span id=""cb11-14""><a href=""generalized-linear-models.html#cb11-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## ------------------------------------------------------------ </span></span>
-<span id=""cb11-15""><a href=""generalized-linear-models.html#cb11-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## wcgs$chd: yes</span></span>
-<span id=""cb11-16""><a href=""generalized-linear-models.html#cb11-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
-<span id=""cb11-17""><a href=""generalized-linear-models.html#cb11-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##      18     934    1666    1655    2400    3229</span></span>
-<span id=""cb11-18""><a href=""generalized-linear-models.html#cb11-18"" aria-hidden=""true"" tabindex=""-1""></a></span>
-<span id=""cb11-19""><a href=""generalized-linear-models.html#cb11-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># `behave` has more detailed info of `dibep` -&gt; exact collinearity</span></span>
-<span id=""cb11-20""><a href=""generalized-linear-models.html#cb11-20"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">with</span>(wcgs, <span class=""fu"">table</span>(behave, dibep))</span>
-<span id=""cb11-21""><a href=""generalized-linear-models.html#cb11-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##       dibep</span></span>
-<span id=""cb11-22""><a href=""generalized-linear-models.html#cb11-22"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## behave    A    B</span></span>
-<span id=""cb11-23""><a href=""generalized-linear-models.html#cb11-23"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     A1    0  264</span></span>
-<span id=""cb11-24""><a href=""generalized-linear-models.html#cb11-24"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     A2    0 1325</span></span>
-<span id=""cb11-25""><a href=""generalized-linear-models.html#cb11-25"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     B3 1216    0</span></span>
-<span id=""cb11-26""><a href=""generalized-linear-models.html#cb11-26"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     B4  349    0</span></span></code></pre></div>
+<div class=""sourceCode"" id=""cb11""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb11-1""><a href=""generalized-linear-models.html#cb11-1"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># `chd` and `typechd` were correlated.</span></span>
+<span id=""cb11-2""><a href=""generalized-linear-models.html#cb11-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">with</span>(wcgs, <span class=""fu"">table</span>(chd, typechd))</span>
+<span id=""cb11-3""><a href=""generalized-linear-models.html#cb11-3"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##      typechd</span></span>
+<span id=""cb11-4""><a href=""generalized-linear-models.html#cb11-4"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## chd   angina infdeath none silent</span></span>
+<span id=""cb11-5""><a href=""generalized-linear-models.html#cb11-5"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##   no       0        0 2897      0</span></span>
+<span id=""cb11-6""><a href=""generalized-linear-models.html#cb11-6"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##   yes     51      135    0     71</span></span>
+<span id=""cb11-7""><a href=""generalized-linear-models.html#cb11-7"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># `timechd` is an outcome variable affected by `chd`.</span></span>
+<span id=""cb11-8""><a href=""generalized-linear-models.html#cb11-8"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">by</span>(wcgs<span class=""sc"">$</span>timechd, wcgs<span class=""sc"">$</span>chd, summary)</span>
+<span id=""cb11-9""><a href=""generalized-linear-models.html#cb11-9"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## wcgs$chd: no</span></span>
+<span id=""cb11-10""><a href=""generalized-linear-models.html#cb11-10"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
+<span id=""cb11-11""><a href=""generalized-linear-models.html#cb11-11"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     238    2864    2952    2775    3048    3430 </span></span>
+<span id=""cb11-12""><a href=""generalized-linear-models.html#cb11-12"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## ------------------------------------------------------------ </span></span>
+<span id=""cb11-13""><a href=""generalized-linear-models.html#cb11-13"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## wcgs$chd: yes</span></span>
+<span id=""cb11-14""><a href=""generalized-linear-models.html#cb11-14"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
+<span id=""cb11-15""><a href=""generalized-linear-models.html#cb11-15"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##      18     934    1666    1655    2400    3229</span></span>
+<span id=""cb11-16""><a href=""generalized-linear-models.html#cb11-16"" aria-hidden=""true"" tabindex=""-1""></a><span class=""co""># `behave` has more detailed info of `dibep` -&gt; exact collinearity</span></span>
+<span id=""cb11-17""><a href=""generalized-linear-models.html#cb11-17"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">with</span>(wcgs, <span class=""fu"">table</span>(behave, dibep))</span>
+<span id=""cb11-18""><a href=""generalized-linear-models.html#cb11-18"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##       dibep</span></span>
+<span id=""cb11-19""><a href=""generalized-linear-models.html#cb11-19"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">## behave    A    B</span></span>
+<span id=""cb11-20""><a href=""generalized-linear-models.html#cb11-20"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     A1    0  264</span></span>
+<span id=""cb11-21""><a href=""generalized-linear-models.html#cb11-21"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     A2    0 1325</span></span>
+<span id=""cb11-22""><a href=""generalized-linear-models.html#cb11-22"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     B3 1216    0</span></span>
+<span id=""cb11-23""><a href=""generalized-linear-models.html#cb11-23"" aria-hidden=""true"" tabindex=""-1""></a><span class=""do"">##     B4  349    0</span></span></code></pre></div>
 <p>We fit logistic regression model to explain the probability of developing cardiac disease (<code>chd</code>) given the remaining variables</p>
 <div class=""sourceCode"" id=""cb12""><pre class=""sourceCode r""><code class=""sourceCode r""><span id=""cb12-1""><a href=""generalized-linear-models.html#cb12-1"" aria-hidden=""true"" tabindex=""-1""></a>model1 <span class=""ot"">&lt;-</span> <span class=""fu"">glm</span>(chd <span class=""sc"">~</span> . <span class=""sc"">-</span> typechd <span class=""sc"">-</span> timechd <span class=""sc"">-</span> dibep, <span class=""at"">data =</span> wcgs, <span class=""at"">family =</span> binomial)</span>
 <span id=""cb12-2""><a href=""generalized-linear-models.html#cb12-2"" aria-hidden=""true"" tabindex=""-1""></a><span class=""fu"">summary</span>(model1)</span></code></pre></div>
@@ -663,7 +658,6 @@ <h2><span class=""header-section-number"">1.4</span> Exercises (GLMs)</h2>
     </div>
   </div>
 <script src=""libs/gitbook-2.6.7/js/app.min.js""></script>
-<script src=""libs/gitbook-2.6.7/js/lunr.js""></script>
 <script src=""libs/gitbook-2.6.7/js/clipboard.min.js""></script>
 <script src=""libs/gitbook-2.6.7/js/plugin-search.js""></script>
 <script src=""libs/gitbook-2.6.7/js/plugin-sharing.js""></script>
@@ -682,6 +676,7 @@ <h2><span class=""header-section-number"">1.4</span> Exercises (GLMs)</h2>
 ""weibo"": false,
 ""instapaper"": false,
 ""vk"": false,
+""whatsapp"": false,
 ""all"": [""facebook"", ""twitter"", ""linkedin"", ""weibo"", ""instapaper""]
 },
 ""fontsettings"": {
@@ -702,6 +697,10 @@ <h2><span class=""header-section-number"">1.4</span> Exercises (GLMs)</h2>
 ""text"": null
 },
 ""download"": null,
+""search"": {
+""engine"": ""fuse"",
+""options"": null
+},
 ""toc"": {
 ""collapse"": ""section"",
 ""scroll_highlight"": true

---FILE: session-glm/docs/index.html---
@@ -6,7 +6,7 @@
   <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
   <title>Introduction to GLM</title>
   <meta name=""description"" content=""Introduction to GLM"" />
-  <meta name=""generator"" content=""bookdown 0.21 and GitBook 2.6.7"" />
+  <meta name=""generator"" content=""bookdown 0.24 and GitBook 2.6.7"" />
 
   <meta property=""og:title"" content=""Introduction to GLM"" />
   <meta property=""og:type"" content=""book"" />
@@ -24,7 +24,7 @@
 <meta name=""author"" content=""Olga Dethlefsen"" />
 
 
-<meta name=""date"" content=""2022-09-01"" />
+<meta name=""date"" content=""2022-09-02"" />
 
   <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />
   <meta name=""apple-mobile-web-app-capable"" content=""yes"" />
@@ -33,8 +33,9 @@
   
 
 <link rel=""next"" href=""generalized-linear-models.html""/>
-<script src=""libs/header-attrs-2.14/header-attrs.js""></script>
-<script src=""libs/jquery-2.2.3/jquery.min.js""></script>
+<script src=""libs/header-attrs-2.10/header-attrs.js""></script>
+<script src=""libs/jquery-3.6.0/jquery-3.6.0.min.js""></script>
+<script src=""https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js""></script>
 <link href=""libs/gitbook-2.6.7/css/style.css"" rel=""stylesheet"" />
 <link href=""libs/gitbook-2.6.7/css/plugin-table.css"" rel=""stylesheet"" />
 <link href=""libs/gitbook-2.6.7/css/plugin-bookdown.css"" rel=""stylesheet"" />
@@ -50,7 +51,8 @@
 
 
 
-
+<link href=""libs/anchor-sections-1.0.1/anchor-sections.css"" rel=""stylesheet"" />
+<script src=""libs/anchor-sections-1.0.1/anchor-sections.js""></script>
 
 
 <style type=""text/css"">
@@ -118,6 +120,7 @@
 code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
 </style>
 
+
 <link rel=""stylesheet"" href=""style.css"" type=""text/css"" />
 </head>
 
@@ -138,7 +141,7 @@
 <li><a href=""generalized-linear-models.html#generalized-linear-models"" id=""toc-generalized-linear-models""><span class=""toc-section-number"">1</span> Generalized linear models</a>
 <ul>
 <li><a href=""generalized-linear-models.html#why-generalized-linear-models-glms"" id=""toc-why-generalized-linear-models-glms""><span class=""toc-section-number"">1.1</span> Why Generalized Linear Models (GLMs)</a></li>
-<li><a href=""generalized-linear-models.html#logisitc-regression"" id=""toc-logisitc-regression""><span class=""toc-section-number"">1.2</span> Logisitc regression</a></li>
+<li><a href=""generalized-linear-models.html#logistic-regression"" id=""toc-logistic-regression""><span class=""toc-section-number"">1.2</span> Logistic regression</a></li>
 <li><a href=""generalized-linear-models.html#poisson-regression"" id=""toc-poisson-regression""><span class=""toc-section-number"">1.3</span> Poisson regression</a></li>
 <li><a href=""generalized-linear-models.html#exercises-glms"" id=""toc-exercises-glms""><span class=""toc-section-number"">1.4</span> Exercises (GLMs)</a></li>
 </ul></li>
@@ -162,7 +165,7 @@ <h1>
 <div id=""header"">
 <h1 class=""title"">Introduction to GLM</h1>
 <p class=""author""><em>Olga Dethlefsen</em></p>
-<p class=""date""><em>2022-09-01</em></p>
+<p class=""date""><em>2022-09-02</em></p>
 </div>
 <div id=""preface"" class=""section level1 unnumbered"">
 <h1>Preface</h1>
@@ -181,7 +184,6 @@ <h1>Preface</h1>
     </div>
   </div>
 <script src=""libs/gitbook-2.6.7/js/app.min.js""></script>
-<script src=""libs/gitbook-2.6.7/js/lunr.js""></script>
 <script src=""libs/gitbook-2.6.7/js/clipboard.min.js""></script>
 <script src=""libs/gitbook-2.6.7/js/plugin-search.js""></script>
 <script src=""libs/gitbook-2.6.7/js/plugin-sharing.js""></script>
@@ -200,6 +202,7 @@ <h1>Preface</h1>
 ""weibo"": false,
 ""instapaper"": false,
 ""vk"": false,
+""whatsapp"": false,
 ""all"": [""facebook"", ""twitter"", ""linkedin"", ""weibo"", ""instapaper""]
 },
 ""fontsettings"": {
@@ -220,6 +223,10 @@ <h1>Preface</h1>
 ""text"": null
 },
 ""download"": null,
+""search"": {
+""engine"": ""fuse"",
+""options"": null
+},
 ""toc"": {
 ""collapse"": ""section"",
 ""scroll_highlight"": true

---FILE: session-glm/docs/index.md---
@@ -3,7 +3,7 @@ title: ""Introduction to GLM""
 author: ""Olga Dethlefsen""
 site: bookdown::bookdown_site
 documentclass: book
-date: ""2022-09-01""  
+date: ""2022-09-02""  
 bibliography: [book.bib]
 biblio-style: apalike
 link-citations: yes

---FILE: session-glm/docs/libs/gitbook-2.6.7/css/plugin-bookdown.css---
@@ -97,3 +97,9 @@ div.proof>*:last-child:after {
 .header-section-number {
   padding-right: .5em;
 }
+#header .multi-author {
+  margin: 0.5em 0 -0.5em 0;
+}
+#header .date {
+  margin-top: 1.5em;
+}

---FILE: session-glm/docs/libs/gitbook-2.6.7/css/plugin-fontsettings.css---
@@ -44,13 +44,24 @@
   line-height: 30px;
   font-size: 1em;
 }
+
+/* sidebar transition background */
+div.book.color-theme-1 {
+  background: #f3eacb;
+}
 .book.color-theme-1 .book-body {
   color: #704214;
   background: #f3eacb;
 }
 .book.color-theme-1 .book-body .page-wrapper .page-inner section {
   background: #f3eacb;
 }
+
+/* sidebar transition background */
+div.book.color-theme-2 {
+  background: #1c1f2b;
+}
+
 .book.color-theme-2 .book-body {
   color: #bdcadb;
   background: #1c1f2b;

---FILE: session-glm/docs/libs/gitbook-2.6.7/css/style.css---
@@ -4,7 +4,12 @@
  * Open sourced under MIT license by @mdo.
  * Some variables and mixins from Bootstrap (Apache 2 license).
  */.link-inherit,.link-inherit:focus,.link-inherit:hover{color:inherit}.fa,.fa-stack{display:inline-block}/*!
- *  Font Awesome 4.1.0 by @davegandy - http://fontawesome.io - @fontawesome
+ *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
  *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
- */@font-face{font-family:FontAwesome;src:url(./fontawesome/fontawesome-webfont.ttf?v=4.1.0) format('truetype');font-weight:400;font-style:normal}.fa{font-family:FontAwesome;font-style:normal;font-weight:400;line-height:1;-moz-osx-font-smoothing:grayscale}.book .book-header,.book .book-summary{font-family:""Helvetica Neue"",Helvetica,Arial,sans-serif}.fa-lg{font-size:1.33333333em;line-height:.75em;vertical-align:-15%}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-fw{width:1.28571429em;text-align:center}.fa-ul{padding-left:0;margin-left:2.14285714em;list-style-type:none}.fa-li{position:absolute;left:-2.14285714em;width:2.14285714em;top:.14285714em;text-align:center}.fa-li.fa-lg{left:-1.85714286em}.fa-border{padding:.2em .25em .15em;border:.08em solid #eee;border-radius:.1em}.pull-right{float:right}.pull-left{float:left}.fa.pull-left{margin-right:.3em}.fa.pull-right{margin-left:.3em}.fa-spin{-webkit-animation:spin 2s infinite linear;-moz-animation:spin 2s infinite linear;-o-animation:spin 2s infinite linear;animation:spin 2s infinite linear}@-moz-keyframes spin{0%{-moz-transform:rotate(0)}100%{-moz-transform:rotate(359deg)}}@-webkit-keyframes spin{0%{-webkit-transform:rotate(0)}100%{-webkit-transform:rotate(359deg)}}@-o-keyframes spin{0%{-o-transform:rotate(0)}100%{-o-transform:rotate(359deg)}}@keyframes spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(359deg);transform:rotate(359deg)}}.fa-rotate-90{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=1);-webkit-transform:rotate(90deg);-moz-transform:rotate(90deg);-ms-transform:rotate(90deg);-o-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=2);-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=3);-webkit-transform:rotate(270deg);-moz-transform:rotate(270deg);-ms-transform:rotate(270deg);-o-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);-webkit-transform:scale(-1,1);-moz-transform:scale(-1,1);-ms-transform:scale(-1,1);-o-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);-webkit-transform:scale(1,-1);-moz-transform:scale(1,-1);-ms-transform:scale(1,-1);-o-transform:scale(1,-1);transform:scale(1,-1)}.fa-stack{position:relative;width:2em;height:2em;line-height:2em;vertical-align:middle}.fa-stack-1x,.fa-stack-2x{position:absolute;left:0;width:100%;text-align:center}.fa-stack-1x{line-height:inherit}.fa-stack-2x{font-size:2em}.fa-inverse{color:#fff}.fa-glass:before{content:""\f000""}.fa-music:before{content:""\f001""}.fa-search:before{content:""\f002""}.fa-envelope-o:before{content:""\f003""}.fa-heart:before{content:""\f004""}.fa-star:before{content:""\f005""}.fa-star-o:before{content:""\f006""}.fa-user:before{content:""\f007""}.fa-film:before{content:""\f008""}.fa-th-large:before{content:""\f009""}.fa-th:before{content:""\f00a""}.fa-th-list:before{content:""\f00b""}.fa-check:before{content:""\f00c""}.fa-times:before{content:""\f00d""}.fa-search-plus:before{content:""\f00e""}.fa-search-minus:before{content:""\f010""}.fa-power-off:before{content:""\f011""}.fa-signal:before{content:""\f012""}.fa-cog:before,.fa-gear:before{content:""\f013""}.fa-trash-o:before{content:""\f014""}.fa-home:before{content:""\f015""}.fa-file-o:before{content:""\f016""}.fa-clock-o:before{content:""\f017""}.fa-road:before{content:""\f018""}.fa-download:before{content:""\f019""}.fa-arrow-circle-o-down:before{content:""\f01a""}.fa-arrow-circle-o-up:before{content:""\f01b""}.fa-inbox:before{content:""\f01c""}.fa-play-circle-o:before{content:""\f01d""}.fa-repeat:before,.fa-rotate-right:before{content:""\f01e""}.fa-refresh:before{content:""\f021""}.fa-list-alt:before{content:""\f022""}.fa-lock:before{content:""\f023""}.fa-flag:before{content:""\f024""}.fa-headphones:before{content:""\f025""}.fa-volume-off:before{content:""\f026""}.fa-volume-down:before{content:""\f027""}.fa-volume-up:before{content:""\f028""}.fa-qrcode:before{content:""\f029""}.fa-barcode:before{content:""\f02a""}.fa-tag:before{content:""\f02b""}.fa-tags:before{content:""\f02c""}.fa-book:before{content:""\f02d""}.fa-bookmark:before{content:""\f02e""}.fa-print:before{content:""\f02f""}.fa-camera:before{content:""\f030""}.fa-font:before{content:""\f031""}.fa-bold:before{content:""\f032""}.fa-italic:before{content:""\f033""}.fa-text-height:before{content:""\f034""}.fa-text-width:before{content:""\f035""}.fa-align-left:before{content:""\f036""}.fa-align-center:before{content:""\f037""}.fa-align-right:before{content:""\f038""}.fa-align-justify:before{content:""\f039""}.fa-list:before{content:""\f03a""}.fa-dedent:before,.fa-outdent:before{content:""\f03b""}.fa-indent:before{content:""\f03c""}.fa-video-camera:before{content:""\f03d""}.fa-image:before,.fa-photo:before,.fa-picture-o:before{content:""\f03e""}.fa-pencil:before{content:""\f040""}.fa-map-marker:before{content:""\f041""}.fa-adjust:before{content:""\f042""}.fa-tint:before{content:""\f043""}.fa-edit:before,.fa-pencil-square-o:before{content:""\f044""}.fa-share-square-o:before{content:""\f045""}.fa-check-square-o:before{content:""\f046""}.fa-arrows:before{content:""\f047""}.fa-step-backward:before{content:""\f048""}.fa-fast-backward:before{content:""\f049""}.fa-backward:before{content:""\f04a""}.fa-play:before{content:""\f04b""}.fa-pause:before{content:""\f04c""}.fa-stop:before{content:""\f04d""}.fa-forward:before{content:""\f04e""}.fa-fast-forward:before{content:""\f050""}.fa-step-forward:before{content:""\f051""}.fa-eject:before{content:""\f052""}.fa-chevron-left:before{content:""\f053""}.fa-chevron-right:before{content:""\f054""}.fa-plus-circle:before{content:""\f055""}.fa-minus-circle:before{content:""\f056""}.fa-times-circle:before{content:""\f057""}.fa-check-circle:before{content:""\f058""}.fa-question-circle:before{content:""\f059""}.fa-info-circle:before{content:""\f05a""}.fa-crosshairs:before{content:""\f05b""}.fa-times-circle-o:before{content:""\f05c""}.fa-check-circle-o:before{content:""\f05d""}.fa-ban:before{content:""\f05e""}.fa-arrow-left:before{content:""\f060""}.fa-arrow-right:before{content:""\f061""}.fa-arrow-up:before{content:""\f062""}.fa-arrow-down:before{content:""\f063""}.fa-mail-forward:before,.fa-share:before{content:""\f064""}.fa-expand:before{content:""\f065""}.fa-compress:before{content:""\f066""}.fa-plus:before{content:""\f067""}.fa-minus:before{content:""\f068""}.fa-asterisk:before{content:""\f069""}.fa-exclamation-circle:before{content:""\f06a""}.fa-gift:before{content:""\f06b""}.fa-leaf:before{content:""\f06c""}.fa-fire:before{content:""\f06d""}.fa-eye:before{content:""\f06e""}.fa-eye-slash:before{content:""\f070""}.fa-exclamation-triangle:before,.fa-warning:before{content:""\f071""}.fa-plane:before{content:""\f072""}.fa-calendar:before{content:""\f073""}.fa-random:before{content:""\f074""}.fa-comment:before{content:""\f075""}.fa-magnet:before{content:""\f076""}.fa-chevron-up:before{content:""\f077""}.fa-chevron-down:before{content:""\f078""}.fa-retweet:before{content:""\f079""}.fa-shopping-cart:before{content:""\f07a""}.fa-folder:before{content:""\f07b""}.fa-folder-open:before{content:""\f07c""}.fa-arrows-v:before{content:""\f07d""}.fa-arrows-h:before{content:""\f07e""}.fa-bar-chart-o:before{content:""\f080""}.fa-twitter-square:before{content:""\f081""}.fa-facebook-square:before{content:""\f082""}.fa-camera-retro:before{content:""\f083""}.fa-key:before{content:""\f084""}.fa-cogs:before,.fa-gears:before{content:""\f085""}.fa-comments:before{content:""\f086""}.fa-thumbs-o-up:before{content:""\f087""}.fa-thumbs-o-down:before{content:""\f088""}.fa-star-half:before{content:""\f089""}.fa-heart-o:before{content:""\f08a""}.fa-sign-out:before{content:""\f08b""}.fa-linkedin-square:before{content:""\f08c""}.fa-thumb-tack:before{content:""\f08d""}.fa-external-link:before{content:""\f08e""}.fa-sign-in:before{content:""\f090""}.fa-trophy:before{content:""\f091""}.fa-github-square:before{content:""\f092""}.fa-upload:before{content:""\f093""}.fa-lemon-o:before{content:""\f094""}.fa-phone:before{content:""\f095""}.fa-square-o:before{content:""\f096""}.fa-bookmark-o:before{content:""\f097""}.fa-phone-square:before{content:""\f098""}.fa-twitter:before{content:""\f099""}.fa-facebook:before{content:""\f09a""}.fa-github:before{content:""\f09b""}.fa-unlock:before{content:""\f09c""}.fa-credit-card:before{content:""\f09d""}.fa-rss:before{content:""\f09e""}.fa-hdd-o:before{content:""\f0a0""}.fa-bullhorn:before{content:""\f0a1""}.fa-bell:before{content:""\f0f3""}.fa-certificate:before{content:""\f0a3""}.fa-hand-o-right:before{content:""\f0a4""}.fa-hand-o-left:before{content:""\f0a5""}.fa-hand-o-up:before{content:""\f0a6""}.fa-hand-o-down:before{content:""\f0a7""}.fa-arrow-circle-left:before{content:""\f0a8""}.fa-arrow-circle-right:before{content:""\f0a9""}.fa-arrow-circle-up:before{content:""\f0aa""}.fa-arrow-circle-down:before{content:""\f0ab""}.fa-globe:before{content:""\f0ac""}.fa-wrench:before{content:""\f0ad""}.fa-tasks:before{content:""\f0ae""}.fa-filter:before{content:""\f0b0""}.fa-briefcase:before{content:""\f0b1""}.fa-arrows-alt:before{content:""\f0b2""}.fa-group:before,.fa-users:before{content:""\f0c0""}.fa-chain:before,.fa-link:before{content:""\f0c1""}.fa-cloud:before{content:""\f0c2""}.fa-flask:before{content:""\f0c3""}.fa-cut:before,.fa-scissors:before{content:""\f0c4""}.fa-copy:before,.fa-files-o:before{content:""\f0c5""}.fa-paperclip:before{content:""\f0c6""}.fa-floppy-o:before,.fa-save:before{content:""\f0c7""}.fa-square:before{content:""\f0c8""}.fa-bars:before,.fa-navicon:before,.fa-reorder:before{content:""\f0c9""}.fa-list-ul:before{content:""\f0ca""}.fa-list-ol:before{content:""\f0cb""}.fa-strikethrough:before{content:""\f0cc""}.fa-underline:before{content:""\f0cd""}.fa-table:before{content:""\f0ce""}.fa-magic:before{content:""\f0d0""}.fa-truck:before{content:""\f0d1""}.fa-pinterest:before{content:""\f0d2""}.fa-pinterest-square:before{content:""\f0d3""}.fa-google-plus-square:before{content:""\f0d4""}.fa-google-plus:before{content:""\f0d5""}.fa-money:before{content:""\f0d6""}.fa-caret-down:before{content:""\f0d7""}.fa-caret-up:before{content:""\f0d8""}.fa-caret-left:before{content:""\f0d9""}.fa-caret-right:before{content:""\f0da""}.fa-columns:before{content:""\f0db""}.fa-sort:before,.fa-unsorted:before{content:""\f0dc""}.fa-sort-desc:before,.fa-sort-down:before{content:""\f0dd""}.fa-sort-asc:before,.fa-sort-up:before{content:""\f0de""}.fa-envelope:before{content:""\f0e0""}.fa-linkedin:before{content:""\f0e1""}.fa-rotate-left:before,.fa-undo:before{content:""\f0e2""}.fa-gavel:before,.fa-legal:before{content:""\f0e3""}.fa-dashboard:before,.fa-tachometer:before{content:""\f0e4""}.fa-comment-o:before{content:""\f0e5""}.fa-comments-o:before{content:""\f0e6""}.fa-bolt:before,.fa-flash:before{content:""\f0e7""}.fa-sitemap:before{content:""\f0e8""}.fa-umbrella:before{content:""\f0e9""}.fa-clipboard:before,.fa-paste:before{content:""\f0ea""}.fa-lightbulb-o:before{content:""\f0eb""}.fa-exchange:before{content:""\f0ec""}.fa-cloud-download:before{content:""\f0ed""}.fa-cloud-upload:before{content:""\f0ee""}.fa-user-md:before{content:""\f0f0""}.fa-stethoscope:before{content:""\f0f1""}.fa-suitcase:before{content:""\f0f2""}.fa-bell-o:before{content:""\f0a2""}.fa-coffee:before{content:""\f0f4""}.fa-cutlery:before{content:""\f0f5""}.fa-file-text-o:before{content:""\f0f6""}.fa-building-o:before{content:""\f0f7""}.fa-hospital-o:before{content:""\f0f8""}.fa-ambulance:before{content:""\f0f9""}.fa-medkit:before{content:""\f0fa""}.fa-fighter-jet:before{content:""\f0fb""}.fa-beer:before{content:""\f0fc""}.fa-h-square:before{content:""\f0fd""}.fa-plus-square:before{content:""\f0fe""}.fa-angle-double-left:before{content:""\f100""}.fa-angle-double-right:before{content:""\f101""}.fa-angle-double-up:before{content:""\f102""}.fa-angle-double-down:before{content:""\f103""}.fa-angle-left:before{content:""\f104""}.fa-angle-right:before{content:""\f105""}.fa-angle-up:before{content:""\f106""}.fa-angle-down:before{content:""\f107""}.fa-desktop:before{content:""\f108""}.fa-laptop:before{content:""\f109""}.fa-tablet:before{content:""\f10a""}.fa-mobile-phone:before,.fa-mobile:before{content:""\f10b""}.fa-circle-o:before{content:""\f10c""}.fa-quote-left:before{content:""\f10d""}.fa-quote-right:before{content:""\f10e""}.fa-spinner:before{content:""\f110""}.fa-circle:before{content:""\f111""}.fa-mail-reply:before,.fa-reply:before{content:""\f112""}.fa-github-alt:before{content:""\f113""}.fa-folder-o:before{content:""\f114""}.fa-folder-open-o:before{content:""\f115""}.fa-smile-o:before{content:""\f118""}.fa-frown-o:before{content:""\f119""}.fa-meh-o:before{content:""\f11a""}.fa-gamepad:before{content:""\f11b""}.fa-keyboard-o:before{content:""\f11c""}.fa-flag-o:before{content:""\f11d""}.fa-flag-checkered:before{content:""\f11e""}.fa-terminal:before{content:""\f120""}.fa-code:before{content:""\f121""}.fa-mail-reply-all:before,.fa-reply-all:before{content:""\f122""}.fa-star-half-empty:before,.fa-star-half-full:before,.fa-star-half-o:before{content:""\f123""}.fa-location-arrow:before{content:""\f124""}.fa-crop:before{content:""\f125""}.fa-code-fork:before{content:""\f126""}.fa-chain-broken:before,.fa-unlink:before{content:""\f127""}.fa-question:before{content:""\f128""}.fa-info:before{content:""\f129""}.fa-exclamation:before{content:""\f12a""}.fa-superscript:before{content:""\f12b""}.fa-subscript:before{content:""\f12c""}.fa-eraser:before{content:""\f12d""}.fa-puzzle-piece:before{content:""\f12e""}.fa-microphone:before{content:""\f130""}.fa-microphone-slash:before{content:""\f131""}.fa-shield:before{content:""\f132""}.fa-calendar-o:before{content:""\f133""}.fa-fire-extinguisher:before{content:""\f134""}.fa-rocket:before{content:""\f135""}.fa-maxcdn:before{content:""\f136""}.fa-chevron-circle-left:before{content:""\f137""}.fa-chevron-circle-right:before{content:""\f138""}.fa-chevron-circle-up:before{content:""\f139""}.fa-chevron-circle-down:before{content:""\f13a""}.fa-html5:before{content:""\f13b""}.fa-css3:before{content:""\f13c""}.fa-anchor:before{content:""\f13d""}.fa-unlock-alt:before{content:""\f13e""}.fa-bullseye:before{content:""\f140""}.fa-ellipsis-h:before{content:""\f141""}.fa-ellipsis-v:before{content:""\f142""}.fa-rss-square:before{content:""\f143""}.fa-play-circle:before{content:""\f144""}.fa-ticket:before{content:""\f145""}.fa-minus-square:before{content:""\f146""}.fa-minus-square-o:before{content:""\f147""}.fa-level-up:before{content:""\f148""}.fa-level-down:before{content:""\f149""}.fa-check-square:before{content:""\f14a""}.fa-pencil-square:before{content:""\f14b""}.fa-external-link-square:before{content:""\f14c""}.fa-share-square:before{content:""\f14d""}.fa-compass:before{content:""\f14e""}.fa-caret-square-o-down:before,.fa-toggle-down:before{content:""\f150""}.fa-caret-square-o-up:before,.fa-toggle-up:before{content:""\f151""}.fa-caret-square-o-right:before,.fa-toggle-right:before{content:""\f152""}.fa-eur:before,.fa-euro:before{content:""\f153""}.fa-gbp:before{content:""\f154""}.fa-dollar:before,.fa-usd:before{content:""\f155""}.fa-inr:before,.fa-rupee:before{content:""\f156""}.fa-cny:before,.fa-jpy:before,.fa-rmb:before,.fa-yen:before{content:""\f157""}.fa-rouble:before,.fa-rub:before,.fa-ruble:before{content:""\f158""}.fa-krw:before,.fa-won:before{content:""\f159""}.fa-bitcoin:before,.fa-btc:before{content:""\f15a""}.fa-file:before{content:""\f15b""}.fa-file-text:before{content:""\f15c""}.fa-sort-alpha-asc:before{content:""\f15d""}.fa-sort-alpha-desc:before{content:""\f15e""}.fa-sort-amount-asc:before{content:""\f160""}.fa-sort-amount-desc:before{content:""\f161""}.fa-sort-numeric-asc:before{content:""\f162""}.fa-sort-numeric-desc:before{content:""\f163""}.fa-thumbs-up:before{content:""\f164""}.fa-thumbs-down:before{content:""\f165""}.fa-youtube-square:before{content:""\f166""}.fa-youtube:before{content:""\f167""}.fa-xing:before{content:""\f168""}.fa-xing-square:before{content:""\f169""}.fa-youtube-play:before{content:""\f16a""}.fa-dropbox:before{content:""\f16b""}.fa-stack-overflow:before{content:""\f16c""}.fa-instagram:before{content:""\f16d""}.fa-flickr:before{content:""\f16e""}.fa-adn:before{content:""\f170""}.fa-bitbucket:before{content:""\f171""}.fa-bitbucket-square:before{content:""\f172""}.fa-tumblr:before{content:""\f173""}.fa-tumblr-square:before{content:""\f174""}.fa-long-arrow-down:before{content:""\f175""}.fa-long-arrow-up:before{content:""\f176""}.fa-long-arrow-left:before{content:""\f177""}.fa-long-arrow-right:before{content:""\f178""}.fa-apple:before{content:""\f179""}.fa-windows:before{content:""\f17a""}.fa-android:before{content:""\f17b""}.fa-linux:before{content:""\f17c""}.fa-dribbble:before{content:""\f17d""}.fa-skype:before{content:""\f17e""}.fa-foursquare:before{content:""\f180""}.fa-trello:before{content:""\f181""}.fa-female:before{content:""\f182""}.fa-male:before{content:""\f183""}.fa-gittip:before{content:""\f184""}.fa-sun-o:before{content:""\f185""}.fa-moon-o:before{content:""\f186""}.fa-archive:before{content:""\f187""}.fa-bug:before{content:""\f188""}.fa-vk:before{content:""\f189""}.fa-weibo:before{content:""\f18a""}.fa-renren:before{content:""\f18b""}.fa-pagelines:before{content:""\f18c""}.fa-stack-exchange:before{content:""\f18d""}.fa-arrow-circle-o-right:before{content:""\f18e""}.fa-arrow-circle-o-left:before{content:""\f190""}.fa-caret-square-o-left:before,.fa-toggle-left:before{content:""\f191""}.fa-dot-circle-o:before{content:""\f192""}.fa-wheelchair:before{content:""\f193""}.fa-vimeo-square:before{content:""\f194""}.fa-try:before,.fa-turkish-lira:before{content:""\f195""}.fa-plus-square-o:before{content:""\f196""}.fa-space-shuttle:before{content:""\f197""}.fa-slack:before{content:""\f198""}.fa-envelope-square:before{content:""\f199""}.fa-wordpress:before{content:""\f19a""}.fa-openid:before{content:""\f19b""}.fa-bank:before,.fa-institution:before,.fa-university:before{content:""\f19c""}.fa-graduation-cap:before,.fa-mortar-board:before{content:""\f19d""}.fa-yahoo:before{content:""\f19e""}.fa-google:before{content:""\f1a0""}.fa-reddit:before{content:""\f1a1""}.fa-reddit-square:before{content:""\f1a2""}.fa-stumbleupon-circle:before{content:""\f1a3""}.fa-stumbleupon:before{content:""\f1a4""}.fa-delicious:before{content:""\f1a5""}.fa-digg:before{content:""\f1a6""}.fa-pied-piper-square:before,.fa-pied-piper:before{content:""\f1a7""}.fa-pied-piper-alt:before{content:""\f1a8""}.fa-drupal:before{content:""\f1a9""}.fa-joomla:before{content:""\f1aa""}.fa-language:before{content:""\f1ab""}.fa-fax:before{content:""\f1ac""}.fa-building:before{content:""\f1ad""}.fa-child:before{content:""\f1ae""}.fa-paw:before{content:""\f1b0""}.fa-spoon:before{content:""\f1b1""}.fa-cube:before{content:""\f1b2""}.fa-cubes:before{content:""\f1b3""}.fa-behance:before{content:""\f1b4""}.fa-behance-square:before{content:""\f1b5""}.fa-steam:before{content:""\f1b6""}.fa-steam-square:before{content:""\f1b7""}.fa-recycle:before{content:""\f1b8""}.fa-automobile:before,.fa-car:before{content:""\f1b9""}.fa-cab:before,.fa-taxi:before{content:""\f1ba""}.fa-tree:before{content:""\f1bb""}.fa-spotify:before{content:""\f1bc""}.fa-deviantart:before{content:""\f1bd""}.fa-soundcloud:before{content:""\f1be""}.fa-database:before{content:""\f1c0""}.fa-file-pdf-o:before{content:""\f1c1""}.fa-file-word-o:before{content:""\f1c2""}.fa-file-excel-o:before{content:""\f1c3""}.fa-file-powerpoint-o:before{content:""\f1c4""}.fa-file-image-o:before,.fa-file-photo-o:before,.fa-file-picture-o:before{content:""\f1c5""}.fa-file-archive-o:before,.fa-file-zip-o:before{content:""\f1c6""}.fa-file-audio-o:before,.fa-file-sound-o:before{content:""\f1c7""}.fa-file-movie-o:before,.fa-file-video-o:before{content:""\f1c8""}.fa-file-code-o:before{content:""\f1c9""}.fa-vine:before{content:""\f1ca""}.fa-codepen:before{content:""\f1cb""}.fa-jsfiddle:before{content:""\f1cc""}.fa-life-bouy:before,.fa-life-ring:before,.fa-life-saver:before,.fa-support:before{content:""\f1cd""}.fa-circle-o-notch:before{content:""\f1ce""}.fa-ra:before,.fa-rebel:before{content:""\f1d0""}.fa-empire:before,.fa-ge:before{content:""\f1d1""}.fa-git-square:before{content:""\f1d2""}.fa-git:before{content:""\f1d3""}.fa-hacker-news:before{content:""\f1d4""}.fa-tencent-weibo:before{content:""\f1d5""}.fa-qq:before{content:""\f1d6""}.fa-wechat:before,.fa-weixin:before{content:""\f1d7""}.fa-paper-plane:before,.fa-send:before{content:""\f1d8""}.fa-paper-plane-o:before,.fa-send-o:before{content:""\f1d9""}.fa-history:before{content:""\f1da""}.fa-circle-thin:before{content:""\f1db""}.fa-header:before{content:""\f1dc""}.fa-paragraph:before{content:""\f1dd""}.fa-sliders:before{content:""\f1de""}.fa-share-alt:before{content:""\f1e0""}.fa-share-alt-square:before{content:""\f1e1""}.fa-bomb:before{content:""\f1e2""}.book-langs-index{width:100%;height:100%;padding:40px 0;margin:0;overflow:auto}@media (max-width:600px){.book-langs-index{padding:0}}.book-langs-index .inner{max-width:600px;width:100%;margin:0 auto;padding:30px;background:#fff;border-radius:3px}.book-langs-index .inner h3{margin:0}.book-langs-index .inner .languages{list-style:none;padding:20px 30px;margin-top:20px;border-top:1px solid #eee}.book-langs-index .inner .languages:after,.book-langs-index .inner .languages:before{content:"" "";display:table;line-height:0}.book-langs-index .inner .languages li{width:50%;float:left;padding:10px 5px;font-size:16px}@media (max-width:600px){.book-langs-index .inner .languages li{width:100%;max-width:100%}}.book .book-header{overflow:visible;height:50px;padding:0 8px;z-index:2;font-size:.85em;color:#7e888b;background:0 0}.book .book-header .btn{display:block;height:50px;padding:0 15px;border-bottom:none;color:#ccc;text-transform:uppercase;line-height:50px;-webkit-box-shadow:none!important;box-shadow:none!important;position:relative;font-size:14px}.book .book-header .btn:hover{position:relative;text-decoration:none;color:#444;background:0 0}.book .book-header h1{margin:0;font-size:20px;font-weight:200;text-align:center;line-height:50px;opacity:0;padding-left:200px;padding-right:200px;-webkit-transition:opacity .2s ease;-moz-transition:opacity .2s ease;-o-transition:opacity .2s ease;transition:opacity .2s ease;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.book .book-header h1 a,.book .book-header h1 a:hover{color:inherit;text-decoration:none}@media (max-width:1000px){.book .book-header h1{display:none}}.book .book-header h1 i{display:none}.book .book-header:hover h1{opacity:1}.book.is-loading .book-header h1 i{display:inline-block}.book.is-loading .book-header h1 a{display:none}.dropdown{position:relative}.dropdown-menu{position:absolute;top:100%;left:0;z-index:100;display:none;float:left;min-width:160px;padding:0;margin:2px 0 0;list-style:none;font-size:14px;background-color:#fafafa;border:1px solid rgba(0,0,0,.07);border-radius:1px;-webkit-box-shadow:0 6px 12px rgba(0,0,0,.175);box-shadow:0 6px 12px rgba(0,0,0,.175);background-clip:padding-box}.dropdown-menu.open{display:block}.dropdown-menu.dropdown-left{left:auto;right:4%}.dropdown-menu.dropdown-left .dropdown-caret{right:14px;left:auto}.dropdown-menu .dropdown-caret{position:absolute;top:-8px;left:14px;width:18px;height:10px;float:left;overflow:hidden}.dropdown-menu .dropdown-caret .caret-inner,.dropdown-menu .dropdown-caret .caret-outer{display:inline-block;top:0;border-left:9px solid transparent;border-right:9px solid transparent;position:absolute}.dropdown-menu .dropdown-caret .caret-outer{border-bottom:9px solid rgba(0,0,0,.1);height:auto;left:0;width:auto;margin-left:-1px}.dropdown-menu .dropdown-caret .caret-inner{margin-top:-1px;top:1px;border-bottom:9px solid #fafafa}.dropdown-menu .buttons{border-bottom:1px solid rgba(0,0,0,.07)}.dropdown-menu .buttons:after,.dropdown-menu .buttons:before{content:"" "";display:table;line-height:0}.dropdown-menu .buttons:last-child{border-bottom:none}.dropdown-menu .buttons .button{border:0;background-color:transparent;color:#a6a6a6;width:100%;text-align:center;float:left;line-height:1.42857143;padding:8px 4px}.alert,.dropdown-menu .buttons .button:hover{color:#444}.dropdown-menu .buttons .button:focus,.dropdown-menu .buttons .button:hover{outline:0}.dropdown-menu .buttons .button.size-2{width:50%}.dropdown-menu .buttons .button.size-3{width:33%}.alert{padding:15px;margin-bottom:20px;background:#eee;border-bottom:5px solid #ddd}.alert-success{background:#dff0d8;border-color:#d6e9c6;color:#3c763d}.alert-info{background:#d9edf7;border-color:#bce8f1;color:#31708f}.alert-danger{background:#f2dede;border-color:#ebccd1;color:#a94442}.alert-warning{background:#fcf8e3;border-color:#faebcc;color:#8a6d3b}.book .book-summary{position:absolute;top:0;left:-300px;bottom:0;z-index:1;width:300px;color:#364149;background:#fafafa;border-right:1px solid rgba(0,0,0,.07);-webkit-transition:left 250ms ease;-moz-transition:left 250ms ease;-o-transition:left 250ms ease;transition:left 250ms ease}.book .book-summary ul.summary{position:absolute;top:0;left:0;right:0;bottom:0;overflow-y:auto;list-style:none;margin:0;padding:0;-webkit-transition:top .5s ease;-moz-transition:top .5s ease;-o-transition:top .5s ease;transition:top .5s ease}.book .book-summary ul.summary li{list-style:none}.book .book-summary ul.summary li.divider{height:1px;margin:7px 0;overflow:hidden;background:rgba(0,0,0,.07)}.book .book-summary ul.summary li i.fa-check{display:none;position:absolute;right:9px;top:16px;font-size:9px;color:#3c3}.book .book-summary ul.summary li.done>a{color:#364149;font-weight:400}.book .book-summary ul.summary li.done>a i{display:inline}.book .book-summary ul.summary li a,.book .book-summary ul.summary li span{display:block;padding:10px 15px;border-bottom:none;color:#364149;background:0 0;text-overflow:ellipsis;overflow:hidden;white-space:nowrap;position:relative}.book .book-summary ul.summary li span{cursor:not-allowed;opacity:.3;filter:alpha(opacity=30)}.book .book-summary ul.summary li a:hover,.book .book-summary ul.summary li.active>a{color:#008cff;background:0 0;text-decoration:none}.book .book-summary ul.summary li ul{padding-left:20px}@media (max-width:600px){.book .book-summary{width:calc(100% - 60px);bottom:0;left:-100%}}.book.with-summary .book-summary{left:0}.book.without-animation .book-summary{-webkit-transition:none!important;-moz-transition:none!important;-o-transition:none!important;transition:none!important}.book{position:relative;width:100%;height:100%}.book .book-body,.book .book-body .body-inner{position:absolute;top:0;left:0;overflow-y:auto;bottom:0;right:0}.book .book-body{color:#000;background:#fff;-webkit-transition:left 250ms ease;-moz-transition:left 250ms ease;-o-transition:left 250ms ease;transition:left 250ms ease}.book .book-body .page-wrapper{position:relative;outline:0}.book .book-body .page-wrapper .page-inner{max-width:800px;margin:0 auto;padding:20px 0 40px}.book .book-body .page-wrapper .page-inner section{margin:0;padding:5px 15px;background:#fff;border-radius:2px;line-height:1.7;font-size:1.6rem}.book .book-body .page-wrapper .page-inner .btn-group .btn{border-radius:0;background:#eee;border:0}@media (max-width:1240px){.book .book-body{-webkit-transition:-webkit-transform 250ms ease;-moz-transition:-moz-transform 250ms ease;-o-transition:-o-transform 250ms ease;transition:transform 250ms ease;padding-bottom:20px}.book .book-body .body-inner{position:static;min-height:calc(100% - 50px)}}@media (min-width:600px){.book.with-summary .book-body{left:300px}}@media (max-width:600px){.book.with-summary{overflow:hidden}.book.with-summary .book-body{-webkit-transform:translate(calc(100% - 60px),0);-moz-transform:translate(calc(100% - 60px),0);-ms-transform:translate(calc(100% - 60px),0);-o-transform:translate(calc(100% - 60px),0);transform:translate(calc(100% - 60px),0)}}.book.without-animation .book-body{-webkit-transition:none!important;-moz-transition:none!important;-o-transition:none!important;transition:none!important}.buttons:after,.buttons:before{content:"" "";display:table;line-height:0}.button{border:0;background:#eee;color:#666;width:100%;text-align:center;float:left;line-height:1.42857143;padding:8px 4px}.button:hover{color:#444}.button:focus,.button:hover{outline:0}.button.size-2{width:50%}.button.size-3{width:33%}.book .book-body .page-wrapper .page-inner section{display:none}.book .book-body .page-wrapper .page-inner section.normal{display:block;word-wrap:break-word;overflow:hidden;color:#333;line-height:1.7;text-size-adjust:100%;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-text-size-adjust:100%}.book .book-body .page-wrapper .page-inner section.normal *{box-sizing:border-box;-webkit-box-sizing:border-box;}.book .book-body .page-wrapper .page-inner section.normal>:first-child{margin-top:0!important}.book .book-body .page-wrapper .page-inner section.normal>:last-child{margin-bottom:0!important}.book .book-body .page-wrapper .page-inner section.normal blockquote,.book .book-body .page-wrapper .page-inner section.normal code,.book .book-body .page-wrapper .page-inner section.normal figure,.book .book-body .page-wrapper .page-inner section.normal img,.book .book-body .page-wrapper .page-inner section.normal pre,.book .book-body .page-wrapper .page-inner section.normal table,.book .book-body .page-wrapper .page-inner section.normal tr{page-break-inside:avoid}.book .book-body .page-wrapper .page-inner section.normal h2,.book .book-body .page-wrapper .page-inner section.normal h3,.book .book-body .page-wrapper .page-inner section.normal h4,.book .book-body .page-wrapper .page-inner section.normal h5,.book .book-body .page-wrapper .page-inner section.normal p{orphans:3;widows:3}.book .book-body .page-wrapper .page-inner section.normal h1,.book .book-body .page-wrapper .page-inner section.normal h2,.book .book-body .page-wrapper .page-inner section.normal h3,.book .book-body .page-wrapper .page-inner section.normal h4,.book .book-body .page-wrapper .page-inner section.normal h5{page-break-after:avoid}.book .book-body .page-wrapper .page-inner section.normal b,.book .book-body .page-wrapper .page-inner section.normal strong{font-weight:700}.book .book-body .page-wrapper .page-inner section.normal em{font-style:italic}.book .book-body .page-wrapper .page-inner section.normal blockquote,.book .book-body .page-wrapper .page-inner section.normal dl,.book .book-body .page-wrapper .page-inner section.normal ol,.book .book-body .page-wrapper .page-inner section.normal p,.book .book-body .page-wrapper .page-inner section.normal table,.book .book-body .page-wrapper .page-inner section.normal ul{margin-top:0;margin-bottom:.85em}.book .book-body .page-wrapper .page-inner section.normal a{color:#4183c4;text-decoration:none;background:0 0}.book .book-body .page-wrapper .page-inner section.normal a:active,.book .book-body .page-wrapper .page-inner section.normal a:focus,.book .book-body .page-wrapper .page-inner section.normal a:hover{outline:0;text-decoration:underline}.book .book-body .page-wrapper .page-inner section.normal img{border:0;max-width:100%}.book .book-body .page-wrapper .page-inner section.normal hr{height:4px;padding:0;margin:1.7em 0;overflow:hidden;background-color:#e7e7e7;border:none}.book .book-body .page-wrapper .page-inner section.normal hr:after,.book .book-body .page-wrapper .page-inner section.normal hr:before{display:table;content:"" ""}.book .book-body .page-wrapper .page-inner section.normal h1,.book .book-body .page-wrapper .page-inner section.normal h2,.book .book-body .page-wrapper .page-inner section.normal h3,.book .book-body .page-wrapper .page-inner section.normal h4,.book .book-body .page-wrapper .page-inner section.normal h5,.book .book-body .page-wrapper .page-inner section.normal h6{margin-top:1.275em;margin-bottom:.85em;}.book .book-body .page-wrapper .page-inner section.normal h1{font-size:2em}.book .book-body .page-wrapper .page-inner section.normal h2{font-size:1.75em}.book .book-body .page-wrapper .page-inner section.normal h3{font-size:1.5em}.book .book-body .page-wrapper .page-inner section.normal h4{font-size:1.25em}.book .book-body .page-wrapper .page-inner section.normal h5{font-size:1em}.book .book-body .page-wrapper .page-inner section.normal h6{font-size:1em;color:#777}.book .book-body .page-wrapper .page-inner section.normal code,.book .book-body .page-wrapper .page-inner section.normal pre{font-family:Consolas,""Liberation Mono"",Menlo,Courier,monospace;direction:ltr;border:none;color:inherit}.book .book-body .page-wrapper .page-inner section.normal pre{overflow:auto;word-wrap:normal;margin:0 0 1.275em;padding:.85em 1em;background:#f7f7f7}.book .book-body .page-wrapper .page-inner section.normal pre>code{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;font-size:.85em;white-space:pre;background:0 0}.book .book-body .page-wrapper .page-inner section.normal pre>code:after,.book .book-body .page-wrapper .page-inner section.normal pre>code:before{content:normal}.book .book-body .page-wrapper .page-inner section.normal code{padding:.2em;margin:0;font-size:.85em;background-color:#f7f7f7}.book .book-body .page-wrapper .page-inner section.normal code:after,.book .book-body .page-wrapper .page-inner section.normal code:before{letter-spacing:-.2em;content:""\00a0""}.book .book-body .page-wrapper .page-inner section.normal ol,.book .book-body .page-wrapper .page-inner section.normal ul{padding:0 0 0 2em;margin:0 0 .85em}.book .book-body .page-wrapper .page-inner section.normal ol ol,.book .book-body .page-wrapper .page-inner section.normal ol ul,.book .book-body .page-wrapper .page-inner section.normal ul ol,.book .book-body .page-wrapper .page-inner section.normal ul ul{margin-top:0;margin-bottom:0}.book .book-body .page-wrapper .page-inner section.normal ol ol{list-style-type:lower-roman}.book .book-body .page-wrapper .page-inner section.normal blockquote{margin:0 0 .85em;padding:0 15px;opacity:0.75;border-left:4px solid #dcdcdc}.book .book-body .page-wrapper .page-inner section.normal blockquote:first-child{margin-top:0}.book .book-body .page-wrapper .page-inner section.normal blockquote:last-child{margin-bottom:0}.book .book-body .page-wrapper .page-inner section.normal dl{padding:0}.book .book-body .page-wrapper .page-inner section.normal dl dt{padding:0;margin-top:.85em;font-style:italic;font-weight:700}.book .book-body .page-wrapper .page-inner section.normal dl dd{padding:0 .85em;margin-bottom:.85em}.book .book-body .page-wrapper .page-inner section.normal dd{margin-left:0}.book .book-body .page-wrapper .page-inner section.normal .glossary-term{cursor:help;text-decoration:underline}.book .book-body .navigation{position:absolute;top:50px;bottom:0;margin:0;max-width:150px;min-width:90px;display:flex;justify-content:center;align-content:center;flex-direction:column;font-size:40px;color:#ccc;text-align:center;-webkit-transition:all 350ms ease;-moz-transition:all 350ms ease;-o-transition:all 350ms ease;transition:all 350ms ease}.book .book-body .navigation:hover{text-decoration:none;color:#444}.book .book-body .navigation.navigation-next{right:0}.book .book-body .navigation.navigation-prev{left:0}@media (max-width:1240px){.book .book-body .navigation{position:static;top:auto;max-width:50%;width:50%;display:inline-block;float:left}.book .book-body .navigation.navigation-unique{max-width:100%;width:100%}}.book .book-body .page-wrapper .page-inner section.glossary{margin-bottom:40px}.book .book-body .page-wrapper .page-inner section.glossary h2 a,.book .book-body .page-wrapper .page-inner section.glossary h2 a:hover{color:inherit;text-decoration:none}.book .book-body .page-wrapper .page-inner section.glossary .glossary-index{list-style:none;margin:0;padding:0}.book .book-body .page-wrapper .page-inner section.glossary .glossary-index li{display:inline;margin:0 8px;white-space:nowrap}*{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;-webkit-overflow-scrolling:touch;-webkit-tap-highlight-color:transparent;-webkit-text-size-adjust:none;-webkit-touch-callout:none}a{text-decoration:none}body,html{height:100%}html{font-size:62.5%}body{text-rendering:optimizeLegibility;font-smoothing:antialiased;font-family:""Helvetica Neue"",Helvetica,Arial,sans-serif;font-size:14px;letter-spacing:.2px;text-size-adjust:100%}
+ */@font-face{font-family:FontAwesome;src:url(./fontawesome/fontawesome-webfont.ttf?v=4.7.0) format('truetype');font-weight:400;font-style:normal}.fa{font-family:FontAwesome;font-style:normal;font-weight:400;line-height:1;-moz-osx-font-smoothing:grayscale}.book .book-header,.book .book-summary{font-family:""Helvetica Neue"",Helvetica,Arial,sans-serif}.fa-lg{font-size:1.33333333em;line-height:.75em;vertical-align:-15%}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-fw{width:1.28571429em;text-align:center}.fa-ul{padding-left:0;margin-left:2.14285714em;list-style-type:none}.fa-li{position:absolute;left:-2.14285714em;width:2.14285714em;top:.14285714em;text-align:center}.fa-li.fa-lg{left:-1.85714286em}.fa-border{padding:.2em .25em .15em;border:.08em solid #eee;border-radius:.1em}.pull-right{float:right}.pull-left{float:left}.fa.pull-left{margin-right:.3em}.fa.pull-right{margin-left:.3em}.fa-spin{-webkit-animation:spin 2s infinite linear;-moz-animation:spin 2s infinite linear;-o-animation:spin 2s infinite linear;animation:spin 2s infinite linear}@-moz-keyframes spin{0%{-moz-transform:rotate(0)}100%{-moz-transform:rotate(359deg)}}@-webkit-keyframes spin{0%{-webkit-transform:rotate(0)}100%{-webkit-transform:rotate(359deg)}}@-o-keyframes spin{0%{-o-transform:rotate(0)}100%{-o-transform:rotate(359deg)}}@keyframes spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(359deg);transform:rotate(359deg)}}.fa-rotate-90{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=1);-webkit-transform:rotate(90deg);-moz-transform:rotate(90deg);-ms-transform:rotate(90deg);-o-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=2);-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=3);-webkit-transform:rotate(270deg);-moz-transform:rotate(270deg);-ms-transform:rotate(270deg);-o-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);-webkit-transform:scale(-1,1);-moz-transform:scale(-1,1);-ms-transform:scale(-1,1);-o-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{filter:progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);-webkit-transform:scale(1,-1);-moz-transform:scale(1,-1);-ms-transform:scale(1,-1);-o-transform:scale(1,-1);transform:scale(1,-1)}.fa-stack{position:relative;width:2em;height:2em;line-height:2em;vertical-align:middle}.fa-stack-1x,.fa-stack-2x{position:absolute;left:0;width:100%;text-align:center}.fa-stack-1x{line-height:inherit}.fa-stack-2x{font-size:2em}.fa-inverse{color:#fff}.fa-glass:before{content:""\f000""}.fa-music:before{content:""\f001""}.fa-search:before{content:""\f002""}.fa-envelope-o:before{content:""\f003""}.fa-heart:before{content:""\f004""}.fa-star:before{content:""\f005""}.fa-star-o:before{content:""\f006""}.fa-user:before{content:""\f007""}.fa-film:before{content:""\f008""}.fa-th-large:before{content:""\f009""}.fa-th:before{content:""\f00a""}.fa-th-list:before{content:""\f00b""}.fa-check:before{content:""\f00c""}.fa-times:before{content:""\f00d""}.fa-search-plus:before{content:""\f00e""}.fa-search-minus:before{content:""\f010""}.fa-power-off:before{content:""\f011""}.fa-signal:before{content:""\f012""}.fa-cog:before,.fa-gear:before{content:""\f013""}.fa-trash-o:before{content:""\f014""}.fa-home:before{content:""\f015""}.fa-file-o:before{content:""\f016""}.fa-clock-o:before{content:""\f017""}.fa-road:before{content:""\f018""}.fa-download:before{content:""\f019""}.fa-arrow-circle-o-down:before{content:""\f01a""}.fa-arrow-circle-o-up:before{content:""\f01b""}.fa-inbox:before{content:""\f01c""}.fa-play-circle-o:before{content:""\f01d""}.fa-repeat:before,.fa-rotate-right:before{content:""\f01e""}.fa-refresh:before{content:""\f021""}.fa-list-alt:before{content:""\f022""}.fa-lock:before{content:""\f023""}.fa-flag:before{content:""\f024""}.fa-headphones:before{content:""\f025""}.fa-volume-off:before{content:""\f026""}.fa-volume-down:before{content:""\f027""}.fa-volume-up:before{content:""\f028""}.fa-qrcode:before{content:""\f029""}.fa-barcode:before{content:""\f02a""}.fa-tag:before{content:""\f02b""}.fa-tags:before{content:""\f02c""}.fa-book:before{content:""\f02d""}.fa-bookmark:before{content:""\f02e""}.fa-print:before{content:""\f02f""}.fa-camera:before{content:""\f030""}.fa-font:before{content:""\f031""}.fa-bold:before{content:""\f032""}.fa-italic:before{content:""\f033""}.fa-text-height:before{content:""\f034""}.fa-text-width:before{content:""\f035""}.fa-align-left:before{content:""\f036""}.fa-align-center:before{content:""\f037""}.fa-align-right:before{content:""\f038""}.fa-align-justify:before{content:""\f039""}.fa-list:before{content:""\f03a""}.fa-dedent:before,.fa-outdent:before{content:""\f03b""}.fa-indent:before{content:""\f03c""}.fa-video-camera:before{content:""\f03d""}.fa-image:before,.fa-photo:before,.fa-picture-o:before{content:""\f03e""}.fa-pencil:before{content:""\f040""}.fa-map-marker:before{content:""\f041""}.fa-adjust:before{content:""\f042""}.fa-tint:before{content:""\f043""}.fa-edit:before,.fa-pencil-square-o:before{content:""\f044""}.fa-share-square-o:before{content:""\f045""}.fa-check-square-o:before{content:""\f046""}.fa-arrows:before{content:""\f047""}.fa-step-backward:before{content:""\f048""}.fa-fast-backward:before{content:""\f049""}.fa-backward:before{content:""\f04a""}.fa-play:before{content:""\f04b""}.fa-pause:before{content:""\f04c""}.fa-stop:before{content:""\f04d""}.fa-forward:before{content:""\f04e""}.fa-fast-forward:before{content:""\f050""}.fa-step-forward:before{content:""\f051""}.fa-eject:before{content:""\f052""}.fa-chevron-left:before{content:""\f053""}.fa-chevron-right:before{content:""\f054""}.fa-plus-circle:before{content:""\f055""}.fa-minus-circle:before{content:""\f056""}.fa-times-circle:before{content:""\f057""}.fa-check-circle:before{content:""\f058""}.fa-question-circle:before{content:""\f059""}.fa-info-circle:before{content:""\f05a""}.fa-crosshairs:before{content:""\f05b""}.fa-times-circle-o:before{content:""\f05c""}.fa-check-circle-o:before{content:""\f05d""}.fa-ban:before{content:""\f05e""}.fa-arrow-left:before{content:""\f060""}.fa-arrow-right:before{content:""\f061""}.fa-arrow-up:before{content:""\f062""}.fa-arrow-down:before{content:""\f063""}.fa-mail-forward:before,.fa-share:before{content:""\f064""}.fa-expand:before{content:""\f065""}.fa-compress:before{content:""\f066""}.fa-plus:before{content:""\f067""}.fa-minus:before{content:""\f068""}.fa-asterisk:before{content:""\f069""}.fa-exclamation-circle:before{content:""\f06a""}.fa-gift:before{content:""\f06b""}.fa-leaf:before{content:""\f06c""}.fa-fire:before{content:""\f06d""}.fa-eye:before{content:""\f06e""}.fa-eye-slash:before{content:""\f070""}.fa-exclamation-triangle:before,.fa-warning:before{content:""\f071""}.fa-plane:before{content:""\f072""}.fa-calendar:before{content:""\f073""}.fa-random:before{content:""\f074""}.fa-comment:before{content:""\f075""}.fa-magnet:before{content:""\f076""}.fa-chevron-up:before{content:""\f077""}.fa-chevron-down:before{content:""\f078""}.fa-retweet:before{content:""\f079""}.fa-shopping-cart:before{content:""\f07a""}.fa-folder:before{content:""\f07b""}.fa-folder-open:before{content:""\f07c""}.fa-arrows-v:before{content:""\f07d""}.fa-arrows-h:before{content:""\f07e""}.fa-bar-chart-o:before{content:""\f080""}.fa-twitter-square:before{content:""\f081""}.fa-facebook-square:before{content:""\f082""}.fa-camera-retro:before{content:""\f083""}.fa-key:before{content:""\f084""}.fa-cogs:before,.fa-gears:before{content:""\f085""}.fa-comments:before{content:""\f086""}.fa-thumbs-o-up:before{content:""\f087""}.fa-thumbs-o-down:before{content:""\f088""}.fa-star-half:before{content:""\f089""}.fa-heart-o:before{content:""\f08a""}.fa-sign-out:before{content:""\f08b""}.fa-linkedin-square:before{content:""\f08c""}.fa-thumb-tack:before{content:""\f08d""}.fa-external-link:before{content:""\f08e""}.fa-sign-in:before{content:""\f090""}.fa-trophy:before{content:""\f091""}.fa-github-square:before{content:""\f092""}.fa-upload:before{content:""\f093""}.fa-lemon-o:before{content:""\f094""}.fa-phone:before{content:""\f095""}.fa-square-o:before{content:""\f096""}.fa-bookmark-o:before{content:""\f097""}.fa-phone-square:before{content:""\f098""}.fa-twitter:before{content:""\f099""}.fa-facebook:before{content:""\f09a""}.fa-github:before{content:""\f09b""}.fa-unlock:before{content:""\f09c""}.fa-credit-card:before{content:""\f09d""}.fa-rss:before{content:""\f09e""}.fa-hdd-o:before{content:""\f0a0""}.fa-bullhorn:before{content:""\f0a1""}.fa-bell:before{content:""\f0f3""}.fa-certificate:before{content:""\f0a3""}.fa-hand-o-right:before{content:""\f0a4""}.fa-hand-o-left:before{content:""\f0a5""}.fa-hand-o-up:before{content:""\f0a6""}.fa-hand-o-down:before{content:""\f0a7""}.fa-arrow-circle-left:before{content:""\f0a8""}.fa-arrow-circle-right:before{content:""\f0a9""}.fa-arrow-circle-up:before{content:""\f0aa""}.fa-arrow-circle-down:before{content:""\f0ab""}.fa-globe:before{content:""\f0ac""}.fa-wrench:before{content:""\f0ad""}.fa-tasks:before{content:""\f0ae""}.fa-filter:before{content:""\f0b0""}.fa-briefcase:before{content:""\f0b1""}.fa-arrows-alt:before{content:""\f0b2""}.fa-group:before,.fa-users:before{content:""\f0c0""}.fa-chain:before,.fa-link:before{content:""\f0c1""}.fa-cloud:before{content:""\f0c2""}.fa-flask:before{content:""\f0c3""}.fa-cut:before,.fa-scissors:before{content:""\f0c4""}.fa-copy:before,.fa-files-o:before{content:""\f0c5""}.fa-paperclip:before{content:""\f0c6""}.fa-floppy-o:before,.fa-save:before{content:""\f0c7""}.fa-square:before{content:""\f0c8""}.fa-bars:before,.fa-navicon:before,.fa-reorder:before{content:""\f0c9""}.fa-list-ul:before{content:""\f0ca""}.fa-list-ol:before{content:""\f0cb""}.fa-strikethrough:before{content:""\f0cc""}.fa-underline:before{content:""\f0cd""}.fa-table:before{content:""\f0ce""}.fa-magic:before{content:""\f0d0""}.fa-truck:before{content:""\f0d1""}.fa-pinterest:before{content:""\f0d2""}.fa-pinterest-square:before{content:""\f0d3""}.fa-google-plus-square:before{content:""\f0d4""}.fa-google-plus:before{content:""\f0d5""}.fa-money:before{content:""\f0d6""}.fa-caret-down:before{content:""\f0d7""}.fa-caret-up:before{content:""\f0d8""}.fa-caret-left:before{content:""\f0d9""}.fa-caret-right:before{content:""\f0da""}.fa-columns:before{content:""\f0db""}.fa-sort:before,.fa-unsorted:before{content:""\f0dc""}.fa-sort-desc:before,.fa-sort-down:before{content:""\f0dd""}.fa-sort-asc:before,.fa-sort-up:before{content:""\f0de""}.fa-envelope:before{content:""\f0e0""}.fa-linkedin:before{content:""\f0e1""}.fa-rotate-left:before,.fa-undo:before{content:""\f0e2""}.fa-gavel:before,.fa-legal:before{content:""\f0e3""}.fa-dashboard:before,.fa-tachometer:before{content:""\f0e4""}.fa-comment-o:before{content:""\f0e5""}.fa-comments-o:before{content:""\f0e6""}.fa-bolt:before,.fa-flash:before{content:""\f0e7""}.fa-sitemap:before{content:""\f0e8""}.fa-umbrella:before{content:""\f0e9""}.fa-clipboard:before,.fa-paste:before{content:""\f0ea""}.fa-lightbulb-o:before{content:""\f0eb""}.fa-exchange:before{content:""\f0ec""}.fa-cloud-download:before{content:""\f0ed""}.fa-cloud-upload:before{content:""\f0ee""}.fa-user-md:before{content:""\f0f0""}.fa-stethoscope:before{content:""\f0f1""}.fa-suitcase:before{content:""\f0f2""}.fa-bell-o:before{content:""\f0a2""}.fa-coffee:before{content:""\f0f4""}.fa-cutlery:before{content:""\f0f5""}.fa-file-text-o:before{content:""\f0f6""}.fa-building-o:before{content:""\f0f7""}.fa-hospital-o:before{content:""\f0f8""}.fa-ambulance:before{content:""\f0f9""}.fa-medkit:before{content:""\f0fa""}.fa-fighter-jet:before{content:""\f0fb""}.fa-beer:before{content:""\f0fc""}.fa-h-square:before{content:""\f0fd""}.fa-plus-square:before{content:""\f0fe""}.fa-angle-double-left:before{content:""\f100""}.fa-angle-double-right:before{content:""\f101""}.fa-angle-double-up:before{content:""\f102""}.fa-angle-double-down:before{content:""\f103""}.fa-angle-left:before{content:""\f104""}.fa-angle-right:before{content:""\f105""}.fa-angle-up:before{content:""\f106""}.fa-angle-down:before{content:""\f107""}.fa-desktop:before{content:""\f108""}.fa-laptop:before{content:""\f109""}.fa-tablet:before{content:""\f10a""}.fa-mobile-phone:before,.fa-mobile:before{content:""\f10b""}.fa-circle-o:before{content:""\f10c""}.fa-quote-left:before{content:""\f10d""}.fa-quote-right:before{content:""\f10e""}.fa-spinner:before{content:""\f110""}.fa-circle:before{content:""\f111""}.fa-mail-reply:before,.fa-reply:before{content:""\f112""}.fa-github-alt:before{content:""\f113""}.fa-folder-o:before{content:""\f114""}.fa-folder-open-o:before{content:""\f115""}.fa-smile-o:before{content:""\f118""}.fa-frown-o:before{content:""\f119""}.fa-meh-o:before{content:""\f11a""}.fa-gamepad:before{content:""\f11b""}.fa-keyboard-o:before{content:""\f11c""}.fa-flag-o:before{content:""\f11d""}.fa-flag-checkered:before{content:""\f11e""}.fa-terminal:before{content:""\f120""}.fa-code:before{content:""\f121""}.fa-mail-reply-all:before,.fa-reply-all:before{content:""\f122""}.fa-star-half-empty:before,.fa-star-half-full:before,.fa-star-half-o:before{content:""\f123""}.fa-location-arrow:before{content:""\f124""}.fa-crop:before{content:""\f125""}.fa-code-fork:before{content:""\f126""}.fa-chain-broken:before,.fa-unlink:before{content:""\f127""}.fa-question:before{content:""\f128""}.fa-info:before{content:""\f129""}.fa-exclamation:before{content:""\f12a""}.fa-superscript:before{content:""\f12b""}.fa-subscript:before{content:""\f12c""}.fa-eraser:before{content:""\f12d""}.fa-puzzle-piece:before{content:""\f12e""}.fa-microphone:before{content:""\f130""}.fa-microphone-slash:before{content:""\f131""}.fa-shield:before{content:""\f132""}.fa-calendar-o:before{content:""\f133""}.fa-fire-extinguisher:before{content:""\f134""}.fa-rocket:before{content:""\f135""}.fa-maxcdn:before{content:""\f136""}.fa-chevron-circle-left:before{content:""\f137""}.fa-chevron-circle-right:before{content:""\f138""}.fa-chevron-circle-up:before{content:""\f139""}.fa-chevron-circle-down:before{content:""\f13a""}.fa-html5:before{content:""\f13b""}.fa-css3:before{content:""\f13c""}.fa-anchor:before{content:""\f13d""}.fa-unlock-alt:before{content:""\f13e""}.fa-bullseye:before{content:""\f140""}.fa-ellipsis-h:before{content:""\f141""}.fa-ellipsis-v:before{content:""\f142""}.fa-rss-square:before{content:""\f143""}.fa-play-circle:before{content:""\f144""}.fa-ticket:before{content:""\f145""}.fa-minus-square:before{content:""\f146""}.fa-minus-square-o:before{content:""\f147""}.fa-level-up:before{content:""\f148""}.fa-level-down:before{content:""\f149""}.fa-check-square:before{content:""\f14a""}.fa-pencil-square:before{content:""\f14b""}.fa-external-link-square:before{content:""\f14c""}.fa-share-square:before{content:""\f14d""}.fa-compass:before{content:""\f14e""}.fa-caret-square-o-down:before,.fa-toggle-down:before{content:""\f150""}.fa-caret-square-o-up:before,.fa-toggle-up:before{content:""\f151""}.fa-caret-square-o-right:before,.fa-toggle-right:before{content:""\f152""}.fa-eur:before,.fa-euro:before{content:""\f153""}.fa-gbp:before{content:""\f154""}.fa-dollar:before,.fa-usd:before{content:""\f155""}.fa-inr:before,.fa-rupee:before{content:""\f156""}.fa-cny:before,.fa-jpy:before,.fa-rmb:before,.fa-yen:before{content:""\f157""}.fa-rouble:before,.fa-rub:before,.fa-ruble:before{content:""\f158""}.fa-krw:before,.fa-won:before{content:""\f159""}.fa-bitcoin:before,.fa-btc:before{content:""\f15a""}.fa-file:before{content:""\f15b""}.fa-file-text:before{content:""\f15c""}.fa-sort-alpha-asc:before{content:""\f15d""}.fa-sort-alpha-desc:before{content:""\f15e""}.fa-sort-amount-asc:before{content:""\f160""}.fa-sort-amount-desc:before{content:""\f161""}.fa-sort-numeric-asc:before{content:""\f162""}.fa-sort-numeric-desc:before{content:""\f163""}.fa-thumbs-up:before{content:""\f164""}.fa-thumbs-down:before{content:""\f165""}.fa-youtube-square:before{content:""\f166""}.fa-youtube:before{content:""\f167""}.fa-xing:before{content:""\f168""}.fa-xing-square:before{content:""\f169""}.fa-youtube-play:before{content:""\f16a""}.fa-dropbox:before{content:""\f16b""}.fa-stack-overflow:before{content:""\f16c""}.fa-instagram:before{content:""\f16d""}.fa-flickr:before{content:""\f16e""}.fa-adn:before{content:""\f170""}.fa-bitbucket:before{content:""\f171""}.fa-bitbucket-square:before{content:""\f172""}.fa-tumblr:before{content:""\f173""}.fa-tumblr-square:before{content:""\f174""}.fa-long-arrow-down:before{content:""\f175""}.fa-long-arrow-up:before{content:""\f176""}.fa-long-arrow-left:before{content:""\f177""}.fa-long-arrow-right:before{content:""\f178""}.fa-apple:before{content:""\f179""}.fa-windows:before{content:""\f17a""}.fa-android:before{content:""\f17b""}.fa-linux:before{content:""\f17c""}.fa-dribbble:before{content:""\f17d""}.fa-skype:before{content:""\f17e""}.fa-foursquare:before{content:""\f180""}.fa-trello:before{content:""\f181""}.fa-female:before{content:""\f182""}.fa-male:before{content:""\f183""}.fa-gittip:before{content:""\f184""}.fa-sun-o:before{content:""\f185""}.fa-moon-o:before{content:""\f186""}.fa-archive:before{content:""\f187""}.fa-bug:before{content:""\f188""}.fa-vk:before{content:""\f189""}.fa-weibo:before{content:""\f18a""}.fa-renren:before{content:""\f18b""}.fa-pagelines:before{content:""\f18c""}.fa-stack-exchange:before{content:""\f18d""}.fa-arrow-circle-o-right:before{content:""\f18e""}.fa-arrow-circle-o-left:before{content:""\f190""}.fa-caret-square-o-left:before,.fa-toggle-left:before{content:""\f191""}.fa-dot-circle-o:before{content:""\f192""}.fa-wheelchair:before{content:""\f193""}.fa-vimeo-square:before{content:""\f194""}.fa-try:before,.fa-turkish-lira:before{content:""\f195""}.fa-plus-square-o:before{content:""\f196""}.fa-space-shuttle:before{content:""\f197""}.fa-slack:before{content:""\f198""}.fa-envelope-square:before{content:""\f199""}.fa-wordpress:before{content:""\f19a""}.fa-openid:before{content:""\f19b""}.fa-bank:before,.fa-institution:before,.fa-university:before{content:""\f19c""}.fa-graduation-cap:before,.fa-mortar-board:before{content:""\f19d""}.fa-yahoo:before{content:""\f19e""}.fa-google:before{content:""\f1a0""}.fa-reddit:before{content:""\f1a1""}.fa-reddit-square:before{content:""\f1a2""}.fa-stumbleupon-circle:before{content:""\f1a3""}.fa-stumbleupon:before{content:""\f1a4""}.fa-delicious:before{content:""\f1a5""}.fa-digg:before{content:""\f1a6""}.fa-pied-piper-square:before,.fa-pied-piper:before{content:""\f1a7""}.fa-pied-piper-alt:before{content:""\f1a8""}.fa-drupal:before{content:""\f1a9""}.fa-joomla:before{content:""\f1aa""}.fa-language:before{content:""\f1ab""}.fa-fax:before{content:""\f1ac""}.fa-building:before{content:""\f1ad""}.fa-child:before{content:""\f1ae""}.fa-paw:before{content:""\f1b0""}.fa-spoon:before{content:""\f1b1""}.fa-cube:before{content:""\f1b2""}.fa-cubes:before{content:""\f1b3""}.fa-behance:before{content:""\f1b4""}.fa-behance-square:before{content:""\f1b5""}.fa-steam:before{content:""\f1b6""}.fa-steam-square:before{content:""\f1b7""}.fa-recycle:before{content:""\f1b8""}.fa-automobile:before,.fa-car:before{content:""\f1b9""}.fa-cab:before,.fa-taxi:before{content:""\f1ba""}.fa-tree:before{content:""\f1bb""}.fa-spotify:before{content:""\f1bc""}.fa-deviantart:before{content:""\f1bd""}.fa-soundcloud:before{content:""\f1be""}.fa-database:before{content:""\f1c0""}.fa-file-pdf-o:before{content:""\f1c1""}.fa-file-word-o:before{content:""\f1c2""}.fa-file-excel-o:before{content:""\f1c3""}.fa-file-powerpoint-o:before{content:""\f1c4""}.fa-file-image-o:before,.fa-file-photo-o:before,.fa-file-picture-o:before{content:""\f1c5""}.fa-file-archive-o:before,.fa-file-zip-o:before{content:""\f1c6""}.fa-file-audio-o:before,.fa-file-sound-o:before{content:""\f1c7""}.fa-file-movie-o:before,.fa-file-video-o:before{content:""\f1c8""}.fa-file-code-o:before{content:""\f1c9""}.fa-vine:before{content:""\f1ca""}.fa-codepen:before{content:""\f1cb""}.fa-jsfiddle:before{content:""\f1cc""}.fa-life-bouy:before,.fa-life-ring:before,.fa-life-saver:before,.fa-support:before{content:""\f1cd""}.fa-circle-o-notch:before{content:""\f1ce""}.fa-ra:before,.fa-rebel:before{content:""\f1d0""}.fa-empire:before,.fa-ge:before{content:""\f1d1""}.fa-git-square:before{content:""\f1d2""}.fa-git:before{content:""\f1d3""}.fa-hacker-news:before{content:""\f1d4""}.fa-tencent-weibo:before{content:""\f1d5""}.fa-qq:before{content:""\f1d6""}.fa-wechat:before,.fa-weixin:before{content:""\f1d7""}.fa-paper-plane:before,.fa-send:before{content:""\f1d8""}.fa-paper-plane-o:before,.fa-send-o:before{content:""\f1d9""}.fa-history:before{content:""\f1da""}.fa-circle-thin:before{content:""\f1db""}.fa-header:before{content:""\f1dc""}.fa-paragraph:before{content:""\f1dd""}.fa-sliders:before{content:""\f1de""}.fa-share-alt:before{content:""\f1e0""}.fa-share-alt-square:before{content:""\f1e1""}.fa-bomb:before{content:""\f1e2""}.book-langs-index{width:100%;height:100%;padding:40px 0;margin:0;overflow:auto}@media (max-width:600px){.book-langs-index{padding:0}}.book-langs-index .inner{max-width:600px;width:100%;margin:0 auto;padding:30px;background:#fff;border-radius:3px}.book-langs-index .inner h3{margin:0}.book-langs-index .inner .languages{list-style:none;padding:20px 30px;margin-top:20px;border-top:1px solid #eee}.book-langs-index .inner .languages:after,.book-langs-index .inner .languages:before{content:"" "";display:table;line-height:0}.book-langs-index .inner .languages li{width:50%;float:left;padding:10px 5px;font-size:16px}@media (max-width:600px){.book-langs-index .inner .languages li{width:100%;max-width:100%}}.book .book-header{overflow:visible;height:50px;padding:0 8px;z-index:2;font-size:.85em;color:#7e888b;background:0 0}.book .book-header .btn{display:block;height:50px;padding:0 15px;border-bottom:none;color:#ccc;text-transform:uppercase;line-height:50px;-webkit-box-shadow:none!important;box-shadow:none!important;position:relative;font-size:14px}.book .book-header .btn:hover{position:relative;text-decoration:none;color:#444;background:0 0}.book .book-header h1{margin:0;font-size:20px;font-weight:200;text-align:center;line-height:50px;opacity:0;padding-left:200px;padding-right:200px;-webkit-transition:opacity .2s ease;-moz-transition:opacity .2s ease;-o-transition:opacity .2s ease;transition:opacity .2s ease;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.book .book-header h1 a,.book .book-header h1 a:hover{color:inherit;text-decoration:none}@media (max-width:1000px){.book .book-header h1{display:none}}.book .book-header h1 i{display:none}.book .book-header:hover h1{opacity:1}.book.is-loading .book-header h1 i{display:inline-block}.book.is-loading .book-header h1 a{display:none}.dropdown{position:relative}.dropdown-menu{position:absolute;top:100%;left:0;z-index:100;display:none;float:left;min-width:160px;padding:0;margin:2px 0 0;list-style:none;font-size:14px;background-color:#fafafa;border:1px solid rgba(0,0,0,.07);border-radius:1px;-webkit-box-shadow:0 6px 12px rgba(0,0,0,.175);box-shadow:0 6px 12px rgba(0,0,0,.175);background-clip:padding-box}.dropdown-menu.open{display:block}.dropdown-menu.dropdown-left{left:auto;right:4%}.dropdown-menu.dropdown-left .dropdown-caret{right:14px;left:auto}.dropdown-menu .dropdown-caret{position:absolute;top:-8px;left:14px;width:18px;height:10px;float:left;overflow:hidden}.dropdown-menu .dropdown-caret .caret-inner,.dropdown-menu .dropdown-caret .caret-outer{display:inline-block;top:0;border-left:9px solid transparent;border-right:9px solid transparent;position:absolute}.dropdown-menu .dropdown-caret .caret-outer{border-bottom:9px solid rgba(0,0,0,.1);height:auto;left:0;width:auto;margin-left:-1px}.dropdown-menu .dropdown-caret .caret-inner{margin-top:-1px;top:1px;border-bottom:9px solid #fafafa}.dropdown-menu .buttons{border-bottom:1px solid rgba(0,0,0,.07)}.dropdown-menu .buttons:after,.dropdown-menu .buttons:before{content:"" "";display:table;line-height:0}.dropdown-menu .buttons:last-child{border-bottom:none}.dropdown-menu .buttons .button{border:0;background-color:transparent;color:#a6a6a6;width:100%;text-align:center;float:left;line-height:1.42857143;padding:8px 4px}.alert,.dropdown-menu .buttons .button:hover{color:#444}.dropdown-menu .buttons .button:focus,.dropdown-menu .buttons .button:hover{outline:0}.dropdown-menu .buttons .button.size-2{width:50%}.dropdown-menu .buttons .button.size-3{width:33%}.alert{padding:15px;margin-bottom:20px;background:#eee;border-bottom:5px solid #ddd}.alert-success{background:#dff0d8;border-color:#d6e9c6;color:#3c763d}.alert-info{background:#d9edf7;border-color:#bce8f1;color:#31708f}.alert-danger{background:#f2dede;border-color:#ebccd1;color:#a94442}.alert-warning{background:#fcf8e3;border-color:#faebcc;color:#8a6d3b}.book .book-summary{position:absolute;top:0;left:-300px;bottom:0;z-index:1;width:300px;color:#364149;background:#fafafa;border-right:1px solid rgba(0,0,0,.07);-webkit-transition:left 250ms ease;-moz-transition:left 250ms ease;-o-transition:left 250ms ease;transition:left 250ms ease}.book .book-summary ul.summary{position:absolute;top:0;left:0;right:0;bottom:0;overflow-y:auto;list-style:none;margin:0;padding:0;-webkit-transition:top .5s ease;-moz-transition:top .5s ease;-o-transition:top .5s ease;transition:top .5s ease}.book .book-summary ul.summary li{list-style:none}.book .book-summary ul.summary li.divider{height:1px;margin:7px 0;overflow:hidden;background:rgba(0,0,0,.07)}.book .book-summary ul.summary li i.fa-check{display:none;position:absolute;right:9px;top:16px;font-size:9px;color:#3c3}.book .book-summary ul.summary li.done>a{color:#364149;font-weight:400}.book .book-summary ul.summary li.done>a i{display:inline}.book .book-summary ul.summary li a,.book .book-summary ul.summary li span{display:block;padding:10px 15px;border-bottom:none;color:#364149;background:0 0;text-overflow:ellipsis;overflow:hidden;white-space:nowrap;position:relative}.book .book-summary ul.summary li span{cursor:not-allowed;opacity:.3;filter:alpha(opacity=30)}.book .book-summary ul.summary li a:hover,.book .book-summary ul.summary li.active>a{color:#008cff;background:0 0;text-decoration:none}.book .book-summary ul.summary li ul{padding-left:20px}@media (max-width:600px){.book .book-summary{width:calc(100% - 60px);bottom:0;left:-100%}}.book.with-summary .book-summary{left:0}.book.without-animation .book-summary{-webkit-transition:none!important;-moz-transition:none!important;-o-transition:none!important;transition:none!important}.book{position:relative;width:100%;height:100%}.book .book-body,.book .book-body .body-inner{position:absolute;top:0;left:0;overflow-y:auto;bottom:0;right:0}.book .book-body{color:#000;background:#fff;-webkit-transition:left 250ms ease;-moz-transition:left 250ms ease;-o-transition:left 250ms ease;transition:left 250ms ease}.book .book-body .page-wrapper{position:relative;outline:0}.book .book-body .page-wrapper .page-inner{max-width:800px;margin:0 auto;padding:20px 0 40px}.book .book-body .page-wrapper .page-inner section{margin:0;padding:5px 15px;background:#fff;border-radius:2px;line-height:1.7;font-size:1.6rem}.book .book-body .page-wrapper .page-inner .btn-group .btn{border-radius:0;background:#eee;border:0}@media (max-width:1240px){.book .book-body{-webkit-transition:-webkit-transform 250ms ease;-moz-transition:-moz-transform 250ms ease;-o-transition:-o-transform 250ms ease;transition:transform 250ms ease;padding-bottom:20px}.book .book-body .body-inner{position:static;min-height:calc(100% - 50px)}}@media (min-width:600px){.book.with-summary .book-body{left:300px}}@media (max-width:600px){.book.with-summary{overflow:hidden}.book.with-summary .book-body{-webkit-transform:translate(calc(100% - 60px),0);-moz-transform:translate(calc(100% - 60px),0);-ms-transform:translate(calc(100% - 60px),0);-o-transform:translate(calc(100% - 60px),0);transform:translate(calc(100% - 60px),0)}}.book.without-animation .book-body{-webkit-transition:none!important;-moz-transition:none!important;-o-transition:none!important;transition:none!important}.buttons:after,.buttons:before{content:"" "";display:table;line-height:0}.button{border:0;background:#eee;color:#666;width:100%;text-align:center;float:left;line-height:1.42857143;padding:8px 4px}.button:hover{color:#444}.button:focus,.button:hover{outline:0}.button.size-2{width:50%}.button.size-3{width:33%}.book .book-body .page-wrapper .page-inner section{display:none}.book .book-body .page-wrapper .page-inner section.normal{display:block;word-wrap:break-word;overflow:hidden;color:#333;line-height:1.7;text-size-adjust:100%;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-text-size-adjust:100%}.book .book-body .page-wrapper .page-inner section.normal *{box-sizing:border-box;-webkit-box-sizing:border-box;}.book .book-body .page-wrapper .page-inner section.normal>:first-child{margin-top:0!important}.book .book-body .page-wrapper .page-inner section.normal>:last-child{margin-bottom:0!important}.book .book-body .page-wrapper .page-inner section.normal blockquote,.book .book-body .page-wrapper .page-inner section.normal code,.book .book-body .page-wrapper .page-inner section.normal figure,.book .book-body .page-wrapper .page-inner section.normal img,.book .book-body .page-wrapper .page-inner section.normal pre,.book .book-body .page-wrapper .page-inner section.normal table,.book .book-body .page-wrapper .page-inner section.normal tr{page-break-inside:avoid}.book .book-body .page-wrapper .page-inner section.normal h2,.book .book-body .page-wrapper .page-inner section.normal h3,.book .book-body .page-wrapper .page-inner section.normal h4,.book .book-body .page-wrapper .page-inner section.normal h5,.book .book-body .page-wrapper .page-inner section.normal p{orphans:3;widows:3}.book .book-body .page-wrapper .page-inner section.normal h1,.book .book-body .page-wrapper .page-inner section.normal h2,.book .book-body .page-wrapper .page-inner section.normal h3,.book .book-body .page-wrapper .page-inner section.normal h4,.book .book-body .page-wrapper .page-inner section.normal h5{page-break-after:avoid}.book .book-body .page-wrapper .page-inner section.normal b,.book .book-body .page-wrapper .page-inner section.normal strong{font-weight:700}.book .book-body .page-wrapper .page-inner section.normal em{font-style:italic}.book .book-body .page-wrapper .page-inner section.normal blockquote,.book .book-body .page-wrapper .page-inner section.normal dl,.book .book-body .page-wrapper .page-inner section.normal ol,.book .book-body .page-wrapper .page-inner section.normal p,.book .book-body .page-wrapper .page-inner section.normal table,.book .book-body .page-wrapper .page-inner section.normal ul{margin-top:0;margin-bottom:.85em}.book .book-body .page-wrapper .page-inner section.normal a{color:#4183c4;text-decoration:none;background:0 0}.book .book-body .page-wrapper .page-inner section.normal a:active,.book .book-body .page-wrapper .page-inner section.normal a:focus,.book .book-body .page-wrapper .page-inner section.normal a:hover{outline:0;text-decoration:underline}.book .book-body .page-wrapper .page-inner section.normal img{border:0;max-width:100%}.book .book-body .page-wrapper .page-inner section.normal hr{height:4px;padding:0;margin:1.7em 0;overflow:hidden;background-color:#e7e7e7;border:none}.book .book-body .page-wrapper .page-inner section.normal hr:after,.book .book-body .page-wrapper .page-inner section.normal hr:before{display:table;content:"" ""}.book .book-body .page-wrapper .page-inner section.normal h1,.book .book-body .page-wrapper .page-inner section.normal h2,.book .book-body .page-wrapper .page-inner section.normal h3,.book .book-body .page-wrapper .page-inner section.normal h4,.book .book-body .page-wrapper .page-inner section.normal h5,.book .book-body .page-wrapper .page-inner section.normal h6{margin-top:1.275em;margin-bottom:.85em;}.book .book-body .page-wrapper .page-inner section.normal h1{font-size:2em}.book .book-body .page-wrapper .page-inner section.normal h2{font-size:1.75em}.book .book-body .page-wrapper .page-inner section.normal h3{font-size:1.5em}.book .book-body .page-wrapper .page-inner section.normal h4{font-size:1.25em}.book .book-body .page-wrapper .page-inner section.normal h5{font-size:1em}.book .book-body .page-wrapper .page-inner section.normal h6{font-size:1em;color:#777}.book .book-body .page-wrapper .page-inner section.normal code,.book .book-body .page-wrapper .page-inner section.normal pre{font-family:Consolas,""Liberation Mono"",Menlo,Courier,monospace;direction:ltr;border:none;color:inherit}.book .book-body .page-wrapper .page-inner section.normal pre{overflow:auto;word-wrap:normal;margin:0 0 1.275em;padding:.85em 1em;background:#f7f7f7}.book .book-body .page-wrapper .page-inner section.normal pre>code{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;font-size:.85em;white-space:pre;background:0 0}.book .book-body .page-wrapper .page-inner section.normal pre>code:after,.book .book-body .page-wrapper .page-inner section.normal pre>code:before{content:normal}.book .book-body .page-wrapper .page-inner section.normal code{padding:.2em;margin:0;font-size:.85em;background-color:#f7f7f7}.book .book-body .page-wrapper .page-inner section.normal code:after,.book .book-body .page-wrapper .page-inner section.normal code:before{letter-spacing:-.2em;content:""\00a0""}.book .book-body .page-wrapper .page-inner section.normal ol,.book .book-body .page-wrapper .page-inner section.normal ul{padding:0 0 0 2em;margin:0 0 .85em}.book .book-body .page-wrapper .page-inner section.normal ol ol,.book .book-body .page-wrapper .page-inner section.normal ol ul,.book .book-body .page-wrapper .page-inner section.normal ul ol,.book .book-body .page-wrapper .page-inner section.normal ul ul{margin-top:0;margin-bottom:0}.book .book-body .page-wrapper .page-inner section.normal ol ol{list-style-type:lower-roman}.book .book-body .page-wrapper .page-inner section.normal blockquote{margin:0 0 .85em;padding:0 15px;opacity:0.75;border-left:4px solid #dcdcdc}.book .book-body .page-wrapper .page-inner section.normal blockquote:first-child{margin-top:0}.book .book-body .page-wrapper .page-inner section.normal blockquote:last-child{margin-bottom:0}.book .book-body .page-wrapper .page-inner section.normal dl{padding:0}.book .book-body .page-wrapper .page-inner section.normal dl dt{padding:0;margin-top:.85em;font-style:italic;font-weight:700}.book .book-body .page-wrapper .page-inner section.normal dl dd{padding:0 .85em;margin-bottom:.85em}.book .book-body .page-wrapper .page-inner section.normal dd{margin-left:0}.book .book-body .page-wrapper .page-inner section.normal .glossary-term{cursor:help;text-decoration:underline}.book .book-body .navigation{position:absolute;top:50px;bottom:0;margin:0;max-width:150px;min-width:90px;display:flex;justify-content:center;align-content:center;flex-direction:column;font-size:40px;color:#ccc;text-align:center;-webkit-transition:all 350ms ease;-moz-transition:all 350ms ease;-o-transition:all 350ms ease;transition:all 350ms ease}.book .book-body .navigation:hover{text-decoration:none;color:#444}.book .book-body .navigation.navigation-next{right:0}.book .book-body .navigation.navigation-prev{left:0}@media (max-width:1240px){.book .book-body .navigation{position:static;top:auto;max-width:50%;width:50%;display:inline-block;float:left}.book .book-body .navigation.navigation-unique{max-width:100%;width:100%}}.book .book-body .page-wrapper .page-inner section.glossary{margin-bottom:40px}.book .book-body .page-wrapper .page-inner section.glossary h2 a,.book .book-body .page-wrapper .page-inner section.glossary h2 a:hover{color:inherit;text-decoration:none}.book .book-body .page-wrapper .page-inner section.glossary .glossary-index{list-style:none;margin:0;padding:0}.book .book-body .page-wrapper .page-inner section.glossary .glossary-index li{display:inline;margin:0 8px;white-space:nowrap}*{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;-webkit-overflow-scrolling:touch;-webkit-tap-highlight-color:transparent;-webkit-text-size-adjust:none;-webkit-touch-callout:none}a{text-decoration:none}body,html{height:100%}html{font-size:62.5%}body{text-rendering:optimizeLegibility;font-smoothing:antialiased;font-family:""Helvetica Neue"",Helvetica,Arial,sans-serif;font-size:14px;letter-spacing:.2px;text-size-adjust:100%}
 .book .book-summary ul.summary li a span {display:inline;padding:initial;overflow:visible;cursor:auto;opacity:1;}
+/* show arrow before summary tag as in bootstrap */
+details > summary {display:list-item;cursor:pointer;}
+/*add whatsapp icon from FA 5.1.1
+TODO: remove when updating fontawesome*/
+.fa-whatsapp:before{content:""\f232""}

---FILE: session-glm/docs/libs/gitbook-2.6.7/js/plugin-search.js---
@@ -1,9 +1,24 @@
 gitbook.require([""gitbook"", ""lodash"", ""jQuery""], function(gitbook, _, $) {
     var index = null;
+    var fuse = null;
+    var _search = {engine: 'lunr', opts: {}};
     var $searchInput, $searchLabel, $searchForm;
     var $highlighted = [], hi, hiOpts = { className: 'search-highlight' };
     var collapse = false, toc_visible = [];
 
+    function init(config) {
+        // Instantiate search settings
+        _search = gitbook.storage.get(""search"", {
+            engine: config.search.engine || 'lunr',
+            opts: config.search.options || {},
+        });
+    };
+
+    // Save current search settings
+    function saveSearchSettings() {
+        gitbook.storage.set(""search"", _search);
+    }
+
     // Use a specific index
     function loadIndex(data) {
         // [Yihui] In bookdown, I use a character matrix to store the chapter
@@ -14,18 +29,36 @@ gitbook.require([""gitbook"", ""lodash"", ""jQuery""], function(gitbook, _, $) {
         // lunr cannot handle non-English text very well, e.g. the default
         // tokenizer cannot deal with Chinese text, so we may want to replace
         // lunr with a dumb simple text matching approach.
-        index = lunr(function () {
-          this.ref('url');
-          this.field('title', { boost: 10 });
-          this.field('body');
-        });
-        data.map(function(item) {
-          index.add({
-            url: item[0],
-            title: item[1],
-            body: item[2]
+        if (_search.engine === 'lunr') {
+          index = lunr(function () {
+            this.ref('url');
+            this.field('title', { boost: 10 });
+            this.field('body');
           });
-        });
+          data.map(function(item) {
+            index.add({
+              url: item[0],
+              title: item[1],
+              body: item[2]
+            });
+          });
+          return;
+        }
+        fuse = new Fuse(data.map((_data => {
+            return {
+                url: _data[0],
+                title: _data[1],
+                body: _data[2]
+            };
+        })), Object.assign(
+            {
+                includeScore: true,
+                threshold: 0.1,
+                ignoreLocation: true,
+                keys: [""title"", ""body""]
+            },
+            _search.opts
+        ));
     }
 
     // Fetch the search index
@@ -36,20 +69,33 @@ gitbook.require([""gitbook"", ""lodash"", ""jQuery""], function(gitbook, _, $) {
 
     // Search for a term and return results
     function search(q) {
-        if (!index) return;
-
-        var results = _.chain(index.search(q))
-        .map(function(result) {
-            var parts = result.ref.split(""#"");
-            return {
-                path: parts[0],
-                hash: parts[1]
-            };
-        })
-        .value();
+        let results = [];
+        switch (_search.engine) {
+            case 'fuse':
+                if (!fuse) return;
+                results = fuse.search(q).map(function(result) {
+                    var parts = result.item.url.split('#');
+                    return {
+                        path: parts[0],
+                        hash: parts[1]
+                    };
+                });
+                break;
+            case 'lunr':
+            default:
+                if (!index) return;
+                results = _.chain(index.search(q)).map(function(result) {
+                    var parts = result.ref.split(""#"");
+                    return {
+                        path: parts[0],
+                        hash: parts[1]
+                    };
+                })
+                .value();
+        }
 
         // [Yihui] Highlight the search keyword on current page
-        $highlighted = results.length === 0 ? [] : $('.page-inner')
+        $highlighted = $('.page-inner')
           .unhighlight(hiOpts).highlight(q, hiOpts).find('span.search-highlight');
         scrollToHighlighted(0);
 
@@ -172,6 +218,7 @@ gitbook.require([""gitbook"", ""lodash"", ""jQuery""], function(gitbook, _, $) {
     gitbook.events.bind(""start"", function(e, config) {
         // [Yihui] disable search
         if (config.search === false) return;
+        init(config);
         collapse = !config.toc || config.toc.collapse === 'section' ||
           config.toc.collapse === 'subsection';
 

---FILE: session-glm/docs/libs/gitbook-2.6.7/js/plugin-sharing.js---
@@ -57,10 +57,21 @@ gitbook.require([""gitbook"", ""lodash"", ""jQuery""], function(gitbook, _, $) {
                 e.preventDefault();
                 window.open(""http://vkontakte.ru/share.php?url=""+encodeURIComponent(location.href));
             }
-        }
+        },
+        'whatsapp': {
+            'label': 'Whatsapp',
+            'icon': 'fa fa-whatsapp',
+            'onClick': function(e) {
+                e.preventDefault();
+                var url = encodeURIComponent(location.href);
+                window.open((isMobile() ? ""whatsapp://send"" : ""https://web.whatsapp.com/send"") + ""?text="" + url);
+            }
+        },
     };
 
-
+    function isMobile() {
+      return !!navigator.maxTouchPoints;
+    }
 
     gitbook.events.bind(""start"", function(e, config) {
         var opts = config.sharing;

---FILE: session-glm/docs/reference-keys.txt---
@@ -6,6 +6,6 @@ exr:glm-rerun
 exr:glm-wcgs
 generalized-linear-models
 why-generalized-linear-models-glms
-logisitc-regression
+logistic-regression
 poisson-regression
 exercises-glms

---FILE: session-glm/docs/search_index.json---
@@ -1 +1 @@
-[[""index.html"", ""Introduction to GLM Preface"", "" Introduction to GLM Olga Dethlefsen 2022-09-01 Preface Generalized Linear Models, GLMs, extend linear model framework to outcome variables that do not follow normal distribution. They are most frequently used to model binary, categorical or count data. Do you see a mistake or a typo? I would be grateful if you let me know via edu.ml-biostats@nbis.se This repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/ ""],[""generalized-linear-models.html"", ""Chapter 1 Generalized linear models 1.1 Why Generalized Linear Models (GLMs) 1.2 Logisitc regression 1.3 Poisson regression 1.4 Exercises (GLMs)"", "" Chapter 1 Generalized linear models Aims to briefly introduce GLMs via examples of modeling binary and count response to test if repo works Learning outcomes to understand the limits of linear regression and the application of GLMs to be able to use glm() function to fit and interpret logistic and Poisson regression 1.1 Why Generalized Linear Models (GLMs) GLMs extend linear model framework to outcome variables that do not follow normal distribution They are most frequently used to model binary, categorical or count data In the Galapagos Island example we have tried to model Species using linear model It kind of worked but the predicted counts were not counts (natural numbers) but rational numbers instead that make no sense when taking about count data Similarly, fitting a regression line to binary data yields predicted values that could take any value, including \\(&lt;0\\) not to mention that it is hard to argue that the values of 0 and 1s are normally distributed Figure 1.1: Example of fitting linear model to binary data, to model the acceptance to medical school, coded as 1 (Yes) and 0 (No) using GPA school scores. Linear model does not fit the data well in this case 1.2 Logisitc regression Yanny or Laurel auditory illusion appeared online in May 2018. You could find lots of information about it, together with some plausible explanations why some people hear Yanny and some year Laurel One of the explanation is that with age we lose the ability to hear certain sounds To see if there is evidence for that, someone has already collected some data for 53 people including their age and gender # Read in and preview data yl &lt;- read.csv(&quot;data/lm/yanny-laurel.csv&quot;) head(yl) ## hear age gender ## 1 Yanny 40 Female ## 2 Yanny 48 Male ## 3 Yanny 32 Female ## 4 Laurel 47 Female ## 5 Laurel 60 Male ## 6 Yanny 11 Female # Recode Laurel to 0 and Yanny as 1 in new variable (what) yl$word &lt;- 0 yl$word[yl$hear==&quot;Laurel&quot;] &lt;- 1 # Make some exploratory plots par(mfrow=c(1,2)) plot(yl$age, yl$word, pch=19, xlab=&quot;age&quot;, ylab=&quot;&quot;, las=1) boxplot(yl$age~yl$hear, xlab=&quot;&quot;, ylab=&quot;age&quot;, col=&quot;lightblue&quot;) Figure 1.2: Yanny and Laurel auditory illusion data, Yanny (1), Luarel (0) Since the response variable takes only two values (Yanny or Laurel) we use GLM model to fit logistic regression model for the probability of hearing Yanny we let \\(p_i=P(Y_i=1)\\) denote the probability of hearing Yanny (success) and we assume that the response follows binomial distribution: \\(Y_i \\sim Bi(1, p_i)\\) distribution We can write the regression model now as: \\[log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i\\] and given the properties of logarithms this is also equivalent to: \\[p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}\\] In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function. Here, the link function \\(log(\\frac{p_i}{1-p_i})\\) provides the link between the binomial distribution of \\(Y_i\\) (hearing Yanny) the linear predictor (age) Thus GLM model can be written as \\[g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta\\] where g() is the link function. We use glm() function in R to fit GLM models # fit logistic regression model logmodel.1 &lt;- glm(word ~ age, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.1)) ## ## Call: ## glm(formula = word ~ age, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.86068 -0.71414 -0.04733 0.64434 2.47887 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.56159 0.95790 -3.718 0.000201 *** ## age 0.08943 0.02297 3.893 9.89e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 83.178 on 59 degrees of freedom ## Residual deviance: 57.967 on 58 degrees of freedom ## AIC: 61.967 ## ## Number of Fisher Scoring iterations: 4 # plot ggPredict(logmodel.1) Figure 1.3: Fitted logistic model to the Yanny and Laurel data # to get predictions use predict() functions # if no new observations is specified predictions are returned for the values of exploratory variables used # we specify response to return prediction on the probability scale predict(logmodel.1, type=&quot;response&quot;) ## 1 2 3 4 5 6 7 ## 0.50394192 0.67507724 0.33187793 0.65516152 0.85868941 0.07057982 0.87903410 ## 8 9 10 11 12 13 14 ## 0.06493370 0.35199768 0.69437930 0.91222027 0.15663558 0.78037334 0.43719992 ## 15 16 17 18 19 20 21 ## 0.39379165 0.59230535 0.20986467 0.45931517 0.69437930 0.22507939 0.48159183 ## 22 23 24 25 26 27 28 ## 0.20986467 0.71302217 0.10615359 0.97322447 0.65516152 0.11494328 0.20986467 ## 29 30 31 32 33 34 35 ## 0.12435950 0.90478991 0.87903410 0.93146108 0.15663558 0.18173908 0.33187793 ## 36 37 38 39 40 41 42 ## 0.59230535 0.94673070 0.06493370 0.82290366 0.91222027 0.82290366 0.92552645 ## 43 44 45 46 47 48 49 ## 0.83556313 0.04630975 0.50394192 0.37265683 0.97751124 0.45931517 0.41533155 ## 50 51 52 53 54 55 56 ## 0.35199768 0.04630975 0.83556313 0.15663558 0.20986467 0.22507939 0.15663558 ## 57 58 59 60 ## 0.73096842 0.35199768 0.43719992 0.87903410 The regression equation for the fitted model is: \\[log(\\frac{\\hat{p_i}}{1-\\hat{p_i}})=-3.56 + 0.09x_i\\] we see from the output that \\(\\hat{\\beta_0} = -3.56\\) and \\(\\hat{\\beta_1} = 0.09\\) these estimates are arrived at via maximum likelihood estimation, something that is out of scope here but similarly to linear models, we can test the null hypothesis \\(H_0:\\beta_1=0\\) by comparing, \\(z = \\frac{\\hat{\\beta_1}}{e.s.e(\\hat{\\beta_1)}} = 3.89\\) with a standard normal distribution, and the associated value is small so there is enough evidence to reject the null, meaning that age is significantly associated with the probability with hearing Laurel and Yanny, Wald test the same conclusion can be reached if we compare the residual deviance Deviance deviance is the number that measures the goodness of fit of a logistic regression model we use saturated and residual deviance to assess model, instead of \\(R^2\\) or \\(R^2(adj)\\) for a GLM model that fits the data well the approximate deviance \\(D\\) is \\[\\chi^2(m-p)\\] where \\(m\\) is the number of parameters in the saturated model (full model) and \\(p\\) is the number of parameters in the model of interest for our above model we have \\(83.178 - 57.967 = 25.21\\) which is larger than 95th percentile of \\(\\chi^2(59-58)\\) qchisq(df=1, p=0.95) ## [1] 3.841459 i.e. \\(25.21 &gt;&gt; 3.84\\) and again we can conclude that age is a significant term in the model Odds ratios In logistic regression we often interpret the model coefficients by taking \\(e^{\\hat{\\beta}}\\) and we talk about odd ratios e.g. we can say, given our above model, \\(e^{0.08943} = 1.093551\\) that for each unit increase in age the odds of hearing Laurel get multiplied by 1.09 Other covariates Finally, we can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical E.g. we can test whether there is a gender effect when hearing Yanny or Laurel # fit logistic regression including age and gender logmodel.2 &lt;- glm(word ~ age + gender, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.2)) ## ## Call: ## glm(formula = word ~ age + gender, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.81723 -0.72585 -0.06218 0.67360 2.44755 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.72679 1.07333 -3.472 0.000516 *** ## age 0.09061 0.02337 3.877 0.000106 *** ## genderMale 0.23919 0.65938 0.363 0.716789 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 83.178 on 59 degrees of freedom ## Residual deviance: 57.835 on 57 degrees of freedom ## AIC: 63.835 ## ## Number of Fisher Scoring iterations: 5 # plot model ggPredict(logmodel.2) Figure 1.4: Yanny Laurel data modelled with logistic regression given age and gender. Regression lines in males and femals are very alike and the model suggest no gender effect 1.3 Poisson regression GLMs can be also applied to count data e.g. hospital admissions due to respiratory disease or number of bird nests in a certain habitat here, we commonly assume that data follow the Poisson distribution \\(Y_i \\sim Pois(\\mu_i)\\) and the corresponding model is \\[E(Y_i)=\\mu_i = \\eta_ie^{\\mathbf{x_i}^T\\boldsymbol\\beta}\\] with a log link \\(\\ln\\mu_i = \\ln \\eta_i + \\mathbf{x_i}^T\\boldsymbol\\beta\\) Data set Suppose we wish to model \\(Y_i\\) the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 regions, a small areas that contain between 2500 and 6000 people. Together with cancer occurrence with have data: Y_all: number of cases of all types of cancer in te IG in 2013 E_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013 pm10: air pollution smoke: percentage of people in an area that smoke ethic: percentage of people who are non-white logpice: natural log of average house price easting and northing: co-ordinates of the central point of the IG divided by 10000 We can model the rate of occurrence of cancer using the very same glm function:¨ - now we use poisson family distribution to model counts - and we will include an offset term to model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions # Read in and preview data cancer &lt;- read.csv(&quot;data/lm/cancer.csv&quot;) head(cancer) ## IG Y_all E_all pm10 smoke ethnic log.price easting northing ## 1 S02000260 133 106.17907 17.8 21.9 5.58 11.59910 26.16245 66.96574 ## 2 S02000261 38 62.43131 18.6 21.8 7.91 11.84940 26.29271 67.00278 ## 3 S02000262 97 120.00694 18.6 20.8 9.58 11.74106 26.21429 67.04280 ## 4 S02000263 80 109.10245 17.0 14.0 10.39 12.30138 25.45705 67.05938 ## 5 S02000264 181 149.77821 18.6 15.2 5.67 11.88449 26.12484 67.09280 ## 6 S02000265 77 82.31156 17.0 14.6 5.61 11.82004 25.37644 67.09826 # fit Poisson regression epid1 &lt;- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), family = poisson, data = cancer) print(summary(epid1)) ## ## Call: ## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + ## northing + offset(log(E_all)), family = poisson, data = cancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.2011 -0.9338 -0.1763 0.8959 3.8416 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8592657 0.8029040 -1.070 0.284531 ## pm10 0.0500269 0.0066724 7.498 6.50e-14 *** ## smoke 0.0033516 0.0009463 3.542 0.000397 *** ## ethnic -0.0049388 0.0006354 -7.773 7.66e-15 *** ## log.price -0.1034461 0.0169943 -6.087 1.15e-09 *** ## easting -0.0331305 0.0103698 -3.195 0.001399 ** ## northing 0.0300213 0.0111013 2.704 0.006845 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 972.94 on 270 degrees of freedom ## Residual deviance: 565.18 on 264 degrees of freedom ## AIC: 2356.2 ## ## Number of Fisher Scoring iterations: 4 Hypothesis testing, model fit and predictions follows stay the same as for logistic regression Rate ratio similarly to logistic regression it common to look at the \\(e^\\beta\\) for instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air poluttion the rate ratio allows us to quantify by how much, here by a factor of \\(e^{0.0500269} = 1.05\\) 1.4 Exercises (GLMs) Exercise 1.1 Make sure you can run and understand the above code for logistic and Poisson regression Exercise 1.2 Additional practice with a bigger more realistic data set. What might affect the chance of getting a heart disease? One of the earliest studies addressing this issue started in 1960 in 3154 healthy men in the San Francisco area. At the start of the study all were free of heart disease. Eight years later the study recorded whether these men now suffered from heart disease (chd), along with many other variables that might be related. The data is available from the faraway package and includes variables: age: age in years height: height in inches weight: weight in pounds sdp: systolic blood pressure in mm Hg dbp: diastolic blood pressure in mm Hg chol: Fasting serum cholesterol in mm % behave: behavior type which is a factor with levels A1 A2 B3 B4 cigs: number of cigarettes smoked per day dibep: behavior type a factor with levels A (Agressive) B (Passive) chd: coronary heat disease developed is a factor with levels no yes typechd: type of coronary heart disease is a factor with levels angina infdeath none silent timechd: Time of CHD event or end of follow-up arcus: arcus senilis is a factor with levels absent present using logistic regression, can you discover anything interesting about the probability of developing heart disease (chd)? using Poisson regression, can you comment about the numbers of cigarettes smoked (cigs)? library(faraway) data(wcgs, package=&quot;faraway&quot;) head(wcgs) ## age height weight sdp dbp chol behave cigs dibep chd typechd timechd ## 2001 49 73 150 110 76 225 A2 25 B no none 1664 ## 2002 42 70 160 154 84 177 A2 20 B no none 3071 ## 2003 42 69 160 110 78 181 B3 0 A no none 3071 ## 2004 41 68 152 124 78 132 B4 20 A no none 3064 ## 2005 59 70 150 144 86 255 B3 20 A yes infdeath 1885 ## 2006 44 72 204 150 90 182 B4 0 A no none 3102 ## arcus ## 2001 absent ## 2002 present ## 2003 absent ## 2004 absent ## 2005 present ## 2006 absent Answers to selected exercises Exr. 1.2 possible solution probability of developing heart disease We first check the relationship between variables to gain more understanding of the data. We discover that a couple of variables are exactly collinear with other variables, including typechd, timechd and dibep. We do not include these in the model. # `chd` and `typechd` were correlated. with(wcgs, table(chd, typechd)) ## typechd ## chd angina infdeath none silent ## no 0 0 2897 0 ## yes 51 135 0 71 # `timechd` is an outcome variable affected by `chd`. by(wcgs$timechd, wcgs$chd, summary) ## wcgs$chd: no ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 238 2864 2952 2775 3048 3430 ## ------------------------------------------------------------ ## wcgs$chd: yes ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 18 934 1666 1655 2400 3229 # `behave` has more detailed info of `dibep` -&gt; exact collinearity with(wcgs, table(behave, dibep)) ## dibep ## behave A B ## A1 0 264 ## A2 0 1325 ## B3 1216 0 ## B4 349 0 We fit logistic regression model to explain the probability of developing cardiac disease (chd) given the remaining variables model1 &lt;- glm(chd ~ . - typechd - timechd - dibep, data = wcgs, family = binomial) summary(model1) ## ## Call: ## glm(formula = chd ~ . - typechd - timechd - dibep, family = binomial, ## data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3653 -0.4362 -0.3128 -0.2208 2.8603 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.331126 2.350347 -5.247 1.55e-07 *** ## age 0.061812 0.012421 4.977 6.47e-07 *** ## height 0.006903 0.033335 0.207 0.83594 ## weight 0.008637 0.003892 2.219 0.02647 * ## sdp 0.018146 0.006435 2.820 0.00481 ** ## dbp -0.000916 0.010903 -0.084 0.93305 ## chol 0.010726 0.001531 7.006 2.45e-12 *** ## behaveA2 0.082920 0.222909 0.372 0.70990 ## behaveB3 -0.618013 0.245032 -2.522 0.01166 * ## behaveB4 -0.487224 0.321325 -1.516 0.12944 ## cigs 0.021036 0.004298 4.895 9.84e-07 *** ## arcuspresent 0.212796 0.143915 1.479 0.13924 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1769.2 on 3139 degrees of freedom ## Residual deviance: 1569.1 on 3128 degrees of freedom ## (14 observations deleted due to missingness) ## AIC: 1593.1 ## ## Number of Fisher Scoring iterations: 6 And we notice that many variables including age, chol, and cigs, were significantly associated with heart disease development. For example, increment of one mm % of Fasting serum cholesterol (chol) elevated the odds of the disease by a factor of \\(e^{0.010726} = 1.010784\\) after adjustment for the effects of the other variables. numbers of cigarettes smoked Many variables were correlated with the number of cigarettes. For example, one mm Hg increase of systolic blood pressure was correlated with the increase of average number of cigarettes smoked by a factor of \\(e^{0.0024264} = 1.002429\\). # check distribution hist(wcgs$cigs, breaks = 25) # Poisson regression for age model2 &lt;- glm(cigs ~ age, data = wcgs, family = poisson) summary(model2) ## ## Call: ## glm(formula = cigs ~ age, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.837 -4.820 -4.787 2.254 15.839 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.5038936 0.0441558 56.706 &lt;2e-16 *** ## age -0.0011423 0.0009481 -1.205 0.228 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62696 on 3152 degrees of freedom ## AIC: 70053 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for weight model3 &lt;- glm(cigs ~ weight, data = wcgs, family = poisson) summary(model3) ## ## Call: ## glm(formula = cigs ~ weight, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.720 -4.803 -4.347 2.441 15.779 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.2939845 0.0430796 76.46 &lt;2e-16 *** ## weight -0.0049918 0.0002548 -19.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62307 on 3152 degrees of freedom ## AIC: 69664 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for systolic blood pressure model4 &lt;- glm(cigs ~ sdp, data = wcgs, family = poisson) summary(model4) ## ## Call: ## glm(formula = cigs ~ sdp, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.445 -4.800 -4.707 2.351 15.922 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1382494 0.0440018 48.595 &lt; 2e-16 *** ## sdp 0.0024264 0.0003382 7.175 7.21e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62647 on 3152 degrees of freedom ## AIC: 70004 ## ## Number of Fisher Scoring iterations: 6 ""]]
+[[""index.html"", ""Introduction to GLM Preface"", "" Introduction to GLM Olga Dethlefsen 2022-09-02 Preface Generalized Linear Models, GLMs, extend linear model framework to outcome variables that do not follow normal distribution. They are most frequently used to model binary, categorical or count data. Do you see a mistake or a typo? I would be grateful if you let me know via edu.ml-biostats@nbis.se This repository contains teaching and learning materials prepared and used during “Introduction to biostatistics and machine learning” course, organized by NBIS, National Bioinformatics Infrastructure Sweden. The course is open for PhD students, postdoctoral researcher and other employees within Swedish universities. The materials are geared towards life scientists wanting to be able to understand and use basic statistical and machine learning methods. More about the course https://nbisweden.github.io/workshop-mlbiostatistics/ ""],[""generalized-linear-models.html"", ""Chapter 1 Generalized linear models 1.1 Why Generalized Linear Models (GLMs) 1.2 Logistic regression 1.3 Poisson regression 1.4 Exercises (GLMs)"", "" Chapter 1 Generalized linear models Aims to briefly introduce GLMs via examples of modeling binary and count response Learning outcomes to understand the limits of linear regression and the application of GLMs to be able to use glm() function to fit and interpret logistic and Poisson regression 1.1 Why Generalized Linear Models (GLMs) GLMs extend linear model framework to outcome variables that do not follow normal distribution They are most frequently used to model binary, categorical or count data In the Galapagos Island example we have tried to model Species using linear model It kind of worked but the predicted counts were not counts (natural numbers) but rational numbers instead that make no sense when taking about count data Similarly, fitting a regression line to binary data yields predicted values that could take any value, including \\(&lt;0\\) not to mention that it is hard to argue that the values of 0 and 1s are normally distributed Figure 1.1: Example of fitting linear model to binary data, to model the acceptance to medical school, coded as 1 (Yes) and 0 (No) using GPA school scores. Linear model does not fit the data well in this case 1.2 Logistic regression Yanny or Laurel auditory illusion appeared online in May 2018. You could find lots of information about it, together with some plausible explanations why some people hear Yanny and some year Laurel One of the explanation is that with age we lose the ability to hear certain sounds To see if there is evidence for that, someone has already collected some data for 53 people including their age and gender # Read in and preview data yl &lt;- read.csv(&quot;data/lm/yanny-laurel.csv&quot;) head(yl) ## hear age gender ## 1 Yanny 40 Female ## 2 Yanny 48 Male ## 3 Yanny 32 Female ## 4 Laurel 47 Female ## 5 Laurel 60 Male ## 6 Yanny 11 Female # Recode Laurel to 0 and Yanny as 1 in new variable yl$word &lt;- 0 yl$word[yl$hear==&quot;Yanny&quot;] &lt;- 1 # Make some exploratory plots par(mfrow=c(1,2)) plot(yl$age, yl$word, pch=19, xlab=&quot;age&quot;, ylab=&quot;&quot;, las=1) boxplot(yl$age~yl$hear, xlab=&quot;&quot;, ylab=&quot;age&quot;, col=&quot;lightblue&quot;) Figure 1.2: Yanny and Laurel auditory illusion data, Yanny (1), Laurel (0) Since the response variable takes only two values (Yanny or Laurel) we use GLM model to fit logistic regression model for the probability of hearing Yanny we let \\(p_i=P(Y_i=1)\\) denote the probability of hearing Yanny (success) and we assume that the response follows binomial distribution: \\(Y_i \\sim Bi(1, p_i)\\) distribution We can write the regression model now as: \\[log(\\frac{p_i}{1-p_i})=\\beta_0 + \\beta_1x_i\\] and given the properties of logarithms this is also equivalent to: \\[p_i = \\frac{exp(\\beta_0 + \\beta_1x_i)}{1 + exp(\\beta_0 + \\beta_1x_i)}\\] In essence, the GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function. Here, the link function \\(log(\\frac{p_i}{1-p_i})\\) provides the link between the binomial distribution of \\(Y_i\\) (hearing Yanny) the linear predictor (age) Thus GLM model can be written as \\[g(\\mu_i)=\\mathbf{X}\\boldsymbol\\beta\\] where g() is the link function. We use glm() function in R to fit GLM models # fit logistic regression model logmodel.1 &lt;- glm(word ~ age, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.1)) ## ## Call: ## glm(formula = word ~ age, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.47887 -0.64434 0.04733 0.71414 1.86068 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.56159 0.95790 3.718 0.000201 *** ## age -0.08943 0.02297 -3.893 9.89e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 83.178 on 59 degrees of freedom ## Residual deviance: 57.967 on 58 degrees of freedom ## AIC: 61.967 ## ## Number of Fisher Scoring iterations: 4 # plot ggPredict(logmodel.1) Figure 1.3: Fitted logistic model to the Yanny and Laurel data # to get predictions use predict() functions # if no new observations is specified predictions are returned for the values of exploratory variables used # we specify response to return prediction on the probability scale predict(logmodel.1, type=&quot;response&quot;) ## 1 2 3 4 5 6 7 ## 0.49605808 0.32492276 0.66812207 0.34483848 0.14131059 0.92942018 0.12096590 ## 8 9 10 11 12 13 14 ## 0.93506630 0.64800232 0.30562070 0.08777973 0.84336442 0.21962666 0.56280008 ## 15 16 17 18 19 20 21 ## 0.60620835 0.40769465 0.79013533 0.54068483 0.30562070 0.77492061 0.51840817 ## 22 23 24 25 26 27 28 ## 0.79013533 0.28697783 0.89384641 0.02677553 0.34483848 0.88505672 0.79013533 ## 29 30 31 32 33 34 35 ## 0.87564050 0.09521009 0.12096590 0.06853892 0.84336442 0.81826092 0.66812207 ## 36 37 38 39 40 41 42 ## 0.40769465 0.05326930 0.93506630 0.17709634 0.08777973 0.17709634 0.07447355 ## 43 44 45 46 47 48 49 ## 0.16443687 0.95369025 0.49605808 0.62734317 0.02248876 0.54068483 0.58466845 ## 50 51 52 53 54 55 56 ## 0.64800232 0.95369025 0.16443687 0.84336442 0.79013533 0.77492061 0.84336442 ## 57 58 59 60 ## 0.26903158 0.64800232 0.56280008 0.12096590 The regression equation for the fitted model is: \\[log(\\frac{\\hat{p_i}}{1-\\hat{p_i}})=-3.56 + 0.09x_i\\] we see from the output that \\(\\hat{\\beta_0} = -3.56\\) and \\(\\hat{\\beta_1} = 0.09\\) these estimates are arrived at via maximum likelihood estimation, something that is out of scope here but similarly to linear models, we can test the null hypothesis \\(H_0:\\beta_1=0\\) by comparing, \\(z = \\frac{\\hat{\\beta_1}}{e.s.e(\\hat{\\beta_1)}} = 3.89\\) with a standard normal distribution, and the associated value is small so there is enough evidence to reject the null, meaning that age is significantly associated with the probability with hearing Laurel and Yanny, Wald test the same conclusion can be reached if we compare the residual deviance Deviance deviance is the number that measures the goodness of fit of a logistic regression model we use saturated and residual deviance to assess model, instead of \\(R^2\\) or \\(R^2(adj)\\) for a GLM model that fits the data well the approximate deviance \\(D\\) is \\[\\chi^2(m-p)\\] where \\(m\\) is the number of parameters in the saturated model (full model) and \\(p\\) is the number of parameters in the model of interest for our above model we have \\(83.178 - 57.967 = 25.21\\) which is larger than 95th percentile of \\(\\chi^2(59-58)\\) qchisq(df=1, p=0.95) ## [1] 3.841459 i.e. \\(25.21 &gt;&gt; 3.84\\) and again we can conclude that age is a significant term in the model Odds ratios In logistic regression we often interpret the model coefficients by taking \\(e^{\\hat{\\beta}}\\) and we talk about odd ratios e.g. we can say, given our above model, \\(e^{0.08943} = 1.093551\\) that for each unit increase in age the odds of hearing Laurel get multiplied by 1.09 Other covariates Finally, we can use the same logic as in multiple regression to expand by models by additional variables, numerical, binary or categorical E.g. we can test whether there is a gender effect when hearing Yanny or Laurel # fit logistic regression including age and gender logmodel.2 &lt;- glm(word ~ age + gender, family = binomial(link=&quot;logit&quot;), data = yl) # print model summary print(summary(logmodel.2)) ## ## Call: ## glm(formula = word ~ age + gender, family = binomial(link = &quot;logit&quot;), ## data = yl) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.44755 -0.67360 0.06218 0.72585 1.81723 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.72679 1.07333 3.472 0.000516 *** ## age -0.09061 0.02337 -3.877 0.000106 *** ## genderMale -0.23919 0.65938 -0.363 0.716789 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 83.178 on 59 degrees of freedom ## Residual deviance: 57.835 on 57 degrees of freedom ## AIC: 63.835 ## ## Number of Fisher Scoring iterations: 5 # plot model ggPredict(logmodel.2) Figure 1.4: Yanny Laurel data modelled with logistic regression given age and gender. Regression lines in males and femals are very alike and the model suggest no gender effect 1.3 Poisson regression GLMs can be also applied to count data e.g. hospital admissions due to respiratory disease or number of bird nests in a certain habitat here, we commonly assume that data follow the Poisson distribution \\(Y_i \\sim Pois(\\mu_i)\\) and the corresponding model is \\[E(Y_i)=\\mu_i = \\eta_ie^{\\mathbf{x_i}^T\\boldsymbol\\beta}\\] with a log link \\(\\ln\\mu_i = \\ln \\eta_i + \\mathbf{x_i}^T\\boldsymbol\\beta\\) Data set Suppose we wish to model \\(Y_i\\) the number of cancer cases in the i-th intermediate geographical location (IG) in Glasgow. We have collected data for 271 regions, a small areas that contain between 2500 and 6000 people. Together with cancer occurrence with have data: Y_all: number of cases of all types of cancer in te IG in 2013 E_all: expected number of cases of all types of cancer for the IG based on the population size and demographics of the IG in 2013 pm10: air pollution smoke: percentage of people in an area that smoke ethic: percentage of people who are non-white logpice: natural log of average house price easting and northing: co-ordinates of the central point of the IG divided by 10000 We can model the rate of occurrence of cancer using the very same glm function:¨ - now we use poisson family distribution to model counts - and we will include an offset term to model as we are modeling the rate of occurrence of the cancer that has to be adjusted by different number of people living in different regions # Read in and preview data cancer &lt;- read.csv(&quot;data/lm/cancer.csv&quot;) head(cancer) ## IG Y_all E_all pm10 smoke ethnic log.price easting northing ## 1 S02000260 133 106.17907 17.8 21.9 5.58 11.59910 26.16245 66.96574 ## 2 S02000261 38 62.43131 18.6 21.8 7.91 11.84940 26.29271 67.00278 ## 3 S02000262 97 120.00694 18.6 20.8 9.58 11.74106 26.21429 67.04280 ## 4 S02000263 80 109.10245 17.0 14.0 10.39 12.30138 25.45705 67.05938 ## 5 S02000264 181 149.77821 18.6 15.2 5.67 11.88449 26.12484 67.09280 ## 6 S02000265 77 82.31156 17.0 14.6 5.61 11.82004 25.37644 67.09826 # fit Poisson regression epid1 &lt;- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), family = poisson, data = cancer) print(summary(epid1)) ## ## Call: ## glm(formula = Y_all ~ pm10 + smoke + ethnic + log.price + easting + ## northing + offset(log(E_all)), family = poisson, data = cancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.2011 -0.9338 -0.1763 0.8959 3.8416 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8592657 0.8029040 -1.070 0.284531 ## pm10 0.0500269 0.0066724 7.498 6.50e-14 *** ## smoke 0.0033516 0.0009463 3.542 0.000397 *** ## ethnic -0.0049388 0.0006354 -7.773 7.66e-15 *** ## log.price -0.1034461 0.0169943 -6.087 1.15e-09 *** ## easting -0.0331305 0.0103698 -3.195 0.001399 ** ## northing 0.0300213 0.0111013 2.704 0.006845 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 972.94 on 270 degrees of freedom ## Residual deviance: 565.18 on 264 degrees of freedom ## AIC: 2356.2 ## ## Number of Fisher Scoring iterations: 4 Hypothesis testing, model fit and predictions follows stay the same as for logistic regression Rate ratio similarly to logistic regression it common to look at the \\(e^\\beta\\) for instance we are interested in the effect of air pollution on health, we could look at the pm10 coefficient coefficient is positive, 0.0500269, indicating that cancer incidence rate increase with increased air poluttion the rate ratio allows us to quantify by how much, here by a factor of \\(e^{0.0500269} = 1.05\\) 1.4 Exercises (GLMs) Exercise 1.1 Make sure you can run and understand the above code for logistic and Poisson regression Exercise 1.2 Additional practice with a bigger more realistic data set. What might affect the chance of getting a heart disease? One of the earliest studies addressing this issue started in 1960 in 3154 healthy men in the San Francisco area. At the start of the study all were free of heart disease. Eight years later the study recorded whether these men now suffered from heart disease (chd), along with many other variables that might be related. The data is available from the faraway package and includes variables: age: age in years height: height in inches weight: weight in pounds sdp: systolic blood pressure in mm Hg dbp: diastolic blood pressure in mm Hg chol: Fasting serum cholesterol in mm % behave: behavior type which is a factor with levels A1 A2 B3 B4 cigs: number of cigarettes smoked per day dibep: behavior type a factor with levels A (Agressive) B (Passive) chd: coronary heat disease developed is a factor with levels no yes typechd: type of coronary heart disease is a factor with levels angina infdeath none silent timechd: Time of CHD event or end of follow-up arcus: arcus senilis is a factor with levels absent present using logistic regression, can you discover anything interesting about the probability of developing heart disease (chd)? using Poisson regression, can you comment about the numbers of cigarettes smoked (cigs)? library(faraway) data(wcgs, package=&quot;faraway&quot;) head(wcgs) ## age height weight sdp dbp chol behave cigs dibep chd typechd timechd ## 2001 49 73 150 110 76 225 A2 25 B no none 1664 ## 2002 42 70 160 154 84 177 A2 20 B no none 3071 ## 2003 42 69 160 110 78 181 B3 0 A no none 3071 ## 2004 41 68 152 124 78 132 B4 20 A no none 3064 ## 2005 59 70 150 144 86 255 B3 20 A yes infdeath 1885 ## 2006 44 72 204 150 90 182 B4 0 A no none 3102 ## arcus ## 2001 absent ## 2002 present ## 2003 absent ## 2004 absent ## 2005 present ## 2006 absent Answers to selected exercises Exr. 1.2 possible solution probability of developing heart disease We first check the relationship between variables to gain more understanding of the data. We discover that a couple of variables are exactly collinear with other variables, including typechd, timechd and dibep. We do not include these in the model. # `chd` and `typechd` were correlated. with(wcgs, table(chd, typechd)) ## typechd ## chd angina infdeath none silent ## no 0 0 2897 0 ## yes 51 135 0 71 # `timechd` is an outcome variable affected by `chd`. by(wcgs$timechd, wcgs$chd, summary) ## wcgs$chd: no ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 238 2864 2952 2775 3048 3430 ## ------------------------------------------------------------ ## wcgs$chd: yes ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 18 934 1666 1655 2400 3229 # `behave` has more detailed info of `dibep` -&gt; exact collinearity with(wcgs, table(behave, dibep)) ## dibep ## behave A B ## A1 0 264 ## A2 0 1325 ## B3 1216 0 ## B4 349 0 We fit logistic regression model to explain the probability of developing cardiac disease (chd) given the remaining variables model1 &lt;- glm(chd ~ . - typechd - timechd - dibep, data = wcgs, family = binomial) summary(model1) ## ## Call: ## glm(formula = chd ~ . - typechd - timechd - dibep, family = binomial, ## data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3653 -0.4362 -0.3128 -0.2208 2.8603 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.331126 2.350347 -5.247 1.55e-07 *** ## age 0.061812 0.012421 4.977 6.47e-07 *** ## height 0.006903 0.033335 0.207 0.83594 ## weight 0.008637 0.003892 2.219 0.02647 * ## sdp 0.018146 0.006435 2.820 0.00481 ** ## dbp -0.000916 0.010903 -0.084 0.93305 ## chol 0.010726 0.001531 7.006 2.45e-12 *** ## behaveA2 0.082920 0.222909 0.372 0.70990 ## behaveB3 -0.618013 0.245032 -2.522 0.01166 * ## behaveB4 -0.487224 0.321325 -1.516 0.12944 ## cigs 0.021036 0.004298 4.895 9.84e-07 *** ## arcuspresent 0.212796 0.143915 1.479 0.13924 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1769.2 on 3139 degrees of freedom ## Residual deviance: 1569.1 on 3128 degrees of freedom ## (14 observations deleted due to missingness) ## AIC: 1593.1 ## ## Number of Fisher Scoring iterations: 6 And we notice that many variables including age, chol, and cigs, were significantly associated with heart disease development. For example, increment of one mm % of Fasting serum cholesterol (chol) elevated the odds of the disease by a factor of \\(e^{0.010726} = 1.010784\\) after adjustment for the effects of the other variables. numbers of cigarettes smoked Many variables were correlated with the number of cigarettes. For example, one mm Hg increase of systolic blood pressure was correlated with the increase of average number of cigarettes smoked by a factor of \\(e^{0.0024264} = 1.002429\\). # check distribution hist(wcgs$cigs, breaks = 25) # Poisson regression for age model2 &lt;- glm(cigs ~ age, data = wcgs, family = poisson) summary(model2) ## ## Call: ## glm(formula = cigs ~ age, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.837 -4.820 -4.787 2.254 15.839 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.5038936 0.0441558 56.706 &lt;2e-16 *** ## age -0.0011423 0.0009481 -1.205 0.228 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62696 on 3152 degrees of freedom ## AIC: 70053 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for weight model3 &lt;- glm(cigs ~ weight, data = wcgs, family = poisson) summary(model3) ## ## Call: ## glm(formula = cigs ~ weight, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.720 -4.803 -4.347 2.441 15.779 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.2939845 0.0430796 76.46 &lt;2e-16 *** ## weight -0.0049918 0.0002548 -19.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62307 on 3152 degrees of freedom ## AIC: 69664 ## ## Number of Fisher Scoring iterations: 6 # Poisson regression for systolic blood pressure model4 &lt;- glm(cigs ~ sdp, data = wcgs, family = poisson) summary(model4) ## ## Call: ## glm(formula = cigs ~ sdp, family = poisson, data = wcgs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.445 -4.800 -4.707 2.351 15.922 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1382494 0.0440018 48.595 &lt; 2e-16 *** ## sdp 0.0024264 0.0003382 7.175 7.21e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 62697 on 3153 degrees of freedom ## Residual deviance: 62647 on 3152 degrees of freedom ## AIC: 70004 ## ## Number of Fisher Scoring iterations: 6 ""],[""404.html"", ""Page not found"", "" Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. ""]]"
NBISweden,workshop-mlbiostatistics,b05408409905840a72b438fbf9593ee268e4589e,olgadet,,2022-08-30T11:35:34Z,olgadet,,2022-08-30T11:35:34Z,Fix link to session-glm,schedule.md,False,False,False,False,2,2,4,"---FILE: schedule.md---
@@ -79,9 +79,9 @@ title:  'Schedule'
 
 *14.30 - 15.00 break*
 
-**15.00 - 15.30** [Generalized linear models](session-glms/docs/)
+**15.00 - 15.30** [Generalized linear models](session-glm/docs/)
 
-**15.30 - 16.30** [Exercises](session-glms/docs/generalized-linear-models.html#exercises-glms)
+**15.30 - 16.30** [Exercises](session-glm/docs/generalized-linear-models.html#exercises-glms)
 
 **16.30 - 17.00** Daily challenge
 "
NBISweden,workshop-mlbiostatistics,63acd3fb9e646369e1e49311e28a3692583f26c2,evaf,eva@freyhult.net,2022-08-30T09:48:13Z,evaf,eva@freyhult.net,2022-08-30T09:48:13Z,Fix typos,session-precourse-math/101-math-notations.Rmd;session-precourse-math/102-math-sets.Rmd;session-precourse-math/103-math-functions.Rmd;session-precourse-math/107-math-matrices.Rmd,True,False,True,False,8,8,16,"---FILE: session-precourse-math/101-math-notations.Rmd---
@@ -8,7 +8,7 @@
 
 **Learning outcomes**
 
-- to recognize natural numbers, integrals and real numbers
+- to recognize natural numbers, integers and real numbers
 - to understand the differences between variables and constants
 - to use symbols, especially Sigma and product notations, to represent mathematical operations
 
@@ -75,8 +75,8 @@ Equal sign
 
 **Sigma and Product notation**
 
-- the $\Sigma$ notation, read as **Sigma notation**, provides a convenient way of writing longs sums, e.g. the sum of $x_1 + x_2 + x_3 + ... + x_{20}$ is written as $\displaystyle \sum_{i=1}^{i=20}x_i$
-- the $\Pi$ notation, read as **Product notation**, provides a convenient way of writing longs products, e.g.  $x_1 \cdot x_2 \cdot x_3 \cdot ... \cdot x_{20}$ is written as $\displaystyle \prod_{i=1}^{i=20}x_i$
+- the $\Sigma$ notation, read as **Sigma notation**, provides a convenient way of writing long sums, e.g. the sum of $x_1 + x_2 + x_3 + ... + x_{20}$ is written as $\displaystyle \sum_{i=1}^{i=20}x_i$
+- the $\Pi$ notation, read as **Product notation**, provides a convenient way of writing long products, e.g.  $x_1 \cdot x_2 \cdot x_3 \cdot ... \cdot x_{20}$ is written as $\displaystyle \prod_{i=1}^{i=20}x_i$
 
 ## Inequalities
 Given any two real numbers $a$ and $b$ there are three mutually exclusive possibilities:

---FILE: session-precourse-math/102-math-sets.Rmd---
@@ -47,7 +47,7 @@ library(""ggvenn"")
 <br/>
 
 - **complement of a set, $A'$, $A^c$**: are the elements not in A
-- **difference of two sets, $\setminus$**: two sets can be ""substracted"", denoted by $A \setminus B$, by taking all elements that are members of A but are not members of B, e.g. $\{1, 2, 3, 4\} \setminus \{1, 3\} = \{2, 4\}$. This is also in other words a relative complement of A with respect to B.
+- **difference of two sets, $\setminus$**: two sets can be ""subtracted"", denoted by $A \setminus B$, by taking all elements that are members of A but are not members of B, e.g. $\{1, 2, 3, 4\} \setminus \{1, 3\} = \{2, 4\}$. This is also in other words a relative complement of A with respect to B.
 
 <br/>
 

---FILE: session-precourse-math/103-math-functions.Rmd---
@@ -55,7 +55,7 @@ knitr::kable(
   
 ```
 
-|x (Celsius degrees) | evaluates | f(x) (Farenheit degress)|
+|x (Celsius degrees) | evaluates | f(x) (Fahrenheit degrees)|
 | :-----------: | :-----------: | :-------: |
 |-20 | $f(-20) = 1.8 \cdot (-20) + 32$| -4 |
 |-10 | $f(-10) = 1.8 \cdot (-10) + 32$| 14 |
@@ -82,10 +82,10 @@ plot(x, y, xlab=""temperature [Celsius]"", ylab=""temperature [Farenheit]"", type=""b
 - quadratic function $f(x) = a + bx + cx^2$
 - cubic function $fx() = a + bx + cx^2 + dx^3$
 
-**Transcedental functions**: functions that are not algebraic, e.g. 
+**Transcendental functions**: functions that are not algebraic, e.g. 
 
 - exponential function $f(x) = e^x$
-- logarithimic function $f(x) = log(x)$
+- logarithmic function $f(x) = log(x)$
 - trigonometric function $f(x) = -3sin(2x)$
 
 ```{r, echo=F, fig.align=""center"", fig.cap=""Examples of the standard classess of functions""}

---FILE: session-precourse-math/107-math-matrices.Rmd---
@@ -54,7 +54,7 @@ $$\mathbf{A}=\begin{bmatrix}
   0 & 2 & 5  \\
   0 & 0 & 3
 \end{bmatrix}$$
-- A **lower-triangular matrix** is a square matrix in which all entries above the digonal are 0, that is hat is $x_{ij}=0$ for $i>j$ e.g. 
+- A **lower-triangular matrix** is a square matrix in which all entries above the diagonal are 0, that is hat is $x_{ij}=0$ for $i>j$ e.g. 
 $$\mathbf{A}=\begin{bmatrix}
   1 & 0 & 0  \\
   1 & 1 & 0  \\"
NBISweden,workshop-mlbiostatistics,a1168569342679b51da0b07070e48d322e3ff600,evaf,eva@freyhult.net,2022-08-30T07:43:30Z,evaf,eva@freyhult.net,2022-08-30T07:43:30Z,Fix typo,Rmdprobdescinfe/02probability.Rmd,True,False,True,False,2,1,3,"---FILE: Rmdprobdescinfe/02probability.Rmd---
@@ -155,7 +155,7 @@ plot(data.frame(x=1:6, p=1/6) %>% ggplot(aes(x=x, y=p)) + geom_bar(stat=""identit
 ```{example, label=""nuclsite"", echo=TRUE}
 **Nucleotide in a given site**
   
-The nucleotide at a given genomic site can be one of the four nucleotides; {A, C, T, G}. Unlike a die the four nulceotides are ususlly not equally likely. Thenucleotide in the site can be described by a random variable, $X$. The probability mass function can be summarized in a table or a bar plot.
+The nucleotide at a given genomic site can be one of the four nucleotides; {A, C, T, G}. Unlike the sides on a die the four nucleotides are usually not equally likely. The nucleotide at the site can be described by a random variable, $X$. The probability mass function can be summarized in a table or a bar plot.
 ```
 
 ```{r nucl, fig.height=3, fig.width=7, fig.cap=""Probability mass function of a nucleotide site."", out.width=""45%"", fig.align='center'}
@@ -406,6 +406,7 @@ The Poisson distribution can approximate the binomial distribution if $n$ is lar
 
 
 Examples of Poisson random variables;
+
 * A rare disease has a very low probability for a single individual. The number of individuals in a large population that catch the disease in a certain time period is a Poisson random variable.
 * Number of reads aligned to a gene region
 "
NBISweden,workshop-mlbiostatistics,c4fe478946991c40aa9b9b605f547b6d9f80bb5c,bsennblad,bengt.sennblad@scilifelab.se,2021-10-07T15:55:52Z,bsennblad,bengt.sennblad@scilifelab.se,2021-10-07T15:55:52Z,Fix minor bugs and typos,session-regularization/slides/lecture-regularization.Rmd;session-regularization/slides/lecture-regularization.html,True,False,True,False,484,233,717,"---FILE: session-regularization/slides/lecture-regularization.Rmd---
@@ -1,5 +1,7 @@
 ---
 title: ""Session Regularization""
+author: Bengt Sennblad, NBIS
+date: 8 October 2021
 output: 
   xaringan::moon_reader:
     encoding: 'UTF-8'
@@ -60,20 +62,25 @@ layout: true
 
 ---
 
-- Given a generative model, we can compute $$Pr[Y| X, \theta],$$ the probability that the model with parameters $\theta$ and given $X$ generates $Y$... 
+- Given a generative model for $X\rightarrow Y$, with parameters $\theta$, we can compute $$Pr[Y| X, \theta],$$ the probability that the model with parameters $\theta$ and given $X$ has generated $Y$... 
 
 --
 
 - .. but we are often more interested in the probability that the parameters, $\theta$, are correct given the observed data, $X,Y$, i.e., ideally we want $$Pr[\theta|X,Y].$$
 
+???
+
+- However, it is not obvious how to compute this probability
+- This is where likelihood comes in.
+
 ---
 
 ### Likelihood intuition:
 
-- If parameters $\theta_T$ is closer to the 'truth' than $\theta_W$, then $$Pr[Y| X, \theta = \theta_T] > Pr[Y| X, \theta = \theta_T]$$
+- If parameters $\theta_T$ is closer to the 'truth' than $\theta_W$, then we would expect $$Pr[Y| X, \theta = \theta_T] > Pr[Y| X, \theta = \theta_T]$$
 --
 
-- We should therefore select the $\theta$ that maximizes $Pr[Y| X, \theta]$; this is called  
+- We should therefore select the $\theta$ that maximizes $Pr[Y| X, \theta]$ as the best estimate; this is called  
 
 .center[
 **Maximum Likelihood estimation (MLE) of $\theta$.**
@@ -93,13 +100,24 @@ Since statistical model contain an element of randomness, the reasoning above mi
 
 - The likelihood of model parameters being true given observed data is $$L[\theta|Y,X] = k\times Pr[Y|X, \theta],$$ with $k$ being an arbitrary konstant (Edwards, 1972)
 
+???
+
+- **Proportional**
+- The proportionality (indicated by '$\propto$') means there are some unknown constant factor, $k$,
+- However, the factor $k$ is assumed to be constant over $\theta$s and over models. 
+
 --
 
 - $L[\theta|Y,X]$ is not a proper probability, hence the term *likelihood* is used.
 
+???
+
+- Sum/integral of likelihoods $\neq 1$
+- possibly bring up Bayes formula $$Pr[\theta|Y,X] = \frac{Pr[Y|\theta,X]}{Pr[Y,X]}$$
+
 --
 
-- In practice, proportionality is ignored and we set
+- In practice, proportionality is (almost alwyays) ignored and we set
 
 $$L[\theta|Y,X] = Pr[Y|X, \theta]$$
 --
@@ -109,30 +127,37 @@ $$\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]} = \frac{k Pr[Y|X, \theta_1]}{ k  Pr[Y|
 
 ???
 
-The proportionality (indicated by '$\propto$') means there are some unknown constant factor, $k$, such that $L[\theta|Y,X] = k Pr[Y|X, \theta]$. However, the factor $k$ is assumed to be constant over $\theta$s and over models. 
-
--- 
-
 - When the likelihood of two $\theta$s (or models) are compared this is almost always done as a _likelihood ratio_, 
-
-$$\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]} = \frac{k Pr[Y|X, \theta_1]}{ k  Pr[Y|X, \theta_0]} =\frac{Pr[Y|X, \theta_1]}{ Pr[Y|X, \theta_0]}$$
-
-which means that the factor $k$ disappears. Hence the factor $k$ is always ignored. Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization... 
+- The proportionality  factor, $k$, such that $L[\theta|Y,X] = k Pr[Y|X, \theta]$ csancels out
+- Hence the factor $k$ is always ignored. 
+- Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization... 
 
 ---
 
 ### Maximum Likelihood estimation
 
-- Select the estimates $\widehat\theta$ that gives the highest likelihood, i.e.,  
+- Select the estimate $\widehat\theta$ that gives the highest likelihood, i.e.,  
 $$\widehat{\theta} = argmax_{\theta}L[\theta|X,Y] \qquad \Leftrightarrow \qquad  max_{\theta}L[\theta|X,Y] = L[\widehat\theta|X,Y].$$
+???
+
+- Select as the best estimate that which...
+- $\argmax_{\theta}$ is the argument $\theta$ that maximizes...
 
 --
 
 - Often, it is practical to use the logarithm of the likelihood, the _logLikelihood_, $$\log L[\theta_1|Y,X].$$ Notice that
 
 --
-  - $\widehat{\theta} = argmin_\theta -\log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]$
-  
+  - $\widehat{\theta} = argmax_\theta \log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]$ 
+
+--
+  - $\widehat{\theta} = argmin_\theta -\log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]$ 
+
+???
+
+- Also the negative logLikelihood is used
+- minimization instead of maximization
+
 --
   - $\log\left(\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]}\right) = \frac{\log L[\theta_1|Y,X]}{\log L[\theta_0|Y,X]} = \log L[\theta_1|Y,X] - \log L[\theta_0|Y,X].$
 
@@ -144,26 +169,42 @@ Likelihood and maximum likelihood estimation are central concepts in statistics.
 
   - A likelihood ratio corresponds to a logLikelihood difference,
 
+---
+
 ## Likelihood | `Likelihood and OLS for linear models`
 
 
-So, why have we used ordinary least squares (OLS), i.e., minimization of  RSS when estimating linear model parameters $\beta$ rather than maximum likelihood estimation?
+- Why have we used ordinary least squares (OLS), $\min_\theta RSS$ when estimating linear model parameters
 
-Linear models is a special case with some nice properties when it comes to  likelihood. Consider a simple linear regression model,
+???
+
+- Why have we used ordinary least squares (OLS), $\min_\theta RSS$ when estimating linear model parameters $\beta$ rather than maximum likelihood estimation?
+
+--
+
+Consider a simple linear regression model, $$ y = \beta x + \epsilon, \epsilon\sim N(0,\sigma^2).$$ 
+
+???
+
+- Linear models is a special case with some nice properties when it comes to  likelihood. 
+
+--
+
+- The maximum likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS, and $$\log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS$$
 
-$$ y = \beta x + \epsilon, $$
+  - This means that $$\widehat{\beta}_{ML} = \widehat{\beta}_{OLS}$$
 
-where the residuals $\epsilon\sim N(0,\sigma^2)$. 
+???
 
-It turns out that the maximum likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS of the residuals, so that the likelihood can be approximated by
+- It turns out that the maximum likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS of the residuals, so that the likelihood can be very well approximated by
 
-$$  \log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS$$
+--
 
-This means that the maximum likelihood estimates of $\beta$ is exactly the same as those of the minimum RSS.
+- _**NB!** This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using a maximum likelihood estimation _
 
-_**NB!** This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using a maximum likelihood estimation _
+???
 
-In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed, e.g., OLS.
+- In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed, e.g., OLS.
 
 ---
 layout: false
@@ -173,7 +214,8 @@ layout: true
 
 ---
 
-- kalle
+ $\quad$ 
+
 ???
 
 We will now look at a general problem in statistical modeling that can be visualized quite well with Likelihoods. We will later look at some solutions to this problem.
@@ -183,22 +225,17 @@ We will now look at a general problem in statistical modeling that can be visual
 ### Task |  Example Data
 
 ```{r, echo=TRUE}
-set.seed(85) # To obtain exactly the same result as in the demo
-
+# To obtain exactly the same result as in the demo, set seed to 85
+set.seed(85)
 N=100 # number of samples
 P=10 # number of variables
-
-# Draw variables, x_{i,1},...,x_{i,P} for all N individuals, uniformly
-# 1. runif generates N*P+1 values, round them off to 2 decimals
-# 2. Put values into a matrix with N rows and P columns
+# Generate X from uniform distribution
 X=matrix(round(runif(N*(P+1),min=0, max=2), 2), nrow=N, ncol=P)
-
 # generate Y from a multivariate lm of 3 first X variables only
 b0=3 #intercept
 b=c(runif(3, min=0.5, max=1.0)) # effect sizes for 3 first X variables
-
 # generate Y for all samples (=rows in X) 
-Y <- b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
+Y = b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
 ```
 
 ```{text include = FALSE}
@@ -210,26 +247,36 @@ Y <- b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
     - $\beta_0 = 3$ 
     - $\beta_1 = `r b[1]`$, $\beta_2 = `r b[2]`$, $\beta_3 = `r b[3]`$ generated from $Unif(0.5, 1.0)$
 ```
+
+???
+- Draw variables, x_{i,1},...,x_{i,P} for all N individuals, uniformly
+- 1. runif generates N*P+1 values, round them off to 2 decimals
+- 2. Put values into a matrix with N rows and P columns
+
 --
 
 #### Think about: What can simulated data be used for?
 --
 
-* Oracle knowledge when evaluating performance of methods, e.g., Type I and II errors 
 * Estimating probabilities and probability distributions of, e.g., data and summary statistics of data
+* Oracle knowledge when evaluating performance of methods, e.g., Type I and II errors 
+
+???
+
+- NULL distribution
 
 ---
 
 ### Model comparison
 
-Now consider the following two models for our data
+Now consider the following two models for our example data
 
 \begin{eqnarray}
 y & \sim & \beta_0 + \beta_1 x_1 & (1) \\
 y & \sim &  \beta_0 + \beta_1 x_1 + \beta_2 x_2 & (2)
 \end{eqnarray}
 
-What are the max Likelihood estimates of the two models?
+What are the max Likelihood estimates of the two models? Let's plot them!
 
 ---
 
@@ -306,13 +353,22 @@ _Nested models_
 * Therefore Model (2) will always have equal or better ML than Model (1)
 {{content}}
 ]
+
+???
+
+- ML increases the more variables we add
 --
 
 _Overfitting_
 * Since the simulated data was generated from the 3 first variables...
   + ...thus, the subsequent variables increase ML by modeling noise in data
   
 {{content}}  
+
+???
+
+- The ML model is the one with all variables
+
 --
   
 * Solution:  Regularization
@@ -333,8 +389,13 @@ layout: true
 - Typically, the regularization term is a function of parameters $\beta$:
 
 $$\log rL[\beta | X, Y]  = \log L[\beta | X, Y] - f(\beta).$$
+???
+
+- ""Simplast"" = few parameters/variables
 
-#### Example | `A very simple regularization model` 
+--
+
+#### Example | A very simple regularization model
 
 - $f(\beta) = \#\beta = \#X$ (that is the number of $X$ variables).  
 $$\log rL[{\beta} | X, Y]  = \log L[\beta | X, Y] - \#X,$$
@@ -376,16 +437,14 @@ Applying this rL to our example data, solves the overfitting problem.
 
 ### Mini-task: Think about:
 
-* Which is the best model? Is this correct?
-* How good it is compared to the other models?
+* Which is the best model? Is this correct according to *oracle knowledge*?
 * Can you see a drawback in our model testing approach above? If so, how can we solve that?
 
 --
 .pull-right[
 #### Some possible answers 
 
 * The best model, 3, is the correct one.
-* The likelihood of second best model (with the first 4 X-variables) is $\approx 40\%$ of the best likelihood.
 * What if the right variables had been among the last? 
 
 {{content}}
@@ -403,17 +462,18 @@ Applying this rL to our example data, solves the overfitting problem.
   - Lasso; see next section
 ---
 
-####  LASSO (*Least absolute shrinkage and selection operator*) 
+###  LASSO (*Least absolute shrinkage and selection operator*) 
 
 - Feature selection
 - Include all variables and apply regularization on $\beta$, so that only important variables have a $\beta_i> 0$.
- read.table
+
+
 --
 
 - **LASSO model**
   - linear regression model (or GLM) $Y \sim X{\beta}$
   - regularization $f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i|$
-      -$\lambda$ defines strength of regularization
+      - $\lambda$ defines strength of regularization
           - higher $\lambda \Rightarrow$  stricter limit on individual $\beta_i$ values. 
 
 --
@@ -424,7 +484,7 @@ Applying this rL to our example data, solves the overfitting problem.
 
 --
 
-- **Optimization**
+- **Optimization algorithms**
   - _lars_ or _coordinate descent_
 
 ???
@@ -439,15 +499,15 @@ Other common notation for LASSO:
 
 ---
 
-### Example | `Lasso using the glmnet R-package`
+### Example | Lasso using the `glmnet` R-package
 
-* preprocessing:  
+* preprocessing (actually done by `glmnet`):  
   - center and standardize variables to ensure equal weighting
-      + standardization of $X$ to unit variance in `glmnet` is obtained  by setting the argument `standardize=TRUE` which is the default
-      +  the values of $Y$ are always standardized (?) for `family=gaussian` (LASSO)
-        + and the coefficients are back-standardized before reported
-* linear regression (`family='gaussian'` = default)
-* LASSO (`alpha=1` = default)
+      + standardization of $X$ to unit variance (`standardize=TRUE` = default)
+      + the values of $Y$ are always standardized (?) for `family=gaussian` (LASSO)
+          + and the coefficients are back-standardized before reported
+* Linear regression (`family='gaussian'` = default)
+* preform LASSO (`alpha=1` = default)
 
 ```{r, echo=T, eval=TRUE} 
 library(glmnet)
@@ -493,27 +553,31 @@ par(mar = c(4, 4, 0.1, 0.1))
 plot(fit, xvar=""lambda"",label=T)
 ```
 
-#### Mini-task | `Think about`
+#### Mini-task | Think about
 * In which order are variables included (i.e., when $\beta_i > 0$)?
 * In which direction is the effect?
 * Which lambda should we select?
   - Given our *oracle knowledge*, where would an appropriate $\lambda$ be?
   - Is this useful for the general case?
   - Can we use anything else?
 ]
+
 --
+
 .pull-right[
 ##### Some possible answers
 {{content}}
 ]
 --
 
-* The order appears to be $(1,2,3,7,6,5,10,9,4,8)$
-* $\beta_i > 0, i\in \{1,2,3,4,7,9\}$, while $\beta_i<0, i\in \{5,6,8,10\}$
-* Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-1), \approx\exp(-2.5)]$
+* The order appears to be $(2,1,3,6,4,10,5,8,9,7)$
+* $\beta_i > 0, i\in \{1,2,3,4,78\}$, while $\beta_i<0, i\in \{5,6,9,10\}$
+* Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-1).1, \approx\exp(-2.5)]$
 * In the normal case, we do not have *oracle knowledge*.
+{{content}}
+
 --
-* Cross validation
+* Next: **Cross validation**
 
 
 ???
@@ -533,27 +597,39 @@ How to decide the optimal $\lambda$ to use.
 
 ---
 .pull-left[
-#### Validation of estimated model
+#### Validation by replication
 
 * Estimate a model on your data
-* Test the general validity of this model by
-  - running the model on other data
+* Test the general validity of this model 
+  - run the estimated model on other data
   - measure the error it does
-    - e.g., using _mean squared error_ $(MSE = RSS/N)$
+      - e.g., using _mean squared error_ $(MSE = RSS/N)$
 ].pull-right[
 
 {{content}}
 
 ]
+
+???
+
+- Classsical/Optimal way of validation
+- The ultimate way of testing an estimated model (with parameters) would be to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, $MSE$ $(=RSS/N)$.
+Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO application, this means that we want to select the $\lambda$ that minimizes the $MSE$
+
 --
 #### Cross-validation
 
 * Split data into _training_ and _validation_ data
-* train model on _trining data_
+* train model on _training data_
 * validate on _validatation_ data
 
 {{content}}
 
+???
+
+- In cross validation, this approach is emulated by partitioning the data at hand into a *training* and  *validation* data set. The model parameters are estimated ('trained') on the the training data and the validated on the validation data. (Optionally, a *test* partition can be assigned in cross-validation on which the final, selected model is evaluated; this is not employed here).
+- By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partitioning.
+
 --
 #### $k$-fold cross validation
 
@@ -563,14 +639,7 @@ How to decide the optimal $\lambda$ to use.
   - validate on $i$
 * Error distribution
 
-???
-- The ultimate way of testing an estimated model (with parameters) would be to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, $MSE$ $(=RSS/N)$.
-Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO application, this means that we want to select the $\lambda$ that minimizes the $MSE$
-
-- In cross validation, this approach is emulated by partitioning the data at hand into a *training* and  *validation* data set. The model parameters are estimated ('trained') on the the training data and the validated on the validation data. (Optionally, a *test* partition can be assigned in cross-validation on which the final, selected model is evaluated; this is not employed here).
-
-- By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partitioning.
-
+??? 
 - In $k$-fold cross validation, the original data is split into $k$ sub-datasets $\{D_1,D_2,\ldots, D_k\}$.
 - For $i \in \{1,2,\ldots, k\}$, set $D_i$ as the validation data set and the union of the other datasets be the training data. Perform cross validation as above.
 
@@ -594,6 +663,15 @@ plot(cvglm)
 ```
 
 
+???
+
+* Use the function `cv.glmnet` to perform cross validation (same options as for `glmnet`), store it in a R variable, e.g., `cvglm`,
+* `plot` the cross-validation results 
+* Compare with the plot of estimated $\beta_i$ under different $\lambda$ (these can be accessed from the result as `cvglm$glmnet.fix`).
+* Determine the optimal $\lambda$ (the one with minimal error, can be found in `cvglm$lambda.min`) 
+
+--
+
 ```{r, echo=FALSE,fig.height=3}
 library(glmnet)
 par(mfrow=c(1,1))
@@ -606,32 +684,41 @@ minlambda=cvglm$lambda.min
 print(minlambda)
 ```
 
-
-???
-
-* Use the function `cv.glmnet` to perform cross validation (same options as for `glmnet`), store it in a R variable, e.g., `cvglm`,
-* `plot` the cross-validation results 
-* Compare with the plot of estimated $\beta_i$ under different $\lambda$ (these can be accessed from the result as `cvglm$glmnet.fix`).
-* Determine the optimal $\lambda$ (the one with minimal error, can be found in `cvglm$lambda.min`) 
 ---
+
 .pull-left[
 ```{r,echo=FALSE, fig.height=3}
 plot(cvglm)
 plot(cvglm$glmnet.fit, xvar=""lambda"",label=T)
-
 ```
 ]
+
+--
+
 .pull-right[
+
 ##### Think about
 * Which is the $\lambda$ selected by `cv.glmnet`?
 * Does this make sense given our *oracle knowledge*?
 {{content}}
+
+]
+
+???
+
+- optimal \lambda giving the minimal MSE can be read from plot
+
 --
 
 ##### Some possible answers</h4>
 * Cross-validation-selected optimal $\lambda$ is `r minlambda` $(\log(\lambda) = `r log(minlambda)`)$
 * Yes, this includes only the *oracle knowledge* correct variables $X_1, X_2, X_3$
 
+???
+
+- optimal \lambda -> include variables read from plot
+
+
 
 ---
 #### Example| Final LASSO effect sizes
@@ -682,22 +769,26 @@ layout: true
 
 ### General cost function
 
-Regularization can be generalized to apply not only to linear models. For example `glmnet` allows using regularized likelihood expression for generalized linear models, GLMs. 
+- Regularization can be generalized to apply not only to linear models, for example, GLMs
+
+???
 
-Even more general, regularization can be formulated to apply some general loss (or cost or error) function, $\mathcal{L}(Y|X, \beta),$ commonly used in Machine learning. The modified loss function (including the regularization) can be written
+- (as in glmnet).
+--
 
-$$\mathcal{L}(Y|\beta, X, \lambda) = \mathcal{L}(Y|\beta, X) + \lambda f(\beta).$$
-This formulation allows application to various regresssion and classification problems.
+- Regularization can be formulated for a general (Machine Learning) loss function, $\mathcal{L}(Y|X, \beta).$
+- The modified loss function (including the regularization) can be written $$\mathcal{L}(Y|\beta, X, \lambda) = \mathcal{L}(Y|\beta, X) + \lambda f(\beta).$$
+    - This formulation allows application to various regresssion and classification problems.
 
-Examples of loss functions:
+--
 
-- $RSS$ (least squares method)
-- mean square error (MSE) = $RSS/N$
-- cross entropy $\sum_i y_i \times Pr[y_i|\beta, x]$
+- Examples of loss functions:
+    - $RSS$ (least squares method)
+    - mean square error (MSE) = $RSS/N$
+    - cross entropy $\sum_i y_i \times Pr[y_i|\beta, x]$
 
 ???
 
-
 - Here, for consistency, I use $\beta$ for the parameters of the model, but other notation is commonly also used, e.g., $w$ (for weights). In fact, $\beta$ is often associated with regression parameters; when more types of parameters are used, it is common to use, e.g., $\theta$ as a collective notation for all those parameters.
     
 
@@ -709,15 +800,19 @@ Examples of loss functions:
 
 - Avoid false positives dues to overfitting
 
+???
+
+- In inference, where you want to identify the causative variables that contribute to the outcome. overfitting may cause false positives, that is, variables that simply explains random noise in the training data.
+  - Applying regularization reduces these kinds of false positives.
+
 --
 
 - Model selection 
   - compare competing models
 
---
+???
 
-- Generalization to other data 
-  - prediction
+- Compare competing models, how much better is the best model. (anive regularization example)
 
 --
 
@@ -726,16 +821,20 @@ Examples of loss functions:
   - easier interpretation
 
 ???
-
-- In inference, where you want to identify the causative variables that contribute to the outcome. overfitting may cause false positives, that is, variables that simply explains random noise in the training data.
-  - Applying regularization reduces these kinds of false positives.
-- Compare competing models, how much better is the best model. (anive regularization example)
-- In Machine learning the goal is often prediction, i.e., using the estimated model to predict outcomes $Y'$ from a completely new set of predictors, $X'$. Overfitting will make the estimated model to specialized to prediction of the training outcomes including the noise it contains. 
-  - Regularization can therefore be motivated as a technique to improve the generalizability of a learned model by reducing its overfitting to the training data..
 - In many applications, e.g., for clinical diagnosis, a small model with few variables explaining the bulk of the outcome is preferred over a detailed model that explains as much of the outcome as possible. In these applications, regularization can be used for selecting the most important features and ignoring the other variables.
   - More generally, sparsity may also lead to more easily interpreted models, e.g., for biological questions.
   - LASSO
 
+--
+
+- Generalization to other data 
+  - prediction
+
+???
+
+- In Machine learning the goal is often prediction, i.e., using the estimated model to predict outcomes $Y'$ from a completely new set of predictors, $X'$. Overfitting will make the estimated model to specialized to prediction of the training outcomes including the noise it contains. 
+  - Regularization can therefore be motivated as a technique to improve the generalizability of a learned model by reducing its overfitting to the training data..
+
 ---
 
 ### Cross validation as ""regularization""
@@ -744,6 +843,15 @@ Examples of loss functions:
   - Number of iterations balance between
       - suboptimal model (too few)
       - risk of overfitting (too many)
+      
+???
+
+- Many Machine learning techniques uses iteration to improve the model estimates incrementally. 
+- The number of iterations then becomes an important parameter 
+    - enough to get a good model, 
+    - but avoid overfitting. 
+    
+--
 
 ##### Cross-validation as regularization
 - Partition the data into three sets, *training*, *validation*, and *test* data sets. 
@@ -753,19 +861,9 @@ Examples of loss functions:
   - continue until the _validation_ loss score does not get better
 - Evaluate final model against the _test_ data.
 
-???
-
-- Many Machine learning techniques uses iteration to improve the model estimates incrementally. 
-- The number of iterations then becomes an important parameter 
-    - enough to get a good model, 
-    - but avoid overfitting. 
-    
 ---
-layout: false
-layout: true
-# Norms 
 
----
+### Norms 
 
 - Norms are a kind of summary statistic over vectors 
   - function that has a vector as input and a (non-negative) number as output. 
@@ -779,14 +877,13 @@ layout: true
 
 - *Norms* is a concept that commonly pops up when regularization is discussed.
 - Since this special notation is used for norms, they can at first look very incomprehensible, but we have already used a number of them!
+- Here I will use $\beta $ as example but could be any vector or expression describing a vector
 
 ---
 
 #### The $L_0$ ""norm""
 
-- the number of non-zero $\beta$ 
-  - $||\beta||_0 = \sum_i I(\beta_i=0),$
-      - where $I(expr)$ equals 1 if $expr$ is true and 0 otherwise. 
+- the number of non-zero $\beta$ $$||\beta||_0 = \sum_i I(\beta_i=0),$$ where $I(expr)$ equals 1 if $expr$ is true and 0 otherwise. 
   - corresponds to $\#X$ in our initial simple naive regularization approach
 
 --
@@ -821,16 +918,20 @@ layout: true
 
 - There is also a $L_2$ norm, $$||\beta||_2 = \sqrt{\sum_{\beta_i\in{\beta}} \beta_i^2}.$$
 
+???
+
+- If you substitute $\beta$ for some other expressin, does this feeel familiar?
+
 --
 
 - You have already have been working with  $L_2$ norms
-  - $RSS = ||Y-X\beta||_2^2$ is the square of the $L_2$ norm of the residuals.
+  - $RSS = ||Y-X\beta||_2^2$ is the square of the $L_2$ norm of the residual vector.
   - the sample standard deviation can be written using a $L_2$ norm, $$sd(x) = \frac{||x-\bar{x}||_2}{\sqrt{n-1}}.$$
 
 --
 
 - Uses in regularization:
- - For regularization, the $L_2$-norm is used in *ridge regression*.
+ - For regularization, the $L_2$-norm is used in *Ridge Regression*.
     - `glmnet(..., alpha = 0,...)`
  
 ???
@@ -843,6 +944,13 @@ layout: true
 #### Combination of norms
 
 - *Elastic net* regularization uses a mixed model combination of the $L_1$ norm and the $L_2$ norm,$$RSS+\alpha\lambda_1 ||\beta||_1 + (1-\alpha) \lambda_2 ||\beta||_2.,$$ where $\alpha$ determines the weight of the different regularizations
+
+--
+
+- ""intermediate"" between LASSO and Ridge Regression
+
+--
+
 - The aim is to avoid some drawbacks of LASSO.
 
 
@@ -859,29 +967,45 @@ Norms can also have a geometry interpretation:
 {{content}}
 ]
 .pull-right[
-![manhattan distance](images/manhattan_distance.jpg)
+![manhattan distance](../images/manhattan_distance.jpg)
 - B is a 2-dimensional vector (4,3)
 - A is the _origo_-vector (0,0)
 ]
 --
-- The $L_2$-norm is also the *Euclidean distance* from origo. 
+- The $L_2$-norm is the *Euclidean distance* from origo. 
 {{content}}
 --
 
-- The $L_1$-norm is also the *Manhattan distance* from origo. 
+- The $L_1$-norm is the *Manhattan distance* from origo. 
 {{content}}
 --
 
-- The $L_0$-""norm"" is also the *Hamming distance* from origo. the Hamming distance between two vectors is the number of places they disagree.
+- The $L_0$-""norm"" is the *Hamming distance* from origo. the Hamming distance between two vectors is the number of places they disagree.
 
 
 ???
 - Hamming dist would be 2 here
-- In formal mathematics, the $L_0$-""norm"" does not properly fulfill all criteria for a norm; hence it is often, as here, written within quotation marks.
+- In formal mathematics, the $L_0$ ""norm"" does not properly fulfill all criteria for a norm; hence it is often, as here, written within quotation marks.
 
 ---
 layout: false
 
+
+# That's it
+
+- End of lecture...
+
+--
+
+- Lecture notes for the session is available from Schedule.
+  - contains some extra reading
+
+--
+
+- Next is the Exercises...
+
+---
+
 # Exercises
 
 ### Data 

---FILE: session-regularization/slides/lecture-regularization.html---
@@ -3,6 +3,8 @@
   <head>
     <title>Session Regularization</title>
     <meta charset=""utf-8"" />
+    <meta name=""author"" content=""Bengt Sennblad, NBIS"" />
+    <meta name=""date"" content=""2021-10-08"" />
     <script src=""libs/header-attrs-2.11/header-attrs.js""></script>
     <link href=""libs/remark-css-0.0.1/default.css"" rel=""stylesheet"" />
     <link href=""libs/remark-css-0.0.1/metropolis.css"" rel=""stylesheet"" />
@@ -15,6 +17,8 @@
 class: center, middle, inverse, title-slide
 
 # Session Regularization
+### Bengt Sennblad, NBIS
+### 8 October 2021
 
 ---
 
@@ -47,20 +51,25 @@
 
 ---
 
-- Given a generative model, we can compute `$$Pr[Y| X, \theta],$$` the probability that the model with parameters `\(\theta\)` and given `\(X\)` generates `\(Y\)`... 
+- Given a generative model for `\(X\rightarrow Y\)`, with parameters `\(\theta\)`, we can compute `$$Pr[Y| X, \theta],$$` the probability that the model with parameters `\(\theta\)` and given `\(X\)` has generated `\(Y\)`... 
 
 --
 
 - .. but we are often more interested in the probability that the parameters, `\(\theta\)`, are correct given the observed data, `\(X,Y\)`, i.e., ideally we want `$$Pr[\theta|X,Y].$$`
 
+???
+
+- However, it is not obvious how to compute this probability
+- This is where likelihood comes in.
+
 ---
 
 ### Likelihood intuition:
 
-- If parameters `\(\theta_T\)` is closer to the 'truth' than `\(\theta_W\)`, then `$$Pr[Y| X, \theta = \theta_T] &gt; Pr[Y| X, \theta = \theta_T]$$`
+- If parameters `\(\theta_T\)` is closer to the 'truth' than `\(\theta_W\)`, then we would expect `$$Pr[Y| X, \theta = \theta_T] &gt; Pr[Y| X, \theta = \theta_T]$$`
 --
 
-- We should therefore select the `\(\theta\)` that maximizes `\(Pr[Y| X, \theta]\)`; this is called  
+- We should therefore select the `\(\theta\)` that maximizes `\(Pr[Y| X, \theta]\)` as the best estimate; this is called  
 
 .center[
 **Maximum Likelihood estimation (MLE) of `\(\theta\)`.**
@@ -80,13 +89,24 @@
 
 - The likelihood of model parameters being true given observed data is `$$L[\theta|Y,X] = k\times Pr[Y|X, \theta],$$` with `\(k\)` being an arbitrary konstant (Edwards, 1972)
 
+???
+
+- **Proportional**
+- The proportionality (indicated by '$\propto$') means there are some unknown constant factor, `\(k\)`,
+- However, the factor `\(k\)` is assumed to be constant over `\(\theta\)`s and over models. 
+
 --
 
 - `\(L[\theta|Y,X]\)` is not a proper probability, hence the term *likelihood* is used.
 
+???
+
+- Sum/integral of likelihoods `\(\neq 1\)`
+- possibly bring up Bayes formula `$$Pr[\theta|Y,X] = \frac{Pr[Y|\theta,X]}{Pr[Y,X]}$$`
+
 --
 
-- In practice, proportionality is ignored and we set
+- In practice, proportionality is (almost alwyays) ignored and we set
 
 `$$L[\theta|Y,X] = Pr[Y|X, \theta]$$`
 --
@@ -96,30 +116,37 @@
 
 ???
 
-The proportionality (indicated by '$\propto$') means there are some unknown constant factor, `\(k\)`, such that `\(L[\theta|Y,X] = k Pr[Y|X, \theta]\)`. However, the factor `\(k\)` is assumed to be constant over `\(\theta\)`s and over models. 
-
--- 
-
 - When the likelihood of two `\(\theta\)`s (or models) are compared this is almost always done as a _likelihood ratio_, 
-
-`$$\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]} = \frac{k Pr[Y|X, \theta_1]}{ k  Pr[Y|X, \theta_0]} =\frac{Pr[Y|X, \theta_1]}{ Pr[Y|X, \theta_0]}$$`
-
-which means that the factor `\(k\)` disappears. Hence the factor `\(k\)` is always ignored. Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization... 
+- The proportionality  factor, `\(k\)`, such that `\(L[\theta|Y,X] = k Pr[Y|X, \theta]\)` csancels out
+- Hence the factor `\(k\)` is always ignored. 
+- Likelihood ratios is the basis of most model comparison statistics, e.g., the Wald test, the Score test, regularization... 
 
 ---
 
 ### Maximum Likelihood estimation
 
-- Select the estimates `\(\widehat\theta\)` that gives the highest likelihood, i.e.,  
+- Select the estimate `\(\widehat\theta\)` that gives the highest likelihood, i.e.,  
 `$$\widehat{\theta} = argmax_{\theta}L[\theta|X,Y] \qquad \Leftrightarrow \qquad  max_{\theta}L[\theta|X,Y] = L[\widehat\theta|X,Y].$$`
+???
+
+- Select as the best estimate that which...
+- `\(\argmax_{\theta}\)` is the argument `\(\theta\)` that maximizes...
 
 --
 
 - Often, it is practical to use the logarithm of the likelihood, the _logLikelihood_, `$$\log L[\theta_1|Y,X].$$` Notice that
 
 --
-  - `\(\widehat{\theta} = argmin_\theta -\log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]\)`
-  
+  - `\(\widehat{\theta} = argmax_\theta \log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]\)` 
+
+--
+  - `\(\widehat{\theta} = argmin_\theta -\log L[\theta|Y,X]  = argmax_\theta L[\theta|Y,X]\)` 
+
+???
+
+- Also the negative logLikelihood is used
+- minimization instead of maximization
+
 --
   - `\(\log\left(\frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]}\right) = \frac{\log L[\theta_1|Y,X]}{\log L[\theta_0|Y,X]} = \log L[\theta_1|Y,X] - \log L[\theta_0|Y,X].\)`
 
@@ -131,26 +158,42 @@
 
   - A likelihood ratio corresponds to a logLikelihood difference,
 
+---
+
 ## Likelihood | `Likelihood and OLS for linear models`
 
 
-So, why have we used ordinary least squares (OLS), i.e., minimization of  RSS when estimating linear model parameters `\(\beta\)` rather than maximum likelihood estimation?
+- Why have we used ordinary least squares (OLS), `\(\min_\theta RSS\)` when estimating linear model parameters
 
-Linear models is a special case with some nice properties when it comes to  likelihood. Consider a simple linear regression model,
+???
+
+- Why have we used ordinary least squares (OLS), `\(\min_\theta RSS\)` when estimating linear model parameters `\(\beta\)` rather than maximum likelihood estimation?
+
+--
+
+Consider a simple linear regression model, $$ y = \beta x + \epsilon, \epsilon\sim N(0,\sigma^2).$$ 
+
+???
+
+- Linear models is a special case with some nice properties when it comes to  likelihood. 
 
-$$ y = \beta x + \epsilon, $$
+--
 
-where the residuals `\(\epsilon\sim N(0,\sigma^2)\)`. 
+- The maximum likelihood estimates of both `\(\beta\)` and `\(\sigma^2\)` are functions of the  RSS, and `$$\log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS$$`
 
-It turns out that the maximum likelihood estimates of both `\(\beta\)` and `\(\sigma^2\)` are functions of the  RSS of the residuals, so that the likelihood can be approximated by
+  - This means that `$$\widehat{\beta}_{ML} = \widehat{\beta}_{OLS}$$`
 
-$$  \log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS$$
+???
 
-This means that the maximum likelihood estimates of `\(\beta\)` is exactly the same as those of the minimum RSS.
+- It turns out that the maximum likelihood estimates of both `\(\beta\)` and `\(\sigma^2\)` are functions of the  RSS of the residuals, so that the likelihood can be very well approximated by
 
-_**NB!** This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using a maximum likelihood estimation _
+--
 
-In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed, e.g., OLS.
+- _**NB!** This is a special case for linear models and are not generally true for other models. For example, logistic regression is typically fitted using a maximum likelihood estimation _
+
+???
+
+- In general, full-on likelihood computation and maximum likelihood estimation is relatively slow, so alternative and faster methods has been developed, e.g., OLS.
 
 ---
 layout: false
@@ -160,7 +203,8 @@
 
 ---
 
-- kalle
+ `\(\quad\)` 
+
 ???
 
 We will now look at a general problem in statistical modeling that can be visualized quite well with Likelihoods. We will later look at some solutions to this problem.
@@ -171,45 +215,50 @@
 
 
 ```r
-set.seed(85) # To obtain exactly the same result as in the demo
-
+# To obtain exactly the same result as in the demo, set seed to 85
+set.seed(85)
 N=100 # number of samples
 P=10 # number of variables
-
-# Draw variables, x_{i,1},...,x_{i,P} for all N individuals, uniformly
-# 1. runif generates N*P+1 values, round them off to 2 decimals
-# 2. Put values into a matrix with N rows and P columns
+# Generate X from uniform distribution
 X=matrix(round(runif(N*(P+1),min=0, max=2), 2), nrow=N, ncol=P)
-
 # generate Y from a multivariate lm of 3 first X variables only
 b0=3 #intercept
 b=c(runif(3, min=0.5, max=1.0)) # effect sizes for 3 first X variables
-
 # generate Y for all samples (=rows in X) 
-Y &lt;- b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
+Y = b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
 ```
 
 
+
+???
+- Draw variables, x_{i,1},...,x_{i,P} for all N individuals, uniformly
+- 1. runif generates N*P+1 values, round them off to 2 decimals
+- 2. Put values into a matrix with N rows and P columns
+
 --
 
 #### Think about: What can simulated data be used for?
 --
 
-* Oracle knowledge when evaluating performance of methods, e.g., Type I and II errors 
 * Estimating probabilities and probability distributions of, e.g., data and summary statistics of data
+* Oracle knowledge when evaluating performance of methods, e.g., Type I and II errors 
+
+???
+
+- NULL distribution
 
 ---
 
 ### Model comparison
 
-Now consider the following two models for our data
+Now consider the following two models for our example data
 
 `\begin{eqnarray}
 y &amp; \sim &amp; \beta_0 + \beta_1 x_1 &amp; (1) \\
 y &amp; \sim &amp;  \beta_0 + \beta_1 x_1 + \beta_2 x_2 &amp; (2)
 \end{eqnarray}`
 
-What are the max Likelihood estimates of the two models?
+What are the max Likelihood estimates of the two models? Let's plot them!
 
 ---
 
@@ -263,13 +312,22 @@
 * Therefore Model (2) will always have equal or better ML than Model (1)
 {{content}}
 ]
+
+???
+
+- ML increases the more variables we add
 --
 
 _Overfitting_
 * Since the simulated data was generated from the 3 first variables...
   + ...thus, the subsequent variables increase ML by modeling noise in data
   
 {{content}}  
+
+???
+
+- The ML model is the one with all variables
+
 --
   
 * Solution:  Regularization
@@ -290,8 +348,13 @@
 - Typically, the regularization term is a function of parameters `\(\beta\)`:
 
 `$$\log rL[\beta | X, Y]  = \log L[\beta | X, Y] - f(\beta).$$`
+???
 
-#### Example | `A very simple regularization model` 
+- ""Simplast"" = few parameters/variables
+
+--
+
+#### Example | A very simple regularization model
 
 - `\(f(\beta) = \#\beta = \#X\)` (that is the number of `\(X\)` variables).  
 `$$\log rL[{\beta} | X, Y]  = \log L[\beta | X, Y] - \#X,$$`
@@ -323,16 +386,14 @@
 
 ### Mini-task: Think about:
 
-* Which is the best model? Is this correct?
-* How good it is compared to the other models?
+* Which is the best model? Is this correct according to *oracle knowledge*?
 * Can you see a drawback in our model testing approach above? If so, how can we solve that?
 
 --
 .pull-right[
 #### Some possible answers 
 
 * The best model, 3, is the correct one.
-* The likelihood of second best model (with the first 4 X-variables) is `\(\approx 40\%\)` of the best likelihood.
 * What if the right variables had been among the last? 
 
 {{content}}
@@ -350,17 +411,18 @@
   - Lasso; see next section
 ---
 
-####  LASSO (*Least absolute shrinkage and selection operator*) 
+###  LASSO (*Least absolute shrinkage and selection operator*) 
 
 - Feature selection
 - Include all variables and apply regularization on `\(\beta\)`, so that only important variables have a `\(\beta_i&gt; 0\)`.
- read.table
+
+
 --
 
 - **LASSO model**
   - linear regression model (or GLM) `\(Y \sim X{\beta}\)`
   - regularization `\(f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i|\)`
-      -$\lambda$ defines strength of regularization
+      - `\(\lambda\)` defines strength of regularization
           - higher `\(\lambda \Rightarrow\)`  stricter limit on individual `\(\beta_i\)` values. 
 
 --
@@ -371,7 +433,7 @@
 
 --
 
-- **Optimization**
+- **Optimization algorithms**
   - _lars_ or _coordinate descent_
 
 ???
@@ -386,15 +448,15 @@
 
 ---
 
-### Example | `Lasso using the glmnet R-package`
+### Example | Lasso using the `glmnet` R-package
 
-* preprocessing:  
+* preprocessing (actually done by `glmnet`):  
   - center and standardize variables to ensure equal weighting
-      + standardization of `\(X\)` to unit variance in `glmnet` is obtained  by setting the argument `standardize=TRUE` which is the default
-      +  the values of `\(Y\)` are always standardized (?) for `family=gaussian` (LASSO)
-        + and the coefficients are back-standardized before reported
-* linear regression (`family='gaussian'` = default)
-* LASSO (`alpha=1` = default)
+      + standardization of `\(X\)` to unit variance (`standardize=TRUE` = default)
+      + the values of `\(Y\)` are always standardized (?) for `family=gaussian` (LASSO)
+          + and the coefficients are back-standardized before reported
+* Linear regression (`family='gaussian'` = default)
+* preform LASSO (`alpha=1` = default)
 
 
 ```r
@@ -437,27 +499,31 @@
 .pull-left[
 &lt;img src=""lecture-regularization_files/figure-html/unnamed-chunk-10-1.png"" width=""100%"" /&gt;
 
-#### Mini-task | `Think about`
+#### Mini-task | Think about
 * In which order are variables included (i.e., when `\(\beta_i &gt; 0\)`)?
 * In which direction is the effect?
 * Which lambda should we select?
   - Given our *oracle knowledge*, where would an appropriate `\(\lambda\)` be?
   - Is this useful for the general case?
   - Can we use anything else?
 ]
+
 --
+
 .pull-right[
 ##### Some possible answers
 {{content}}
 ]
 --
 
-* The order appears to be `\((1,2,3,7,6,5,10,9,4,8)\)`
-* `\(\beta_i &gt; 0, i\in \{1,2,3,4,7,9\}\)`, while `\(\beta_i&lt;0, i\in \{5,6,8,10\}\)`
-* Given *oracle knowledge*, the correct `\(\lambda\)` appears lie somewhere in the interval `\([\approx \exp(-1), \approx\exp(-2.5)]\)`
+* The order appears to be `\((2,1,3,6,4,10,5,8,9,7)\)`
+* `\(\beta_i &gt; 0, i\in \{1,2,3,4,78\}\)`, while `\(\beta_i&lt;0, i\in \{5,6,9,10\}\)`
+* Given *oracle knowledge*, the correct `\(\lambda\)` appears lie somewhere in the interval `\([\approx \exp(-1).1, \approx\exp(-2.5)]\)`
 * In the normal case, we do not have *oracle knowledge*.
+{{content}}
+
 --
-* Cross validation
+* Next: **Cross validation**
 
 
 ???
@@ -477,27 +543,39 @@
 
 ---
 .pull-left[
-#### Validation of estimated model
+#### Validation by replication
 
 * Estimate a model on your data
-* Test the general validity of this model by
-  - running the model on other data
+* Test the general validity of this model 
+  - run the estimated model on other data
   - measure the error it does
-    - e.g., using _mean squared error_ `\((MSE = RSS/N)\)`
+      - e.g., using _mean squared error_ `\((MSE = RSS/N)\)`
 ].pull-right[
 
 {{content}}
 
 ]
+
+???
+
+- Classsical/Optimal way of validation
+- The ultimate way of testing an estimated model (with parameters) would be to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, `\(MSE\)` `\((=RSS/N)\)`.
+Naturally, we want to minimize `\(MSE\)`, i.e., the error of the model. In our LASSO application, this means that we want to select the `\(\lambda\)` that minimizes the `\(MSE\)`
+
 --
 #### Cross-validation
 
 * Split data into _training_ and _validation_ data
-* train model on _trining data_
+* train model on _training data_
 * validate on _validatation_ data
 
 {{content}}
 
+???
+
+- In cross validation, this approach is emulated by partitioning the data at hand into a *training* and  *validation* data set. The model parameters are estimated ('trained') on the the training data and the validated on the validation data. (Optionally, a *test* partition can be assigned in cross-validation on which the final, selected model is evaluated; this is not employed here).
+- By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partitioning.
+
 --
 #### `\(k\)`-fold cross validation
 
@@ -507,14 +585,7 @@
   - validate on `\(i\)`
 * Error distribution
 
-???
-- The ultimate way of testing an estimated model (with parameters) would be to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, `\(MSE\)` `\((=RSS/N)\)`.
-Naturally, we want to minimize `\(MSE\)`, i.e., the error of the model. In our LASSO application, this means that we want to select the `\(\lambda\)` that minimizes the `\(MSE\)`
-
-- In cross validation, this approach is emulated by partitioning the data at hand into a *training* and  *validation* data set. The model parameters are estimated ('trained') on the the training data and the validated on the validation data. (Optionally, a *test* partition can be assigned in cross-validation on which the final, selected model is evaluated; this is not employed here).
-
-- By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partitioning.
-
+??? 
 - In `\(k\)`-fold cross validation, the original data is split into `\(k\)` sub-datasets `\(\{D_1,D_2,\ldots, D_k\}\)`.
 - For `\(i \in \{1,2,\ldots, k\}\)`, set `\(D_i\)` as the validation data set and the union of the other datasets be the training data. Perform cross validation as above.
 
@@ -539,34 +610,53 @@
 ```
 
 
-&lt;img src=""lecture-regularization_files/figure-html/unnamed-chunk-12-1.png"" width=""100%"" /&gt;
-
-```
-## [1] 0.08415953
-```
-
-
 ???
 
 * Use the function `cv.glmnet` to perform cross validation (same options as for `glmnet`), store it in a R variable, e.g., `cvglm`,
 * `plot` the cross-validation results 
 * Compare with the plot of estimated `\(\beta_i\)` under different `\(\lambda\)` (these can be accessed from the result as `cvglm$glmnet.fix`).
 * Determine the optimal `\(\lambda\)` (the one with minimal error, can be found in `cvglm$lambda.min`) 
+
+--
+
+&lt;img src=""lecture-regularization_files/figure-html/unnamed-chunk-12-1.png"" width=""100%"" /&gt;
+
+```
+## [1] 0.08415953
+```
+
 ---
+
 .pull-left[
 &lt;img src=""lecture-regularization_files/figure-html/unnamed-chunk-13-1.png"" width=""100%"" /&gt;&lt;img src=""lecture-regularization_files/figure-html/unnamed-chunk-13-2.png"" width=""100%"" /&gt;
 ]
+
+--
+
 .pull-right[
+
 ##### Think about
 * Which is the `\(\lambda\)` selected by `cv.glmnet`?
 * Does this make sense given our *oracle knowledge*?
 {{content}}
+
+]
+
+???
+
+- optimal \lambda giving the minimal MSE can be read from plot
+
 --
 
 ##### Some possible answers&lt;/h4&gt;
 * Cross-validation-selected optimal `\(\lambda\)` is 0.084 `\((\log(\lambda) = -2.475)\)`
 * Yes, this includes only the *oracle knowledge* correct variables `\(X_1, X_2, X_3\)`
 
+???
+
+- optimal \lambda -&gt; include variables read from plot
+
+
 
 ---
 #### Example| Final LASSO effect sizes
@@ -675,22 +765,26 @@
 
 ### General cost function
 
-Regularization can be generalized to apply not only to linear models. For example `glmnet` allows using regularized likelihood expression for generalized linear models, GLMs. 
+- Regularization can be generalized to apply not only to linear models, for example, GLMs
+
+???
 
-Even more general, regularization can be formulated to apply some general loss (or cost or error) function, `\(\mathcal{L}(Y|X, \beta),\)` commonly used in Machine learning. The modified loss function (including the regularization) can be written
+- (as in glmnet).
+--
 
-`$$\mathcal{L}(Y|\beta, X, \lambda) = \mathcal{L}(Y|\beta, X) + \lambda f(\beta).$$`
-This formulation allows application to various regresssion and classification problems.
+- Regularization can be formulated for a general (Machine Learning) loss function, `\(\mathcal{L}(Y|X, \beta).\)`
+- The modified loss function (including the regularization) can be written `$$\mathcal{L}(Y|\beta, X, \lambda) = \mathcal{L}(Y|\beta, X) + \lambda f(\beta).$$`
+    - This formulation allows application to various regresssion and classification problems.
 
-Examples of loss functions:
+--
 
-- `\(RSS\)` (least squares method)
-- mean square error (MSE) = `\(RSS/N\)`
-- cross entropy `\(\sum_i y_i \times Pr[y_i|\beta, x]\)`
+- Examples of loss functions:
+    - `\(RSS\)` (least squares method)
+    - mean square error (MSE) = `\(RSS/N\)`
+    - cross entropy `\(\sum_i y_i \times Pr[y_i|\beta, x]\)`
 
 ???
 
-
 - Here, for consistency, I use `\(\beta\)` for the parameters of the model, but other notation is commonly also used, e.g., `\(w\)` (for weights). In fact, `\(\beta\)` is often associated with regression parameters; when more types of parameters are used, it is common to use, e.g., `\(\theta\)` as a collective notation for all those parameters.
     
 
@@ -702,15 +796,19 @@
 
 - Avoid false positives dues to overfitting
 
+???
+
+- In inference, where you want to identify the causative variables that contribute to the outcome. overfitting may cause false positives, that is, variables that simply explains random noise in the training data.
+  - Applying regularization reduces these kinds of false positives.
+
 --
 
 - Model selection 
   - compare competing models
 
---
+???
 
-- Generalization to other data 
-  - prediction
+- Compare competing models, how much better is the best model. (anive regularization example)
 
 --
 
@@ -719,16 +817,20 @@
   - easier interpretation
 
 ???
-
-- In inference, where you want to identify the causative variables that contribute to the outcome. overfitting may cause false positives, that is, variables that simply explains random noise in the training data.
-  - Applying regularization reduces these kinds of false positives.
-- Compare competing models, how much better is the best model. (anive regularization example)
-- In Machine learning the goal is often prediction, i.e., using the estimated model to predict outcomes `\(Y'\)` from a completely new set of predictors, `\(X'\)`. Overfitting will make the estimated model to specialized to prediction of the training outcomes including the noise it contains. 
-  - Regularization can therefore be motivated as a technique to improve the generalizability of a learned model by reducing its overfitting to the training data..
 - In many applications, e.g., for clinical diagnosis, a small model with few variables explaining the bulk of the outcome is preferred over a detailed model that explains as much of the outcome as possible. In these applications, regularization can be used for selecting the most important features and ignoring the other variables.
   - More generally, sparsity may also lead to more easily interpreted models, e.g., for biological questions.
   - LASSO
 
+--
+
+- Generalization to other data 
+  - prediction
+
+???
+
+- In Machine learning the goal is often prediction, i.e., using the estimated model to predict outcomes `\(Y'\)` from a completely new set of predictors, `\(X'\)`. Overfitting will make the estimated model to specialized to prediction of the training outcomes including the noise it contains. 
+  - Regularization can therefore be motivated as a technique to improve the generalizability of a learned model by reducing its overfitting to the training data..
+
 ---
 
 ### Cross validation as ""regularization""
@@ -737,6 +839,15 @@
   - Number of iterations balance between
       - suboptimal model (too few)
       - risk of overfitting (too many)
+      
+???
+
+- Many Machine learning techniques uses iteration to improve the model estimates incrementally. 
+- The number of iterations then becomes an important parameter 
+    - enough to get a good model, 
+    - but avoid overfitting. 
+    
+--
 
 ##### Cross-validation as regularization
 - Partition the data into three sets, *training*, *validation*, and *test* data sets. 
@@ -746,19 +857,9 @@
   - continue until the _validation_ loss score does not get better
 - Evaluate final model against the _test_ data.
 
-???
-
-- Many Machine learning techniques uses iteration to improve the model estimates incrementally. 
-- The number of iterations then becomes an important parameter 
-    - enough to get a good model, 
-    - but avoid overfitting. 
-    
 ---
-layout: false
-layout: true
-# Norms 
 
----
+### Norms 
 
 - Norms are a kind of summary statistic over vectors 
   - function that has a vector as input and a (non-negative) number as output. 
@@ -772,14 +873,13 @@
 
 - *Norms* is a concept that commonly pops up when regularization is discussed.
 - Since this special notation is used for norms, they can at first look very incomprehensible, but we have already used a number of them!
+- Here I will use $\beta $ as example but could be any vector or expression describing a vector
 
 ---
 
 #### The `\(L_0\)` ""norm""
 
-- the number of non-zero `\(\beta\)` 
-  - `\(||\beta||_0 = \sum_i I(\beta_i=0),\)`
-      - where `\(I(expr)\)` equals 1 if `\(expr\)` is true and 0 otherwise. 
+- the number of non-zero `\(\beta\)` `$$||\beta||_0 = \sum_i I(\beta_i=0),$$` where `\(I(expr)\)` equals 1 if `\(expr\)` is true and 0 otherwise. 
   - corresponds to `\(\#X\)` in our initial simple naive regularization approach
 
 --
@@ -814,16 +914,20 @@
 
 - There is also a `\(L_2\)` norm, `$$||\beta||_2 = \sqrt{\sum_{\beta_i\in{\beta}} \beta_i^2}.$$`
 
+???
+
+- If you substitute `\(\beta\)` for some other expressin, does this feeel familiar?
+
 --
 
 - You have already have been working with  `\(L_2\)` norms
-  - `\(RSS = ||Y-X\beta||_2^2\)` is the square of the `\(L_2\)` norm of the residuals.
+  - `\(RSS = ||Y-X\beta||_2^2\)` is the square of the `\(L_2\)` norm of the residual vector.
   - the sample standard deviation can be written using a `\(L_2\)` norm, `$$sd(x) = \frac{||x-\bar{x}||_2}{\sqrt{n-1}}.$$`
 
 --
 
 - Uses in regularization:
- - For regularization, the `\(L_2\)`-norm is used in *ridge regression*.
+ - For regularization, the `\(L_2\)`-norm is used in *Ridge Regression*.
     - `glmnet(..., alpha = 0,...)`
  
 ???
@@ -836,6 +940,13 @@
 #### Combination of norms
 
 - *Elastic net* regularization uses a mixed model combination of the `\(L_1\)` norm and the `\(L_2\)` norm,$$RSS+\alpha\lambda_1 ||\beta||_1 + (1-\alpha) \lambda_2 ||\beta||_2.,$$ where `\(\alpha\)` determines the weight of the different regularizations
+
+--
+
+- ""intermediate"" between LASSO and Ridge Regression
+
+--
+
 - The aim is to avoid some drawbacks of LASSO.
 
 
@@ -852,29 +963,45 @@
 {{content}}
 ]
 .pull-right[
-![manhattan distance](images/manhattan_distance.jpg)
+![manhattan distance](../images/manhattan_distance.jpg)
 - B is a 2-dimensional vector (4,3)
 - A is the _origo_-vector (0,0)
 ]
 --
-- The `\(L_2\)`-norm is also the *Euclidean distance* from origo. 
+- The `\(L_2\)`-norm is the *Euclidean distance* from origo. 
 {{content}}
 --
 
-- The `\(L_1\)`-norm is also the *Manhattan distance* from origo. 
+- The `\(L_1\)`-norm is the *Manhattan distance* from origo. 
 {{content}}
 --
 
-- The `\(L_0\)`-""norm"" is also the *Hamming distance* from origo. the Hamming distance between two vectors is the number of places they disagree.
+- The `\(L_0\)`-""norm"" is the *Hamming distance* from origo. the Hamming distance between two vectors is the number of places they disagree.
 
 
 ???
 - Hamming dist would be 2 here
-- In formal mathematics, the `\(L_0\)`-""norm"" does not properly fulfill all criteria for a norm; hence it is often, as here, written within quotation marks.
+- In formal mathematics, the `\(L_0\)` ""norm"" does not properly fulfill all criteria for a norm; hence it is often, as here, written within quotation marks.
 
 ---
 layout: false
 
+
+# That's it
+
+- End of lecture...
+
+--
+
+- Lecture notes for the session is available from Schedule.
+  - contains some extra reading
+
+--
+
+- Next is the Exercises...
+
+---
+
 # Exercises
 
 ### Data "
NBISweden,workshop-mlbiostatistics,a518d9a00a0bbc3e1cb6f0bb1520ac00f3cc9a8a,bsennblad,bengt.sennblad@scilifelab.se,2021-10-06T13:32:04Z,bsennblad,bengt.sennblad@scilifelab.se,2021-10-06T13:32:04Z,Fix typos,session-intro2/intro2_files/figure-html/mice1-1.png;session-regularization/01RegularizationLecture.Rmd;session-regularization/02RegularizationExercises.Rmd,True,False,True,False,8,8,16,"---FILE: session-regularization/01RegularizationLecture.Rmd---
@@ -56,7 +56,7 @@ In practice, the proportionality is ignored and we set
 $$L[\theta|Y,X] = Pr[Y|X, \theta]$$
 
 <details>
-<summary> <span style=""color:gray"">Extra Reading</span> </summary>
+<summary> <span style=""color:gray"">Extra manhattan_Reading</span> </summary>
 The proportionality (indicated by '$\propto$') means there are some unknown constant factor, $k$, such that $L[\theta|Y,X] = k Pr[Y|X, \theta]$. However, the factor $k$ is assumed to be constant over $\theta$s and over models. 
 
 When the likelihood of two $\theta$s (or models) are compared this is almost always done as a _likelihood ratio_, 
@@ -89,8 +89,6 @@ Likelihood and maximum likelihood estimation are central concepts in statistics.
 
 ## Likelihood | `Likelihood and OLS for linear models`
 
-<details>
-<summary> Lecture notes </summary>
 
 So, why have we used ordinary least squares (OLS), i.e., minimization of  RSS when estimating linear model parameters $\beta$ rather than maximum likelihood estimation?
 
@@ -180,6 +178,8 @@ X=matrix(round(runif(N*(P+1),min=0, max=2)), nrow=N, ncol=P)
 b0=3
 # effect sizes for first three variables
 b=c(runif(3, min=0.5, max=1.0))
+# print the true parameter values 
+#print(paste(""True effect sizes b_1 ="", b[1], ""b_2 ="", b[2], ""b_3 ="", b[3], ""; intercept ="", b0))
 
 # generate y for all samples (=rows in X) using only the 3 first variables (columns) of X
 Y <- b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
@@ -788,7 +788,7 @@ $$ ||\beta||_2 = \sqrt{\sum_{\beta_i\in{\beta}} \beta_i^2}.$$
 
 Geometrically, the $L_2$-norm is also the *Euclidean distance* from origo. 
 
-![Euclidean distance](assets/manhattan_distance.jpg)
+![Euclidean distance](images/manhattan_distance.jpg)
 
 ***
 

---FILE: session-regularization/02RegularizationExercises.Rmd---
@@ -3,7 +3,7 @@
 
 ## Data
 
-We will here use biological experimental data, more specifically a skeletal muscle gene expression subset (randomly sampled 1000 genes) from GTEX Human Tissue Gene Expression Consortium ([Lonsdale et al. 2013](https://www.nature.com/articles/ng.2653)). We will use the approaches we learned above to perform feature selection on this data with respect to a logistic regression analysis.
+We will here use biological experimental data, more specifically a skeletal muscle gene expression subset (randomly sampled 1000 genes) from GTEX Human Tissue Gene Expression Consortium ([Lonsdale et al. 2013](https://www.nature.com/articles/ng.2653)). We will use the approaches we learned above to perform model testing feature selection on this data with respect to a logistic regression analysis.
 
 ### Task| `Load the GTEX muscle expression data`
 
@@ -161,12 +161,12 @@ However, as suggested by the PCA, the majority of genes are probably not associa
 
 Coming from a _information theory_ base, Hirotugu Akaike came up with a very similar approach for the overfitting problem.
 
-The Akaike information criterion (AIC), for a model $m$ with variables $X$, is defined as
+Akaike information criterion (AIC), for a model $m$ with variables $X$, is defined as
 
-  $$AIC_m = 2\# X - 2\log \max L[{\beta}|X,Y]$$
+  $$AIC_m = 2\# X - 2\log L[{\beta}|X,Y]$$
 The Bayesian information criterion, ($BIC$), differs by the weighting of $L_0$-norm:
 
- $$BIC_m = \log(n)\# X - 2\log \max L[{\beta}|X,Y],$$
+ $$BIC_m = \log(n)\# X - 2\log L[{\beta}|X,Y],$$
 where $n$ is the sample size (number of observations). The two criteria can be written more general as (scaled) regularized likelihood:
 
 $$-2\left( \log L[Y\beta|X,Y] - \lambda ||\beta||_0\right),$$"
NBISweden,workshop-mlbiostatistics,3c47703a7b45b1288e5149307f1dbd990cfd5fc5,Eva Freyhult,eva@freyhult.net,2021-09-30T06:35:30Z,Eva Freyhult,eva@freyhult.net,2021-09-30T06:35:30Z,Fix schedule links,schedule.md,False,False,False,False,3,3,6,"---FILE: schedule.md---
@@ -21,7 +21,7 @@ title:  'Schedule'
 
 *14.30 - 15.00 break*
 
-**15:00 - 15:30** [Probability: Introduction and discrete distributions](sessions-probability)
+**15:00 - 15:30** [Probability: Introduction and discrete distributions](session-probability)
 
 **13.30 - 16.30** [Exercises](session-probability/prob-exercise-discrv.html)
 
@@ -32,7 +32,7 @@ title:  'Schedule'
 
 **09:00 - 09.30** Group discussions: recap of the previous day
 
-**09:30 - 10.00** [Probability: continuous distributions](session-probability/prob-conrv.html)
+**09:30 - 10.00** [Probability: continuous distributions](session-probability/prob-contrv.html)
 
 **10:00 - 10.45** [Exercises](session-probability/prob-exercise-contrv.html) and [Exercises](session-probability/prob-exercise-sample.html)
 
@@ -92,7 +92,7 @@ title:  'Schedule'
 
 **13.00 - 13.30** [Clustering: k-means and hierarchical clustering](session-clustering)
 
-**13.30-14.30** [Exercises](sessions-probdescinfe/session-clustering.html#exercises-clustering)
+**13.30-14.30** [Exercises](session-clustering/clust-exercises.html)
 
 *14.30 - 15.00 break*
 "
NBISweden,workshop-mlbiostatistics,f781a882b989339bb9d317c056d0eb0bda8a64f0,Eva Freyhult,eva@freyhult.net,2021-09-17T08:09:52Z,Eva Freyhult,eva@freyhult.net,2021-09-17T08:09:52Z,Fix tex problem,session-clustering/404.html;session-clustering/clustering-dissimilarity.html;session-clustering/clustering-exercises.html;session-clustering/clustering-hierarchical.html;session-clustering/clustering-intro.html;session-clustering/clustering-kmeans.html;session-clustering/session-clustering.Rmd,True,False,True,False,753,469,1222,"---FILE: session-clustering/session-clustering.Rmd---
@@ -2,9 +2,11 @@
 title: ""Clustering""
 output:
   bookdown::gitbook:
-    keep_md: false
+    in_header:
+      style.css
+    keep_md: true
     number_sections: true
-    self_contained: true
+    css: style.css
   toc: false
 ---
 
@@ -15,7 +17,7 @@ require(knitr)
 require(kableExtra)
 require(ggdendro)
 require(tidyverse)
-knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, dev=""svg"", dpi=96)
+knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5, echo = FALSE, cache=TRUE, error=FALSE, warnings=FALSE, dev=""png"", dpi=96)
 options(digits=2)
 ```
 
@@ -144,7 +146,7 @@ $$d_{pear}(x,y) = \sqrt{1-r}$$
 
 Hierarchical clustering does not require the number of clusters to be specified. Instead of creating a single set of clusters it creates a hierarchy of clusterings based on pairwise dissimilarities.
 
-```{r, fig.cap=""A hierarchical cluster dendrogram."", fig.width=7, fig.height=3.5}
+```{r dendro, fig.cap=""A hierarchical cluster dendrogram."", fig.width=7, fig.height=3.5}
 require(ggdendro)
 h <- hclust(dist(iris[, 1:4]))
 ggdendrogram(h)
@@ -193,7 +195,7 @@ where $m_A, m_B, m_{A \cup B}$ are the center of the clusters $A$, $B$ and $A \c
 
 Note that Ward's linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function `hclus` either the Euclidean or squared Euclidean distance can be used in combination with the linkage `method='ward.D'` or `method='ward.D2`, respectively.
 
-```{r, fig.cap=""Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods."", fig.show=""hold"", fig.width=7, fig.height=3.5}
+```{r dendro4link, fig.cap=""Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods."", fig.show=""hold"", fig.width=7, fig.height=3.5}
 require(ggdendro)
 hsl <- hclust(dist(iris[, 1:4]), method=""single"")
 hcl <- hclust(dist(iris[, 1:4]), method=""complete"")
@@ -209,13 +211,12 @@ ggdendrogram(hwl, labels=FALSE) + ggtitle(""Ward's linkage"")
 
 # Exercises: Clustering {#clustering-exercises}
 
-```{exercise}
-Download the two-dimensional data set.
+```{exercise, label=""kmeans"", echo=TRUE}
 
-Based on a two dimensional data set you will investigate K-means clustering.
+Based on a two dimensional data set in the below figure,  you will investigate K-means clustering.
 ```
 
-```{r fiveclusters}
+```{r fiveclusters, fig.cap=""A two dimensional data set of five clusters.""}
 set.seed(1919)
 x <- c(1,3,1.5,3.5,2)
 y <- c(1,1,3,3,5)
@@ -226,47 +227,26 @@ df <- data.frame(klass=rep(1:5, c(30, 15, 25, 10,18)), stringsAsFactors = FALSE)
 df %>% ggplot(aes(x=x,y=y, color=group)) + geom_point()
 write.csv(df, file=""fiveclasses.csv"")
 ```
+
+Download the two dimensional [data set](fiveclasses.csv).
+
 a) Try to cluster the data using the k-means algorithm (function `kmeans` in R). Use Forgy's method for initialization and select a value for $k$.
 b) Plot the data and color by cluster id
 c) Run the same analysis again, do you get the same results? (You can compare two vectors of class identities using the function `table`.)
 d) By setting the argument `nstart` to 4 the algorithm will automatically try four different (random) starting points.
 e) Try different values for $k$, run the k-means algorithm and collect the WSS ('tot.withinss'). Plot WSS vs k. Thw WSS is always decreasing as k increases, but the curve can still give you a hint of which $k$ to choose. The Elbow method for selecting $k$ is to look at this curve and choose the $k$ that you find at the bend of the curve, at the 'elbow'.
 Which $k$ would you pick based on this?
 
-```{r}
-df <- read.csv(""fiveclasses.csv"")
-
-## K-means with k=5
-df$kmeans <- kmeans(df[, c('x','y')], centers=5, algorithm =""Forgy"")$cluster
-
-df %>% ggplot(aes(x=x, y=y, color=factor(kmeans))) + geom_point() + theme_bw() + theme(legend.position = ""none"")
-
-#X <- read.csv2(""../../workshop-mlbiostatistics/session-pca_clustering/data/cpm.csv"",row.names = 1)
-#log2X <- log2(X+1)
-set.seed(2871)
-k <- 5
-l <- 10
-X0 <- matrix(rnorm(k*l, runif(l, 2.5,2.5), 0.2), k, l, byrow=TRUE)
-r <- sample(3:15, size=k, replace=TRUE)
-X1 <- do.call(""rbind"", lapply(1:k, function(i) matrix(X0[i,] + rnorm(r[i]*l, 0, runif(1, 0, 0.2)), r[i], l, byrow=TRUE)))
-
-rownames(X1) <- paste0(rep(toupper(letters[1:5]), r), 1:sum(r))
-
-q <- sample(10:50, size=l, replace=TRUE)
-X <- do.call(""cbind"", lapply(1:l, function(i) matrix(X1[,i] + rnorm(q[i]*nrow(X1), 0, runif(1, 0, 0.01)), nrow(X1), q[i])))
-
-plot(hclust(dist(X)))
-```
+```{exercise, label=""clusterNCI60"", echo=TRUE}
 
-```{exercise}
 The NCI60 data set consists of gene expression values for 6830 genes for 64 cell lines.
 
 Using this data set investigate a few hierarchical clustering distances and linkage methods.
 ```
 
 The data can be downloaded in R using the following command
 
-```{r NCI60, eval=FALSE}
+```{r NCI60, eval=FALSE, echo=TRUE}
 nci.data <- read.table(url(""https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv""), sep="","",row.names=1,header=TRUE)
 nci.label <-scan(url(""https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label""),what="""")
 ```
@@ -285,3 +265,66 @@ Pick the tree resulting from the method you think is 'best'. How many clusters a
 
 You can cut the tree on any level to get between 1 and 64 clusters. The function `cutree` either on a specific height (dissimilarity) or to get a specific number of clusters.
 
+## Solutions
+
+\@ref(exr:kmeans)
+
+First read the data
+
+```{r}
+df <- read.csv(""fiveclasses.csv"")
+```
+
+a) Try to cluster the data using the k-means algorithm (function `kmeans` in R). Use Forgy's method for initialization and select a value for $k$.
+
+```{r}
+## k-means with k=5 and Forgy's algorithm
+km <- kmeans(df[, c('x','y')], centers=5, algorithm =""Forgy"")
+```
+
+b) Plot the data and color by cluster id
+
+```{r}
+## The cluster identities are stored in km$cluster, same order as input 
+df$kmeansclusters <- km$cluster
+## Plot using base R graphics
+plot(df$x, df$y, col=df$kmeansclusters)
+##alternatively, plot using ggplot
+ggplot(df, aes(x=x, y=y, color=factor(kmeansclusters))) + geom_point() + scale_color_discrete(""Cluster id"") + theme_bw()
+```
+
+c) Run the same analysis again, do you get the same results? (You can compare two vectors of class identities using the function `table`.)
+
+```{r}
+km2 <- kmeans(df[, c('x','y')], centers=5, algorithm =""Forgy"")
+table(km$cluster, km2$cluster)
+```
+
+d) By setting the argument `nstart` to 4 the algorithm will automatically try four different (random) starting points. Try this out and compare how stable two runs are.
+
+```{r}
+km4.1 <- kmeans(df[, c('x','y')], centers=5, algorithm =""Forgy"", nstart=4)
+km4.2 <- kmeans(df[, c('x','y')], centers=5, algorithm =""Forgy"", nstart=4)
+table(km4.1$cluster, km4.2$cluster)
+```
+
+e) Try different values for $k$, run the k-means algorithm and collect the WSS ('tot.withinss'). Plot WSS vs k. The WSS is always decreasing as k increases, but the curve can still give you a hint of which $k$ to choose. The Elbow method for selecting $k$ is to look at this curve and choose the $k$ that you find at the bend of the curve, at the 'elbow'.
+Which $k$ would you pick based on this?
+  
+  
+
+```{r tmp, eval=FALSE}
+set.seed(2871)
+k <- 5
+l <- 10
+X0 <- matrix(rnorm(k*l, runif(l, 2.5,2.5), 0.2), k, l, byrow=TRUE)
+r <- sample(3:15, size=k, replace=TRUE)
+X1 <- do.call(""rbind"", lapply(1:k, function(i) matrix(X0[i,] + rnorm(r[i]*l, 0, runif(1, 0, 0.2)), r[i], l, byrow=TRUE)))
+
+rownames(X1) <- paste0(rep(toupper(letters[1:5]), r), 1:sum(r))
+
+q <- sample(10:50, size=l, replace=TRUE)
+X <- do.call(""cbind"", lapply(1:l, function(i) matrix(X1[,i] + rnorm(q[i]*nrow(X1), 0, runif(1, 0, 0.01)), nrow(X1), q[i])))
+
+plot(hclust(dist(X)))
+```"
NBISweden,workshop-mlbiostatistics,1f5c9a784169660a68c25678aaac49a5fa6c3124,olgadet,,2021-09-08T09:20:44Z,olgadet,,2021-09-08T09:20:44Z,Fix typos,precourse-R/01-install.Rmd;precourse-R/01-install.html;precourse-R/precourse-R.html;precourse-R/precourse-setup.Rmd;precourse-R/preface.html,True,False,True,False,24,24,48,"---FILE: precourse-R/01-install.Rmd---
@@ -20,8 +20,8 @@ Install R version 3.5.0 or higher
 
 - Go to [https://cran.rstudio.com](https://cran.rstudio.com)
 - Click on the link corresponding to your operating system
-- Download the recommended files for your system.
-- Run the installer or move the downloaded files to suitable place on your computer.
+- Download the recommended files for your system
+- Run the installer or move the downloaded files to suitable place on your computer
 
 ## RStudio
 
@@ -35,12 +35,12 @@ include_graphics(""figures/RStudio.png"", dpi = 600)
 - A short introduction to RStudio: [https://www.rstudio.com/products/rstudio/?wvideo=520zbd3tij](https://www.rstudio.com/products/rstudio/?wvideo=520zbd3tij)
 
 ## Markdown 
-- Markdown (.md) is a lightweight markup language that you can use to add formatting elements to plain text text documents. [https://www.markdownguide.org/getting-started/](https://www.markdownguide.org/getting-started/). Originally, Markdown had nothing to do with R. 
-- The common formatting options: [https://www.markdownguide.org/cheat-sheet/](https://www.markdownguide.org/cheat-sheet/) 
+- Markdown (.md) is a lightweight markup language that you can use to add formatting elements to plain text documents. [https://www.markdownguide.org/getting-started/](https://www.markdownguide.org/getting-started/). Originally, Markdown had nothing to do with R. 
+- The common formatting options in Markdown are summarized in [https://www.markdownguide.org/cheat-sheet/](https://www.markdownguide.org/cheat-sheet/) 
 
 ## R Markdown 
-- R Markdown (.Rmd) allows to combine typing text in Markdown and execute code in R. It is thus great to document data analysis. 
-- [https://rmarkdown.rstudio.com/articles_intro.html](https://rmarkdown.rstudio.com/articles_intro.html)
+- R Markdown (.Rmd) allows to combine typing text in `Markdown` and execute code in `R`. It is thus great to document data analysis. 
+- A short introduction to R Markdown is under [https://rmarkdown.rstudio.com/articles_intro.html](https://rmarkdown.rstudio.com/articles_intro.html)
 - To open R Markdown in RStudio click `File` -> `New File` -> `R Markdown`
 
 ```{r, fig.align='center', out.width = ""600px"", fig.cap = ""Opening new R Markdown file within RStudio""}

---FILE: precourse-R/01-install.html---
@@ -175,8 +175,8 @@ <h2>R</h2>
 <ul>
 <li>Go to <a href=""https://cran.rstudio.com"">https://cran.rstudio.com</a></li>
 <li>Click on the link corresponding to your operating system</li>
-<li>Download the recommended files for your system.</li>
-<li>Run the installer or move the downloaded files to suitable place on your computer.</li>
+<li>Download the recommended files for your system</li>
+<li>Run the installer or move the downloaded files to suitable place on your computer</li>
 </ul>
 </div>
 <div id=""rstudio"" class=""section level2"">
@@ -198,15 +198,15 @@ <h2>RStudio</h2>
 <div id=""markdown"" class=""section level2"">
 <h2>Markdown</h2>
 <ul>
-<li>Markdown (.md) is a lightweight markup language that you can use to add formatting elements to plain text text documents. <a href=""https://www.markdownguide.org/getting-started/"">https://www.markdownguide.org/getting-started/</a>. Originally, Markdown had nothing to do with R.</li>
-<li>The common formatting options: <a href=""https://www.markdownguide.org/cheat-sheet/"">https://www.markdownguide.org/cheat-sheet/</a></li>
+<li>Markdown (.md) is a lightweight markup language that you can use to add formatting elements to plain text documents. <a href=""https://www.markdownguide.org/getting-started/"">https://www.markdownguide.org/getting-started/</a>. Originally, Markdown had nothing to do with R.</li>
+<li>The common formatting options in Markdown are summarized in <a href=""https://www.markdownguide.org/cheat-sheet/"">https://www.markdownguide.org/cheat-sheet/</a></li>
 </ul>
 </div>
 <div id=""r-markdown"" class=""section level2"">
 <h2>R Markdown</h2>
 <ul>
-<li>R Markdown (.Rmd) allows to combine typing text in Markdown and execute code in R. It is thus great to document data analysis.</li>
-<li><a href=""https://rmarkdown.rstudio.com/articles_intro.html"">https://rmarkdown.rstudio.com/articles_intro.html</a></li>
+<li>R Markdown (.Rmd) allows to combine typing text in <code>Markdown</code> and execute code in <code>R</code>. It is thus great to document data analysis.</li>
+<li>A short introduction to R Markdown is under <a href=""https://rmarkdown.rstudio.com/articles_intro.html"">https://rmarkdown.rstudio.com/articles_intro.html</a></li>
 <li>To open R Markdown in RStudio click <code>File</code> -&gt; <code>New File</code> -&gt; <code>R Markdown</code></li>
 </ul>
 <div class=""figure"" style=""text-align: center"">

---FILE: precourse-R/precourse-R.html---
@@ -100,8 +100,8 @@ <h2><span class=""header-section-number"">1.1</span> R</h2>
 <ul>
 <li>Go to <a href=""https://cran.rstudio.com"">https://cran.rstudio.com</a></li>
 <li>Click on the link corresponding to your operating system</li>
-<li>Download the recommended files for your system.</li>
-<li>Run the installer or move the downloaded files to suitable place on your computer.</li>
+<li>Download the recommended files for your system</li>
+<li>Run the installer or move the downloaded files to suitable place on your computer</li>
 </ul>
 </div>
 <div id=""rstudio"" class=""section level2"" number=""1.2"">
@@ -123,15 +123,15 @@ <h2><span class=""header-section-number"">1.2</span> RStudio</h2>
 <div id=""markdown"" class=""section level2"" number=""1.3"">
 <h2><span class=""header-section-number"">1.3</span> Markdown</h2>
 <ul>
-<li>Markdown (.md) is a lightweight markup language that you can use to add formatting elements to plain text text documents. <a href=""https://www.markdownguide.org/getting-started/"">https://www.markdownguide.org/getting-started/</a>. Originally, Markdown had nothing to do with R.</li>
-<li>The common formatting options: <a href=""https://www.markdownguide.org/cheat-sheet/"">https://www.markdownguide.org/cheat-sheet/</a></li>
+<li>Markdown (.md) is a lightweight markup language that you can use to add formatting elements to plain text documents. <a href=""https://www.markdownguide.org/getting-started/"">https://www.markdownguide.org/getting-started/</a>. Originally, Markdown had nothing to do with R.</li>
+<li>The common formatting options in Markdown are summarized in <a href=""https://www.markdownguide.org/cheat-sheet/"">https://www.markdownguide.org/cheat-sheet/</a></li>
 </ul>
 </div>
 <div id=""r-markdown"" class=""section level2"" number=""1.4"">
 <h2><span class=""header-section-number"">1.4</span> R Markdown</h2>
 <ul>
-<li>R Markdown (.Rmd) allows to combine typing text in Markdown and execute code in R. It is thus great to document data analysis.</li>
-<li><a href=""https://rmarkdown.rstudio.com/articles_intro.html"">https://rmarkdown.rstudio.com/articles_intro.html</a></li>
+<li>R Markdown (.Rmd) allows to combine typing text in <code>Markdown</code> and execute code in <code>R</code>. It is thus great to document data analysis.</li>
+<li>A short introduction to R Markdown is under <a href=""https://rmarkdown.rstudio.com/articles_intro.html"">https://rmarkdown.rstudio.com/articles_intro.html</a></li>
 <li>To open R Markdown in RStudio click <code>File</code> -&gt; <code>New File</code> -&gt; <code>R Markdown</code></li>
 </ul>
 <div class=""figure"" style=""text-align: center""><span id=""fig:unnamed-chunk-4""></span>

---FILE: precourse-R/precourse-setup.Rmd---
@@ -8,17 +8,17 @@ output:
 
 # Preface {-}
 
-During the course we will be using `R` programming language within `RStudio Desktop` editor. We will be writing scripts using `R Markdown` (.Rmd). We will try to keep coding as simple as possible, but we do assume that you have a basic understanding of R and your computer setup with RStudio and R version 3.5.0 or higher.  
+During the course we will be using `R` programming language within `RStudio Desktop` editor and writing scripts using `R Markdown` (.Rmd). We will try to keep coding as simple as possible, but we do assume that you have a basic understanding of R and your computer is setup with RStudio and R version 3.5.0 or higher.  
 
 R skills that will be useful during the course are: 
 
 - using R as calculator incl. raising values to a power
 - being able to work with vectors and matrices, incl. subsetting and matrices multiplication 
 - reading in data from .csv files, e.g. with `read.delim()`
-- printing top few rows or last few rows, e.g. with `head()` and `tail()`
+- printing top few rows or last few rows of a data frame, e.g. with `head()` and `tail()`
 - using in-built summary functions such as `sum()`, `min()` or `max()`
 - being able to use documentation pages for R functions, e.g. with `help()` or `?()`
-- using if else statements, writing simple loops and functions.
+- using `if else` statements, writing simple loops and functions
 - making simple scatter plots of one numerical variable against another, both with `plot()` and `ggplot()`
 - being able to install CRAN packages e.g. with `install.packages()`
 - being familiar with R Markdown format

---FILE: precourse-R/preface.html---
@@ -97,16 +97,16 @@ <h1 class=""title"">R, RStudio and R Markdown</h1>
 </div>
 <div id=""preface"" class=""section level1 unnumbered"">
 <h1>Preface</h1>
-<p>During the course we will be using <code>R</code> programming language within <code>RStudio Desktop</code> editor. We will be writing scripts using <code>R Markdown</code> (.Rmd). We will try to keep coding as simple as possible, but we do assume that you have a basic understanding of R and your computer setup with RStudio and R version 3.5.0 or higher.</p>
+<p>During the course we will be using <code>R</code> programming language within <code>RStudio Desktop</code> editor and writing scripts using <code>R Markdown</code> (.Rmd). We will try to keep coding as simple as possible, but we do assume that you have a basic understanding of R and your computer is setup with RStudio and R version 3.5.0 or higher.</p>
 <p>R skills that will be useful during the course are:</p>
 <ul>
 <li>using R as calculator incl. raising values to a power</li>
 <li>being able to work with vectors and matrices, incl. subsetting and matrices multiplication</li>
 <li>reading in data from .csv files, e.g. with <code>read.delim()</code></li>
-<li>printing top few rows or last few rows, e.g. with <code>head()</code> and <code>tail()</code></li>
+<li>printing top few rows or last few rows of a data frame, e.g. with <code>head()</code> and <code>tail()</code></li>
 <li>using in-built summary functions such as <code>sum()</code>, <code>min()</code> or <code>max()</code></li>
 <li>being able to use documentation pages for R functions, e.g. with <code>help()</code> or <code>?()</code></li>
-<li>using if else statements, writing simple loops and functions.</li>
+<li>using <code>if else</code> statements, writing simple loops and functions</li>
 <li>making simple scatter plots of one numerical variable against another, both with <code>plot()</code> and <code>ggplot()</code></li>
 <li>being able to install CRAN packages e.g. with <code>install.packages()</code></li>
 <li>being familiar with R Markdown format</li>"
NBISweden,workshop-mlbiostatistics,30cf8c3114c396519e4032bc6e12a3a718657c7f,bsennblad,bengt.sennblad@scilifelab.se,2021-08-30T11:36:39Z,bsennblad,bengt.sennblad@scilifelab.se,2021-08-30T11:36:39Z,Fix premature norm reference. Fix LRT error,session-regularization/01RegularizationLecture.Rmd,True,False,True,False,14,17,31,"---FILE: session-regularization/01RegularizationLecture.Rmd---
@@ -2,7 +2,7 @@
 # Learning outcomes
 
 * ML pipeline
-    + preprocessing (scalin,l nrmalization/standardization);
+    + preprocessing (scaling, normalization/standardization);
     + Feature selection
     + Cross-validation
     + Model evaluation
@@ -322,7 +322,9 @@ _Overfitting_
 <summary> <span style=""color:gray"">Extra Reading</span> </summary>
 <h2> Model comparison | `Likelihood ratio test`</h1>
 
-For nested models $-2 \max LRT$ is $\chi^2(d)$-distributed, with $d=$ the difference in free params in the two models.
+
+For nested models $-2 \max \frac{L[\theta_1|Y,X]}{L[\theta_0|Y,X]}$ is $\chi^2(d)$-distributed, with $d=$ the difference in free parameters in the two models.
+This fors the basis of the *likelihood ratio test*. This is very frequently used with in statistics, but does not perform any regularization. Applying a likelihood ratio test to our data yields the following result:
 
 ```{r, lrt, echo=F}
 library(lmtest)
@@ -462,22 +464,16 @@ The *coordinate descent* algorithm is used in the R package `glmnet`:
 ### Example | `Lasso using the glmnet R-package`
 
 * Use function `glmnet` to perform LASSO analysis on our example data; relevant arguments of the function:
+    + preprocessing
+        + Standardization in `glmnet`:
+$x' = \frac{x-\bar{x}}{\sqrt{\frac{1}{n}\sum_i(x_i-\bar{x})^2}},$
+where $\sqrt{\frac{1}{n}\sum_i(x_i-\bar{x})^2}$ is the uncorrected sample standard deviation.
+          - The variables Y and X must be centered and standardized to ensure that all variables are given equal weight in the model selection.
+          - standardization of $X$ to unit variance in `glmnet` is obtained  by setting the argument `standardize=TRUE` which is the default
+          - the values of $Y$ is always standardized (?) for `family=gaussian` (LASSO)
+            + and the coefficients are back-standardized before reported
     + linear regression (`family='gaussian'` = default)
     + LASSO (`alpha=1` = default)
-    + standardization
-        + The variables Y and X must be centered and standardized to ensure that all variables are given equal weight in the model selection.
-        + standardization of $X$ to unit variance in `glmnet` is obtained  by setting the argument `standardize=TRUE` which is the default
-        + the values of $Y$ is always standardized (?) for `family=gaussian` (LASSO)
-            + and the coefficients are back-standardized before reported
-
-<details> 
-<summary> <span style=""color:gray"">Extra Reading</span> </summary>
-
-Standardization in `glmnet`:
-$x' = \frac{x-\bar{x}}{1/\sqrt{N}||X-\bar{x}||_2}$
-
-***
-</details>
 
 ```{r, echo=T, eval=TRUE} 
 library(glmnet)
@@ -712,7 +708,7 @@ Many Machine learning techniques uses iteration to improve the model estimate in
 
 A common approach to this problem is to use cross-validation. Here you partition the data into three sets, *training*, *validation*, and *test* data sets. The test data is is not used in the iterations. In each iteration, you train the model on the training data and then evaluate it on the validation data using some loss function. You continue the iterations until the  evaluated loss score is not better than the loss score from the previous iteration; this will be your final model, which is evaluated against the test data.
  
-### Norms
+### Norms {#norms}
 
 *Norms* is a concept that commonly pops up when regularization is discussed.
 
@@ -799,6 +795,7 @@ Geometrically, the $L_2$-norm is also the *Euclidean distance* from origo.
 </details>
 
 We note that you already have been working with an $L_2-norm$: since $RSS = ||Y-X\beta||_2^2$ is simply the square of the $L_2$ norm of the residuals.
+Similarly, the sample standard deviation could be written using the $L_2$-norm, $sd(x) = \frac{||x-\bar{x}||_2}{\sqrt{n-1}}.$
 
 For regularization, the $L_2$-norm is used in *ridge regression*. Computationally, regularization with an $L_2$-norm is even more efficient than with an $L_1$-norm. It works well to pevent over-fitting, but because it is bad at shrinking $\beta_i$'s all the way to zero, it is not so good for feature selection.
 "
NBISweden,workshop-mlbiostatistics,f757808218031cc2ee80870a514db910daf6bfa3,Eva Freyhult,eva@freyhult.net,2021-08-30T07:06:00Z,Eva Freyhult,eva@freyhult.net,2021-08-30T07:06:00Z,Fix typos,session-regularization/01RegularizationLecture.Rmd;session-regularization/02RegularizationExercises.Rmd,True,False,True,False,26,26,52,"---FILE: session-regularization/01RegularizationLecture.Rmd---
@@ -2,7 +2,7 @@
 # Learning outcomes
 
 * ML pipeline
-    + preprocessing (scalin,l nrmalization/standardization);
+    + preprocessing (scaling, normalization/standardization);
     + Feature selection
     + Cross-validation
     + Model evaluation
@@ -29,7 +29,7 @@ that is, the probability that the model with parameters $\theta$ and given $X$ g
 
 Likelihood builds on the intuition that if parameters $\theta_T$ is close to the 'truth', then $Pr[Y| X, \theta]$ will be higher for $\theta_T$ than for any more ""wrong"" parameter $\theta_W$. We should therefore select the $\theta$ that maximizes $Pr[Y| X, \theta]$; this is called maximum likelihood estimation (MLE) of $\theta$.
 
-Since statistical model contain an element of randomness, the reasoning above might not always be correct for any single obeservation. However, if we sum over a large number of observations it will be true on average. Hence the need for datasets that are large enough.
+Since statistical model contain an element of randomness, the reasoning above might not always be correct for any single observation. However, if we sum over a large number of observations it will be true on average. Hence the need for datasets that are large enough.
 
 To formalize this intuition, Edwards (1972) defined the likelihood of model parameters being true given observed data as
 
@@ -84,7 +84,7 @@ As mentioned above, the logarithm of the likelihood, the logLikelihood, $\log L[
 
 Likelihood and maximum likelihood estimation are central concepts in statistics. Many statistical tests and methods uses or is based on the concept of maximum likelihood.
 
-(In the following, I will simplify notation and not differentiate between etimates and random variables, e.g., $\theta$ will be used also for $\widehat\theta$.)
+(In the following, I will simplify notation and not differentiate between estimates and random variables, e.g., $\theta$ will be used also for $\widehat\theta$.)
 
 
 ## Likelihood | `Likelihood and OLS for linear models`
@@ -175,7 +175,7 @@ P=10 # number of variables
 # 2. Put values into a matrix with N rows and P columns
 X=matrix(round(runif(N*(P+1),min=0, max=2)), nrow=N, ncol=P)
 
-# generate a y variable from a multivarite lm of 3 first X variables only
+# generate a y variable from a multivariate lm of 3 first X variables only
 # intercept
 b0=3
 # effect sizes for first three variables
@@ -340,7 +340,7 @@ library(kableExtra)
 kable(lrt,""html"",row.names=F, col.names=c(""Compared models"",""logL 1st model"",""logL 2nd model"",""logLR"", ""P-value"", ""Sign at 0.05""),digits=30, format.args=list(snsmall=0)) %>% kable_styling(bootstrap=""striped"", font_size = 14, full_width=F)
 
 ```
-In our simple test case, the LRT also succeed in picking the correct model. It should be noted that certain issues, such as *linkage disequilibriium*, may cause problems for LRT (*the example is not optimized to show this*).
+In our simple test case, the LRT also succeed in picking the correct model. It should be noted that certain issues, such as *linkage disequilibrium*, may cause problems for LRT (*the example is not optimized to show this*).
 
 
 ***
@@ -385,7 +385,7 @@ print(paste(""Model"",which(ifelse(rl==maxrl,TRUE,FALSE)), ""has the maximum rL""))
 ### Mini-task: Think about:
 
 * Which is the best model? Is this correct compared to our *oracle knowledge*?
-* What an we say about how good is it compared to the other models?
+* What can we say about how good it is compared to the other models?
 * Can you see a drawback in our model testing approach above? If so, how can we solve that?
 
 <details>
@@ -397,7 +397,7 @@ print(paste(""Model"",which(ifelse(rl==maxrl,TRUE,FALSE)), ""has the maximum rL""))
 <details>
 <summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
-* Sometimes it is desirable to compute a significance for rejecting a model in favour of another model. A NULL distribution for the $relL$ statistic is usally obtained through simulation, e.g., using parameteric bootstrapping.
+* Sometimes it is desirable to compute a significance for rejecting a model in favour of another model. A NULL distribution for the $relL$ statistic is usually obtained through simulation, e.g., using parametric bootstrapping.
 
 ***
 </details>
@@ -414,9 +414,9 @@ print(paste(""Model"",which(ifelse(rl==maxrl,TRUE,FALSE)), ""has the maximum rL""))
 
 LASSO  stands for *Least absolute shrinkage and selection operator* (""shrinkage"" is another common term for regularization) and is a method for selecting variables to include in a multivariate model.
 
-Here, we do not explicitly compare two models with different number of variables. Instead all variables are included and the regularization acts upon the values of $\beta$ so that only important variabels have a $\beta_i> 0$.
+Here, we do not explicitly compare two models with different number of variables. Instead all variables are included and the regularization acts upon the values of $\beta$ so that only important variables have a $\beta_i> 0$.
 
-Classical LASSO builds on RSS of a linear regression model $Y \sim X{\beta}$ with regularization
+Classical LASSO builds on RSS (residual sum of squares) of a linear regression model $Y \sim X{\beta}$ with regularization
 
 
 The regularization term $f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i|$
@@ -485,7 +485,7 @@ library(glmnet)
 fit = glmnet(X,Y, family=""gaussian"", alpha=1, standardize=T)
 ```
 
-* A graphical way to view the result is to `plot` the paths of $\beta$ for increasing vaules of $\lambda$. This plot shows how the $\beta_i$ for different variables $i$ changes with $\lambda$. The plot is perhaps best read from right to left, going from higher and thereby stricter, $\lambda$ values to lower $\lambda$ values including more and more variables/non-zero $\beta_i$.
+* A graphical way to view the result is to `plot` the paths of $\beta$ for increasing values of $\lambda$. This plot shows how the $\beta_i$ for different variables $i$ changes with $\lambda$. The plot is perhaps best read from right to left, going from higher and thereby stricter, $\lambda$ values to lower $\lambda$ values including more and more variables/non-zero $\beta_i$.
 
 ```{r, echo=T, fig.height=5, eval=FALSE} 
 par(mfrow=c(1,1))
@@ -538,7 +538,7 @@ Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO
 
 In cross validation, this approach is emulated by partitioning the data at hand into a *training* and  *validation* data set. The model parameters are estimated ('trained') on the the training data and the validated on the validation data. (Optionally, a *test* partition can be assigned in cross-validation on which the final, selected model is evaluated; this is not employed here).
 
-By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partioning.
+By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partitioning.
 
 In $k$-fold cross validation, the original data is split into $k$ sub-datasets $\{D_1,D_2,\ldots, D_k\}$.
 For $i \in \{1,2,\ldots, k\}$, set $D_i$ as the validation data set and the union of the other datasets be the training data. Perform cross validation as above.
@@ -696,13 +696,13 @@ Compare competing models, how much better is the best model.
 
 #### Generalization
 
-In Machine learning the goal is often prediction, i.e., using the estimated model to predict outcoomes $Y'$ from a completely new set of predictords, $X'$. Overfitting will make the estimated model to specialized to prediction of the training outcomes including the noise it contains. 
+In Machine learning the goal is often prediction, i.e., using the estimated model to predict outcomes $Y'$ from a completely new set of predictors, $X'$. Overfitting will make the estimated model to specialized to prediction of the training outcomes including the noise it contains. 
 
 Regularization can therefore be motivated as a technique to improve the generalizability of a learned model by reducing its overfitting to the training data..
 
 #### Feature selection/sparsity
 
-In many applications, e.g., for clinical diagnosis, a small model with few variables explaining the bulk of the outcome is preferred over a detailed model that explains as much of the outcome as possible. In these applications, regularization can be used for selecting the most important features and ignoring the other vasriables.
+In many applications, e.g., for clinical diagnosis, a small model with few variables explaining the bulk of the outcome is preferred over a detailed model that explains as much of the outcome as possible. In these applications, regularization can be used for selecting the most important features and ignoring the other variables.
 
 More generally, sparsity may also lead to more easily interpreted models, e.g., for biological questions.
 
@@ -749,7 +749,7 @@ Here, the $L_0$-norm can also be viewed as the number of variables, $\#X$, inclu
 
 The *Akaike Information Criterion*, $AIC$, as well as the *Bayesian Information Criterion*, $BIC$, for model selection use a logLikelihood penalized with an $L_0$ ""norm"", with different values of $\lambda$ ($1$ and $ln(n)/2$, respectively).
 
-In fact, regularization with the $L_0$-norm would be the desired approachin most cases. However, as we have seen, finding the optimal model is really hard; in fact this is an *NP-hard* problem. Hence, alternative approaches using other norms has been investigated. 
+In fact, regularization with the $L_0$-norm would be the desired approaching most cases. However, as we have seen, finding the optimal model is really hard; in fact this is an *NP-hard* problem. Hence, alternative approaches using other norms has been investigated. 
 
 #### The $L_1$ norm
 
@@ -771,12 +771,12 @@ Geometrically, the $L_1$-norm is also the *Manhattan distance* from origo.
 So another way of writing LASSO is
 
 $$min\left\{RSS\right\} + \lambda ||\beta||_1.$$
-As we have seen, LASSO is an effective algoritm for regularization with the $L_1$-norm. It also does a decent job of mimicking the $L_0$-""norm"", i.e., setting $\beta_i=0$ for certain variables $i$. However, it has been shown that it can occasionally produce non-unique solutions.
+As we have seen, LASSO is an effective algorithm for regularization with the $L_1$-norm. It also does a decent job of mimicking the $L_0$-""norm"", i.e., setting $\beta_i=0$ for certain variables $i$. However, it has been shown that it can occasionally produce non-unique solutions.
 
 <details>
 <summary> *Extra reading*</summary>
 
-The absolute value $|x|$ is not differentiable at $x=0$, so neither is the $L_1$-norm. This is a drawback in some Machine Learing approaches, e.g., those using so-called *gradient descent*.
+The absolute value $|x|$ is not differentiable at $x=0$, so neither is the $L_1$-norm. This is a drawback in some Machine Learning approaches, e.g., those using so-called *gradient descent*.
 
 ***
 
@@ -800,12 +800,12 @@ Geometrically, the $L_2$-norm is also the *Euclidean distance* from origo.
 
 We note that you already have been working with an $L_2-norm$: since $RSS = ||Y-X\beta||_2^2$ is simply the square of the $L_2$ norm of the residuals.
 
-For regularization, the $L_2$-norm is used in *ridge regression*. Computationally, regularization with an $L_2$-norm is even more efficient than with an $L_1$-norm. It works well to pevent over-fitting, but because it is bad at shrinking $\beta_i$'s all the way to zero, it is not so good for feature selection.
+For regularization, the $L_2$-norm is used in *ridge regression*. Computationally, regularization with an $L_2$-norm is even more efficient than with an $L_1$-norm. It works well to prevent over-fitting, but because it is bad at shrinking $\beta_i$'s all the way to zero, it is not so good for feature selection.
 
 <details>
 <summary> *Extra reading*</summary>
 
-The $L_2$-norm is differentiable at all values, making it usable in Machine Learing approaches using so-called *gradient descent*.
+The $L_2$-norm is differentiable at all values, making it usable in Machine Learning approaches using so-called *gradient descent*.
 
 ***
 

---FILE: session-regularization/02RegularizationExercises.Rmd---
@@ -3,7 +3,7 @@
 
 ## Data
 
-We will here use biological experimental data, more specifically a skeletal muscle gene expression subset (randomly sampled 1000 genes) from GTEX Human Tissue Gene Expression Consortium ([Lonsdale et al. 2013](https://www.nature.com/articles/ng.2653)). We will use the approaches we learned above to perform feature selection on this data with respect to a logicit regression analysis.
+We will here use biological experimental data, more specifically a skeletal muscle gene expression subset (randomly sampled 1000 genes) from GTEX Human Tissue Gene Expression Consortium ([Lonsdale et al. 2013](https://www.nature.com/articles/ng.2653)). We will use the approaches we learned above to perform feature selection on this data with respect to a logistic regression analysis.
 
 ### Task| `Load the GTEX muscle expression data`
 
@@ -45,11 +45,11 @@ datatable(X)
 </details>
 
 <br>
-The phenotype of interest that we address address is *Gender*, i.e. we will figure out which of the `r ncol(X)` genes expressed in human skeletal muscles drive the phenotypic difference between Males and Females. 
+The phenotype of interest that we address is *Gender*, i.e. we will figure out which of the `r ncol(X)` genes expressed in human skeletal muscles drive the phenotypic difference between Males and Females. 
 
 ### Task| `Load the GTEX Gender data`
 
-* Load the Gender data for the GTEX samples from the file `GTEX/GTEX_SkeletalMuscles_157Samples_Gender.txt` in to a variale `Y` (Hint: keep only the variable `Gender`)
+* Load the Gender data for the GTEX samples from the file `GTEX/GTEX_SkeletalMuscles_157Samples_Gender.txt` in to a variable `Y` (Hint: keep only the variable `Gender`)
 * Check that the the number of samples corresponds to `X` and how many women and men there are.
 
 ```{r, echo =T, eval=FALSE}
@@ -218,7 +218,7 @@ From a information theory perspective, the difference in $AIC$ between two model
 </details>
 
 
-$AIC$ and $BIC$ are implemented in the R base `stat` packages as `AIC(m)` and `BIC(m)`, respecitvely. In their simplest use, a `lm` or `glm` model is given as argument and the value of its $AIC$ and $BIC$, respectively, is returned.
+$AIC$ and $BIC$ are implemented in the R base `stat` packages as `AIC(m)` and `BIC(m)`, respectively. In their simplest use, a `lm` or `glm` model is given as argument and the value of its $AIC$ and $BIC$, respectively, is returned.
 Simple example use is:
 
 ```
@@ -232,7 +232,7 @@ We will first try to use these criteria to determine the best multivariate model
 
 
 * Perform separate univariate logistic regression ($logit(Y)\sim\beta X$) analyses for all genes of `X`
-    - Collect the Gene name, P-values and Odds ratios (or alternatively the coeficients) in a data.frame *univariateResults* (one row per gene)
+    - Collect the Gene name, P-values and Odds ratios (or alternatively the coefficients) in a data.frame *univariateResults* (one row per gene)
         - Hint: use `coef(summary(<your glm model>)` to extract P-values and coefficients
     - Also add a column with adjusted p-values to the data.frame
     - Optionally show the new data.frame in a `datatable`
@@ -415,7 +415,7 @@ The same approach as the one used in [the LASSO example](# Regularization | `LAS
 
 ### Task| `LASSO/Ridge Regression/Elastic Net`
 
-* Pick one approach among LSSSO, Ridge Regression or Elastic Net and run it  on `X` and `Y` with 100-fold cross-validation (for elastic net, select an arbitrary value of `alpha`)
+* Pick one approach among LASSO, Ridge Regression or Elastic Net and run it  on `X` and `Y` with 100-fold cross-validation (for elastic net, select an arbitrary value of `alpha`)
   - Identify the optimal $\lambda$
       - Optionally, plot the result of the cross-validation
       - Optionally, plot the traces of $\beta$ for the inclusion of variables in the model.
@@ -490,12 +490,12 @@ doAIC(X[,genes], Y)
 <summary> Some possible answers </summary>
 <h4>Some possible answers</h4>
 
-* The features selected by LASSO/RIdge Regression/Elastic Net includes the gene from the univariate approach, but also several others
+* The features selected by LASSO/Ridge Regression/Elastic Net includes the gene from the univariate approach, but also several others
   - This is often the case in practice 
   - Multiple test correction reduces the power of the univariate approach
 * The optimal LASSO model is different from those obtained from the various AIC/BIC approach (most likely:); moreover, the optimal LASSO model has a lower AIC/BIC than those investigated in the AIC/BIC approach (most likely)
 * The optimal Ridge Regression model may contain many variables, much more than LASSO, and maybe also than the the best AIC/BIC model. This is because Ridge Regression tend to give unimportant variables very low, but still non-zero $beta$ and is therefore, less effective for feature selection.
-* The optimal Elastic Net model will depend on the arbitrarily chosen values fo `alpha`, but will somewhere in between LASSO and Ridge Regression
+* The optimal Elastic Net model will depend on the arbitrarily chosen values of `alpha`, but will somewhere in between LASSO and Ridge Regression
   - In general, unless we test all models corresponding all possible combinatorial subsets of variables or apply some clever search algorithm (similar to the one LASSO uses), we are unlikely to find the optimal model using AIC/BIC.
      - Even then, we might get different answers, because the regularization applied in AIC/BIC and LASSO are different.
   - When applied to the data with features selected by LASSO, `doAIC` choses a model that includes all variables -- this is in line with the notion that LASSO approximates $L_0$-based regularization well."
NBISweden,workshop-mlbiostatistics,2de0338350fc6d673d7c3de258678828afaf6484,bsennblad,bengt.sennblad@scilifelab.se,2020-11-24T08:58:58Z,bsennblad,bengt.sennblad@scilifelab.se,2020-11-24T08:58:58Z,Correct errors found during lecture. Add a TODO.md file for next year,session-ann/TODO.md;session-ann/session-ann.Rmd;session-ann/session-ann.html;session-ann/session-ann_files/figure-html/hyperplanes1-1.png;session-ann/session-ann_files/figure-html/hyperplanes2-1.png;session-ann/session-ann_files/figure-html/logistic-1.png,True,False,True,False,82,460,542,"---FILE: session-ann/TODO.md---
@@ -1,4 +1,2 @@
 TODO for 2021:
-1. Remove Bayesian stuff
-2. Preferably Likelihood is introduced earlier in course
-3. Preferably cross-validation is introduced earlier in course
+1. Remove Back-propagation

---FILE: session-ann/session-ann.Rmd---
@@ -1190,7 +1190,8 @@ $f(t) = t^2$
 
 ---
 
-# Chain rule more complex example (optional)
+# Chain rule more complex example 
+## (optional)
 
 .pull-left[
 ##### A reminder
@@ -1478,7 +1479,7 @@ import numpy as np
 
 eta = 0.3
 
-dcda2 = y-a2
+dcda2 = a2-y
 da2dz2= a2*(1-a1)
 dcdz2 = dcda2 * da2dz2
 dz2da1 = w2
@@ -1579,11 +1580,11 @@ kable(tab2, booktabs = TRUE,
       escape = FALSE)
 ```
 --
-$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = (y-a_2)$$
+$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = -(y-a_2) = a_2 -y$$
 ---
 .pull-right[
 ### Backward pass 1
-$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = (y-a_2)$$
+$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = -(y-a_2) = a_2 -y$$
 
 ]
 

---FILE: session-ann/session-ann.html---
@@ -154,7 +154,7 @@
 
 It turns out that the perceptorn is a suboptimal for ANN learning. The situation can be substantially improved by relaxing the on/off output of the perceptron -- Enter the *sigmoid neuron*:
 .pull-left[
-&lt;img src=""session-ann_files/figure-html/sigmoid-1.png"" width=""100%"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""session-ann_files/figure-html/sigmoid-1.png"" width=""1152"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
 ].pull-right[
 
 - *Multiple input (on/off):* 
@@ -188,7 +188,7 @@
     `$$\sigma(z) = \frac{1}{1+e^{-z}}$$`
 ]
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/logistic-1.png"" width=""100%"" height=""300"" /&gt;
+&lt;img src=""session-ann_files/figure-html/logistic-1.png"" height=""300"" /&gt;
 
 ]
 
@@ -221,7 +221,7 @@
 
 
 .pull-left[
-&lt;img src=""session-ann_files/figure-html/sigmoidAnn-1.png"" width=""100%"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""session-ann_files/figure-html/sigmoidAnn-1.png"" width=""1152"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
 ]
 .pull-right[
 In a *feedforward* ANN, neurons are arranged into **layers**: 
@@ -311,7 +311,7 @@
 ]
 
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/hyperplanes1-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/hyperplanes1-1.png)&lt;!-- --&gt;
 ]
 
 ---
@@ -333,7 +333,7 @@
 ]
 '.pull-right[
 
-&lt;img src=""session-ann_files/figure-html/hyperplanes2-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/hyperplanes2-1.png)&lt;!-- --&gt;
 ]
 
 ---
@@ -347,15 +347,15 @@
 - `\(\tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}} = 2\sigma(2z) -1\)`
 ]
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/unnamed-chunk-1-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;
 ]
 .pull-left[
 #### *Rectified Linear Unit* (*ReLU*)
 - `\(relu(z) =\begin{cases}z &amp;\textrm{if } z &gt;0\\0 &amp;\textrm{otherwise}\end{cases}\)`
 
 ]
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/unnamed-chunk-2-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
 ]
 ---
 .pull-left[
@@ -370,7 +370,7 @@
 
 ]
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/unnamed-chunk-3-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
 {{content}}
 ]
 
@@ -473,7 +473,7 @@
 
 .pull-right[
 
-&lt;img src=""session-ann_files/figure-html/rss-1.png"" width=""100%"" height=""175"" /&gt;
+&lt;img src=""session-ann_files/figure-html/rss-1.png"" height=""175"" /&gt;
 
 ]
 
@@ -510,7 +510,7 @@
 ]
 
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/descent1-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/descent1-1.png)&lt;!-- --&gt;
 ]
 
 
@@ -533,7 +533,7 @@
 ]
 
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/descent3-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/descent3-1.png)&lt;!-- --&gt;
 ]
 
 ---
@@ -555,7 +555,7 @@
 ]
 
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/descent4-1.png"" width=""100%"" /&gt;
+![](session-ann_files/figure-html/descent4-1.png)&lt;!-- --&gt;
 ]
 
 ???
@@ -684,7 +684,8 @@
 
 ---
 
-# Chain rule more complex example (optional)
+# Chain rule more complex example 
+## (optional)
 
 .pull-left[
 ##### A reminder
@@ -781,59 +782,15 @@
 
 ---
 
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(x\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(y\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(z_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(a_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(z_2\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(a_2\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\hat{y}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(C(w,b\vert x)\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.1 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+|  `\(x\)`| `\(y\)`| `\(z_1\)`| `\(a_1\)`| `\(z_2\)`| `\(a_2\)`| `\(\hat{y}\)`| `\(C(w,b\vert x)\)`|
+|----:|---:|-----:|-----:|-----:|-----:|---------:|---------------:|
+| 0.05| 0.1|     0|     0|     0|     0|         0|               0|
 ---
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(x\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(y\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(z_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(a_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(z_2\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(a_2\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\hat{y}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(C(w,b\vert x)\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.1 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.095 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.48 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.44 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.13 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+|  `\(x\)`| `\(y\)`|  `\(z_1\)`| `\(a_1\)`| `\(z_2\)`| `\(a_2\)`| `\(\hat{y}\)`| `\(C(w,b\vert x)\)`|
+|----:|---:|------:|-----:|-----:|-----:|---------:|---------------:|
+| 0.05| 0.1| -0.095|  0.48|  0.44|  0.61|      0.61|            0.13|
 
 ---
 layout: false
@@ -845,32 +802,10 @@
 
 &lt;img src=""session-ann_files/figure-html/sigmoidAnn3b-1.png"" width=""600"" height=""300"" style=""display: block; margin: auto;"" /&gt;
 
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(x\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(y\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(z_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(a_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(z_2\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(a_2\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\hat{y}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(C(w,b\vert x)\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.1 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.095 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.48 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.44 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.13 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+|  `\(x\)`| `\(y\)`|  `\(z_1\)`| `\(a_1\)`| `\(z_2\)`| `\(a_2\)`| `\(\hat{y}\)`| `\(C(w,b\vert x)\)`|
+|----:|---:|------:|-----:|-----:|-----:|---------:|---------------:|
+| 0.05| 0.1| -0.095|  0.48|  0.44|  0.61|      0.61|            0.13|
 
 
 
@@ -884,111 +819,33 @@
 
 ]
 
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|
 --
-`$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = (y-a_2)$$`
+`$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = -(y-a_2) = a_2 -y$$`
 ---
 .pull-right[
 ### Backward pass 1
-`$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = (y-a_2)$$`
+`$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = -(y-a_2) = a_2 -y$$`
 
 ]
 
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                          0.51|
 
 ---
 .pull-right[
 ### Backward pass 2
 {{content}}
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                          0.51|
 --
 `$$\frac{\partial a_2}{\partial z_2} =  \frac{\partial \sigma(z)}{\partial a} = \sigma(z)(1-\sigma(z))$$`
 
@@ -1002,72 +859,20 @@
 `$$\frac{\partial C(w,b|x)}{\partial z_2} =  \frac{\partial a_2}{\partial z_2} \times \frac{\partial C(w,b|x)}{\partial a_2}$$`
 
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                        0.1623|                              0.3189|                                          0.51|
 
 ---
 .pull-right[
 ### Backward pass 3
 {{content}}
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                        0.1623|                              0.3189|                                          0.51|
 --
 `$$\frac{\partial z_2}{\partial a_1} =  \frac{\partial w_2*a_1+b_1}{\partial a} = w_2$$`
 
@@ -1080,72 +885,20 @@
 
 `$$\frac{\partial C(w,b|x)}{\partial a_1} =  \frac{\partial z_2}{\partial a_1} \times \frac{\partial C(w,b|x)}{\partial z_2}$$`
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                             0|                                   0|                                       0.04869|                                 0.3|                                        0.1623|                              0.3189|                                          0.51|
 
 ---
 .pull-right[
 ### Backward pass 4
 {{content}}
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                             0|                                   0|                                       0.04869|                                 0.3|                                        0.1623|                              0.3189|                                          0.51|
 
 --
 `$$\frac{\partial a_1}{\partial z_1} =  \frac{\partial \sigma(z_1)}{\partial a_1} = \sigma(z_1)(1-\sigma(z_1))$$`
@@ -1159,72 +912,20 @@
 
 `$$\frac{\partial C(w,b|x)}{\partial z_1} =  \frac{\partial a_1}{\partial z_1} \times \frac{\partial C(w,b|x)}{\partial a_1}$$`
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                       0.04869|                              0.2494|                                       0.04869|                                 0.3|                                        0.1623|                              0.3189|                                          0.51|
 
 ---
 .pull-right[
 ### Backward pass 5
 {{content}}
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                             0|                                   0|                                       0.04869|                              0.2494|                                       0.04869|                                 0.3|                                        0.1623|                              0.3189|                                          0.51|
 
 --
 `$$\frac{\partial z_1}{\partial w_1} =  \frac{\partial w_1a_1+b_1}{\partial a_1} = a_1$$`
@@ -1239,36 +940,10 @@
 
 `$$\frac{\partial C(w_1,b|x)}{\partial z_1} =  \frac{\partial z_1}{\partial w_1} \times \frac{\partial C(w_1,b|x)}{\partial z_1}$$`
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.002435 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                      0.002435|                                0.05|                                       0.04869|                              0.2494|                                       0.04869|                                 0.3|                                        0.1623|                              0.3189|                                          0.51|
 
 ---
 .pull-right[
@@ -1280,36 +955,10 @@
 
 `$$w'_1 = w_1 - \eta \frac{\partial C(w_1,b|x)}{\partial z_1}$$`
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.002435 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+|      0|                                      0.002435|                                0.05|                                       0.04869|                              0.2494|                                       0.04869|                                 0.3|                                        0.1623|                              0.3189|                                          0.51|
 
 ---
 .pull-right[
@@ -1321,36 +970,10 @@
 
 `$$w'_1 = w_1 - \eta \frac{\partial C(w_1,b|x)}{\partial z_1}$$`
 ]
-&lt;table&gt;
- &lt;thead&gt;
-  &lt;tr&gt;
-   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
-   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
-  &lt;/tr&gt;
- &lt;/thead&gt;
-&lt;tbody&gt;
-  &lt;tr&gt;
-   &lt;td style=""text-align:right;""&gt; 0.1007 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.002435 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
-   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
-  &lt;/tr&gt;
-&lt;/tbody&gt;
-&lt;/table&gt;
+
+|  `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
+|-------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
+| 0.09927|                                      0.002435|                                0.05|                                       0.04869|                              0.2494|                                       0.04869|                                 0.3|                                        0.1623|                              0.3189|                                          0.51|
 ---
 layout: false
 # Back-propagation"
NBISweden,workshop-mlbiostatistics,54a6dfa5e96ad77d8cf01a51debeb5cfecd3a8cc,bsennblad,bengt.sennblad@scilifelab.se,2020-11-24T08:58:12Z,bsennblad,bengt.sennblad@scilifelab.se,2020-11-24T08:58:12Z,Correct errors found during lecture. Add a TODO.md file for next year,session-regularization/TODO.md;session-regularization/session-regularization.Rmd;session-regularization/session-regularization.html,True,False,True,False,117,88,205,"---FILE: session-regularization/TODO.md---
@@ -0,0 +1,4 @@
+TODO for 2021:
+1. Remove Bayesian stuff
+2. Preferably Likelihood is introduced earlier in course
+3. Preferably cross-validation is introduced earlier in course

---FILE: session-regularization/session-regularization.Rmd---
@@ -172,10 +172,10 @@ In general, full-on likelihood computation and maximum likelihood estimation is
 Bayes' theorem (Thomas Bayes, 1702-1761) provides a way to obtain the requested $P[\theta|X,Y]$
 
 
-$$Pr[\theta|D] = \frac{Pr[D| \theta]Pr[\theta]}{Pr[D]}$$
+$$Pr[\theta|X,Y] = \frac{Pr[Y| X, \theta]Pr[\theta]}{Pr[Y|X]}$$
 **Posterior probability**
 
-$Pr[\theta|D],$ the probability, computed posterior to analysis, of the parameters $\theta$ conditioned on the observed data, i.e, our requested probability.
+$Pr[\theta|X,Y],$ the probability, computed posterior to analysis, of the parameters $\theta$ conditioned on the observed data, i.e, our requested probability.
 
 One important characteristic of Bayesian statistics is that, typically, the focus is not on point estimates, but on the posterior probability distribution over the parameter space of $\theta$, which provides a measure of uncertainty (probabilities) in comparison to other values.
 
@@ -199,7 +199,7 @@ It can be shown that the effect of the prior on the posterior probbility is larg
 
 **Marginal Probability of $D$**
 
-$Pr[D]=\int_{\theta}Pr[D| \theta]Pr[\theta]$ is the probability of $D$ regardless of $\theta$. This can often be difficult difficult to calculate and, for this reason, Bayesian models are often designed so that this can be calculated analytically or using some approximation approach, such as Markov chain Monte Carlo (MCMC) is used.
+$Pr[Y|X]=\int_{\theta}Pr[Y| X, \theta]Pr[\theta]$ is the probability of $Y|X$ regardless of $\theta$. This can often be difficult difficult to calculate and, for this reason, Bayesian models are often designed so that this can be calculated analytically or using some approximation approach, such as Markov chain Monte Carlo (MCMC) is used.
 
 <details>
 <summary> <span style=""color:gray"">Extra Reading</span> </summary>
@@ -277,6 +277,8 @@ N=100 # number of samples
 P=10 # number of variables
 
 # Draw variables, x_{i,1},...,x_{i,P} for all N individuals, from a uniform distribution in interval (0,1) (this is the default interval for runif)
+# 1. runif generates N*P+1 values, round them off to 2 decimals
+# 2. Put values into a matrix with N rows and P columns
 X=matrix(round(runif(N*(P+1),min=0, max=2)), nrow=N, ncol=P)
 
 # generate a y variable from a multivarite lm of 3 first X variables only
@@ -285,7 +287,7 @@ b0=3
 # effect sizes for first three variables
 b=c(runif(3, min=0.5, max=1.0))
 
-# generate y
+# generate y for all samples (=rows in X) using only the 3 first variables (columns) of X
 Y <- b0 + X[,1] * b[1] + X[,2] * b[2] + X[,3] * b[3] + rnorm(N)
 
 ```
@@ -324,10 +326,10 @@ What are the max Likelihood estimates of the two models? (we can use the R funct
 library(stats)
 ll= vector()
 for(i in seq(1,2)){
-  Xi=X[,seq(1,i)]
-  ll[i] <- logLik(lm(Y~Xi))
+  Xi=X[,seq(1,i)] # use variables 1..i
+  ll[i] <- logLik(lm(Y~Xi)) # logLik extract loglikelihood from lm 
 }
-# plot likelihoods for models with 1 and 2 vaiables
+# plot likelihoods for models with 1 and 2 variables
 plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 # xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
 ```
@@ -457,28 +459,26 @@ In our simple test case, the LRT also succeed in picking the correct model. It s
 
 Regularization is a concept that adds auxiliary criteria, so-called *regularization terms*,  to probabilistic models.  This is called regularized likelihood models or penalized likelihood models. Typically, the regularization term is a function of parameters $\beta$:
 
-$$\log rL[\beta | X, Y]  = \log Pr[Y | X, \beta] - f(\beta),$$
+$$\log rL[\beta | X, Y]  = \log L[\beta | X, Y] - f(\beta),$$
 
 
 A very simple regularized likelihood model uses $f(\beta) = \#\beta = \#X$, that is the number of $X$ variables.  
-$$\log rL[{\beta} | X, Y]  = \log Pr[Y | X, {\beta}] - \#X, $$
+$$\log rL[{\beta} | X, Y]  = \log L[\beta | X, Y] - \#X, $$
 
 
 Applying this rL to our example data, solves the overfitting problem.
 
 ```{r,echo=F, fig.height=4, echo=TRUE}
 # compute loglikelihood (ll) for all models including 1-P variables
-pl= vector() 
+rl= vector() 
 for(i in seq(1,P)){
-  xi=X[,seq(1,i)]
-  xi=cbind(rep(1,N), xi)
+  xi=X[,seq(1,i), drop=FALSE]
   fit = lm(Y~xi)
-  # To make the code simple, we forestall next step and use the AIC function here
-  # AIC= -2(pl) so convert back
-  pl[i] = -AIC(fit)/2
+  # Compute the regularized Likelihood
+  rl[i] = logLik(fit) - i
 }
 # plot ll of all models
-plot(pl, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""log pL"", xlab=""model #"", type = ""b"")
+plot(rl, xlim=c(1,P), ylim=c(floor(min(rl)),ceiling(max(rl))),ylab=""log rL"", xlab=""model #"", type = ""b"")
 ```  
 
 </details>
@@ -489,12 +489,12 @@ plot(pl, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""log pL"", xla
 
 Regularization is a canonical example where Bayesian and frequentist statistics meet.
 
-The standard way of writing a regularized likelihood is using the logLikelihood, but what if 'de-log' it:
+The standard way of writing a regularized likelihood is using the logLikelihood, but what if we 'de-log' it:
 
 \begin{eqnarray*}
-\log rL[\beta | X, Y]  &=& \log Pr[Y | X, \beta] - f(\beta) \\
-\Downarrow\\
-rL[\beta | X, Y]  &=& Pr[Y | X, \beta] * e^{- f(\beta)}
+\log rL[\beta | X, Y]  &= \log L[\beta | X, Y] - f(\beta) &=\log Pr[Y | X, \beta] - f(\beta)  \\
+&\Downarrow\\
+rL[\beta | X, Y]  &=  L[\beta | X, Y]\times e^{-f(\beta)}&= Pr[Y | X, \beta] \times e^{- f(\beta)}
 \end{eqnarray*}
 
 This looks suspiciously like an un-normalized posterior probability (i.e., lacking the denominator), with an exponential prior $Pr[\beta]=e^{-f(\beta)}.$
@@ -563,7 +563,7 @@ library(dplyr)      # used for nice table formatting
 library(kableExtra) # used for nice table formatting
 
 mprev <- lm(Y ~ X[,1]) # current miminimum AIC model
-# dummyentry to be replaced
+# dummy entry to be replaced
 aic=data.frame(models=0, aic=0, isAICmin=""-"") 
 
 for(i in seq(1,P)){
@@ -573,13 +573,16 @@ for(i in seq(1,P)){
   if(i==2){ #include also the first model
     aic[i-1,] = list(paste0(i-1,"" variable""), signif(fit$AIC[1],5), ""-"") 
   }
-  aic[i,] = list(paste0(i,"" variables""), signif(fit$AIC[2],5), ""-"") 
+  aic[i,] = list(paste0(i,"" variables""), signif(fit$AIC[2],5), ""-"")
 }
+# Identify the model with min AIC and compare all others to that
 minaic=min(aic$aic)
 aic$relL=format(exp((minaic-aic$aic)/2), digits=4)
 aic$isAICmin = ifelse(aic$aic==minaic,""Yes"",""-"")
 
-kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Minimum AIC"",""relL""),digits=30,format.args=list(snsmall=0))  %>%  kable_styling( font_size = 14)
+kable(aic, format='html', row.names=F, 
+      col.names=c(""Compared models"",""AIC"",""Minimum AIC"",""relL""))%>%
+  kable_styling( font_size = 14)
 ```
 
 <details>
@@ -603,7 +606,9 @@ minaic=min(aic$aic)
 aic$rl=format(exp((minaic-aic$aic)/2), digits=4)
 aic$lowest = ifelse(aic$aic==minaic,""Yes"",""-"")
 
-kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Minimum AIC"",""rL""),digits=30,format.args=list(snsmall=0))  %>%  kable_styling( font_size = 14)
+kable(aic, format='html', row.names=F, 
+      col.names=c(""Compared models"",""AIC"",""Minimum AIC"",""relL""))%>% 
+  kable_styling( font_size = 14)
 ```
 
 ***
@@ -615,10 +620,13 @@ kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Mini
 library(stats)
 
 # plot AIC of all models
-plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab=""AIC"", xlab=""model #"", type = ""b"")
+plot(aic$aic, xlim=c(1,P), 
+     ylim=c(floor(min(aic$aic)), ceiling(max(aic$aic))),
+     ylab=""AIC"", xlab=""model #"", type = ""b"")
 
 # plot relL of all models
-plot(aic$relL, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
+plot(aic$relL, xlim=c(1,P), 
+     ylab=""relL"", xlab=""model #"", type = ""b"")
 ```  
 
 <details>
@@ -627,10 +635,13 @@ plot(aic$relL, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
 library(stats)
 
 # plot AIC of all models
-plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab=""AIC"", xlab=""model #"", type = ""b"")
+plot(aic$aic, xlim=c(1,P), 
+     ylim=c(floor(min(aic$aic)), ceiling(max(aic$aic))),
+     ylab=""AIC"", xlab=""model #"", type = ""b"")
 
 # plot relL of all models
-plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
+plot(aic$rl, xlim=c(1,P), 
+     ylab=""relL"", xlab=""model #"", type = ""b"")
 ```  
 
 ***
@@ -683,7 +694,7 @@ Extensions to glms exists, but then using a regularized likelihood expression
 ***
 </details>
 
-The regularization term $f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i-0|= \lambda\sum_{\beta_i\in\beta} |\beta_i|$
+The regularization term $f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i|$
 
 <details>
 <summary> <span style=""color:gray"">Extra Reading</span> </summary>
@@ -700,12 +711,14 @@ We note, BTW, that you already have been working with an $\ell_2-norm$: since $R
 ***
 </details>  
     
-The $\lambda$ parameter sets a limit on the estimation of $\beta$. 
+The $\lambda$ parameter defines how strong the regularization becomes; a higher $\lambda$ will put a stricter limit on the estimation of individual $\beta_i$ values. 
 
 
 Lasso is traditionally described as RSS with an auxiliary criterion/constraint: 
 
-$$min\left\{RSS\right\} + \lambda\sum_{\beta_i\in\beta} |\beta_i|.$$
+$$min\left\{RSS\right\} + \lambda\sum_{\beta_i\in\beta} |\beta_i|,$$
+but, equivalently, it can be written in terms of the likelihood:
+$$max \log L[\beta|X,Y] - \lambda\sum_{\beta_i\in\beta} |\beta_i|.$$
 Lasso can also be viewed as a un-normalized Bayesian posterior probability, with a LaPlacean prior on $\beta$: $\beta_j ∼ LaPlace(0, 1/\lambda)$ 
 
 <details>
@@ -715,7 +728,6 @@ Other common notation for LASSO:
 
 * You might often see the notation $$min_{{\beta}}\left\{RSS\right\} \textrm{ subject to } ||{\beta}||_1 <= t$$
   where $t$ is related to $\lambda$.
-* Lasso can also be viewed as a Bayesian posterior probability, with a LaPlacean prior on $\beta$: $\beta_j ∼ LaPlace(0, 1/\lambda)$ 
 
 ***
 </details>
@@ -740,11 +752,12 @@ The *Coordinate descent* algorithm is used in the R package `glmnet`:
 <details>
 <summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
-Alternatives to LASSO, differing mainly in the auxiliary criterion
+Alternatives to LASSO, differing mainly in the auxiliary criterion, include
 
   - *Ridge regression* which uses a $\ell_2$ norm
-  - *Elastic-net*, which uses a mixed model combination of the  $\ell_1$ norm and the $\ell_2$ norm.
+  - *Elastic-net*, which uses a mixed model combination of the  $\ell_1$ norm and the $\ell_2$ norm
 
+ (both these are implemented in the R package `glmnet`, which we will use below).
 *** 
 </details>
 
@@ -776,7 +789,7 @@ library(glmnet)
 fit = glmnet(X,Y, family=""gaussian"", alpha=1, standardize=T)
 ```
 
-* A graphical way to view the result is to `plot` the paths of $\beta$ for increasing vaules of $\lambda$.
+* A graphical way to view the result is to `plot` the paths of $\beta$ for increasing vaules of $\lambda$. This plot shows how the $\beta_i$ for different variables changes with $\lambda$. The plot is perhaps best read from right to left, going from higher and thereby stricter, $\lambda$ values to lower $\lambda$ values including more and more variables/non-zero $\beta_i$.
 
 ```{r, echo=T, fig.height=5, eval=FALSE} 
 par(mfrow=c(1,1))
@@ -806,7 +819,7 @@ plot(fit, xvar=""lambda"",label=T)
 <h4>Some possible answers</h4>
 * The order appears to be $(1,2,3,7,6,5,10,9,4,8)$
 * $\beta_i > 0, i\in \{1,2,3,4,7,9\}$, while $\beta_i<0, i\in \{5,6,8,10\}$
-* Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-2.1), \approx\exp(-2.5)]$
+* Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-1), \approx\exp(-2.5)]$
 * In the normal case, we do not have *oracle knowledge*.
 
 ***
@@ -834,12 +847,12 @@ The LASSO model will be different depending on how we set $\lambda$. A problem i
 The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, $MSE$ ($=RSS/N$).
 Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO application, this means that we want to select the $\lambda$ that minimizes the $MSE$
 
-In cross validation, this approach is emaulated by partioning the data at hand into a *training* and  *test* (or *validation*) data set. The model parameters are estimated ('trained') on the the training data and the validated on the test data.
+In cross validation, this approach is emulated by partioning the data at hand into a *training* and  *validation* data set. The model parameters are estimated ('trained') on the the training data and the validated on the validation data. (Optionally, a *test* partition can be assigned in cross-validation on which the final, selected model is evaluated; this is not employed here).
 
 By chance, this may fail if the partitioning is 'non-representative'. A solution is to repeat the cross-validation procedure with another partioning.
 
 In $k$-fold cross validation, the original data is split into $k$ sub-datasets $\{D_1,D_2,\ldots, D_k\}$.
-For $i \in \{1,2,\ldots, k\}$, set $D_i$ as the test data set and the union of the other datasets be the training data. Perform cross validation as above.
+For $i \in \{1,2,\ldots, k\}$, set $D_i$ as the validation data set and the union of the other datasets be the training data. Perform cross validation as above.
 
 This gives a distribution of $MSE$ from which we can estimate, e.g., mean and standard deviation.
 
@@ -856,10 +869,10 @@ Here we will limit ourselves to finding the minimum $\lambda$, called `lambda.mi
 </details>
 
 ### Task | `Determine optimal LASSO `$\lambda$` using cross-validation`
-* Use the function `cv.glmnet` to perform cross validation (same options as for `glmnet`)
+* Use the function `cv.glmnet` to perform cross validation (same options as for `glmnet`), store it in a R variable, e.g., `cvglm`,
 * `plot` the cross-validation results 
-* Compare with the plot of estimated $\beta_i$ under different $\lambda$.
-* Determine the optimal $\lambda$ (the one with minimal error)
+* Compare with the plot of estimated $\beta_i$ under different $\lambda$ (these can be accessed from the result as `cvglm$glmnet.fix`).
+* Determine the optimal $\lambda$ (the one with minimal error, can be found in `cvglm$lambda.min`) 
 
 ```{r, echo=T,fig.height=5, eval=FALSE}
 library(glmnet)
@@ -870,6 +883,7 @@ cvglm=cv.glmnet(X,Y, family=""gaussian"", alpha=1, standardize=T, nfolds=100)
 plot(cvglm)
 plot(cvglm$glmnet.fit, xvar=""lambda"",label=T)
 minlambda=cvglm$lambda.min
+print(minlambda)
 ```
 
 <details>
@@ -883,6 +897,7 @@ cvglm=cv.glmnet(X,Y, family=""gaussian"", alpha=1, standardize=T, nfolds=100)
 plot(cvglm)
 plot(cvglm$glmnet.fit, xvar=""lambda"",label=T)
 minlambda=cvglm$lambda.min
+print(minlambda)
 ```
 
 ***
@@ -903,7 +918,7 @@ minlambda=cvglm$lambda.min
 </details>
 
 ### Task| `Final LASSO effect sizes`
-* Finally print a table with the $\beta$ coefficients (including the intercept, $\beta_0$) for the optimal model (i.e.,  at minimum $\lambda$); compare with oracle knowledge. (Hint: Use function`coef`).
+* Finally print a table with the $\beta$ coefficients (including the intercept, $\beta_0$) for the optimal model (i.e.,  at minimum $\lambda$); compare with oracle knowledge. (Hint: see `? coef.cv.glmnet`).
 
 ```{r, echo =T, eval=FALSE}
 # Actually the following suffice for output on console"
NBISweden,workshop-mlbiostatistics,72137c9ba848bd90e577b3aa9ddc23343b0e0e2f,bsennblad,bengt.sennblad@scilifelab.se,2020-11-24T08:43:55Z,bsennblad,bengt.sennblad@scilifelab.se,2020-11-24T08:43:55Z,Correct errors found during lecture. Add a TODO.md file for next year,session-ann/TODO.md;session-ann/session-ann.html;session-ann/session-ann_files/figure-html/hyperplanes1-1.png;session-ann/session-ann_files/figure-html/hyperplanes2-1.png;session-ann/session-ann_files/figure-html/logistic-1.png,False,False,False,False,454,72,526,"---FILE: session-ann/TODO.md---
@@ -0,0 +1,4 @@
+TODO for 2021:
+1. Remove Bayesian stuff
+2. Preferably Likelihood is introduced earlier in course
+3. Preferably cross-validation is introduced earlier in course

---FILE: session-ann/session-ann.html---
@@ -154,7 +154,7 @@
 
 It turns out that the perceptorn is a suboptimal for ANN learning. The situation can be substantially improved by relaxing the on/off output of the perceptron -- Enter the *sigmoid neuron*:
 .pull-left[
-&lt;img src=""session-ann_files/figure-html/sigmoid-1.png"" width=""1152"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""session-ann_files/figure-html/sigmoid-1.png"" width=""100%"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
 ].pull-right[
 
 - *Multiple input (on/off):* 
@@ -188,7 +188,7 @@
     `$$\sigma(z) = \frac{1}{1+e^{-z}}$$`
 ]
 .pull-right[
-&lt;img src=""session-ann_files/figure-html/logistic-1.png"" height=""300"" /&gt;
+&lt;img src=""session-ann_files/figure-html/logistic-1.png"" width=""100%"" height=""300"" /&gt;
 
 ]
 
@@ -221,7 +221,7 @@
 
 
 .pull-left[
-&lt;img src=""session-ann_files/figure-html/sigmoidAnn-1.png"" width=""1152"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
+&lt;img src=""session-ann_files/figure-html/sigmoidAnn-1.png"" width=""100%"" height=""100%"" style=""display: block; margin: auto;"" /&gt;
 ]
 .pull-right[
 In a *feedforward* ANN, neurons are arranged into **layers**: 
@@ -311,7 +311,7 @@
 ]
 
 .pull-right[
-![](session-ann_files/figure-html/hyperplanes1-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/hyperplanes1-1.png"" width=""100%"" /&gt;
 ]
 
 ---
@@ -333,7 +333,7 @@
 ]
 '.pull-right[
 
-![](session-ann_files/figure-html/hyperplanes2-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/hyperplanes2-1.png"" width=""100%"" /&gt;
 ]
 
 ---
@@ -347,15 +347,15 @@
 - `\(\tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}} = 2\sigma(2z) -1\)`
 ]
 .pull-right[
-![](session-ann_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/unnamed-chunk-1-1.png"" width=""100%"" /&gt;
 ]
 .pull-left[
 #### *Rectified Linear Unit* (*ReLU*)
 - `\(relu(z) =\begin{cases}z &amp;\textrm{if } z &gt;0\\0 &amp;\textrm{otherwise}\end{cases}\)`
 
 ]
 .pull-right[
-![](session-ann_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/unnamed-chunk-2-1.png"" width=""100%"" /&gt;
 ]
 ---
 .pull-left[
@@ -370,7 +370,7 @@
 
 ]
 .pull-right[
-![](session-ann_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/unnamed-chunk-3-1.png"" width=""100%"" /&gt;
 {{content}}
 ]
 
@@ -473,7 +473,7 @@
 
 .pull-right[
 
-&lt;img src=""session-ann_files/figure-html/rss-1.png"" height=""175"" /&gt;
+&lt;img src=""session-ann_files/figure-html/rss-1.png"" width=""100%"" height=""175"" /&gt;
 
 ]
 
@@ -510,7 +510,7 @@
 ]
 
 .pull-right[
-![](session-ann_files/figure-html/descent1-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/descent1-1.png"" width=""100%"" /&gt;
 ]
 
 
@@ -533,7 +533,7 @@
 ]
 
 .pull-right[
-![](session-ann_files/figure-html/descent3-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/descent3-1.png"" width=""100%"" /&gt;
 ]
 
 ---
@@ -555,7 +555,7 @@
 ]
 
 .pull-right[
-![](session-ann_files/figure-html/descent4-1.png)&lt;!-- --&gt;
+&lt;img src=""session-ann_files/figure-html/descent4-1.png"" width=""100%"" /&gt;
 ]
 
 ???
@@ -781,15 +781,59 @@
 
 ---
 
-
-|  `\(x\)`| `\(y\)`| `\(z_1\)`| `\(a_1\)`| `\(z_2\)`| `\(a_2\)`| `\(\hat{y}\)`| `\(C(w,b\vert x)\)`|
-|----:|---:|-----:|-----:|-----:|-----:|---------:|---------------:|
-| 0.05| 0.1|     0|     0|     0|     0|         0|               0|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(x\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(y\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(z_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(a_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(z_2\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(a_2\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\hat{y}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(C(w,b\vert x)\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.1 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 ---
-
-|  `\(x\)`| `\(y\)`|  `\(z_1\)`| `\(a_1\)`| `\(z_2\)`| `\(a_2\)`| `\(\hat{y}\)`| `\(C(w,b\vert x)\)`|
-|----:|---:|------:|-----:|-----:|-----:|---------:|---------------:|
-| 0.05| 0.1| -0.095|  0.48|  0.44|  0.61|      0.61|            0.13|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(x\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(y\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(z_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(a_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(z_2\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(a_2\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\hat{y}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(C(w,b\vert x)\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.1 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.095 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.48 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.44 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.13 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 ---
 layout: false
@@ -801,10 +845,32 @@
 
 &lt;img src=""session-ann_files/figure-html/sigmoidAnn3b-1.png"" width=""600"" height=""300"" style=""display: block; margin: auto;"" /&gt;
 
-
-|  `\(x\)`| `\(y\)`|  `\(z_1\)`| `\(a_1\)`| `\(z_2\)`| `\(a_2\)`| `\(\hat{y}\)`| `\(C(w,b\vert x)\)`|
-|----:|---:|------:|-----:|-----:|-----:|---------:|---------------:|
-| 0.05| 0.1| -0.095|  0.48|  0.44|  0.61|      0.61|            0.13|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(x\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(y\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(z_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(a_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(z_2\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(a_2\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\hat{y}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(C(w,b\vert x)\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.1 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.095 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.48 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.44 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.61 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.13 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 
 
@@ -818,10 +884,36 @@
 
 ]
 
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 --
 `$$\frac{\partial C(w,b|x)}{\partial a_2} =  \frac{\partial \frac{1}{2}(y-a_2)^2}{\partial a_2} = (y-a_2)$$`
 ---
@@ -831,20 +923,72 @@
 
 ]
 
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 ---
 .pull-right[
 ### Backward pass 2
 {{content}}
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 --
 `$$\frac{\partial a_2}{\partial z_2} =  \frac{\partial \sigma(z)}{\partial a} = \sigma(z)(1-\sigma(z))$$`
 
@@ -858,20 +1002,72 @@
 `$$\frac{\partial C(w,b|x)}{\partial z_2} =  \frac{\partial a_2}{\partial z_2} \times \frac{\partial C(w,b|x)}{\partial a_2}$$`
 
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 ---
 .pull-right[
 ### Backward pass 3
 {{content}}
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                             0|                                   0|                                             0|                                   0|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 --
 `$$\frac{\partial z_2}{\partial a_1} =  \frac{\partial w_2*a_1+b_1}{\partial a} = w_2$$`
 
@@ -884,20 +1080,72 @@
 
 `$$\frac{\partial C(w,b|x)}{\partial a_1} =  \frac{\partial z_2}{\partial a_1} \times \frac{\partial C(w,b|x)}{\partial z_2}$$`
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                             0|                                   0|                                      -0.04869|                                 0.3|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 ---
 .pull-right[
 ### Backward pass 4
 {{content}}
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                             0|                                   0|                                      -0.04869|                                 0.3|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 --
 `$$\frac{\partial a_1}{\partial z_1} =  \frac{\partial \sigma(z_1)}{\partial a_1} = \sigma(z_1)(1-\sigma(z_1))$$`
@@ -911,20 +1159,72 @@
 
 `$$\frac{\partial C(w,b|x)}{\partial z_1} =  \frac{\partial a_1}{\partial z_1} \times \frac{\partial C(w,b|x)}{\partial a_1}$$`
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                      -0.04869|                              0.2494|                                      -0.04869|                                 0.3|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 ---
 .pull-right[
 ### Backward pass 5
 {{content}}
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                             0|                                   0|                                      -0.04869|                              0.2494|                                      -0.04869|                                 0.3|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 --
 `$$\frac{\partial z_1}{\partial w_1} =  \frac{\partial w_1a_1+b_1}{\partial a_1} = a_1$$`
@@ -939,10 +1239,36 @@
 
 `$$\frac{\partial C(w_1,b|x)}{\partial z_1} =  \frac{\partial z_1}{\partial w_1} \times \frac{\partial C(w_1,b|x)}{\partial z_1}$$`
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                     -0.002435|                                0.05|                                      -0.04869|                              0.2494|                                      -0.04869|                                 0.3|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.002435 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 ---
 .pull-right[
@@ -954,10 +1280,36 @@
 
 `$$w'_1 = w_1 - \eta \frac{\partial C(w_1,b|x)}{\partial z_1}$$`
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-|      0|                                     -0.002435|                                0.05|                                      -0.04869|                              0.2494|                                      -0.04869|                                 0.3|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.002435 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 
 ---
 .pull-right[
@@ -969,10 +1321,36 @@
 
 `$$w'_1 = w_1 - \eta \frac{\partial C(w_1,b|x)}{\partial z_1}$$`
 ]
-
-| `\(w'_1\)`| `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)`| `\(\frac{\partial z_1}{\partial w_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)`| `\(\frac{\partial a_1}{\partial z_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)`| `\(\frac{\partial z_2}{\partial a_1}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)`| `\(\frac{\partial a_2}{\partial z_2}\)`| `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)`|
-|------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|-----------------------------------:|---------------------------------------------:|
-| 0.1007|                                     -0.002435|                                0.05|                                      -0.04869|                              0.2494|                                      -0.04869|                                 0.3|                                       -0.1623|                              0.3189|                                         -0.51|
+&lt;table&gt;
+ &lt;thead&gt;
+  &lt;tr&gt;
+   &lt;th style=""text-align:right;""&gt; `\(w'_1\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_1}{\partial w_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_1}{\partial z_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial z_2}{\partial a_1}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial a_2}{\partial z_2}\)` &lt;/th&gt;
+   &lt;th style=""text-align:right;""&gt; `\(\frac{\partial C(w,b\vert x)}{\partial a_2}\)` &lt;/th&gt;
+  &lt;/tr&gt;
+ &lt;/thead&gt;
+&lt;tbody&gt;
+  &lt;tr&gt;
+   &lt;td style=""text-align:right;""&gt; 0.1007 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.002435 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.05 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.2494 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.04869 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.1623 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; 0.3189 &lt;/td&gt;
+   &lt;td style=""text-align:right;""&gt; -0.51 &lt;/td&gt;
+  &lt;/tr&gt;
+&lt;/tbody&gt;
+&lt;/table&gt;
 ---
 layout: false
 # Back-propagation"
NBISweden,workshop-mlbiostatistics,53f7d78c24b295fb536ef6665669441df8142610,evaf,eva.freyhult@nbis.se,2020-11-19T07:18:24Z,evaf,eva.freyhult@nbis.se,2020-11-19T07:18:24Z,Fix typos,sessions-probdescinfe/04clustering.Rmd;sessions-probdescinfe/session-clustering.html,True,False,True,False,4,4,8,"---FILE: sessions-probdescinfe/04clustering.Rmd---
@@ -121,7 +121,7 @@ $$d_{cl}(A, B) = \max_{i,j} d(a_i, b_j)$$
 
 #### Average linkage {-}
 
-Average linkage takes as a cluster dissimilarity the distance between the two closest objects in the two clusters.
+Average linkage takes as a cluster dissimilarity the average distance between the objects in the in the two clusters.
 
 $$d_{al}(A, B) = \frac{1}{n_A n_B}\sum_i\sum_j d(a_i, b_j)$$
 
@@ -134,7 +134,7 @@ Ward's linkage method minimize the within variance, by merging clusters with the
 $$d_{wl}(A, B) = \sum_{i=1}^{n_A} (a_i - m_{A \cup B})^2 + \sum_{i=1}^{n_B} (b_i - m_{A \cup B})^2 - \sum_{i=1}^{n_A} (a_i - m_{A})^2 - \sum_{i=1}^{n_B} (b_i - m_{B})^2$$
 where $m_A, m_B, m_{A \cup B}$ are the center of the clusters $A$, $B$ and $A \cup B$, respectively. 
 
-Note that Ward's linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function `hclus` either the Euclidean or squared Euclidean distance can be used in combination with the linkage `method='ward.D'` or `method='ward.D2`, respectively.
+Note that Ward's linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function `hclust` either the Euclidean or squared Euclidean distance can be used in combination with the linkage `method='ward.D'` or `method='ward.D2`, respectively.
 
 ```{r dendromulti, fig.cap=""Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods."", fig.show=""hold"", fig.width=7, fig.height=3.5}
 require(ggdendro)

---FILE: sessions-probdescinfe/session-clustering.html---
@@ -318,7 +318,7 @@ <h4>Complete linkage</h4>
 </div>
 <div id=""average-linkage"" class=""section level4 unnumbered"">
 <h4>Average linkage</h4>
-<p>Average linkage takes as a cluster dissimilarity the distance between the two closest objects in the two clusters.</p>
+<p>Average linkage takes as a cluster dissimilarity the average distance between the objects in the in the two clusters.</p>
 <p><span class=""math display"">\[d_{al}(A, B) = \frac{1}{n_A n_B}\sum_i\sum_j d(a_i, b_j)\]</span></p>
 <p>Single, complete and average linkage are the most common linkage methods and these can be combined with any pairwise dissimilarity measures.</p>
 </div>
@@ -327,7 +327,7 @@ <h4>Ward’s linkage</h4>
 <p>Ward’s linkage method minimize the within variance, by merging clusters with the minimum increase in within sum of squares.</p>
 <p><span class=""math display"">\[d_{wl}(A, B) = \sum_{i=1}^{n_A} (a_i - m_{A \cup B})^2 + \sum_{i=1}^{n_B} (b_i - m_{A \cup B})^2 - \sum_{i=1}^{n_A} (a_i - m_{A})^2 - \sum_{i=1}^{n_B} (b_i - m_{B})^2\]</span>
 where <span class=""math inline"">\(m_A, m_B, m_{A \cup B}\)</span> are the center of the clusters <span class=""math inline"">\(A\)</span>, <span class=""math inline"">\(B\)</span> and <span class=""math inline"">\(A \cup B\)</span>, respectively.</p>
-<p>Note that Ward’s linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function <code>hclus</code> either the Euclidean or squared Euclidean distance can be used in combination with the linkage <code>method='ward.D'</code> or <code>method='ward.D2</code>, respectively.</p>
+<p>Note that Ward’s linkage method should not be combined with any dissimilarity matrix as it is based on the squared Euclidean distance. In the R function <code>hclust</code> either the Euclidean or squared Euclidean distance can be used in combination with the linkage <code>method='ward.D'</code> or <code>method='ward.D2</code>, respectively.</p>
 <div class=""figure""><span id=""fig:dendromulti""></span>
 <img src=""sessions-probdescinfe_files/figure-html/dendromulti-1.png"" alt=""Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods."" width=""4200"" /><img src=""sessions-probdescinfe_files/figure-html/dendromulti-2.png"" alt=""Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods."" width=""4200"" /><img src=""sessions-probdescinfe_files/figure-html/dendromulti-3.png"" alt=""Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods."" width=""4200"" /><img src=""sessions-probdescinfe_files/figure-html/dendromulti-4.png"" alt=""Hierarchical clustering of the same data set using Euclidean distance and four different linkage methods."" width=""4200"" />
 <p class=""caption"">"
NBISweden,workshop-mlbiostatistics,58157362f9730365db5600dc9dbd750269b80f84,Olga,olga.dethlefsen@nbis.se,2020-11-14T17:53:54Z,GitHub,noreply@github.com,2020-11-14T17:53:54Z,fix typo,classroom.md,False,False,False,False,1,1,2,"---FILE: classroom.md---
@@ -83,7 +83,7 @@ _choose to do whatever works best for you (anything goes)_
 Psss...Eva, Payam and Bengt are statistical geeks and Mun-Gwuan, Nima and Lucile are super helpful
 
 
-_Note: you should be able to move between break out room. Try updating your Zomm if this is not the case._
+_Note: you should be able to move between break out room. Try updating your Zoom if this is not the case._
 
 #### Cameras and photos
 * please have your cameras on during online session and group sessions"
NBISweden,workshop-mlbiostatistics,69032ea457db16fe41d04ed06097f997213ec6ad,bsennblad,bengt.sennblad@scilifelab.se,2020-11-14T17:26:59Z,bsennblad,bengt.sennblad@scilifelab.se,2020-11-14T17:26:59Z,Update PCA to fit what Payam teach. Correct interpretation of relL. Fixed layout of final exercises,session-regularization/session-regularization.Rmd;session-regularization/session-regularization.html,True,False,True,False,795,134,929,"---FILE: session-regularization/session-regularization.Rmd---
@@ -2,18 +2,24 @@
 title: ""Session Regularization""
 output:
   bookdown::html_document2:
+    theme: cerulean
     code_folding: hide
     keep_md: false
     number_sections: false
     toc: TRUE
 editor_options:
   chunk_output_type: console
 ---
-
+<style type=""text/css"">
+  body{
+  font-size: 14pt;
+}
+</style>
 ```{r setup, message=FALSE, echo=FALSE, show=FALSE}
 #setwd(""workshop-mlbiostatistics/session-regularization"")
 
 library(knitr)
+library(ggplot2)
 opts_chunk$set(echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE, out.width='100%', fig.height=3) #, fig.width=6)#, knitr.kable.NA = '-')
 ```
 
@@ -51,13 +57,19 @@ Since statistical model contain an element of randomness, the reasoning above mi
 To formalize this intuition, Edwards (1972) defined the likelihood of model parameters being true given observed data as
 
 $$L[\theta|Y,X] \propto Pr[Y|X, \theta]$$
-Notice that this means that $L[\theta|Y,X]$ is not a proper probability, hence the term *likelihood* is used.
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 Notice that this notation is not uncommonly mixed up, so you might also see the notation $L[Y|X,\theta]$ for the likelihood.
 
-Similarly $\propto Pr[Y|X, \theta]$ is often referred to as the *likelihood function*.
+Similarly $Pr[Y|X, \theta]$ is often referred to as the *likelihood function* or simply the likelihood.
+
+***
+</details>
+Notice that this means that $L[\theta|Y,X]$ is not a proper probability, hence the term *likelihood* is used.
+<details>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
+In fact, for continuous $Y$, $Pr[Y|X, \theta]$ may be a density $f[Y|X, \theta]$ rather than a probability.
 
 ***
 </details>
@@ -67,7 +79,7 @@ In practice, the proportionality is ignored and we set
 $$L[\theta|Y,X] = Pr[Y|X, \theta]$$
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 The proportionality (indicated by '$\propto$') means there are some unknown constant factor, $k$, such that $L[\theta|Y,X] = k Pr[Y|X, \theta]$. However, the factor $k$ is assumed to be constant over $\theta$s and over models. 
 
 When the likelihood of two $\theta$s (or models) are compared this is almost always done as a _likelihood ratio_, 
@@ -79,10 +91,10 @@ which means that the factor $k$ disappears. Hence the factor $k$ is always ignor
 ***
 </details>
 
-In maximum likelihood estimation of some parameters $\theta$, one simply selects the estimates $\widehat\theta$ that gives the highest likelihood $max_{\theta}L[\theta|X,Y] = L[\widehat\theta|X,Y]$. In many applications of  likelihood and maximum likelihood, it is practical to instead use the logarithm of the likelihood, the logLikelihood, $\log L[\theta_1|Y,X]$.
+In maximum likelihood estimation, _MLE_, of some parameters $\theta$, one simply selects the estimates $\widehat\theta$ that gives the highest likelihood, i.e.,  $$\widehat{\theta} = max_{\theta}L[\theta|X,Y] = L[\widehat\theta|X,Y].$$ In many applications of  likelihood and maximum likelihood, it is practical to instead use the logarithm of the likelihood, the logLikelihood, $$\log L[\theta_1|Y,X].$$
 <details>
-<summary> Extra Reading </summary>
-As mentioned above, the logarithm of the likelihood, the logLikelihood, $\log L[\theta_1|Y,X]$, or sometimes the negative logLikelihood, $-\log L[\theta_1|Y,X]$, is often used. Notice, that 
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
+As mentioned above, the logarithm of the likelihood, the logLikelihood, $\log L[\theta_1|Y,X],$ or sometimes the negative logLikelihood, $-\log L[\theta_1|Y,X]$, is often used. Notice, that 
 
 1. The $\theta$ estimates that maximizes  $\log L[\theta|Y,X]$ also maximizes $L[\theta|Y,X]$
 2. The $\theta$ estimates that minimizes $-\log L[\theta|Y,X]$ maximizes $L[\theta|Y,X]$
@@ -110,18 +122,18 @@ $$ y = \beta x + \epsilon, $$
 
 where the residuals $\epsilon\sim N(0,\sigma^2)$. 
 
-It turns out that the  likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS of the residuals, so that the likelihood can be approximated by
+It turns out that the maximum likelihood estimates of both $\beta$ and $\sigma^2$ are functions of the  RSS of the residuals, so that the likelihood can be approximated by
 
 $$  \log L[\beta, \sigma^2|Y,X] \approx -\frac{N}{2} \log RSS$$
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 The likelihood for given $\beta$ and $\sigma^2$, given observed data $Y$ and $X$ is given by
 
 $$ L[\beta, \sigma^2|Y,X] = \prod_i pdf_{Normal}(y_i, \mu=\beta x_i, \sigma^2=\sigma^2) = \prod_i \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_i-\beta x_i)^2}{2\sigma^""}} $$
 
-where $pdf_{Normal}$ denotes the probability distribution function for the Normal distribution. If we work with the logLIkelihood instead, we get 
+where $pdf_{Normal}$ denotes the probability distribution function for the Normal distribution. If we work with the logLikelihood instead, we get 
 
 $$\begin{eqnarray*}
 \log L[\beta, \sigma^2|Y,X] 
@@ -165,7 +177,7 @@ $$Pr[\theta|D] = \frac{Pr[D| \theta]Pr[\theta]}{Pr[D]}$$
 
 $Pr[\theta|D],$ the probability, computed posterior to analysis, of the parameters $\theta$ conditioned on the observed data, i.e, our requested probability.
 
-An important characteristic of Bayesian statistics is that the focus is not on point estimates, but on the posterior probability distribution over the parameter space of $\theta$, which provides a measure of uncertainty (probabilities) in comparison to other values.
+One important characteristic of Bayesian statistics is that, typically, the focus is not on point estimates, but on the posterior probability distribution over the parameter space of $\theta$, which provides a measure of uncertainty (probabilities) in comparison to other values.
 
 ```{r, echo=FALSE}
 # fake plots for illustration purpose
@@ -183,19 +195,19 @@ plot(theta,pp_theta, type='l', ylim=c(0,1.0))
 
 $Pr[\theta]$ is the *prior* probability of $\theta$ and should according to Bayesian statistics reflect what we know (or believe to know) about how close $\theta$ is to the true parameters. We can use information from previous studies or we can assign a *uninformative* prior, e.g., $Pr[\theta]$ follows a uniform distribution for all $\theta$ in the interval $[a,b]$. 
 
-It can be shown that the effect of the prior on the posterior probsbiity is largest when the observed data is small. With larger sample sizes, the posterior probability will eventually just depend on $Pr[D|\theta]$.
+It can be shown that the effect of the prior on the posterior probbility is largest when the observed data is small. With larger sample sizes, the posterior probability will eventually just depend on $Pr[D|\theta]$.
 
 **Marginal Probability of $D$**
 
-$Pr[D]=\int_{\theta}Pr[D| \theta]Pr[\theta]$ is the probability of $D$ regardless of $\theta$. This can often be difficult difficult to calculate and, for this reason, Bayesian models are often designed so that this can be calculated analystically or some approximation approach, such as Markov chain Monte Carlo (MCMC) is used.
+$Pr[D]=\int_{\theta}Pr[D| \theta]Pr[\theta]$ is the probability of $D$ regardless of $\theta$. This can often be difficult difficult to calculate and, for this reason, Bayesian models are often designed so that this can be calculated analytically or using some approximation approach, such as Markov chain Monte Carlo (MCMC) is used.
 
 <details>
-<summary> Extra reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 **Probabilistic algebra**
 
 A conditional probability $Pr[A|B]$ is the probability that $A$ happens if we know that $B$ has happened.
-To obtain the probability that both $A$ and $B$ happens we need to first take the probability that $B$ happens and then multiply it with the conditional probability that $A$ hapens given $B$, i.e.,:
+To obtain the probability that both $A$ and $B$ happens we need to first take the probability that $B$ happens and then multiply it with the conditional probability that $A$ hapens given $B$, i.e.:
 
 $$Pr[A,B] = Pr[A|B] Pr[B].$$
 
@@ -227,9 +239,11 @@ There is often described a severe controversy between Bayesians and frequentists
 In reality, there is a large gray-zone where frequentists and Bayesians meet and socialize:
 
 * Bayesian models can be viewed as a type of the hierarchical models often used by frequentists
-* Frequentist bootstrap analysis is often used to estimate uncertainty of point estimates in relation to alternatives, as is done in Bayesian statistics
-* The *Bayes factor* is a Bayesian  version of the likelihood ratio
+* The likelihood could be interpreted as an unnormalized posterior probability under an uninformative prior
+* Frequentist bootstrap analysis is often used to estimate uncertainty of point estimates in relation to alternatives, similarly to what is done in Bayesian statistics
+* Conversely, the maximum _a posteriori_ estimate is a point estimate, similar to the MLE
 * Bayesian *posterior intervals* corresponds to frequentist *confidence intervals* (*Note* however, that there are no Bayesian significance test)
+* The *Bayes factor* is a Bayesian  version of the likelihood ratio
 * etc.
 
 Most practical statisticians use the tool that is adequate for the problem at hand, whether it is Bayesian or frequentist.
@@ -409,7 +423,7 @@ _Overfitting_
 
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 <h2> Model comparison | `Likelihood ratio test`</h1>
 
 For nested models $-2 \max LRT$ is $\chi^2(d)$-distributed, with $d=$ the difference in free params in the two models.
@@ -430,7 +444,7 @@ library(kableExtra)
 kable(lrt,""html"",row.names=F, col.names=c(""Compared models"",""logL 1st model"",""logL 2nd model"",""logLR"", ""P-value"", ""Sign at 0.05""),digits=30, format.args=list(snsmall=0)) %>% kable_styling(bootstrap=""striped"", font_size = 14, full_width=F)
 
 ```
-In our simple test case, the LRT also succeed in picking the correct model. It should be noted that certain issues, such as *lnkage disequiibriium*, may cause problems for LRT (*the example is not optimized to show this*).
+In our simple test case, the LRT also succeed in picking the correct model. It should be noted that certain issues, such as *linkage disequilibriium*, may cause problems for LRT (*the example is not optimized to show this*).
 
 
 ***
@@ -507,9 +521,9 @@ The Akaike information criterion (AIC), for a model $m$ with variables $X$, is d
 We see that $AIC_m = -2 \left(\log \max L[{\beta}|X,Y] - \#X\right)$, i.e., $-2$ times the the simple $\log rL$, we just looked at in our first regularization example. 
     
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
-The difference in $AIC$ between two models is claimed to estimate the information lost by selecting the worse model.
+From a information theory perspective, the difference in $AIC$ between two models is claimed to estimate the information lost by selecting the worse model.
 
 ***
 </details>
@@ -519,16 +533,16 @@ Sometimes, the *relative likelihood* for model $m$ is used, which is
 where $AIC_{min}$ is the minimum AIC among a set of compared models
       
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 * $relL$ can be interpreted as proportional to the probability that the model $m$ minimizes the information loss.
 <!--       and can be interpreted as -->
 <!-- $rL \propto Pr[m\textrm{ minimizes estimated information loss}]$. -->
 
-   * Notice that
-   
-$$\log relL = \frac{\#X_m }{\#X_{min}}\log\frac{\max L[{\beta}_{m}|X_m,Y]}{\max L[{\beta}_{min}|X_{min},Y]}$$
-  we see that $rL$ can be viewed as a  likelihood ratio weighted by the ratio of number of $X$ variables.
+   * Notice that,  
+$$relL = \frac{\max L[{\beta}_{m}|X_m,Y]}{\max L[{\beta}_{min}|X_{min},Y]} \times e^{-\left(\#X_{m}-\#X_{min}\right)}$$
+
+   we see that $rL$ can be viewed as a  likelihood ratio weighted by a function of the difference in number of $X$ variables.
 * However, AIC are not limited to nested models
 
 ***
@@ -562,10 +576,10 @@ for(i in seq(1,P)){
   aic[i,] = list(paste0(i,"" variables""), signif(fit$AIC[2],5), ""-"") 
 }
 minaic=min(aic$aic)
-aic$rl=format(exp((minaic-aic$aic)/2), digits=4)
+aic$relL=format(exp((minaic-aic$aic)/2), digits=4)
 aic$isAICmin = ifelse(aic$aic==minaic,""Yes"",""-"")
 
-kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Minimum AIC"",""rL""),digits=30,format.args=list(snsmall=0))  %>%  kable_styling( font_size = 14)
+kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Minimum AIC"",""relL""),digits=30,format.args=list(snsmall=0))  %>%  kable_styling( font_size = 14)
 ```
 
 <details>
@@ -595,7 +609,7 @@ kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Mini
 ***
 </details>
 
-* Try to plot the $AIC$ and the $reL$ with the different models on the $X$-axis
+* Try to plot the $AIC$ and the $relL$ with the different models on the $X$-axis
 
 ```{r,echo=F, fig.height=4, echo=TRUE, eval=FALSE}
 library(stats)
@@ -604,7 +618,7 @@ library(stats)
 plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab=""AIC"", xlab=""model #"", type = ""b"")
 
 # plot relL of all models
-plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
+plot(aic$relL, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
 ```  
 
 <details>
@@ -633,10 +647,10 @@ plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
 <summary> Some possible answers </summary>
 <h4>Some possible answers</h4>
 
-* We see that the best model is the one with the 3 first X-variables (in line with our *oracle knowledge*) and that the second best model (with the first 2 X-variabels) is $\approx70\%$ worse.
+* We see that the best model is the one with the 3 first X-variables (in line with our *oracle knowledge*) and that the second best model (with the first 4 X-variabels) is $\approx60\%$ worse.
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 * Sometimes it is desirable to compute a significance for rejecting a model in favour of another model. A NULL distribution for the $relL$ statistic is usally obtained through simulation, e.g., using parameteric bootstrapping.
 
@@ -657,10 +671,12 @@ plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
 
 LASSO  stands for Least absolute shrinkage and selection operator (""shrinkage"" is another common term for regularization) and is a method for selecting variables to include in a multivariate model.
 
+Here, we do nott explicitly compare two models with different number of variables. Instead all variables are included and the regularization acts upon the values of $\beta$ so that only important variabels have a $\beta_i> 0$.
+
 Classical LASSO builds on RSS of a linear regression model $Y \sim X{\beta}$ with regularization
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 Extensions to glms exists, but then using a regularized likelihood expression
     
@@ -670,7 +686,7 @@ Extensions to glms exists, but then using a regularized likelihood expression
 The regularization term $f(\beta) = \lambda\sum_{\beta_i\in\beta} |\beta_i-0|= \lambda\sum_{\beta_i\in\beta} |\beta_i|$
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 Often the regularization term is expressed in terms of the $\ell_1-norm$, which can be viewed simply a short-hand notation, e.g., the $\ell_1-norm$ of $\beta$ is
 $$ ||\beta||_1 = \sum_{\beta_i\in\beta} |\beta_i|$$
@@ -689,11 +705,11 @@ The $\lambda$ parameter sets a limit on the estimation of $\beta$.
 
 Lasso is traditionally described as RSS with an auxiliary criterion/constraint: 
 
-$$min_{{\beta}}\left\{RSS\right\} + \lambda\sum_{\beta_i\in\beta} |\beta_i|.$$
+$$min\left\{RSS\right\} + \lambda\sum_{\beta_i\in\beta} |\beta_i|.$$
 Lasso can also be viewed as a un-normalized Bayesian posterior probability, with a LaPlacean prior on $\beta$: $\beta_j ∼ LaPlace(0, 1/\lambda)$ 
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 Other common notation for LASSO:
 
@@ -707,7 +723,7 @@ Other common notation for LASSO:
 The optimal values of $\beta$ are then estimated, using some algorithm (lars or coordinate descent).
 
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 The *Coordinate descent* algorithm is used in the R package `glmnet`:
 
@@ -722,7 +738,7 @@ The *Coordinate descent* algorithm is used in the R package `glmnet`:
 </details>
             
 <details>
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 Alternatives to LASSO, differing mainly in the auxiliary criterion
 
@@ -746,7 +762,7 @@ Alternatives to LASSO, differing mainly in the auxiliary criterion
          + and the coefficients are back-standardized before reported
 
 <details> 
-<summary> Extra Reading </summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 Standardization in `glmnet`:
 $x' = \frac{x-\bar{x}}{1/\sqrt{N}||X-\bar{x}||_2}$
@@ -782,7 +798,7 @@ plot(fit, xvar=""lambda"",label=T)
 * In which direction is the effect
 * Which lambda should we select?
   - Given our *oracle knowledge*, where would an appropriate $\lambda$ be?
-  - Can we use that?
+  - Can we use that in the general case?
 
 <details>
 <summary> Some possible answers </summary>
@@ -828,7 +844,7 @@ For $i \in \{1,2,\ldots, k\}$, set $D_i$ as the test data set and the union of t
 This gives a distribution of $MSE$ from which we can estimate, e.g., mean and standard deviation.
 
 <details>
-<summary>Additional reading</summary>
+<summary> <span style=""color:gray"">Extra Reading</span> </summary>
 
 This distribution allows us to use more elaborate means to select $\lambda$. One common suggestion is to use the largest $\lambda$ whose $MSE$ is within 1 standard error from the minimum value (called `lambda.1se` in `glmnet`). The motivation argued for this choice is *parsimony*, in the sense that larger $\lambda$ will include fewer variables (hence it is parsimonious in terms of number of included variables). 
 
@@ -887,7 +903,7 @@ minlambda=cvglm$lambda.min
 </details>
 
 ### Task| `Final LASSO effect sizes`
-* Finally print a table with the $\beta$ coefficients (including the intercept, $\beta_0$) for the optimal model (i.e.,  at minimum $\lambda$). (Use function`coef`).
+* Finally print a table with the $\beta$ coefficients (including the intercept, $\beta_0$) for the optimal model (i.e.,  at minimum $\lambda$); compare with oracle knowledge. (Hint: Use function`coef`).
 
 ```{r, echo =T, eval=FALSE}
 # Actually the following suffice for output on console
@@ -1011,13 +1027,13 @@ The samples includes 99 Males and 58 Females, it is not perfectly balanced but s
 
 ```{r, echo =T, eval=FALSE}
 ## TODO: Adjust this so that it fit what they learnt from Paya
-pca.gtex <- prcomp(X, scale=TRUE)
+pca.gtex <- prcomp(X, scale=FALSE, center=TRUE)
 
 # sample plot
 pca_plot <- data.frame(x = pca.gtex$x[,""PC1""], y = pca.gtex$x[,""PC2""], Groups = Y)
 ggplot(pca_plot) +
   geom_point(aes(x=x, y=y, color=Groups)) +
-  #scale_color_manual(values=mycolors) +
+  scale_color_manual(values=c(""red"",""blue"")) +
   ggtitle(label=""PCA on GTEX Skeletal Muscles"") +
   xlab(""PC1"") +
   ylab(""PC2"") +
@@ -1026,40 +1042,52 @@ ggplot(pca_plot) +
 
 # barplot prop variation explained
 x = paste0(""PC"",1:10)
-pc_plot = data.frame(x = factor(x, levels=x), y = unlist(summary(pca.gtex)$importance[2,1:10]))
+# calculate variation explained
+y.var <- pca.gtex$sdev ^ 2
+y.pvar <- y.var/sum(y.var)
+y.pvar<-y.pvar*100
+
+pc_plot=0
+pc_plot = data.frame(x = factor(x, levels=x), y = y.pvar[1:10])
 ggplot(data=pc_plot, aes(x=x, y=y)) +
   geom_bar(stat=""identity"") +
-  #    ggtitle(label=) +
+  ggtitle(label=""PCA on GTEX Skeletal Muscles"") +
   xlab(""Principal Components"") +
   ylab(""Variation explained"") +
   theme_bw() +
   theme(title=element_text(face=""bold"")) 
 
 ```
-
 <details>
 <summary> *Show result*</summary>
 ```{r, fig.width=10, fig.height=8}
 ## TODO: Adjust this so that it fit what they learnt from Paya
-pca.gtex <- prcomp(X, scale=TRUE)
+pca.gtex <- prcomp(X, scale=FALSE, center=TRUE)
 
 # sample plot
 pca_plot <- data.frame(x = pca.gtex$x[,""PC1""], y = pca.gtex$x[,""PC2""], Groups = Y)
 ggplot(pca_plot) +
   geom_point(aes(x=x, y=y, color=Groups)) +
-  #scale_color_manual(values=mycolors) +
+  scale_color_manual(values=c(""red"",""blue"")) +
   ggtitle(label=""PCA on GTEX Skeletal Muscles"") +
   xlab(""PC1"") +
   ylab(""PC2"") +
   theme_bw() +
   theme(title=element_text(face=""bold"")) 
 
+# calculate variation explained
+y.var <- pca.gtex$sdev ^ 2
+y.pvar <- y.var/sum(y.var)
+y.pvar<-y.pvar*100
 # barplot prop variation explained
 x = paste0(""PC"",1:10)
-pc_plot = data.frame(x = factor(x, levels=x), y = unlist(summary(pca.gtex)$importance[2,1:10]))
+y = y.pvar[1:10]
+
+pc_plot=0
+pc_plot = data.frame(x = factor(x, levels=x), y = y)
 ggplot(data=pc_plot, aes(x=x, y=y)) +
   geom_bar(stat=""identity"") +
-  #    ggtitle(label=) +
+  ggtitle(label=""PCA on GTEX Skeletal Muscles"") +
   xlab(""Principal Components"") +
   ylab(""Variation explained"") +
   theme_bw() +
@@ -1073,7 +1101,7 @@ The PCA plot demonstrates that there is a lot of variation between samples with
 ***
 </details>
 
-We want to investigate if there are a subset of genes that are associated to the phenotype *Gender*, that is, we want to do find multivariate association analyses. 
+We want to investigate if there are a subset of genes that are associated to the phenotype *Gender*, that is, we want to do find optimal multivariate regression analyses. 
 Since *Gender* is a binary variable, it is appropriate to use logistic *GLM* (`glm` with `family=binomial()`) for all analyses.
 However, as suggested by the PCA, the majority of genes are probably not associated to *Gender*. Hence we want to perform *feature selection* to identify a, hopefully, optimal multivariate model to use. 
 
@@ -1092,7 +1120,7 @@ We will first try to use AIC to determine the best multivariate model. A main de
     - Hints:
         1. Reuse code from the [AIC task above](.Task | `AIC analysis`). 
         2. Use `myX[,seq(1, i)]` to get the use only the `i` first columns of `myX`.
-        3. Use `as.matrix` to convert the given subsetted `X` data.frame to a matrix before unsing it in the GLM.
+        3. Use `as.matrix` to convert the given subsetted `X` data.frame to a matrix before using it in the GLM.
 * Try to run `doAIC` using different orderings and subsets of `X` as `myX`, e.g.:
     - ordered by size of odds ratio, by P-value
     - top 20, top 100 or only significant P-values (unadjusted or adjusted)
@@ -1101,7 +1129,7 @@ We will first try to use AIC to determine the best multivariate model. A main de
 <details>
 <summary> *Possible solutions*</summary>
 #### Univariate analysis
-```{r warning=FALSE, echo=TRUE}
+```{r warning=FALSE, echo=TRUE, eval=FALSE}
 coefs<-vector()
 pvals<-vector()
 a<-seq(from=0,to=dim(X)[2],by=100)
@@ -1113,13 +1141,35 @@ for(i in 1:dim(X)[2])
 }
 univariateResults<-data.frame(GENE=colnames(X), COEFFICIENTS=coefs, PVALUE=pvals)
 univariateResults$FDR<-p.adjust(univariateResults$PVALUE,method=""BH"")
+# Optional
 datatable(univariateResults[order(-abs((1-univariateResults$PVALUE))),]) 
 ```
 
+```{r warning=FALSE, echo=FALSE}
+coefs<-vector()
+pvals<-vector()
+a<-seq(from=0,to=dim(X)[2],by=100)
+for(i in 1:dim(X)[2])
+{
+  model = glm(Y~X[,i], family=binomial)
+  coefs=append(coefs, exp(coef(summary(model))[,1][2])) #
+  pvals=append(pvals, coef(summary(model))[,4][2]) #
+}
+univariateResults<-data.frame(GENE=colnames(X), COEFFICIENTS=coefs, PVALUE=pvals)
+univariateResults$FDR<-p.adjust(univariateResults$PVALUE,method=""BH"")
+# Optionally uncomment
+#datatable(univariateResults[order(-abs((1-univariateResults$PVALUE))),]) 
+
+# Following is just get an output table to the lecture
+univariateResults[order(univariateResults$FDR),][1:10,] %>%
+  kable(, format='html', row.names=F, #col.names=c(""Compared models"",""AIC"",""Minimum AIC"",""rL""),digits=30,format.args=list(snsmall=0
+  )  %>%  kable_styling( font_size = 14)
+```
+
 
 #### doAIC function
 
-```{r warning=FALSE, evho=TRUE}
+```{r warning=FALSE, echo=TRUE}
 library(stats)
 coef<-vector()
 pval<-vector()
@@ -1221,28 +1271,30 @@ doAIC(X[,univariateResults[univariateResults$FDR <= 0.05, ""GENE""]], Y)
       - Clever step-wise addition: Select the most significant univariate model, take its residuals and use as a new phenotype in a second round of univariate analyses, compare AIC to the previous round, iterate!
       - Raw force: Create models for all possible combinations of variables in `X`, find the model among these that maximises AIC.
 * The univariate analyses comprised only one model with a significant adjusted P-value, while our AIC analysis suggest that there may exist multivariate models that has lower AIC
-  - The penalty of multiple testing.
+  - What could be the reason for this?
+      + The penalty of multiple testing.
 
 ***
 </details>
 
 ## LASSO
 
-We now turn to LASSO feature selection.  We will use `glmnet` to run LASSO on a logisitc GLM `Y` vs `X` and find an optimal value of $\lambda$ via 10-fold cross-validation. 
+We now turn to LASSO feature selection.  We will use `glmnet` to run LASSO on a logistic GLM `Y` vs `X` and find an optimal value of $\lambda$ via 10-fold cross-validation. 
 
 ### Task| `LASSO`
 
 * Run LASSO on `X` and `Y` with 100-fold cross-validation
   - Identify the optimal $\lambda$
       - Optionally, plot the result of the cross-validation
-      - Optionally, plot the traces of¢beta$ for the inclusion of variables in the model.
-* Create a table of the genes included in the optimal LASSO model and their $beta$, e.g., using `datatable`
+      - Optionally, plot the traces of $\beta$ for the inclusion of variables in the model.
+* Create a table of the genes included in the optimal LASSO model and their $\beta$, e.g., using `datatable`
 * Run `doAIC` on the subset of `X` corresponding to genes in the optimal LASSO model.
 
 <details>
 <summary> *Possible solutions*</summary>
 
-```{r LASSO,fig.width=10,fig.height=8}
+#### Lasso and optimal lambda
+```{r,fig.width=10,fig.height=8, echo =TRUE}
 library(glmnet)
 par(mfrow=c(1,2))
 # run lasso (alpha=1) for linear model (family=binomial)
@@ -1251,23 +1303,33 @@ cvglm=cv.glmnet(as.matrix(X),Y, family=binomial(), alpha=1, standardize=T, nfold
 plot(cvglm)
 plot(cvglm$glmnet.fit, xvar=""lambda"",label=T)
 minlambda=cvglm$lambda.min
-
-#lasso_fit <- cv.glmnet(as.matrix(X), Y, family=""binomial"", alpha=1)
-#plot(lasso_fit)
-#lasso_fit$lambda.min
-#og(lasso_fit$lambda.min)
+print(minlambda)
 ```
 
+#### Feature selection
 Once we know the optimal $\lambda$, we can display the names of the most informative features selected by LASSO for that optimal $\lambda$.
 
-```{r bestmodel}
+```{r , echo=TRUE, eval=FALSE}
 genes<-colnames(X)[unlist(predict(cvglm, s = ""lambda.min"", type = ""nonzero""))]
-betas= data.frame(betas=unlist(coef(cvglm, s=""lambda.min"")[genes,]))
-betas<-betas[order(-abs(betas$betas)), , drop =FALSE]
+betas= data.frame(genes = genes, 
+                  betas=unlist(coef(cvglm, s=""lambda.min"")[genes,]))
+betas<-betas[order(-abs(betas$betas)), ]
 datatable(betas)
 ```
 
-```{r warning=FALSE, echo=TRUE}
+```{r}
+genes<-colnames(X)[unlist(predict(cvglm, s = ""lambda.min"", type = ""nonzero""))]
+betas= data.frame(genes = genes, 
+                  betas=unlist(coef(cvglm, s=""lambda.min"")[genes,]))
+betas<-betas[order(-abs(betas$betas)), ]
+
+betas %>%
+  kable(format='html', row.names=F)  %>%  
+  kable_styling( font_size = 14)
+```
+
+#### AIC of best model
+```{r, warning=FALSE, echo=TRUE}
 doAIC(X[,genes], Y)
 ```
 </details>
@@ -1285,7 +1347,7 @@ doAIC(X[,genes], Y)
   - Multiple test correction reduces the power of the univariate approach
 * The optimal LASSO model is different from those obtained from the various AIC approach (most likely:); moreover, the optimal LASSO model has a lower AIC than those investigated in the AIC approach (most likely)
   - In general, unless we test all models corresponding all possible combinatorial subsets of variables or apply some clever search algorithm (similar to the one LASSO uses), we are unlikely to find the optimal model.
-      - Even then, we might get idifferent answers, because the regularization applied in AIC and LASSO are different.
+      - Even then, we might get different answers, because the regularization applied in AIC and LASSO are different.
 
 </details>
 "
NBISweden,workshop-mlbiostatistics,def350146cb763e6e79f7795e79b1e7e4b29eb2c,olgadet,olga.dethlefsen@nbis.se,2020-11-12T16:14:09Z,olgadet,olga.dethlefsen@nbis.se,2020-11-12T16:14:09Z,Fix typo,schedule.md,False,False,False,False,2,1,3,"---FILE: schedule.md---
@@ -13,7 +13,7 @@ title:  'Schedule'
 
 **10:15 - 11:15** Introduction to R, R-Studio and R markdown (Olga) (~ 5 min live stream, ~ 25 min online support)
 
-**11.15 - 12:00** Mathematical foundations (Olga) (*live stream)
+**11.15 - 12:00** Mathematical foundations (Olga) (live stream)
 
 
 *12:00 - 13:00 lunch* (*offline*)
@@ -59,6 +59,7 @@ title:  'Schedule'
 
 <br/>
 ##### Wednesday 2020-11-18
+
 **09:00 - 09.30** Group discussions: recap of the previous day (group session)
 
 **09:30 - 10:00** [Linear models: introduction](https://olgadet.github.io/bookdown-mlbiostatistics/introduction-to-linear-models.html) (Olga) (live stream)"
NBISweden,workshop-mlbiostatistics,52d4b35cc341f1f32c8807c2f9384643b4c19a86,bsennblad,bengt.sennblad@scilifelab.se,2020-11-12T10:08:25Z,bsennblad,bengt.sennblad@scilifelab.se,2020-11-12T10:08:25Z,Fix a tag not properly terminated error for {{content},session-intro2/intro2.Rmd,True,False,True,False,1,1,2,"---FILE: session-intro2/intro2.Rmd---
@@ -834,7 +834,7 @@ scatter3D(x=df$x, z=df$z, y=df$y, xlab=""x"", ylab=""y"", zlab=""z"", phi=20, theta=-5
 --
 ![black box warning](assets/blackbox.jpg) 
 
-{{content}
+{{content}}
 --
 
 #### Really Big Data "
NBISweden,workshop-mlbiostatistics,fa3280de7f2a8344f9e7a94301c7c137c9597860,olgadet,olga.dethlefsen@nbis.se,2020-11-02T11:21:27Z,olgadet,olga.dethlefsen@nbis.se,2020-11-02T11:21:27Z,Fix Zoom link,precourse.md,False,False,False,False,7,3,10,"---FILE: precourse.md---
@@ -37,8 +37,8 @@ There are few things **to do** before the course starts. These include both pre-
 - We will be using [Zoom](https://zoom.us) for discussions and [Zulip](https://zulip.com) for chatting in writing
 
 ##### Zoom Meeting
-- https://stockholmuniversity.zoom.us/j/67966236535
-*Password sent via email*
+- [https://stockholmuniversity.zoom.us/j/67966236535](https://stockholmuniversity.zoom.us/j/67966236535)
+- *Password sent via email*
 
 ##### Zulip
 - Join via the invitation link *sent via email*
@@ -85,7 +85,7 @@ Install R version 3.5.0 or higher
 
 # install decision tress packages
 install.packages(""rpart"")
-install.packages(""rpart"")
+install.packages(""rpart.part"")
 install.packages(""randomForest"")
 
 # install artifical neural network package
@@ -94,6 +94,7 @@ install.packages(""neuralnet"")
 ```
 
 - To use the install package we load them by using `library()` or `require()` e.g.
+
 ```R
 
 library(""neuralnet"")
@@ -102,8 +103,11 @@ require(""randomForest"")
 ```
 
 - To see packages index page use `help()` function
+
 ```R
+
 help(package=""neuralnet"")
+
 ```
 
 ##### Install R packages"
NBISweden,workshop-mlbiostatistics,ef66b3f57b38343eb841fc739ca9dbf1b590de9b,Bengt Sennblad,bengt.sennblad@scilfelab.se,2019-05-23T17:07:03Z,Bengt Sennblad,bengt.sennblad@scilfelab.se,2019-05-23T17:07:03Z,fixed ylim for first ML plot,session-regularization/session-regularization.Rmd;session-regularization/session-regularization.html;session-regularization/session-regularization.md,True,False,True,False,5,5,10,"---FILE: session-regularization/session-regularization.Rmd---
@@ -232,12 +232,12 @@ plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min
 ```{r,echo=T, fig.height=4, echo=FALSE}
 require(stats)
 ll= vector()
-for(i in seq(1,2)){
+for(i in seq(1,P)){
   Xi=X[,seq(1,i)]
   ll[i] <- logLik(lm(Y~Xi))
 }
 # plot likelihoods for models with 1 and 2 vaiables
-plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
+plot(ll[1:2], ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 # xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
 ```
 

---FILE: session-regularization/session-regularization.html---
@@ -635,7 +635,7 @@ <h3>Task | <code>plot two likelihoods</code></h3>
 plot(ll, ylab=&quot;log L&quot;, xlab=&quot;model #&quot;, type = &quot;b&quot;, xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 # xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar</code></pre>
 <details>
-<p><summary> <em>Show result</em></summary> <img src=""data:image/png;base64,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"" width=""100%"" /></p>
+<p><summary> <em>Show result</em></summary> <img src=""data:image/png;base64,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"" width=""100%"" /></p>
 <hr />
 </details>
 <p>… 2 variables are clearly better than 1 variable – What if we add more variables?</p>
@@ -1587,7 +1587,7 @@ <h1>Thank you</h1>
 <p><strong>locale:</strong> en_US.UTF-8||en_US.UTF-8||en_US.UTF-8||C||en_US.UTF-8||en_US.UTF-8</p>
 <p><strong>attached base packages:</strong> <em>stats</em>, <em>graphics</em>, <em>grDevices</em>, <em>utils</em>, <em>datasets</em>, <em>methods</em> and <em>base</em></p>
 <p><strong>other attached packages:</strong> <em>pander(v.0.6.3)</em>, <em>glmnet(v.2.0-16)</em>, <em>foreach(v.1.4.4)</em>, <em>Matrix(v.1.2-17)</em>, <em>dplyr(v.0.8.0.1)</em>, <em>kableExtra(v.1.1.0)</em>, <em>lmtest(v.0.9-37)</em>, <em>zoo(v.1.8-5)</em> and <em>knitr(v.1.22)</em></p>
-<p><strong>loaded via a namespace (and not attached):</strong> <em>Rcpp(v.1.0.1)</em>, <em>later(v.0.8.0)</em>, <em>compiler(v.3.5.3)</em>, <em>pillar(v.1.3.1)</em>, <em>highr(v.0.8)</em>, <em>iterators(v.1.0.10)</em>, <em>tools(v.3.5.3)</em>, <em>digest(v.0.6.18)</em>, <em>packrat(v.0.5.0)</em>, <em>jsonlite(v.1.6)</em>, <em>evaluate(v.0.13)</em>, <em>tibble(v.2.1.1)</em>, <em>lattice(v.0.20-38)</em>, <em>viridisLite(v.0.3.0)</em>, <em>pkgconfig(v.2.0.2)</em>, <em>rlang(v.0.3.4)</em>, <em>rstudioapi(v.0.10)</em>, <em>yaml(v.2.2.0)</em>, <em>xfun(v.0.6)</em>, <em>stringr(v.1.4.0)</em>, <em>httr(v.1.4.0)</em>, <em>xml2(v.1.2.0)</em>, <em>hms(v.0.4.2)</em>, <em>tidyselect(v.0.2.5)</em>, <em>grid(v.3.5.3)</em>, <em>webshot(v.0.5.1)</em>, <em>glue(v.1.3.1)</em>, <em>R6(v.2.4.0)</em>, <em>rmarkdown(v.1.12)</em>, <em>xaringan(v.0.9)</em>, <em>servr(v.0.13)</em>, <em>purrr(v.0.3.2)</em>, <em>readr(v.1.3.1)</em>, <em>magrittr(v.1.5)</em>, <em>promises(v.1.0.1)</em>, <em>codetools(v.0.2-16)</em>, <em>scales(v.1.0.0)</em>, <em>htmltools(v.0.3.6)</em>, <em>rsconnect(v.0.8.13)</em>, <em>assertthat(v.0.2.1)</em>, <em>rvest(v.0.3.3)</em>, <em>mime(v.0.6)</em>, <em>colorspace(v.1.4-1)</em>, <em>httpuv(v.1.5.1)</em>, <em>stringi(v.1.4.3)</em>, <em>munsell(v.0.5.0)</em> and <em>crayon(v.1.3.4)</em></p>
+<p><strong>loaded via a namespace (and not attached):</strong> <em>Rcpp(v.1.0.1)</em>, <em>pillar(v.1.3.1)</em>, <em>compiler(v.3.5.3)</em>, <em>highr(v.0.8)</em>, <em>iterators(v.1.0.10)</em>, <em>tools(v.3.5.3)</em>, <em>digest(v.0.6.18)</em>, <em>evaluate(v.0.13)</em>, <em>tibble(v.2.1.1)</em>, <em>lattice(v.0.20-38)</em>, <em>viridisLite(v.0.3.0)</em>, <em>pkgconfig(v.2.0.2)</em>, <em>rlang(v.0.3.4)</em>, <em>rstudioapi(v.0.10)</em>, <em>yaml(v.2.2.0)</em>, <em>xfun(v.0.6)</em>, <em>stringr(v.1.4.0)</em>, <em>httr(v.1.4.0)</em>, <em>xml2(v.1.2.0)</em>, <em>hms(v.0.4.2)</em>, <em>tidyselect(v.0.2.5)</em>, <em>grid(v.3.5.3)</em>, <em>webshot(v.0.5.1)</em>, <em>glue(v.1.3.1)</em>, <em>R6(v.2.4.0)</em>, <em>rmarkdown(v.1.12)</em>, <em>purrr(v.0.3.2)</em>, <em>readr(v.1.3.1)</em>, <em>magrittr(v.1.5)</em>, <em>codetools(v.0.2-16)</em>, <em>scales(v.1.0.0)</em>, <em>htmltools(v.0.3.6)</em>, <em>assertthat(v.0.2.1)</em>, <em>rvest(v.0.3.3)</em>, <em>colorspace(v.1.4-1)</em>, <em>stringi(v.1.4.3)</em>, <em>munsell(v.0.5.0)</em> and <em>crayon(v.1.3.4)</em></p>
 </div>
 
 

---FILE: session-regularization/session-regularization.md---
@@ -983,4 +983,4 @@ _stats_, _graphics_, _grDevices_, _utils_, _datasets_, _methods_ and _base_
 _pander(v.0.6.3)_, _glmnet(v.2.0-16)_, _foreach(v.1.4.4)_, _Matrix(v.1.2-17)_, _dplyr(v.0.8.0.1)_, _kableExtra(v.1.1.0)_, _lmtest(v.0.9-37)_, _zoo(v.1.8-5)_ and _knitr(v.1.22)_
 
 **loaded via a namespace (and not attached):** 
-_Rcpp(v.1.0.1)_, _later(v.0.8.0)_, _compiler(v.3.5.3)_, _pillar(v.1.3.1)_, _highr(v.0.8)_, _iterators(v.1.0.10)_, _tools(v.3.5.3)_, _digest(v.0.6.18)_, _packrat(v.0.5.0)_, _jsonlite(v.1.6)_, _evaluate(v.0.13)_, _tibble(v.2.1.1)_, _lattice(v.0.20-38)_, _viridisLite(v.0.3.0)_, _pkgconfig(v.2.0.2)_, _rlang(v.0.3.4)_, _rstudioapi(v.0.10)_, _yaml(v.2.2.0)_, _xfun(v.0.6)_, _stringr(v.1.4.0)_, _httr(v.1.4.0)_, _xml2(v.1.2.0)_, _hms(v.0.4.2)_, _tidyselect(v.0.2.5)_, _grid(v.3.5.3)_, _webshot(v.0.5.1)_, _glue(v.1.3.1)_, _R6(v.2.4.0)_, _rmarkdown(v.1.12)_, _xaringan(v.0.9)_, _servr(v.0.13)_, _purrr(v.0.3.2)_, _readr(v.1.3.1)_, _magrittr(v.1.5)_, _promises(v.1.0.1)_, _codetools(v.0.2-16)_, _scales(v.1.0.0)_, _htmltools(v.0.3.6)_, _rsconnect(v.0.8.13)_, _assertthat(v.0.2.1)_, _rvest(v.0.3.3)_, _mime(v.0.6)_, _colorspace(v.1.4-1)_, _httpuv(v.1.5.1)_, _stringi(v.1.4.3)_, _munsell(v.0.5.0)_ and _crayon(v.1.3.4)_
+_Rcpp(v.1.0.1)_, _pillar(v.1.3.1)_, _compiler(v.3.5.3)_, _highr(v.0.8)_, _iterators(v.1.0.10)_, _tools(v.3.5.3)_, _digest(v.0.6.18)_, _evaluate(v.0.13)_, _tibble(v.2.1.1)_, _lattice(v.0.20-38)_, _viridisLite(v.0.3.0)_, _pkgconfig(v.2.0.2)_, _rlang(v.0.3.4)_, _rstudioapi(v.0.10)_, _yaml(v.2.2.0)_, _xfun(v.0.6)_, _stringr(v.1.4.0)_, _httr(v.1.4.0)_, _xml2(v.1.2.0)_, _hms(v.0.4.2)_, _tidyselect(v.0.2.5)_, _grid(v.3.5.3)_, _webshot(v.0.5.1)_, _glue(v.1.3.1)_, _R6(v.2.4.0)_, _rmarkdown(v.1.12)_, _purrr(v.0.3.2)_, _readr(v.1.3.1)_, _magrittr(v.1.5)_, _codetools(v.0.2-16)_, _scales(v.1.0.0)_, _htmltools(v.0.3.6)_, _assertthat(v.0.2.1)_, _rvest(v.0.3.3)_, _colorspace(v.1.4-1)_, _stringi(v.1.4.3)_, _munsell(v.0.5.0)_ and _crayon(v.1.3.4)_"
NBISweden,workshop-mlbiostatistics,1bcc0321a72fdba30a9de3a54771502bdd8068f1,Bengt Sennblad,bengt.sennblad@scilfelab.se,2019-05-23T14:01:14Z,Bengt Sennblad,bengt.sennblad@scilfelab.se,2019-05-23T14:01:14Z,Fixed some bugs stupidly introduced at last minute,session-regularization/session-regularization.Rmd;session-regularization/session-regularization.html;session-regularization/session-regularization.md,True,False,True,False,187,169,356,"---FILE: session-regularization/session-regularization.Rmd---
@@ -172,7 +172,7 @@ N=100 # number of samples
 P=10 # number of variables
 
 # Draw variables, x_{i,1},...,x_{i,P} for all N individuals, from a uniform distribution in interval (0,1) (this is the default interval for runif)
-X=matrix(runif(N*(P+1)), nrow=N, ncol=P) 
+X=matrix(round(runif(N*(P+1),min=0, max=2)), nrow=N, ncol=P)
 
 # generate a y variable from a multivarite lm of 3 first X variables only
 # intercept
@@ -223,7 +223,7 @@ for(i in seq(1,2)){
   ll[i] <- logLik(lm(Y~Xi))
 }
 # plot likelihoods for models with 1 and 2 vaiables
-plot(ll[seq(1,2)], ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
+plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 # xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
 ```
 
@@ -237,7 +237,7 @@ for(i in seq(1,2)){
   ll[i] <- logLik(lm(Y~Xi))
 }
 # plot likelihoods for models with 1 and 2 vaiables
-plot(ll[seq(1,2)], ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
+plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 # xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
 ```
 
@@ -260,7 +260,7 @@ for(i in seq(1,P)){
 }
 
 # plot ll for all models
-plot(ll[seq(1,P)], ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
+plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 
 ```
 
@@ -276,7 +276,7 @@ for(i in seq(1,P)){
 }
 
 # plot ll for all models
-plot(ll[seq(1,P)], ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
+plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 
 ```
 
@@ -373,7 +373,7 @@ for(i in seq(1,P)){
   pl[i] = -AIC(fit)/2
 }
 # plot ll of all models
-plot(pl[seq(1,P)], xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""log pL"", xlab=""model #"", type = ""b"")
+plot(pl, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""log pL"", xlab=""model #"", type = ""b"")
 ```  
 
 </details>
@@ -483,9 +483,10 @@ kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Mini
 * Try to plot the $AIC$ and the $reL$ with the different models on the $X$-axis
 
 ```{r,echo=F, fig.height=4, echo=TRUE, eval=FALSE}
+require(stats)
 
 # plot AIC of all models
-plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""AIC"", xlab=""model #"", type = ""b"")
+plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab=""AIC"", xlab=""model #"", type = ""b"")
 
 # plot relL of all models
 plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
@@ -494,9 +495,10 @@ plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
 <details>
 <summary> *Show result*</summary>
 ```{r,echo=F, fig.height=4, echo=FALSE}
+require(stats)
 
 # plot AIC of all models
-plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""AIC"", xlab=""model #"", type = ""b"")
+plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab=""AIC"", xlab=""model #"", type = ""b"")
 
 # plot relL of all models
 plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
@@ -670,8 +672,8 @@ plot(fit, xvar=""lambda"",label=T)
 <summary> Some possible answers </summary>
 
 <h4>Some possible answers</h4>
-* The order appears to be $(3,2,1,6,4,10,5,8,7,9)$
-* $\beta_i > 0, i\in \{1,2,3,4,7,8\}$, while $\beta_i<0, i\in \{5,6,9,19\}$
+* The order appears to be $(1,2,3,7,6,5,10,9,4,8)$
+* $\beta_i > 0, i\in \{1,2,3,4,7,9\}$, while $\beta_i<0, i\in \{5,6,8,10\}$
 * Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-2.1), \approx\exp(-2.5)]$
 * In the normal case, we do not have *oracle knowledge*.
 
@@ -685,8 +687,8 @@ plot(fit, xvar=""lambda"",label=T)
 
 The LASSO model will be different depending on how we set $\lambda$. A problem is to decide the optimal $\lambda$ to use. 
 
-* $\lambda$ too *low*: risk of missing relevant variables
-* $\lambda$ too *high*: risk of overfitting 
+* $\lambda$ too *high*: risk of missing relevant variables
+* $\lambda$ too *low*: risk of overfitting 
 
 `glmnet` addresses this using *$k$-fold cross-validation* -- what is that?
 
@@ -697,7 +699,7 @@ The LASSO model will be different depending on how we set $\lambda$. A problem i
 <details>
 <summary> Lecture notes </summary>
 
-The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error, MSE* ($=RSS/N$).
+The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, $MSE$ ($=RSS/N$).
 Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO application, this means that we want to select the $\lambda$ that minimizes the $MSE$
 
 In cross validation, this approach is emaulated by partioning the data at hand into a *training* and  *test* (or *validation*) data set. The model parameters are estimated ('trained') on the the training data and the validated on the test data.
@@ -772,6 +774,10 @@ minlambda=cvglm$lambda.min
 * Finally print a table with the $\beta$ coefficients (including the intercept, $\beta_0$) for the optimal model (i.e.,  at minimum $\lambda$). (Use function`coef`).
 
 ```{r, echo =T, eval=FALSE}
+# Actually the following suffice for output on console
+#coef(cvglm, s=""lambda.min"")
+
+# But to get a nice table:
 require(dplyr)      # for nice table
 require(kableExtra) #for nice table
 

---FILE: session-regularization/session-regularization.md---
@@ -171,7 +171,7 @@ N=100 # number of samples
 P=10 # number of variables
 
 # Draw variables, x_{i,1},...,x_{i,P} for all N individuals, from a uniform distribution in interval (0,1) (this is the default interval for runif)
-X=matrix(runif(N*(P+1)), nrow=N, ncol=P) 
+X=matrix(round(runif(N*(P+1),min=0, max=2)), nrow=N, ncol=P)
 
 # generate a y variable from a multivarite lm of 3 first X variables only
 # intercept
@@ -222,7 +222,7 @@ for(i in seq(1,2)){
   ll[i] <- logLik(lm(Y~Xi))
 }
 # plot likelihoods for models with 1 and 2 vaiables
-plot(ll[seq(1,2)], ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
+plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 # xlim and ylim not really necessary here, but I can reuse the plot statement below, so the plots look similar
 ```
 
@@ -250,7 +250,7 @@ for(i in seq(1,P)){
 }
 
 # plot ll for all models
-plot(ll[seq(1,P)], ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
+plot(ll, ylab=""log L"", xlab=""model #"", type = ""b"", xlim=c(1,P), ylim=c(floor(min(ll)),ceiling(max(ll)))) 
 ```
 
 <details>
@@ -314,74 +314,74 @@ For nested models $-2 \max LRT$ is $\chi^2(d)$-distributed, with $d=$ the differ
 <tbody>
   <tr>
    <td style=""text-align:left;""> 1 vs 2 variables </td>
-   <td style=""text-align:right;""> -139.86 </td>
-   <td style=""text-align:right;""> -137.37 </td>
-   <td style=""text-align:left;""> -2.492 </td>
-   <td style=""text-align:left;""> 0.0256 </td>
+   <td style=""text-align:right;""> -155.80 </td>
+   <td style=""text-align:right;""> -146.90 </td>
+   <td style=""text-align:left;""> -8.9 </td>
+   <td style=""text-align:left;""> 2.45e-05 </td>
    <td style=""text-align:left;""> yes </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 2 vs 3 variables </td>
-   <td style=""text-align:right;""> -137.37 </td>
-   <td style=""text-align:right;""> -136.00 </td>
-   <td style=""text-align:left;""> -1.366 </td>
-   <td style=""text-align:left;""> 0.0983 </td>
-   <td style=""text-align:left;""> no </td>
+   <td style=""text-align:right;""> -146.90 </td>
+   <td style=""text-align:right;""> -136.73 </td>
+   <td style=""text-align:left;""> -10.17 </td>
+   <td style=""text-align:left;""> 6.48e-06 </td>
+   <td style=""text-align:left;""> yes </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 3 vs 4 variables </td>
-   <td style=""text-align:right;""> -136.00 </td>
-   <td style=""text-align:right;""> -135.85 </td>
-   <td style=""text-align:left;""> -0.1471 </td>
-   <td style=""text-align:left;""> 0.588 </td>
+   <td style=""text-align:right;""> -136.73 </td>
+   <td style=""text-align:right;""> -136.69 </td>
+   <td style=""text-align:left;""> -0.04215 </td>
+   <td style=""text-align:left;""> 0.772 </td>
    <td style=""text-align:left;""> no </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 4 vs 5 variables </td>
-   <td style=""text-align:right;""> -135.85 </td>
-   <td style=""text-align:right;""> -135.73 </td>
-   <td style=""text-align:left;""> -0.1271 </td>
-   <td style=""text-align:left;""> 0.614 </td>
+   <td style=""text-align:right;""> -136.69 </td>
+   <td style=""text-align:right;""> -136.23 </td>
+   <td style=""text-align:left;""> -0.4601 </td>
+   <td style=""text-align:left;""> 0.337 </td>
    <td style=""text-align:left;""> no </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 5 vs 6 variables </td>
-   <td style=""text-align:right;""> -135.73 </td>
-   <td style=""text-align:right;""> -135.27 </td>
-   <td style=""text-align:left;""> -0.4527 </td>
-   <td style=""text-align:left;""> 0.341 </td>
+   <td style=""text-align:right;""> -136.23 </td>
+   <td style=""text-align:right;""> -135.83 </td>
+   <td style=""text-align:left;""> -0.4016 </td>
+   <td style=""text-align:left;""> 0.37 </td>
    <td style=""text-align:left;""> no </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 6 vs 7 variables </td>
-   <td style=""text-align:right;""> -135.27 </td>
-   <td style=""text-align:right;""> -135.25 </td>
-   <td style=""text-align:left;""> -0.0238 </td>
-   <td style=""text-align:left;""> 0.827 </td>
+   <td style=""text-align:right;""> -135.83 </td>
+   <td style=""text-align:right;""> -135.35 </td>
+   <td style=""text-align:left;""> -0.4803 </td>
+   <td style=""text-align:left;""> 0.327 </td>
    <td style=""text-align:left;""> no </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 7 vs 8 variables </td>
-   <td style=""text-align:right;""> -135.25 </td>
-   <td style=""text-align:right;""> -135.22 </td>
-   <td style=""text-align:left;""> -0.02914 </td>
-   <td style=""text-align:left;""> 0.809 </td>
+   <td style=""text-align:right;""> -135.35 </td>
+   <td style=""text-align:right;""> -135.31 </td>
+   <td style=""text-align:left;""> -0.04266 </td>
+   <td style=""text-align:left;""> 0.77 </td>
    <td style=""text-align:left;""> no </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 8 vs 9 variables </td>
-   <td style=""text-align:right;""> -135.22 </td>
-   <td style=""text-align:right;""> -135.19 </td>
-   <td style=""text-align:left;""> -0.02746 </td>
-   <td style=""text-align:left;""> 0.815 </td>
+   <td style=""text-align:right;""> -135.31 </td>
+   <td style=""text-align:right;""> -135.30 </td>
+   <td style=""text-align:left;""> -0.0002981 </td>
+   <td style=""text-align:left;""> 0.981 </td>
    <td style=""text-align:left;""> no </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 9 vs 10 variables </td>
-   <td style=""text-align:right;""> -135.19 </td>
-   <td style=""text-align:right;""> -134.79 </td>
-   <td style=""text-align:left;""> -0.4047 </td>
-   <td style=""text-align:left;""> 0.368 </td>
+   <td style=""text-align:right;""> -135.30 </td>
+   <td style=""text-align:right;""> -134.55 </td>
+   <td style=""text-align:left;""> -0.7536 </td>
+   <td style=""text-align:left;""> 0.22 </td>
    <td style=""text-align:left;""> no </td>
   </tr>
 </tbody>
@@ -421,7 +421,7 @@ for(i in seq(1,P)){
   pl[i] = -AIC(fit)/2
 }
 # plot ll of all models
-plot(pl[seq(1,P)], xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""log pL"", xlab=""model #"", type = ""b"")
+plot(pl, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""log pL"", xlab=""model #"", type = ""b"")
 ```
 
 <img src=""session-regularization_files/figure-html/unnamed-chunk-7-1.png"" width=""100%"" />
@@ -518,63 +518,63 @@ kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Mini
 <tbody>
   <tr>
    <td style=""text-align:left;""> 1 variable </td>
-   <td style=""text-align:right;""> 285.71 </td>
+   <td style=""text-align:right;""> 317.61 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.156453 </td>
+   <td style=""text-align:left;""> 3.841e-08 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 2 variables </td>
-   <td style=""text-align:right;""> 282.73 </td>
+   <td style=""text-align:right;""> 301.80 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.694197 </td>
+   <td style=""text-align:left;""> 1.041e-04 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 3 variables </td>
-   <td style=""text-align:right;""> 282.00 </td>
+   <td style=""text-align:right;""> 283.46 </td>
    <td style=""text-align:left;""> Yes </td>
-   <td style=""text-align:left;""> 1.000000 </td>
+   <td style=""text-align:left;""> 1.000e+00 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 4 variables </td>
-   <td style=""text-align:right;""> 283.70 </td>
+   <td style=""text-align:right;""> 285.38 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.427415 </td>
+   <td style=""text-align:left;""> 3.829e-01 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 5 variables </td>
-   <td style=""text-align:right;""> 285.45 </td>
+   <td style=""text-align:right;""> 286.46 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.178173 </td>
+   <td style=""text-align:left;""> 2.231e-01 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 6 variables </td>
-   <td style=""text-align:right;""> 286.55 </td>
+   <td style=""text-align:right;""> 287.66 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.102797 </td>
+   <td style=""text-align:left;""> 1.225e-01 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 7 variables </td>
-   <td style=""text-align:right;""> 288.50 </td>
+   <td style=""text-align:right;""> 288.70 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.038774 </td>
+   <td style=""text-align:left;""> 7.280e-02 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 8 variables </td>
-   <td style=""text-align:right;""> 290.44 </td>
+   <td style=""text-align:right;""> 290.61 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.014699 </td>
+   <td style=""text-align:left;""> 2.802e-02 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 9 variables </td>
-   <td style=""text-align:right;""> 292.38 </td>
+   <td style=""text-align:right;""> 292.61 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.005572 </td>
+   <td style=""text-align:left;""> 1.031e-02 </td>
   </tr>
   <tr>
    <td style=""text-align:left;""> 10 variables </td>
-   <td style=""text-align:right;""> 293.58 </td>
+   <td style=""text-align:right;""> 293.10 </td>
    <td style=""text-align:left;""> - </td>
-   <td style=""text-align:left;""> 0.003058 </td>
+   <td style=""text-align:left;""> 8.067e-03 </td>
   </tr>
 </tbody>
 </table>
@@ -586,8 +586,10 @@ kable(aic, format='html', row.names=F, col.names=c(""Compared models"",""AIC"",""Mini
 
 
 ```r
+require(stats)
+
 # plot AIC of all models
-plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(pl)),ceiling(max(pl))),ylab=""AIC"", xlab=""model #"", type = ""b"")
+plot(aic$aic, xlim=c(1,P), ylim=c(floor(min(aic$aic)),ceiling(max(aic$aic))),ylab=""AIC"", xlab=""model #"", type = ""b"")
 
 # plot relL of all models
 plot(aic$rl, xlim=c(1,P), ylab=""relL"", xlab=""model #"", type = ""b"")
@@ -764,8 +766,8 @@ plot(fit, xvar=""lambda"",label=T)
 <summary> Some possible answers </summary>
 
 <h4>Some possible answers</h4>
-* The order appears to be $(3,2,1,6,4,10,5,8,7,9)$
-* $\beta_i > 0, i\in \{1,2,3,4,7,8\}$, while $\beta_i<0, i\in \{5,6,9,19\}$
+* The order appears to be $(1,2,3,7,6,5,10,9,4,8)$
+* $\beta_i > 0, i\in \{1,2,3,4,7,9\}$, while $\beta_i<0, i\in \{5,6,8,10\}$
 * Given *oracle knowledge*, the correct $\lambda$ appears lie somewhere in the interval $[\approx \exp(-2.1), \approx\exp(-2.5)]$
 * In the normal case, we do not have *oracle knowledge*.
 
@@ -779,8 +781,8 @@ plot(fit, xvar=""lambda"",label=T)
 
 The LASSO model will be different depending on how we set $\lambda$. A problem is to decide the optimal $\lambda$ to use. 
 
-* $\lambda$ too *low*: risk of missing relevant variables
-* $\lambda$ too *high*: risk of overfitting 
+* $\lambda$ too *high*: risk of missing relevant variables
+* $\lambda$ too *low*: risk of overfitting 
 
 `glmnet` addresses this using *$k$-fold cross-validation* -- what is that?
 
@@ -791,7 +793,7 @@ The LASSO model will be different depending on how we set $\lambda$. A problem i
 <details>
 <summary> Lecture notes </summary>
 
-The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error, MSE* ($=RSS/N$).
+The ultimate way of testing an estimated model (with parameters) is to apply it to new data and evaluate how well it performs, e.g., by measuring the *mean squared error*, $MSE$ ($=RSS/N$).
 Naturally, we want to minimize $MSE$, i.e., the error of the model. In our LASSO application, this means that we want to select the $\lambda$ that minimizes the $MSE$
 
 In cross validation, this approach is emaulated by partioning the data at hand into a *training* and  *test* (or *validation*) data set. The model parameters are estimated ('trained') on the the training data and the validated on the test data.
@@ -848,7 +850,7 @@ minlambda=cvglm$lambda.min
 <summary> Some possible answers </summary>
 
 <h4>Some possible answers</h4>
-* Cross-validation-selected optimal lambda is 0.0875179
+* Cross-validation-selected optimal lambda is 0.1064891
 * Yes, this includes only the *oracle knowledge* correct variables $X_1, X_2, X_3$
 
 ***
@@ -859,6 +861,10 @@ minlambda=cvglm$lambda.min
 
 
 ```r
+# Actually the following suffice for output on console
+#coef(cvglm, s=""lambda.min"")
+
+# But to get a nice table:
 require(dplyr)      # for nice table
 require(kableExtra) #for nice table
 
@@ -876,29 +882,29 @@ kable(coefglm, row.names=F) %>%   kable_styling( font_size = 14)
   <tr>
    <th style=""text-align:right;""> beta </th>
    <th style=""text-align:right;""> value (oracle) </th>
-   <th style=""text-align:right;""> estimate(lambda=0.0875) </th>
+   <th style=""text-align:right;""> estimate(lambda=0.106) </th>
   </tr>
  </thead>
 <tbody>
   <tr>
    <td style=""text-align:right;""> 0 </td>
    <td style=""text-align:right;""> 3.0000000 </td>
-   <td style=""text-align:right;""> 3.7018933 </td>
+   <td style=""text-align:right;""> 3.6046135 </td>
   </tr>
   <tr>
    <td style=""text-align:right;""> 1 </td>
    <td style=""text-align:right;""> 0.8125118 </td>
-   <td style=""text-align:right;""> 0.1485417 </td>
+   <td style=""text-align:right;""> 0.5905522 </td>
   </tr>
   <tr>
    <td style=""text-align:right;""> 2 </td>
    <td style=""text-align:right;""> 0.6009469 </td>
-   <td style=""text-align:right;""> 0.4612657 </td>
+   <td style=""text-align:right;""> 0.4631531 </td>
   </tr>
   <tr>
    <td style=""text-align:right;""> 3 </td>
    <td style=""text-align:right;""> 0.7232911 </td>
-   <td style=""text-align:right;""> 0.2341136 </td>
+   <td style=""text-align:right;""> 0.5204561 </td>
   </tr>
   <tr>
    <td style=""text-align:right;""> 4 </td>
@@ -977,4 +983,4 @@ _stats_, _graphics_, _grDevices_, _utils_, _datasets_, _methods_ and _base_
 _pander(v.0.6.3)_, _glmnet(v.2.0-16)_, _foreach(v.1.4.4)_, _Matrix(v.1.2-17)_, _dplyr(v.0.8.0.1)_, _kableExtra(v.1.1.0)_, _lmtest(v.0.9-37)_, _zoo(v.1.8-5)_ and _knitr(v.1.22)_
 
 **loaded via a namespace (and not attached):** 
-_Rcpp(v.1.0.1)_, _pillar(v.1.3.1)_, _compiler(v.3.5.3)_, _highr(v.0.8)_, _iterators(v.1.0.10)_, _tools(v.3.5.3)_, _digest(v.0.6.18)_, _evaluate(v.0.13)_, _tibble(v.2.1.1)_, _lattice(v.0.20-38)_, _viridisLite(v.0.3.0)_, _pkgconfig(v.2.0.2)_, _rlang(v.0.3.4)_, _rstudioapi(v.0.10)_, _yaml(v.2.2.0)_, _xfun(v.0.6)_, _stringr(v.1.4.0)_, _httr(v.1.4.0)_, _xml2(v.1.2.0)_, _hms(v.0.4.2)_, _tidyselect(v.0.2.5)_, _grid(v.3.5.3)_, _webshot(v.0.5.1)_, _glue(v.1.3.1)_, _R6(v.2.4.0)_, _rmarkdown(v.1.12)_, _purrr(v.0.3.2)_, _readr(v.1.3.1)_, _magrittr(v.1.5)_, _codetools(v.0.2-16)_, _scales(v.1.0.0)_, _htmltools(v.0.3.6)_, _assertthat(v.0.2.1)_, _rvest(v.0.3.3)_, _colorspace(v.1.4-1)_, _stringi(v.1.4.3)_, _munsell(v.0.5.0)_ and _crayon(v.1.3.4)_
+_Rcpp(v.1.0.1)_, _later(v.0.8.0)_, _compiler(v.3.5.3)_, _pillar(v.1.3.1)_, _highr(v.0.8)_, _iterators(v.1.0.10)_, _tools(v.3.5.3)_, _digest(v.0.6.18)_, _packrat(v.0.5.0)_, _jsonlite(v.1.6)_, _evaluate(v.0.13)_, _tibble(v.2.1.1)_, _lattice(v.0.20-38)_, _viridisLite(v.0.3.0)_, _pkgconfig(v.2.0.2)_, _rlang(v.0.3.4)_, _rstudioapi(v.0.10)_, _yaml(v.2.2.0)_, _xfun(v.0.6)_, _stringr(v.1.4.0)_, _httr(v.1.4.0)_, _xml2(v.1.2.0)_, _hms(v.0.4.2)_, _tidyselect(v.0.2.5)_, _grid(v.3.5.3)_, _webshot(v.0.5.1)_, _glue(v.1.3.1)_, _R6(v.2.4.0)_, _rmarkdown(v.1.12)_, _xaringan(v.0.9)_, _servr(v.0.13)_, _purrr(v.0.3.2)_, _readr(v.1.3.1)_, _magrittr(v.1.5)_, _promises(v.1.0.1)_, _codetools(v.0.2-16)_, _scales(v.1.0.0)_, _htmltools(v.0.3.6)_, _rsconnect(v.0.8.13)_, _assertthat(v.0.2.1)_, _rvest(v.0.3.3)_, _mime(v.0.6)_, _colorspace(v.1.4-1)_, _httpuv(v.1.5.1)_, _stringi(v.1.4.3)_, _munsell(v.0.5.0)_ and _crayon(v.1.3.4)_"
NBISweden,workshop-mlbiostatistics,9b623cea9823e98ad13c18ebf7b355fd1353cdaa,Marcin Kierczak,kiero@kiero.pl,2019-05-23T12:26:34Z,Marcin Kierczak,kiero@kiero.pl,2019-05-23T12:26:34Z,Fixed title in ML,session-ml/session-ml-files/.DS_Store;session-ml/session-ml.md,False,False,False,False,1,1,2,"---FILE: session-ml/session-ml.md---
@@ -1,4 +1,4 @@
-Session example
+Session Machine Learning
 ================
 
 ### Learning outcomes"
NBISweden,workshop-mlbiostatistics,356283958a52d5f0f21d3a3c397eb98ec24f1d55,Marcin Kierczak,kiero@kiero.pl,2019-05-23T11:44:04Z,Marcin Kierczak,kiero@kiero.pl,2019-05-23T11:44:04Z,Fixed naming in ML,session-ml/session-ml.md,False,False,False,False,0,0,0,
NBISweden,workshop-mlbiostatistics,32262161f15db44e6a5022d12f0635948e2d3815,olgadet,olga.dethlefsen@nbis.se,2019-05-20T17:03:42Z,olgadet,olga.dethlefsen@nbis.se,2019-05-20T17:03:42Z,Fix Warren's affiliation in intro,schedule.md;session-intro/session-intro.Rmd;session-intro/session-intro.html,True,False,True,False,3,3,6,"---FILE: schedule.md---
@@ -83,7 +83,7 @@ ____
 - [Paulo Czarnewski][paulo], PhD, NBIS, SciLifeLab, University of Stockholm
 - [Marcin Kierczak][marcin], PhD, NBIS, SciLifeLab, Uppsala University
 - [Bengt Sennblad][bengt], PhD, NBIS, SciLifeLab, Uppsala University
-- [Warren Kretzschmar][warren], PhD, Department of Medicine Huddinge, Karolinska Institute
+- [Warren Kretzschmar][warren], PhD, Department of Medicine, Huddinge, Karolinska Institute
 
 
 [eva]: https://nbis.se/about/staff/eva-freyhult/

---FILE: session-intro/session-intro.Rmd---
@@ -28,7 +28,7 @@ knitr::opts_chunk$set(cache = FALSE)
 - [Paulo Czarnewski][paulo], PhD, NBIS, SciLifeLab, University of Stockholm
 - [Marcin Kierczak][marcin], PhD, NBIS, SciLifeLab, Uppsala University
 - [Bengt Sennblad][bengt], PhD, NBIS, SciLifeLab, Uppsala University
-- [Warren Kretzschmar][warren], PhD, Department of Medicine, Karolinska Institutet
+- [Warren Kretzschmar][warren], PhD, Department of Medicine, Huddinge, Karolinska Institutet
 
 <br/><br/>
 

---FILE: session-intro/session-intro.html---
@@ -392,7 +392,7 @@ <h5>Teaching team</h5>
 <li><a href=""https://nbis.se/about/staff/paulo-czarnewski/"">Paulo Czarnewski</a>, PhD, NBIS, SciLifeLab, University of Stockholm</li>
 <li><a href=""https://nbis.se/about/staff/marcin-kierczak/"">Marcin Kierczak</a>, PhD, NBIS, SciLifeLab, Uppsala University</li>
 <li><a href=""https://nbis.se/about/staff/bengt-sennblad/"">Bengt Sennblad</a>, PhD, NBIS, SciLifeLab, Uppsala University</li>
-<li><a href=""https://ki.se/en/people/warkre"">Warren Kretzschmar</a>, PhD, Department of Medicine, Karolinska Institutet</li>
+<li><a href=""https://ki.se/en/people/warkre"">Warren Kretzschmar</a>, PhD, Department of Medicine, Huddinge, Karolinska Institutet</li>
 </ul>
 <p><br /><br /></p>
 </div>"
NBISweden,workshop-mlbiostatistics,bcd426d6fc58577c0325e59384105b19d63fe050,Eva Freyhult,eva.freyhult@medsci.uu.se,2019-05-20T13:15:46Z,Eva Freyhult,eva.freyhult@medsci.uu.se,2019-05-20T13:15:46Z,Fix typos and reorder exercises,session-confidenceinterval/session-confidenceinterval.Rmd;session-confidenceinterval/session-confidenceinterval.html;session-foundations/biostatisticsexercises.Rmd;session-foundations/biostatisticsexercises.html;session-hypothesistest/session-hypothesistest.Rmd;session-hypothesistest/session-hypothesistest.html,True,False,True,False,38,35,73,"---FILE: session-confidenceinterval/session-confidenceinterval.Rmd---
@@ -197,7 +197,7 @@ $P(\mu-1.96\frac{s}{\sqrt{n}} \leq \bar X \leq \mu+1.96\frac{s}{\sqrt{n}}) < 0.9
 
 The statistic
 
-$$t = \frac{\bar X - m}{\frac{s}{\sqrt{n}}}$$
+$$t = \frac{\bar X - \mu}{\frac{s}{\sqrt{n}}}$$
 
 follows a t-distribution with $n-1$ degrees of freedom.
 

---FILE: session-foundations/biostatisticsexercises.Rmd---
@@ -55,13 +55,6 @@ x <- unlist(dice)
 x <- x[x %in% 0:10]
 ```  
 
-## Simulation examples
-
-1. Fourty percent of a large population of men is overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?
-
-2. 20 of the 50 people working in your lab are overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?
-
-
 ## Baby weights: probability
 
 When the entire population is known, probabilities can be computed by summing the number of observations that fulfill the criteria and divide by the total number.
@@ -159,6 +152,13 @@ baby %>% kable() %>% kable_styling(""striped"", full_width = FALSE, font_size = 14
 
 Make up your own distribution. Sample 40 observations from this and compute the sum or mean. Is the sum/mean approximately normally distributed?
 
+
+## Simulation examples
+
+1. Fourty percent of a large population of men is overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?
+
+2. 20 of the 50 people working in your lab are overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?
+
 ------
 
 # Confidence intervals
@@ -226,10 +226,6 @@ If 30% of a large population is allergic to pollen, what is the probability to o
 ## P(P>=0.42)
 ```
 
-## Active substance
-
-A pharmaceutical company produce a pill that should have 100 mg active substance. The company claim that the amount varies abit and follows a normal distribution with $\mu$=100mg and $\sigma$=2mg. You test 10 random pills from one batch and observe that they on average have 97mg active substance. Is there a reason to believe that the amount of active substance in the pills in the batch is less than 100mg on average?
-
 ## Do high fat diet lead to increased body weight in mice?
 
 Study setup:
@@ -315,6 +311,10 @@ $$P(\bar X_2 - \bar X_1 \geq d_{obs} | H_0)$$
 
 ```
 
+## Active substance
+
+A pharmaceutical company produce a pill that should have 100 mg active substance. The company claim that the amount varies abit and follows a normal distribution with $\mu$=100mg and $\sigma$=2mg. You test 10 random pills from one batch and observe that they on average have 97mg active substance. Is there a reason to believe that the amount of active substance in the pills in the batch is less than 100mg on average?
+
 ## Morning melatonin
 
 You study the morning melatonin level in saliva in two patient groups and want to know if there is a difference in mean melatonin levels in the two groups. Based on the observations; group 1: 12.0, 5.0, 3.4, 2.1, 4.3 pg/ml and group 2: 3.1, 4.2, 1.2, 6.3, 4.1, 3.6 pg/ml perform a hypothesis test to investigate the if there is a difference in mean.
@@ -323,7 +323,9 @@ Define null and alternative hypotheis, set the significance level, perform an ap
 
 ## Melatonin during the day
 
-Melatonin in saliva is measured in 5 patients during the day, morning and before lunch values are presente in the table. Is there a reason to believe that the melatonin level is increased in the morning compared to before lunch in this patient group?
+Melatonin in saliva is measured in 5 patients during the day, morning and before lunch values are presented in the table. Is there a reason to believe that the melatonin level is increased in the morning compared to before lunch in this patient group?
+
+Note: paired samples!
 
 ```{r, echo=FALSE}
 kable(data.frame(id=1:5, morning=c(12, 3.2,28,5.8,13), ""before lunch""=c(2.1,2.6,2.4,11.0,2.7)), caption=""Melatonin levels (pg/ml) for five patients in the morning and before lunch."") %>% kable_styling(""striped"", full_width = F)

---FILE: session-foundations/biostatisticsexercises.html---
@@ -426,15 +426,8 @@ <h2><span class=""header-section-number"">1.1</span> Exercise: Dice experiment</h2
 x &lt;- unlist(dice)
 x &lt;- x[x %in% 0:10]</code></pre>
 </div>
-<div id=""simulation-examples"" class=""section level2"">
-<h2><span class=""header-section-number"">1.2</span> Simulation examples</h2>
-<ol style=""list-style-type: decimal"">
-<li><p>Fourty percent of a large population of men is overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?</p></li>
-<li><p>20 of the 50 people working in your lab are overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?</p></li>
-</ol>
-</div>
 <div id=""baby-weights-probability"" class=""section level2"">
-<h2><span class=""header-section-number"">1.3</span> Baby weights: probability</h2>
+<h2><span class=""header-section-number"">1.2</span> Baby weights: probability</h2>
 <p>When the entire population is known, probabilities can be computed by summing the number of observations that fulfill the criteria and divide by the total number.</p>
 <p>Use the <code>babies</code> data set (in the <code>UsingR</code> package) to compute the following probabilities (let <span class=""math inline"">\(W\)</span> denote the baby weight (a random variable)):</p>
 <ul>
@@ -559,7 +552,7 @@ <h2><span class=""header-section-number"">1.3</span> Baby weights: probability</h2
 </ul>
 </div>
 <div id=""diagnostic-tests"" class=""section level2"">
-<h2><span class=""header-section-number"">1.4</span> Diagnostic tests</h2>
+<h2><span class=""header-section-number"">1.3</span> Diagnostic tests</h2>
 <p>The performance of a diagnostic test for cancer type is summarized in the below table. In total 1000 persons are tested, 20 of these did actually have cancer. The test was positive (+) in 114 cases and negative (-) in 886 cases.</p>
 <table class=""table table-striped"" style=""width: auto !important; margin-left: auto; margin-right: auto;"">
 <thead>
@@ -638,7 +631,7 @@ <h2><span class=""header-section-number"">1.4</span> Diagnostic tests</h2>
 </ul>
 </div>
 <div id=""data-summary"" class=""section level2"">
-<h2><span class=""header-section-number"">1.5</span> Data summary</h2>
+<h2><span class=""header-section-number"">1.4</span> Data summary</h2>
 <p>Consider the below data and summarize each of the variables, separately for smokers and non-smokers.</p>
 <table class=""table table-striped"" style=""font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;"">
 <thead>
@@ -934,7 +927,7 @@ <h2><span class=""header-section-number"">1.5</span> Data summary</h2>
 </table>
 </div>
 <div id=""get-familiar-with-the-normal-distribution"" class=""section level2"">
-<h2><span class=""header-section-number"">1.6</span> Get familiar with the normal distribution</h2>
+<h2><span class=""header-section-number"">1.5</span> Get familiar with the normal distribution</h2>
 <ol style=""list-style-type: decimal"">
 <li>Let <span class=""math inline"">\(Z \sim N(0,1)\)</span> be a standard normal random variable, and compute;</li>
 </ol>
@@ -974,8 +967,15 @@ <h2><span class=""header-section-number"">1.6</span> Get familiar with the normal
 </ul>
 </div>
 <div id=""central-limit-theorem"" class=""section level2"">
-<h2><span class=""header-section-number"">1.7</span> Central limit theorem</h2>
+<h2><span class=""header-section-number"">1.6</span> Central limit theorem</h2>
 <p>Make up your own distribution. Sample 40 observations from this and compute the sum or mean. Is the sum/mean approximately normally distributed?</p>
+</div>
+<div id=""simulation-examples"" class=""section level2"">
+<h2><span class=""header-section-number"">1.7</span> Simulation examples</h2>
+<ol style=""list-style-type: decimal"">
+<li><p>Fourty percent of a large population of men is overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?</p></li>
+<li><p>20 of the 50 people working in your lab are overweight. If you randomly select 10 people to participate in your study, what is the probability that at least 7 will be overweight?</p></li>
+</ol>
 <hr />
 </div>
 </div>
@@ -1042,12 +1042,8 @@ <h2><span class=""header-section-number"">3.1</span> Pollen allergy</h2>
 
 ## P(P&gt;=0.42)</code></pre>
 </div>
-<div id=""active-substance"" class=""section level2"">
-<h2><span class=""header-section-number"">3.2</span> Active substance</h2>
-<p>A pharmaceutical company produce a pill that should have 100 mg active substance. The company claim that the amount varies abit and follows a normal distribution with <span class=""math inline"">\(\mu\)</span>=100mg and <span class=""math inline"">\(\sigma\)</span>=2mg. You test 10 random pills from one batch and observe that they on average have 97mg active substance. Is there a reason to believe that the amount of active substance in the pills in the batch is less than 100mg on average?</p>
-</div>
 <div id=""do-high-fat-diet-lead-to-increased-body-weight-in-mice"" class=""section level2"">
-<h2><span class=""header-section-number"">3.3</span> Do high fat diet lead to increased body weight in mice?</h2>
+<h2><span class=""header-section-number"">3.2</span> Do high fat diet lead to increased body weight in mice?</h2>
 <p>Study setup:</p>
 <ol style=""list-style-type: decimal"">
 <li>Order 24 female mice from a lab.</li>
@@ -1150,7 +1146,7 @@ <h2><span class=""header-section-number"">3.3</span> Do high fat diet lead to incr
 <p>Mean weight of mice on high-fat diet: <span class=""math inline"">\(\bar x_2 = 26.71\)</span></p>
 <p>Difference in mean weights: <span class=""math inline"">\(d = \bar x_2 - \bar x_1 = 2.9375\)</span></p>
 <div id=""permutation-test"" class=""section level3"">
-<h3><span class=""header-section-number"">3.3.1</span> Permutation test</h3>
+<h3><span class=""header-section-number"">3.2.1</span> Permutation test</h3>
 <p>We need to compute the sampling distribution under <span class=""math inline"">\(H_0\)</span>.</p>
 <p>If high-fat diet has no effect, i.e. if <span class=""math inline"">\(H_0\)</span> was true, the result would be as if all mice were given the ordinary diet. What can we expect if all mice are fed with the ordinary food? The 24 mice were initially from the same population, depending on how the mice are randomly assigned to high-fat and normal group, the mean weights would differ.</p>
 <p>Assume <span class=""math inline"">\(H_0\)</span> is true, i.e. assume all mice are equivalent and</p>
@@ -1163,14 +1159,19 @@ <h3><span class=""header-section-number"">3.3.1</span> Permutation test</h3>
 <p><span class=""math display"">\[P(\bar X_2 - \bar X_1 \geq d_{obs} | H_0)\]</span></p>
 </div>
 </div>
+<div id=""active-substance"" class=""section level2"">
+<h2><span class=""header-section-number"">3.3</span> Active substance</h2>
+<p>A pharmaceutical company produce a pill that should have 100 mg active substance. The company claim that the amount varies abit and follows a normal distribution with <span class=""math inline"">\(\mu\)</span>=100mg and <span class=""math inline"">\(\sigma\)</span>=2mg. You test 10 random pills from one batch and observe that they on average have 97mg active substance. Is there a reason to believe that the amount of active substance in the pills in the batch is less than 100mg on average?</p>
+</div>
 <div id=""morning-melatonin"" class=""section level2"">
 <h2><span class=""header-section-number"">3.4</span> Morning melatonin</h2>
 <p>You study the morning melatonin level in saliva in two patient groups and want to know if there is a difference in mean melatonin levels in the two groups. Based on the observations; group 1: 12.0, 5.0, 3.4, 2.1, 4.3 pg/ml and group 2: 3.1, 4.2, 1.2, 6.3, 4.1, 3.6 pg/ml perform a hypothesis test to investigate the if there is a difference in mean.</p>
 <p>Define null and alternative hypotheis, set the significance level, perform an appropriate hypotheis test and draw a conclusion.</p>
 </div>
 <div id=""melatonin-during-the-day"" class=""section level2"">
 <h2><span class=""header-section-number"">3.5</span> Melatonin during the day</h2>
-<p>Melatonin in saliva is measured in 5 patients during the day, morning and before lunch values are presente in the table. Is there a reason to believe that the melatonin level is increased in the morning compared to before lunch in this patient group?</p>
+<p>Melatonin in saliva is measured in 5 patients during the day, morning and before lunch values are presented in the table. Is there a reason to believe that the melatonin level is increased in the morning compared to before lunch in this patient group?</p>
+<p>Note: paired samples!</p>
 <table class=""table table-striped"" style=""width: auto !important; margin-left: auto; margin-right: auto;"">
 <caption>
 Melatonin levels (pg/ml) for five patients in the morning and before lunch.

---FILE: session-hypothesistest/session-hypothesistest.Rmd---
@@ -230,7 +230,7 @@ $$P(\bar X_2 - \bar X_1 \geq d_{obs} | H_0) = `r sprintf(""%.3g"",mean(dnull.perm>
 $$H_0: \mu_2 = \mu_1\\
 H_1: \mu_2>\mu_1$$
 
-Lets assume that both the control and treatment populations are normally distributed, with unknown mean, but known standard deviations, $\sigma_1=3.4$ and $\sigma_2=5.1$.
+Lets assume that both mouse body weights in control and treatment groups are independent and normally distributed, with unknown mean, but known standard deviations, $\sigma_1=3.4$ and $\sigma_2=5.1$.
 
 $$X_1 \sim N(\mu_1, \sigma_1) \\
 X_2 \sim N(\mu_2, \sigma_2)$$

---FILE: session-hypothesistest/session-hypothesistest.html---
@@ -596,7 +596,7 @@ <h2>Parametric test</h2>
 <h3>Two samples, known <span class=""math inline"">\(\sigma_1\)</span> and <span class=""math inline"">\(\sigma_2\)</span></h3>
 <p><span class=""math display"">\[H_0: \mu_2 = \mu_1\\
 H_1: \mu_2&gt;\mu_1\]</span></p>
-<p>Lets assume that both the control and treatment populations are normally distributed, with unknown mean, but known standard deviations, <span class=""math inline"">\(\sigma_1=3.4\)</span> and <span class=""math inline"">\(\sigma_2=5.1\)</span>.</p>
+<p>Lets assume that both mouse body weights in control and treatment groups are independent and normally distributed, with unknown mean, but known standard deviations, <span class=""math inline"">\(\sigma_1=3.4\)</span> and <span class=""math inline"">\(\sigma_2=5.1\)</span>.</p>
 <p><span class=""math display"">\[X_1 \sim N(\mu_1, \sigma_1) \\
 X_2 \sim N(\mu_2, \sigma_2)\]</span></p>
 <p>If follows that the sample means are</p>"
NBISweden,workshop-mlbiostatistics,c252f6ffd8a9ba4eb39880b9315777176a81c5cb,Winni Kretzschmar,wkretzsch@gmail.com,2019-05-20T06:33:12Z,GitHub,noreply@github.com,2019-05-20T06:33:12Z,Fix personal web page and affiliation for Warren,index.md,False,False,False,False,2,3,5,"---FILE: index.md---
@@ -61,7 +61,7 @@ In this course we focus on active learning approach. The course participants are
 - [Paulo Czarnewski][paulo], PhD, NBIS, SciLifeLab, University of Stockholm
 - [Marcin Kierczak][marcin], PhD, NBIS, SciLifeLab, Uppsala University
 - [Bengt Sennblad][bengt], PhD, NBIS, SciLifeLab, Uppsala University
-- [Warren Kretzschmar][warren], PhD, Department of Medicine, Karolinska Institutet
+- [Warren Kretzschmar][warren], PhD, Department of Medicine Huddinge, Karolinska Institutet
 
 <br/><br/>
 
@@ -106,8 +106,7 @@ In this course we focus on active learning approach. The course participants are
 [paulo]: https://nbis.se/about/staff/paulo-czarnewski/
 [marcin]: https://nbis.se/about/staff/marcin-kierczak/
 [bengt]: https://nbis.se/about/staff/bengt-sennblad/
-[warren]: https://ki.se/en/people/warkre
-
+[warren]: http://www.warrenwk.com
 
 [book-gj]: https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf
 [book-rai]: http://www.rwdc2.com/files/rafa.pdf"
NBISweden,workshop-mlbiostatistics,ace98594f6d93ad300e309e4461ef800b15f2377,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-19T10:15:41Z,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-19T10:15:41Z,Completes session-regression-II,session-regression-II/session-regression-II.Rmd;session-regression-II/session-regression-II.html;session-regression-II/session-regression-II.pdf,True,False,True,False,161,29,190,"---FILE: session-regression-II/session-regression-II.Rmd---
@@ -1,18 +1,19 @@
 ---
 title: 'Session regression II: multiple linear regression'
 output:
-  html_document:
-    df_print: paged
+  pdf_document: 
     toc: true
     number_sections: true
   md_document:
     toc: true
     number_sections: true
     variant: markdown_github
-  pdf_document: default
+  html_document:
+    df_print: paged
+    toc: true
+    number_sections: true
 header-includes: \usepackage{float}
 urlcolor: blue
-
 ---
 
 
@@ -427,15 +428,90 @@ ggplot(aes(x=TV, y=radio, color=residual), data=plot_dat) + geom_point() +
 - The model underestimates sales generated from investment in both add platforms
   (top right and bottom left)
   
-We will learn how to model this synergy as an ""interaction effect"" later on.
+We can model this synergy as an ""interaction term"". Unfortunately, interaction terms
+are beyond the scope of this session. 
+See [further reading](#further-reading) for more on interaction terms.
 
 ## What is the model's prediction accuracy?
 
-As we saw earlier, we can make predictions from our 
+- Previously: we can make predictions from our model using the `predict()` function.
+
+```{r}
+# making a prediction on a new data point
+predict(fit, newdata=data.frame(TV=100, radio=30))
+```
+
+There are three sources of error when making predictions from a linear model
+
+1. *reducible error*: A result of the difference between the estimates 
+   $$\hat{Y} = \hat\beta_0 + \hat\beta_1 X_1 + ... + \hat\beta_p X_p$$
+   and the *true population regression plane*
+   $$f(x) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p$$
+   We use *confidence intervals* to estimate this error.
+2. *model bias*: When our model differs from the true model 
+   (see for example the previous section)
+3. *irreducible error*: The random noise that is part of our system, $\epsilon$. We can
+   use *prediction intervals* to estimate this error.
+
+If we assume that we have the correct model, then we can ask two kinds of questions:
+
+1. How far is $\hat{Y}$ from $f(x)$? 
+   - We use *confidence intervals* to talk about how 
+     our estimate of average sales will differ from the true average of sales
+2. How far is any one prediction from its true value?
+   - For this, we use *prediction intervals*
 
+Prediction intervals are always larger than confidence intervals because prediction
+intervals quantify both the reducible and irreducible error.
 
-# Qualitative predictors 
-# Interaction terms
-# Non-linear transformation of the predictors 
-# Potential problems: non-linearity, collinearity 
-# Logistic regression
+Let's try and calculate the confidence and prediction intervals around $\hat{Y}$ of our
+(dubious) model fit:
+
+```{r}
+preds = as.data.frame(predict(fit, interval=""confidence""))
+head(preds)
+preds2 = as.data.frame(predict(fit, interval=""prediction""))
+head(preds2)
+preds$lwr_pred = preds2$lwr
+preds$upr_pred = preds2$upr
+
+preds = preds[order(preds$fit),]
+preds$index = 1:nrow(preds)
+head(preds)
+```
+
+```{r, collapse=TRUE}
+plot(preds$fit, type='l')
+lines(preds$index, preds$upr, col='red')
+lines(preds$index, preds$lwr, col='red')
+lines(preds$index, preds$upr_pred, col='blue')
+lines(preds$index, preds$lwr_pred, col='blue')
+```
+
+We can see that the prediction intervals are larger than the confidence intervals. 
+However, neither the confidence intervals nor the prediction intervals are valid here.
+
+**Caveats**:
+
+  - Our model does not fit the data well, and so we are also dealing with model bias.
+    The confidence interval calculations assume a good model fit, which is clearly not 
+    the case here.
+  - Calculating prediction intervals on the data we used to create the model underestimates
+    the prediction error on new data. Generally, we are interested in prediction intervals
+    for new data. For that we need to calculate prediction intervals on a separate 
+    (or held out) data set.
+
+# Further reading
+
+There is much more to linear regression. Section 3.3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/data.html) is worth a read before you start 
+fitting linear models to your data. That section discusses the following topics:
+
+  - Qualitative predictors 
+  - Interaction terms
+  - Non-linear transformation of the predictors 
+  - Potential problems: non-linearity, collinearity, outliers, heteroskedasticity
+  - Logistic regression
+  
+The R builtin functions for visualization are sometimes not as helpful for quickly 
+looking at data in many different ways. I find the `R` library
+[`ggplot2`](https://ggplot2.tidyverse.org/) very useful in such cases."
NBISweden,workshop-mlbiostatistics,5aec7ff54118cec4b89532572846badf7e941291,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T09:07:08Z,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T09:07:08Z,Fix exercise interpretation html,session-regression-II/session-regression-II.html,False,False,False,False,2,2,4,"---FILE: session-regression-II/session-regression-II.html---
@@ -1726,7 +1726,7 @@ <h2>Excercise: fitting linear regressions on multivariate data</h2>
 ## Coefficients:
 ## (Intercept)        radio  
 ##      9.3116       0.2025</code></pre>
-<p>radio appears to help as well, although not as much. newspaper?</p>
+<p>radio appears to help even more!</p>
 <pre class=""r""><code>lm(sales ~ newspaper, data=ads)</code></pre>
 <pre><code>## 
 ## Call:
@@ -1735,7 +1735,7 @@ <h2>Excercise: fitting linear regressions on multivariate data</h2>
 ## Coefficients:
 ## (Intercept)    newspaper  
 ##    12.35141      0.05469</code></pre>
-<p>newspaper appears to have a similar effect to TV. But there seemed to be less correlation in the previous pairs plot. What’s going on?</p>
+<p>newspaper appears to have a similar effect to TV. But there seemed to be much more noise between sales and newspaper in the pairs plot. What’s going on?</p>
 <pre class=""r""><code>summary(lm(sales ~ TV, data=ads))</code></pre>
 <pre><code>## 
 ## Call:"
NBISweden,workshop-mlbiostatistics,8243ad1894d8189b02a438d1c702f8adffac04aa,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T09:05:51Z,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T09:05:51Z,Fix exercise interpretation,session-regression-II/session-regression-II.Rmd;session-regression-II/session-regression-II.html,True,False,True,False,9,6,15,"---FILE: session-regression-II/session-regression-II.Rmd---
@@ -107,20 +107,19 @@ Ah! sales is lower case:
 lm(sales ~ TV, data=ads)
 
 ```
-For every five cents spent on advertising our average sales went up by a dollar.
+For every \$1000 spent on TV ads, our average sales went up by five units.
 Pretty sweet! What about radio?
 
 ```{r}
 lm(sales ~ radio, data=ads)
 ```
-radio appears to help as well, although not as much.
-newspaper?
+radio appears to help even more!
 
 ```{r}
 lm(sales ~ newspaper, data=ads)
 ```
 newspaper appears to have a similar effect to TV. 
-But there seemed to be less correlation in the previous pairs plot.  What's going on?
+But there seemed to be much more noise between sales and newspaper in the pairs plot.  What's going on?
 
 ```{r}
 summary(lm(sales ~ TV, data=ads))
@@ -168,8 +167,11 @@ $$\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$
 , which is equivalent to
 $$\text{RSS} = \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_{i1} - ... - \hat\beta_px_{ip})^2$$
 
-## Quiz: [What was $y_i$ again?](https://forms.gle/XUxpgxJbkTp1QfDG8)
+The mathematical formulas for estimating the model coefficients in multiple linear regression
+work similarly to the formulas for linear regression. However, they are more complex and 
+require some linear algebra to understand, so we will skip those formulas here. 
 
+## Quiz: [What was $y_i$ again?](https://forms.gle/XUxpgxJbkTp1QfDG8)
 
 # Relationship between the response and predictors 
 # Model fit 

---FILE: session-regression-II/session-regression-II.html---
@@ -1717,7 +1717,7 @@ <h2>Excercise: fitting linear regressions on multivariate data</h2>
 ## Coefficients:
 ## (Intercept)           TV  
 ##     7.03259      0.04754</code></pre>
-<p>For every five cents spent on advertising our average sales went up by a dollar. Pretty sweet! What about radio?</p>
+<p>For every $1000 spent on TV ads, our average sales went up by five units. Pretty sweet! What about radio?</p>
 <pre class=""r""><code>lm(sales ~ radio, data=ads)</code></pre>
 <pre><code>## 
 ## Call:
@@ -1797,6 +1797,7 @@ <h1>Estimating regression coefficients</h1>
 <p><span class=""math display"">\[\hat{y} = \hat\beta_0 + \hat\beta_1x_1 + ... + \hat\beta_px_p\]</span></p>
 <p>As in linear regression, we choose <span class=""math inline"">\(\beta_j\)</span> such that we minimize the residual sum of squares:</p>
 <p><span class=""math display"">\[\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span> , which is equivalent to <span class=""math display"">\[\text{RSS} = \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_{i1} - ... - \hat\beta_px_{ip})^2\]</span></p>
+<p>The mathematical formulas for estimating the model coefficients in multiple linear regression work similarly to the formulas for linear regression. However, they are more complex and require some linear algebra to understand, so we will skip those formulas here.</p>
 <div id=""quiz-what-was-y_i-again"" class=""section level2"">
 <h2>Quiz: <a href=""https://forms.gle/XUxpgxJbkTp1QfDG8"">What was <span class=""math inline"">\(y_i\)</span> again?</a></h2>
 </div>"
NBISweden,workshop-mlbiostatistics,6463ae91c05f895dfecc71e8540bcee87825004a,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T08:34:31Z,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T08:34:31Z,Add a linear regression exercise,session-regression-II/session-regression-II.Rmd;session-regression-II/session-regression-II.html,True,False,True,False,152,15,167,"---FILE: session-regression-II/session-regression-II.Rmd---
@@ -34,9 +34,9 @@ After this session, a student should be able to:
 
 # Warmup
 
-[Quiz: revisiting linear model specifications](https://forms.gle/z4qKdrcQj1wqUxBV7)
+## [Quiz: revisiting linear model specifications](https://forms.gle/z4qKdrcQj1wqUxBV7)
 
-# Visualizing the Advertising dataset
+## Visualizing the Advertising dataset
 
 In this session we will use the [Advertising dataset](http://www-bcf.usc.edu/~gareth/ISL/data.html).
 This simple dataset consists of sales data for 200 products along with the amount of money spent 
@@ -81,12 +81,61 @@ Ah, that's better.
 
 From the pairs plot we can see that:
 
-1. TV expenditure appears to be correlated with sales, although as TV expenditure goes up the
-   variance associated with sales increases as well
-2. radio expenditure appears to be correlated with sales
-3. newspaper sales do not look very correlated to sales
+1. TV expenditure appears to be correlated with sales
+2. As TV expenditure goes up, the variance associated with sales increases as well
+3. radio expenditure appears to be correlated with sales
+4. newspaper sales do not look very correlated to sales
 
-It looks like more than one variable could be used to explain sales. This is where multiple linear regression comes in.
+It looks like more than one variable could be used to explain sales. 
+How would we handle this in the simple regression? 
+
+## Excercise: fitting linear regressions on multivariate data
+
+We can fit a linear regression for TV vs Sales this way:
+```{r error=TRUE}
+lm(Sales ~ TV, data=ads)
+```
+
+Oops that did not work! Let's see what's up:
+
+```{r}
+names(ads)
+```
+Ah! sales is lower case:
+```{r}
+
+lm(sales ~ TV, data=ads)
+
+```
+For every five cents spent on advertising our average sales went up by a dollar.
+Pretty sweet! What about radio?
+
+```{r}
+lm(sales ~ radio, data=ads)
+```
+radio appears to help as well, although not as much.
+newspaper?
+
+```{r}
+lm(sales ~ newspaper, data=ads)
+```
+newspaper appears to have a similar effect to TV. 
+But there seemed to be less correlation in the previous pairs plot.  What's going on?
+
+```{r}
+summary(lm(sales ~ TV, data=ads))
+summary(lm(sales ~ newspaper, data=ads))
+```
+The answer is in the $R^2$ values: TV has a much higher $R^2$ than newspaper.
+
+We can fit a linear model for each explanatory variable separately, but we are left
+with two problems:
+
+1. How would be combine the three models into a single model to create a single prediction for Sales?
+2. The pairs plot shows us that the explanatory variables are correlated. The linear regression
+   fits ignore all other explanatory variables, and this can lead to incorrect predictions.
+
+This is where multiple linear regression comes in.
 It allows us to create a single model for predicting sales from multiple explanatory variables, and it allows us to
 create a model that takes correlation between explanatory variables into account.
 
@@ -119,7 +168,8 @@ $$\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$
 , which is equivalent to
 $$\text{RSS} = \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_{i1} - ... - \hat\beta_px_{ip})^2$$
 
-Quiz: [What was $y_i$ again?](https://forms.gle/XUxpgxJbkTp1QfDG8)
+## Quiz: [What was $y_i$ again?](https://forms.gle/XUxpgxJbkTp1QfDG8)
+
 
 # Relationship between the response and predictors 
 # Model fit 

---FILE: session-regression-II/session-regression-II.html---
@@ -1664,10 +1664,11 @@ <h1>Learning outcomes</h1>
 </div>
 <div id=""warmup"" class=""section level1"">
 <h1>Warmup</h1>
-<p><a href=""https://forms.gle/z4qKdrcQj1wqUxBV7"">Quiz: revisiting linear model specifications</a></p>
+<div id=""quiz-revisiting-linear-model-specifications"" class=""section level2"">
+<h2><a href=""https://forms.gle/z4qKdrcQj1wqUxBV7"">Quiz: revisiting linear model specifications</a></h2>
 </div>
-<div id=""visualizing-the-advertising-dataset"" class=""section level1"">
-<h1>Visualizing the Advertising dataset</h1>
+<div id=""visualizing-the-advertising-dataset"" class=""section level2"">
+<h2>Visualizing the Advertising dataset</h2>
 <p>In this session we will use the <a href=""http://www-bcf.usc.edu/~gareth/ISL/data.html"">Advertising dataset</a>. This simple dataset consists of sales data for 200 products along with the amount of money spent on TV, radio, and newspaper ads. We would like to know how best to spend advertizing money to maximize sales.</p>
 <p>To begin with, let us familiarize ourselves with the dataset. The data are stored in the <code>data</code> subdirectory of this session directory.</p>
 <pre class=""r""><code># load the data
@@ -1692,17 +1693,101 @@ <h1>Visualizing the Advertising dataset</h1>
 <p>Ah, that’s better.</p>
 <p>From the pairs plot we can see that:</p>
 <ol style=""list-style-type: decimal"">
-<li>TV expenditure appears to be correlated with sales, although as TV expenditure goes up the variance associated with sales increases as well</li>
+<li>TV expenditure appears to be correlated with sales</li>
+<li>As TV expenditure goes up, the variance associated with sales increases as well</li>
 <li>radio expenditure appears to be correlated with sales</li>
 <li>newspaper sales do not look very correlated to sales</li>
 </ol>
-<p>It looks like more than one variable could be used to explain sales. This is where multiple linear regression comes in. It allows us to create a single model for predicting sales from multiple explanatory variables, and it allows us to create a model that takes correlation between explanatory variables into account.</p>
+<p>It looks like more than one variable could be used to explain sales. How would we handle this in the simple regression?</p>
+</div>
+<div id=""excercise-fitting-linear-regressions-on-multivariate-data"" class=""section level2"">
+<h2>Excercise: fitting linear regressions on multivariate data</h2>
+<p>We can fit a linear regression for TV vs Sales this way:</p>
+<pre class=""r""><code>lm(Sales ~ TV, data=ads)</code></pre>
+<pre><code>## Error in eval(predvars, data, env): object 'Sales' not found</code></pre>
+<p>Oops that did not work! Let’s see what’s up:</p>
+<pre class=""r""><code>names(ads)</code></pre>
+<pre><code>## [1] &quot;TV&quot;        &quot;radio&quot;     &quot;newspaper&quot; &quot;sales&quot;</code></pre>
+<p>Ah! sales is lower case:</p>
+<pre class=""r""><code>lm(sales ~ TV, data=ads)</code></pre>
+<pre><code>## 
+## Call:
+## lm(formula = sales ~ TV, data = ads)
+## 
+## Coefficients:
+## (Intercept)           TV  
+##     7.03259      0.04754</code></pre>
+<p>For every five cents spent on advertising our average sales went up by a dollar. Pretty sweet! What about radio?</p>
+<pre class=""r""><code>lm(sales ~ radio, data=ads)</code></pre>
+<pre><code>## 
+## Call:
+## lm(formula = sales ~ radio, data = ads)
+## 
+## Coefficients:
+## (Intercept)        radio  
+##      9.3116       0.2025</code></pre>
+<p>radio appears to help as well, although not as much. newspaper?</p>
+<pre class=""r""><code>lm(sales ~ newspaper, data=ads)</code></pre>
+<pre><code>## 
+## Call:
+## lm(formula = sales ~ newspaper, data = ads)
+## 
+## Coefficients:
+## (Intercept)    newspaper  
+##    12.35141      0.05469</code></pre>
+<p>newspaper appears to have a similar effect to TV. But there seemed to be less correlation in the previous pairs plot. What’s going on?</p>
+<pre class=""r""><code>summary(lm(sales ~ TV, data=ads))</code></pre>
+<pre><code>## 
+## Call:
+## lm(formula = sales ~ TV, data = ads)
+## 
+## Residuals:
+##     Min      1Q  Median      3Q     Max 
+## -8.3860 -1.9545 -0.1913  2.0671  7.2124 
+## 
+## Coefficients:
+##             Estimate Std. Error t value Pr(&gt;|t|)    
+## (Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***
+## TV          0.047537   0.002691   17.67   &lt;2e-16 ***
+## ---
+## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
+## 
+## Residual standard error: 3.259 on 198 degrees of freedom
+## Multiple R-squared:  0.6119, Adjusted R-squared:  0.6099 
+## F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16</code></pre>
+<pre class=""r""><code>summary(lm(sales ~ newspaper, data=ads))</code></pre>
+<pre><code>## 
+## Call:
+## lm(formula = sales ~ newspaper, data = ads)
+## 
+## Residuals:
+##      Min       1Q   Median       3Q      Max 
+## -11.2272  -3.3873  -0.8392   3.5059  12.7751 
+## 
+## Coefficients:
+##             Estimate Std. Error t value Pr(&gt;|t|)    
+## (Intercept) 12.35141    0.62142   19.88  &lt; 2e-16 ***
+## newspaper    0.05469    0.01658    3.30  0.00115 ** 
+## ---
+## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
+## 
+## Residual standard error: 5.092 on 198 degrees of freedom
+## Multiple R-squared:  0.05212,    Adjusted R-squared:  0.04733 
+## F-statistic: 10.89 on 1 and 198 DF,  p-value: 0.001148</code></pre>
+<p>The answer is in the <span class=""math inline"">\(R^2\)</span> values: TV has a much higher <span class=""math inline"">\(R^2\)</span> than newspaper.</p>
+<p>We can fit a linear model for each explanatory variable separately, but we are left with two problems:</p>
+<ol style=""list-style-type: decimal"">
+<li>How would be combine the three models into a single model to create a single prediction for Sales?</li>
+<li>The pairs plot shows us that the explanatory variables are correlated. The linear regression fits ignore all other explanatory variables, and this can lead to incorrect predictions.</li>
+</ol>
+<p>This is where multiple linear regression comes in. It allows us to create a single model for predicting sales from multiple explanatory variables, and it allows us to create a model that takes correlation between explanatory variables into account.</p>
+</div>
 </div>
 <div id=""multiple-linear-regression-model-specifications"" class=""section level1"">
 <h1>Multiple linear regression model specifications</h1>
 <p>A multiple linear regression model that incorporates <span class=""math inline"">\(p\)</span> explanatory variables can be expressed as:</p>
 <p><span class=""math display"">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_p + \epsilon\]</span></p>
-<p>where <span class=""math inline"">\(Y\)</span> is the response variable, <span class=""math inline"">\(X_j\)</span> is the <span class=""math inline"">\(j^{th}\)</span> explanatory variable, and <span class=""math inline"">\(\beta_j\)</span> is the <span class=""math inline"">\(j^{th}\)</span> model coefficient. <span class=""math inline"">\(\beta_j\)</span> can be interpreted as the average increase in <span class=""math inline"">\(Y\)</span> for one unit increase in <span class=""math inline"">\(X_j\)</span> while holding all other response variables fixed.</p>
+<p>where <span class=""math inline"">\(Y\)</span> is the response variable, <span class=""math inline"">\(X_j\)</span> is the <span class=""math inline"">\(j^{th}\)</span> explanatory variable, and <span class=""math inline"">\(\beta_j\)</span> is the <span class=""math inline"">\(j^{th}\)</span> model coefficient. <span class=""math inline"">\(\beta_j\)</span> can be interpreted as the average increase in <span class=""math inline"">\(Y\)</span> for one unit increase in <span class=""math inline"">\(X_j\)</span> while holding all other explanatory variables fixed.</p>
 <p>For the <code>Advertising</code> dataset we can express a regression model as:</p>
 <p><span class=""math display"">\[Sales = \beta_0 + \beta_1 newspaper + \beta_2 radio + \beta_3 TV + \epsilon\]</span></p>
 </div>
@@ -1712,7 +1797,9 @@ <h1>Estimating regression coefficients</h1>
 <p><span class=""math display"">\[\hat{y} = \hat\beta_0 + \hat\beta_1x_1 + ... + \hat\beta_px_p\]</span></p>
 <p>As in linear regression, we choose <span class=""math inline"">\(\beta_j\)</span> such that we minimize the residual sum of squares:</p>
 <p><span class=""math display"">\[\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span> , which is equivalent to <span class=""math display"">\[\text{RSS} = \sum_{i=1}^n (y_i - \hat\beta_0 - \hat\beta_1x_{i1} - ... - \hat\beta_px_{ip})^2\]</span></p>
-<p>Quiz: <a href=""https://forms.gle/XUxpgxJbkTp1QfDG8"">What was <span class=""math inline"">\(y_i\)</span> again?</a></p>
+<div id=""quiz-what-was-y_i-again"" class=""section level2"">
+<h2>Quiz: <a href=""https://forms.gle/XUxpgxJbkTp1QfDG8"">What was <span class=""math inline"">\(y_i\)</span> again?</a></h2>
+</div>
 </div>
 <div id=""relationship-between-the-response-and-predictors"" class=""section level1"">
 <h1>Relationship between the response and predictors</h1>"
NBISweden,workshop-mlbiostatistics,5ae48d3084d970282278baa38ecbba236335d0f7,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T08:01:00Z,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T08:01:00Z,Fix typo,session-regression-II/session-regression-II.Rmd,True,False,True,False,1,1,2,"---FILE: session-regression-II/session-regression-II.Rmd---
@@ -98,7 +98,7 @@ $$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_p + \epsilon$$
 
 where $Y$ is the response variable, $X_j$ is the $j^{th}$ explanatory variable, and 
 $\beta_j$ is the $j^{th}$ model coefficient. $\beta_j$ can be interpreted as the average
-increase in $Y$ for one unit increase in $X_j$ while holding all other response 
+increase in $Y$ for one unit increase in $X_j$ while holding all other explanatory 
 variables fixed.
 
 For the `Advertising` dataset we can express a regression model as:"
NBISweden,workshop-mlbiostatistics,729107688152bea254fe0af595adaa1b292000fb,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T07:54:12Z,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-18T07:54:12Z,Fix up sessI pdf,session-regression-I/session-regression-I.pdf,False,False,False,False,0,0,0,
NBISweden,workshop-mlbiostatistics,2bd00ed2e7760641886246dd443a7dd603166130,Czarnewski,paulo.czarnewski@scilifelab.se,2019-05-17T06:50:24Z,Czarnewski,paulo.czarnewski@scilifelab.se,2019-05-17T06:50:24Z,fixes presentation file name,session-pca_clustering/Linear_Clustering.pdf,False,False,False,False,0,0,0,
NBISweden,workshop-mlbiostatistics,5f731f602c3cdf81016811d95debe26c38bd68df,Paulo Czarnewski,paulo.czarnewski@scilifelab.se,2019-05-17T06:43:55Z,GitHub,noreply@github.com,2019-05-17T06:43:55Z,Fixes font sizes in the documents,index.md,False,False,False,False,28,8,36,"---FILE: index.md---
@@ -7,17 +7,18 @@ title:  'Biostatistics Essentials: a blackboard approach'
 - 21st - 23rd, May, 2019
 - SciLifeLab, Uppsala University, BMC, Husargatan 3, E10:1308
 
+<br/><br/>
 
+----------
 
--------
-
-##### [Pre-course](precourse.md)
-##### [Travelling](travel.md)
-#### [Schedule](schedule.md)
-#### [Session Links](session-links.md)
+##### [Pre-course materials](precourse.md)
+##### [Travelling information](travel.md)
+##### [Schedule](schedule.md)
+##### [Links to the Sessions](session-links.md)
 
----------
+----------
 
+<br/><br/>
 
 ### About
 
@@ -28,6 +29,8 @@ title:  'Biostatistics Essentials: a blackboard approach'
 
 Responsible teachers:  [Olga Dethlefsen][olga] <<olga.dethlefsen@nbis.se>>, [Eva Freyhult][eva] <<eva.freyhult@nbis.se>>
 
+<br/><br/>
+
 ##### Course content
 - Basic statistical theory and concepts incl. stochastic variables, density and distribution functions, central limit theorem, confidence intervals
 - Hypothesis testing
@@ -36,14 +39,20 @@ Responsible teachers:  [Olga Dethlefsen][olga] <<olga.dethlefsen@nbis.se>>, [Eva
 - Model selection and regularization
 - Unsupervised learning, incl. clustering and dimension reduction methods
 
+<br/><br/>
+
 ##### Education
 In this course we focus on active learning approach. The course participants are expected to do some pre-course reading and exercises, corresponding up to 16h studying. The education consists of teaching blocks alternating between mini-lectures, group discussions, live coding sessions etc. A special focus is put on understanding basic concepts by solving problems with pen and paper. We believe in less-is-more principle, so be ready to say no to digital exhaustion to truly learn something with our modern “blackboard approach”.
 
+<br/><br/>
+
 ##### Literature
 - [An Introduction to Statistical Learning][book-gj], Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani (.pdf freely available)
 - [Data analysis for the life sciences][book-rai], Rafael A. Irizarry and Michael I. Love (.pdf freely available)
 - Additional teachers’ choice
 
+<br/><br/>
+
 ##### Teaching team
 - [Eva Freyhult][eva], PhD, NBIS, SciLifeLab, Uppsala University
 - [Olga Dethlefsen][olga], PhD, NBIS, SciLifeLab, University of Stockholm
@@ -52,31 +61,42 @@ In this course we focus on active learning approach. The course participants are
 - [Marcin Kierczak][marcin], PhD, NBIS, SciLifeLab, Uppsala University
 - [Bengt Sennblad][bengt], PhD, NBIS, SciLifeLab, Uppsala University
 
+<br/><br/>
+
 ----------
+
 #### Next course
 ##### Time and location
 - 2019-05-21 to 2019-05-23
 - SciLifeLab, Uppsala University, BMC, Husargatan 3, E10:1308
 
+<br/><br/>
+
 ##### Application status
 - closed
 - [Sign up](https://forms.gle/9gATt9jkXKRJojPw7) and we will notify you when we open application for the next course
 
+<br/><br/>
+
 ##### Entry requirements
 - No prior knowledge is assumed
 - BYOL (bring your own laptop) with R and RStudio installed
 - Prior experience using R and RStudio is welcome; pre-course exercises will be available for those new to R/RStudio
 
+<br/><br/>
+
 ##### Selection criteria
  - Due to limited space the course can accommodate maximum of 25 participants. If we receive more applications, participants will be selected based on several criteria. Selection criteria include correct entry requirements, motivation to attend the course as well as gender and geographical balance.
 
+<br/><br/>
+
 ##### Course fee
 - A course fee* of 1700 SEK will be invoiced to accepted participants.
 - The fee includes lunches, coffee and snacks.
 
 *Please note that NBIS cannot invoice individuals*
 
-
+<br/><br/>
 
 [eva]: https://nbis.se/about/staff/eva-freyhult/
 [olga]: https://nbis.se/about/staff/olga-dethlefsen/"
NBISweden,workshop-mlbiostatistics,54f0f1780493fdc1a3770a75b1e3c9fd5536146b,olgadet,olga.dethlefsen@nbis.se,2019-05-16T13:21:46Z,olgadet,olga.dethlefsen@nbis.se,2019-05-16T13:21:46Z,"Admin changes

Move things to index.md from readme.md. Fix some links. Add sign-up google form",README.md;ToDo.md;index.md;schedule.md;session-links.md;session-regression-I/.Rhistory;session-regression-I/Statistial-tables.pdf;session-regression-I/ToDo.md;session-regression-I/session-regression-I-files/figures/coeff-residuals-1.pdf;session-regression-I/session-regression-I-files/figures/fig-intro-det-vs-stat-1.pdf;session-regression-I/session-regression-I-files/figures/fig-intro-example-1.pdf;session-regression-I/session-regression-I-files/figures/fig-intro-example-reg-1.pdf;session-regression-I/session-regression-I-files/figures/fig-intro-example-reg-parameters-1.pdf;session-regression-I/session-regression-I-files/figures/rse-1.pdf;session-regression-I/session-regression-I-files/figures/unnamed-chunk-3-1.pdf;session-regression-I/session-regression-I-files/figures/unnamed-chunk-7-1.pdf;session-regression-I/session-regression-I.log;session-regression-I/session-regression-I.pdf,False,False,False,False,226,298,524,"---FILE: README.md---
@@ -1,65 +1,3 @@
 ## Biostatistics Essentials: a blackboard approach
 
 *National course open for PhD students, postdocs, researchers and other employees in need of biostatistical skills within all Swedish universities.  The course is geared towards life scientists wanting to be able to understand and use basic statistical methods. It would also suit those already applying biostatistical methods but have never got a chance to reflect on and truly grasp the basic statistical concepts, such as the commonly misinterpreted p-value!*
-
-### Important dates
-Application open: Click here for registration
-
-Application closes: 2019-03-31
-
-Confirmation to accepted students:  2019-04-16
-
-Responsible teachers:  [Olga Dethlefsen][olga], [Eva Freyhult][eva]
-
-If you do not receive information according to the above dates please contact olga.dethlefsen@nbis.se, eva.freyhult@nbis.se
-
-### Course content
-- Basic statistical theory and concepts incl. stochastic variables, density and distribution functions, central limit theorem, confidence intervals
-- Hypothesis testing
-- Resampling
-- Linear and logistic regression methods
-- Model selection and regularization
-- Unsupervised learning, incl. clustering and dimension reduction methods
-
-### Education
-In this course we focus on active learning approach. The course participants are expected to do some pre-course reading and exercises, corresponding up to 16h studying. The education consists of teaching blocks alternating between mini-lectures, group discussions, live coding sessions etc. A special focus is put on understanding basic concepts by solving problems with pen and paper. We believe in less-is-more principle, so be ready to say no to digital exhaustion to truly learn something with our modern “blackboard approach”.
-
-### Literature
-- [An Introduction to Statistical Learning][book-gj], Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani (.pdf freely available)
-- [Data analysis for the life sciences][book-rai], Rafael A. Irizarry and Michael I. Love (.pdf freely available)
-- Additional teachers’ choice
-
-### Entry requirements
-- No prior knowledge is assumed
-- BYOL (bring your own laptop) with R and RStudio installed
-- Prior experience using R and RStudio is welcome; pre-course exercises will be available for those new to R/RStudio
-
-### Selection criteria
- Due to limited space the course can accommodate maximum of 25 participants. If we receive more applications, participants will be selected based on several criteria. Selection criteria include correct entry requirements, motivation to attend the course as well as gender and geographical balance.
-
-### Course fee
-A course fee* of 1700 SEK will be invoiced to accepted participants.
-
-The fee includes lunches, coffee and snacks.
-
-*Please note that NBIS cannot invoice individuals*
-
-### Teaching team
-- [Eva Freyhult][eva], PhD, NBIS, SciLifeLab, Uppsala University
-- [Olga Dethlefsen][olga], PhD, NBIS, SciLifeLab, University of Stockholm
-- [Payam Emami][payam], PhD, NBIS, SciLifeLab, University of Stockholm
-- [Ashfaq Ali][ashfaq], PhD, NBIS, Lund University
-- [Paulo Czarnewski][paulo], PhD, NBIS, SciLifeLab, University of Stockholm
-- [Marcin Kierczak][marcin], PhD, NBIS, SciLifeLab, Uppsala University
-- [Bengt Sennblad][bengt], PhD, NBIS, SciLifeLab, Uppsala University
-
-[eva]: https://nbis.se/about/staff/eva-freyhult/
-[olga]: https://nbis.se/about/staff/olga-dethlefsen/
-[payam]: https://nbis.se/about/staff/payam-emami/
-[ashfaq]: https://nbis.se/about/staff/ashfaq-ali/
-[paulo]: https://nbis.se/about/staff/paulo-czarnewski/
-[marcin]: https://nbis.se/about/staff/marcin-kierczak/
-[bengt]: https://nbis.se/about/staff/bengt-sennblad/
-
-[book-gj]: https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf
-[book-rai]: http://www.rwdc2.com/files/rafa.pdf

---FILE: ToDo.md---
@@ -5,7 +5,7 @@ title:  'To Do'
 
 # Now
 
-## Other sessions:
+## Other sessions
 - intro: logistics, course philosophy
 - group discussions
 
@@ -64,5 +64,5 @@ title:  'To Do'
 ###  Simple regression
 - cross validation
 - outliers
-- answers to quizzes and pen-and-paper exercises 
+- answers to quizzes and pen-and-paper exercises
 - add confidence interval to regression line; improve confidence vs. prediction intervals

---FILE: index.md---
@@ -48,7 +48,6 @@ In this course we focus on active learning approach. The course participants are
 - [Eva Freyhult][eva], PhD, NBIS, SciLifeLab, Uppsala University
 - [Olga Dethlefsen][olga], PhD, NBIS, SciLifeLab, University of Stockholm
 - [Payam Emami][payam], PhD, NBIS, SciLifeLab, University of Stockholm
-- [Ashfaq Ali][ashfaq], PhD, NBIS, Lund University
 - [Paulo Czarnewski][paulo], PhD, NBIS, SciLifeLab, University of Stockholm
 - [Marcin Kierczak][marcin], PhD, NBIS, SciLifeLab, Uppsala University
 - [Bengt Sennblad][bengt], PhD, NBIS, SciLifeLab, Uppsala University
@@ -61,6 +60,7 @@ In this course we focus on active learning approach. The course participants are
 
 ##### Application status
 - closed
+- [Sign up](https://forms.gle/9gATt9jkXKRJojPw7) and we will notify you when we open application for the next course
 
 ##### Entry requirements
 - No prior knowledge is assumed
@@ -81,7 +81,6 @@ In this course we focus on active learning approach. The course participants are
 [eva]: https://nbis.se/about/staff/eva-freyhult/
 [olga]: https://nbis.se/about/staff/olga-dethlefsen/
 [payam]: https://nbis.se/about/staff/payam-emami/
-[ashfaq]: https://nbis.se/about/staff/ashfaq-ali/
 [paulo]: https://nbis.se/about/staff/paulo-czarnewski/
 [marcin]: https://nbis.se/about/staff/marcin-kierczak/
 [bengt]: https://nbis.se/about/staff/bengt-sennblad/

---FILE: schedule.md---
@@ -17,6 +17,7 @@ Room: E10:1308, BMC
 **09.30 - 10.00** Welcome & introduction (Olga, Eva)
 
 **10.00 - 12.00** Statistical theory and concepts I (Eva)
+
 (mix of lectures, groups discussions, paper exercises, live-coding, leg stretches)
 
 **12.00 - 13.00** *lunch*
@@ -25,9 +26,9 @@ Room: E10:1308, BMC
 
   *(mix of lectures, group discussions, paper exercises, live-coding, leg stretches)*
 
-**15.30 - 16.30** Introduction to experimental design (Payam and Ashfaq)
+**15.30 - 16.30** Introduction to experimental design (Payam)
 
-**16.30 - 17.00** Ask us anything (Eva, Olga, Marcin, Payam, Ashfaq, Bengt)
+**16.30 - 17.00** Ask us anything (Eva, Olga, Marcin, Payam, Bengt)
 
 ----
 
@@ -79,15 +80,13 @@ ____
 - [Eva Freyhult][eva], PhD, NBIS, SciLifeLab, Uppsala University
 - [Olga Dethlefsen][olga], PhD, NBIS, SciLifeLab, University of Stockholm
 - [Payam Emami][payam], PhD, NBIS, SciLifeLab, University of Stockholm
-- [Ashfaq Ali][ashfaq], PhD, NBIS, Lund University
 - [Paulo Czarnewski][paulo], PhD, NBIS, SciLifeLab, University of Stockholm
 - [Marcin Kierczak][marcin], PhD, NBIS, SciLifeLab, Uppsala University
 - [Bengt Sennblad][bengt], PhD, NBIS, SciLifeLab, Uppsala University
 
 [eva]: https://nbis.se/about/staff/eva-freyhult/
 [olga]: https://nbis.se/about/staff/olga-dethlefsen/
 [payam]: https://nbis.se/about/staff/payam-emami/
-[ashfaq]: https://nbis.se/about/staff/ashfaq-ali/
 [paulo]: https://nbis.se/about/staff/paulo-czarnewski/
 [marcin]: https://nbis.se/about/staff/marcin-kierczak/
 [bengt]: https://nbis.se/about/staff/bengt-sennblad/

---FILE: session-links.md---
@@ -5,15 +5,26 @@ title:  'Session links'
 
 ## Session Links
 
-##### Example (to be removed)
-- [example.pdf](session-example/session-example.pdf)
-- [example.md](session-example/session-example.md)
 ##### Statistical theory and concepts I and II
+
 ##### Introduction to experimental design
+
 ##### Regression I
-- [Linear regression and correlation](session-regression-I/session-regression-I.md)
+- [Linear regression and correlation](session-regression-I/session-regression-I.pdf)
+
 ##### Regression II
-- [Multiple linear regression](session-regression-II/session-regression-II.md)
+- [Multiple linear regression](session-regression-II/session-regression-II.pdf)
+
 ##### Unsupervised learning: PCA, K-means, KNN
+
 ##### What comes next & introduction to machine learning
+- [Regularization](session-regularization/session-regularization.md)
+
 ##### Wrap-up session & feedback
+
+
+----
+
+Example
+- [example.pdf](session-example/session-example.pdf)
+- [example.md](session-example/session-example.md)

---FILE: session-regression-I/.Rhistory---
@@ -1,205 +1,3 @@
-6  14 85
-7  16 66
-8  16 79
-9  18 77
-10 19 91
-> # prediction
-> new.df <- data.frame(x=x)
-> predict(reg, my.data)
-1        2        3        4        5        6        7        8        9       10
-84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
-> # y - y.hat
-> sum(y-pred(reg(my.data)))
-Error in pred(reg(my.data)) : could not find function ""pred""
-> # y - y.hat
-> sum(y-predict(reg(my.data)))
-Error in reg(my.data) : could not find function ""reg""
-> # y - y.hat
-> sum(y-predict(reg, my.data)))
-Error: unexpected ')' in ""sum(y-predict(reg, my.data)))""
-> # y - y.hat
-> sum(y-predict(reg, my.data))
-[1] 3.410605e-13
-> RSS=sum((y-predict(reg, my.data))^2)
-> RSS
-[1] 777.5377
-> RSE=sqrt(RSS/8)
-> RSE
-[1] 9.858611
-> SE=RSE/(sum((x-mean(x))^2))
-> SE
-[1] 0.2325144
-> sqrt(SE)
-[1] 0.4821975
-> summary(reg)
-Call:
-lm(formula = y ~ x, data = my.data)
-Residuals:
-Min      1Q  Median      3Q     Max
--14.972  -7.434   1.028   7.939  12.028
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   30.104     23.824   1.264    0.242
-x              3.179      1.514   2.100    0.069 .
----
-Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-Residual standard error: 9.859 on 8 degrees of freedom
-Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
-F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
-> beta.0=30.104
-> beta.hat=30.104
-> alfa.hat=3.179
-> RSS=sum((y - beta.hat - alfa.hat*x)^2)
-> RSS
-[1] 777.5379
-> length(x)
-[1] 10
-> RSE=sqrt(RSS/(length(x-2)))
-> RSE
-[1] 8.817811
-> length(x-2)
-[1] 10
-> RSE=sqrt(RSS/(length(x)-2))
-> RSE
-[1] 9.858612
-> (length(x)-2)
-[1] 8
-> var=RSE
-> SE=RSE/(sum((x-mean(x))^2))
-> sqrt(SE)
-[1] 0.4821975
-> RSS=sum((y - beta.hat - alfa.hat*x)^2)
-> RSS
-[1] 777.5379
-> RSE=sqrt(RSS/(length(x)-2))
-> n=10
-> RSE*(1/n + (mean(x)^2)/(sum((x-mean(x)^2))))
-[1] -0.06752474
-> summary(reg)
-Call:
-lm(formula = y ~ x, data = my.data)
-Residuals:
-Min      1Q  Median      3Q     Max
--14.972  -7.434   1.028   7.939  12.028
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   30.104     23.824   1.264    0.242
-x              3.179      1.514   2.100    0.069 .
----
-Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-Residual standard error: 9.859 on 8 degrees of freedom
-Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
-F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
-> RSE*(1/n + (mean(x)^2)/(sum((x-mean(x)^2))))
-[1] -0.06752474
-> predict(reg, my.data)
-1        2        3        4        5        6        7        8        9       10
-84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
-> y.hat <- predict(reg, my.data)
-> y
-[1] 94 73 59 80 93 85 66 79 77 91
-> y.hat
-1        2        3        4        5        6        7        8        9       10
-84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
-> sqrt(sum((y - y.hat)^2))
-[1] 27.88436
-> sqrt((sum((y - y.hat)^2))/n-2)
-[1] 8.703664
-> summary(reg)
-Call:
-lm(formula = y ~ x, data = my.data)
-Residuals:
-Min      1Q  Median      3Q     Max
--14.972  -7.434   1.028   7.939  12.028
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   30.104     23.824   1.264    0.242
-x              3.179      1.514   2.100    0.069 .
----
-Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-Residual standard error: 9.859 on 8 degrees of freedom
-Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
-F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
-> x <- 1:5
-> y <- c(2,4,5,4,5)
-> lm(y~x)
-Call:
-lm(formula = y ~ x)
-Coefficients:
-(Intercept)            x
-2.2          0.6
-> model <- lm(y~x)
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
-1    2    3    4    5
--0.8  0.6  1.0 -0.6 -0.2
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   2.2000     0.9381   2.345    0.101
-x             0.6000     0.2828   2.121    0.124
-Residual standard error: 0.8944 on 3 degrees of freedom
-Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
-F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
-> y.hat <- predict(model)
-> y.hat
-1   2   3   4   5
-2.8 3.4 4.0 4.6 5.2
-> y
-[1] 2 4 5 4 5
-> y.hat
-1   2   3   4   5
-2.8 3.4 4.0 4.6 5.2
-> y
-[1] 2 4 5 4 5
-> y.hat
-1   2   3   4   5
-2.8 3.4 4.0 4.6 5.2
-> model
-Call:
-lm(formula = y ~ x)
-Coefficients:
-(Intercept)            x
-2.2          0.6
-> names(model)
-[1] ""coefficients""  ""residuals""     ""effects""       ""rank""          ""fitted.values""
-[6] ""assign""        ""qr""            ""df.residual""   ""xlevels""       ""call""
-[11] ""terms""         ""model""
-> model$coefficients
-(Intercept)           x
-2.2         0.6
-> SE=sum((y.hat-y)^2)
-> SE
-[1] 2.4
-> SE=sum(y.hat-y)^2
-> SE
-[1] 9.663546e-30
-> n=length(x)
-> n=length(x)
-> SE=sqrt(sum((y.hat-y)^2)/(n-2))
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
-1    2    3    4    5
--0.8  0.6  1.0 -0.6 -0.2
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   2.2000     0.9381   2.345    0.101
-x             0.6000     0.2828   2.121    0.124
-Residual standard error: 0.8944 on 3 degrees of freedom
-Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
-F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
-> y.hat <- predict(model)
-> n=length(x)
-> SE=sqrt(sum((y.hat-y)^2)/(n-2))
-> SE
-[1] 0.8944272
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
 1    2    3    4    5
 -0.8  0.6  1.0 -0.6 -0.2
 Coefficients:
@@ -510,3 +308,205 @@ abline(lm(plasma~weight), col=""red"") # regression line (y~x)
 255 / 2
 255 / 2 - 30 - 24
 255 / 2 - 30 - 24 - 16
+setwd(""~/Desktop/workshop-biostatistics/session-regression-I"")
+# Chunk 1: setup
+knitr::opts_chunk$set(echo = TRUE)
+knitr::opts_chunk$set(fig.path=""session-regression-I-files/figures/"")
+knitr::opts_chunk$set(fig.pos = 'H')
+knitr::opts_chunk$set(cache.path = ""tmp"")
+knitr::opts_chunk$set(cache = TRUE)
+# Chunk 2
+par(mfrow=c(2,2), mar=c(3,4,3,3))
+# Deterministic relationship example
+x_celcius <- seq(from=0, to=50, by=5)
+y_fahr <- 9/5*x_celcius+32
+plot(x_celcius, y_fahr, type=""b"", pch=19, xlab=""Celcius"", ylab=""Fahrenheit"", main=""a)"", cex.main=0.8)
+# Statistical relationship (increasing)
+x <- seq(from=0, to=100, by=5)
+y_increasing <- 2*x + rnorm(length(x), mean=100, sd=25)
+plot(x, y_increasing, pch=19, xlab=""x"", ylab=""y"", main=""b)"", cex.main=0.8)
+# Statistical relationship (decreasing)
+y_decreasing <- -2*x + rnorm(length(x), mean=100, sd=25)
+plot(x, y_decreasing, pch=19, xlab=""x"", ylab=""y"", main=""c)"", cex.main=0.8)
+# Statistical relationshp (random)
+y_random <- - rnorm(length(x), mean=100, sd=25)
+plot(x, y_random, pch=19, xlab=""x"", ylab=""y"", main=""d)"", cex.main=0.8)
+# Chunk 3
+weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+# Chunk 4
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
+#abline(lm(plasma~weight), col=""red"") # regression line
+# Chunk 5
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
+abline(lm(plasma~weight), col=""red"") # regression line
+# Chunk 6
+par(mfcol=c(2,2), mar=c(4,4,3,2))
+# Values from regression model: plasma_volume = 0.0857 + 0.043615*x
+# Fitted line
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid())
+abline(lm(plasma~weight), col=""red"") # regression line
+text(65, 3.3, ""plasma = 0.0857 + 0.0436 * weight"", cex=1)
+# Beta 1 example b
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid(), xlim=c(60, 70), ylim=c(2.8, 3.2))
+abline(lm(plasma~weight), col=""red"") # regression line
+segments(x0=65, y0=2.92, x1=66, y1=2.92, col=""blue"")
+segments(x0=66, y0=2.92, x1=66, y1=2.964, col=""blue"")
+text(67, 2.92, expression(beta[1]), cex=1.2, col=""blue"")
+# Beta 1 example a
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid())
+abline(lm(plasma~weight), col=""red"") # regression line
+segments(x0=65, y0=2.92, x1=70, y1=2.92, col=""blue"")
+segments(x0=70, y0=2.92, x1=70, y1=3.14, col=""blue"")
+text(72, 2.95, expression(beta[1]), cex=1.2, col=""blue"")
+# Beta 0 example a
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid(), xlim=c(-5, 80), ylim=c(0, 5))
+abline(lm(plasma~weight), col=""red"") # regression line
+abline(h=0.0857) # regression line
+segments(x0=65, y0=2.92, x1=66, y1=2.92, col=""blue"")
+segments(x0=66, y0=2.92, x1=66, y1=2.964, col=""blue"")
+text(0, 0.5, expression(beta[0]), cex=1.2, col=""blue"")
+# Chunk 7
+data.reg <- data.frame(plasma=plasma, weight=weight)
+fit.reg <- lm(plasma~weight, data=data.reg)
+data.reg$predicted <- predict(fit.reg)
+data.reg$residuals <- residuals((fit.reg))
+library(ggplot2)
+ggplot(data.reg, aes(x=data.reg$weight, data.reg$plasma)) + geom_point() +
+geom_smooth(method = ""lm"", se = FALSE, color = ""lightgrey"") +
+geom_segment(aes(xend = weight, yend = predicted)) +
+geom_point(aes(y = predicted), shape = 1) +
+geom_point(aes(y = predicted), shape = 1) +
+theme_bw() + xlab(""body weight [kg]"") + ylab(""plasma volume [liters]"")
+# Chunk 8
+weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+# Chunk 9
+# Data
+weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12)
+# Plot
+plot(weight, plasma)
+# Regression
+reg <- lm(plasma~weight)
+summary(reg)
+# Coefficients
+coef(reg)
+# Confidence intervals
+confint(reg)
+# Add regression line to the plot
+abline(reg)
+# Chunk 10
+# Prediction
+predict(reg, data.frame(weight=60))
+predict(reg, data.frame(weight=c(60, 70)))
+# Prediction with confidence intervals
+predict(reg, data.frame(weight=66), interval=""prediction"")
+# Chunk 11
+weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+data.reg <- data.frame(plasma=plasma, weight=weight)
+fit.reg <- lm(plasma~weight, data=data.reg)
+data.reg$predicted <- predict(fit.reg)
+data.reg$residuals <- residuals((fit.reg))
+library(ggplot2)
+ggplot(data.reg, aes(x=data.reg$weight, data.reg$plasma)) + geom_point() +
+geom_smooth(method = ""lm"", se = FALSE, color = ""lightgrey"") +
+geom_segment(aes(xend = weight, yend = predicted)) +
+geom_point(aes(y = predicted), shape = 1) +
+geom_point(aes(y = predicted), shape = 1) +
+theme_bw() + xlab(""body weight [kg]"") + ylab(""plasma volume [liters]"")
+# Chunk 12
+#weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
+#plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+plasma
+weight
+head(data.reg)
+reg <- lm(data.reg$plasma~data.reg$weight)
+# predict Y given the values of X and regression model reg
+y.pred <- predict(reg, data.frame(weight=data.reg$weight))
+y.pred
+# calculate residuals
+e.terms <- data.reg$plasma-y.pred
+e.terms
+# calculate RSS
+RSS=sum(e.terms^2)
+# calculate RSE
+n=nrow(data.reg)
+RSE <- sqrt((1/(n-2))*RSS)
+# R reg objects contains it all
+names(reg)
+reg$fitted.values
+reg$residuals
+# RSE
+summary(reg)
+#summary(reg)
+names(reg)
+r2 <- cor(data.reg$plasma, data.reg$weight)^2
+#summary(reg)
+names(reg)
+r2 <- cor(data.reg$plasma, data.reg$weight)^2
+reg
+plot(reg$fitted.values)
+graphics.off()
+plot(reg$fitted.values)
+plot(reg, which=1:2)
+par(mfrow=c)
+plot(reg, which=1:2)
+par(mfrow=c(2,2))
+plot(reg, which=1:2)
+par(mfrow=c(2,1))
+plot(reg, which=1:2)
+plot(reg, which=1:4)
+par(mfrow=c(2,2))
+plot(reg, which=1:4)
+par(mfrow=c(2,1))
+plot(reg, which=1:2)
+par(mfrow=c(1,2))
+plot(reg, which=1:2)
+hist(reg$residuals)
+par(mfrow=c(2,2), mar=c(3,4,3,3))
+# Deterministic relationship example
+x_celcius <- seq(from=0, to=50, by=5)
+y_fahr <- 9/5*x_celcius+32
+plot(x_celcius, y_fahr, type=""b"", pch=19, xlab=""Celcius"", ylab=""Fahrenheit"", main=""a)"", cex.main=0.8)
+# Statistical relationship (increasing)
+x <- seq(from=0, to=100, by=5)
+y_increasing <- 2*x + rnorm(length(x), mean=100, sd=25)
+plot(x, y_increasing, pch=19, xlab=""x"", ylab=""y"", main=""b)"", cex.main=0.8)
+# Statistical relationship (decreasing)
+y_decreasing <- -2*x + rnorm(length(x), mean=100, sd=25)
+plot(x, y_decreasing, pch=19, xlab=""x"", ylab=""y"", main=""c)"", cex.main=0.8)
+# Statistical relationship (random)
+y_random <- - rnorm(length(x), mean=100, sd=25)
+plot(x, y_random, pch=19, xlab=""x"", ylab=""y"", main=""d)"", cex.main=0.8)
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
+abline(lm(plasma~weight), col=""red"") # regression line
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid())
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"", panel.first = grid())
+abline(lm(plasma~weight), col=""red"") # regression line
+par(mfcol=c(2,2), mar=c(4,4,3,2))
+# Values from regression model: plasma_volume = 0.0857 + 0.043615*x
+# Fitted line
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid())
+abline(lm(plasma~weight), col=""red"") # regression line
+text(65, 3.3, ""plasma = 0.0857 + 0.0436 * weight"", cex=1)
+# Beta 1 example b
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid(), xlim=c(60, 70), ylim=c(2.8, 3.2))
+abline(lm(plasma~weight), col=""red"") # regression line
+segments(x0=65, y0=2.92, x1=66, y1=2.92, col=""blue"")
+segments(x0=66, y0=2.92, x1=66, y1=2.964, col=""blue"")
+text(67, 2.92, expression(beta[1]), cex=1.2, col=""blue"")
+# Beta 1 example a
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid())
+abline(lm(plasma~weight), col=""red"") # regression line
+segments(x0=65, y0=2.92, x1=70, y1=2.92, col=""blue"")
+segments(x0=70, y0=2.92, x1=70, y1=3.14, col=""blue"")
+text(72, 2.95, expression(beta[1]), cex=1.2, col=""blue"")
+# Beta 0 example a
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"",  panel.first = grid(), xlim=c(-5, 80), ylim=c(0, 5))
+abline(lm(plasma~weight), col=""red"") # regression line
+abline(h=0.0857) # regression line
+segments(x0=65, y0=2.92, x1=66, y1=2.92, col=""blue"")
+segments(x0=66, y0=2.92, x1=66, y1=2.964, col=""blue"")
+text(0, 0.5, expression(beta[0]), cex=1.2, col=""blue"")

---FILE: session-regression-I/ToDo.md---
@@ -1,19 +0,0 @@
----
-layout: default
-title:  'To Do'
----
-
-# Now
-## Simple regression
-- more examples to practice (pen and paper)
-- prepare answers to have them handy
-
-## Multiple regression
-- B plan
-
-# Next time
-## Simple regression
-- cross validation
-- outliers 
-- answers to my quizzes
-- add confidence interval to regression line; improve confidence vs. prediction intervals 
\ No newline at end of file

---FILE: session-regression-I/session-regression-I.log---
@@ -1,4 +1,4 @@
-This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015) (preloaded format=pdflatex 2016.1.18)  16 MAY 2019 12:53
+This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015) (preloaded format=pdflatex 2016.1.18)  16 MAY 2019 14:53
 entering extended mode
  restricted \write18 enabled.
  %&-line parsing enabled.
@@ -1086,7 +1086,7 @@ nts/type1/public/lm/lmsy7.pfb></usr/local/texlive/2015/texmf-dist/fonts/type1/p
 ublic/lm/lmtk10.pfb></usr/local/texlive/2015/texmf-dist/fonts/type1/public/lm/l
 mtt10.pfb></usr/local/texlive/2015/texmf-dist/fonts/type1/public/lm/lmtti10.pfb
 >
-Output written on session-regression-I.pdf (13 pages, 347521 bytes).
+Output written on session-regression-I.pdf (13 pages, 347497 bytes).
 PDF statistics:
  352 PDF objects out of 1000 (max. 8388607)
  294 compressed objects within 3 object streams"
NBISweden,workshop-mlbiostatistics,8ecbd5e7e6ff46520fa0b2de730e5cbe062a8aba,Bengt Sennblad,bengt.sennblad@scilfelab.se,2019-05-14T07:55:25Z,Bengt Sennblad,bengt.sennblad@scilfelab.se,2019-05-14T07:55:25Z,typo fix,session-regularization/session-regularization.md,False,False,False,False,1,1,2,"---FILE: session-regularization/session-regularization.md---
@@ -895,4 +895,4 @@ _stats_, _graphics_, _grDevices_, _utils_, _datasets_, _methods_ and _base_
 _pander(v.0.6.3)_, _glmnet(v.2.0-16)_, _foreach(v.1.4.4)_, _Matrix(v.1.2-17)_, _dplyr(v.0.8.0.1)_, _kableExtra(v.1.1.0)_, _lmtest(v.0.9-37)_, _zoo(v.1.8-5)_ and _knitr(v.1.22)_
 
 **loaded via a namespace (and not attached):** 
-_Rcpp(v.1.0.1)_, _pillar(v.1.3.1)_, _compiler(v.3.5.3)_, _highr(v.0.8)_, _iterators(v.1.0.10)_, _tools(v.3.5.3)_, _digest(v.0.6.18)_, _evaluate(v.0.13)_, _tibble(v.2.1.1)_, _lattice(v.0.20-38)_, _viridisLite(v.0.3.0)_, _pkgconfig(v.2.0.2)_, _rlang(v.0.3.4)_, _rstudioapi(v.0.10)_, _yaml(v.2.2.0)_, _xfun(v.0.6)_, _stringr(v.1.4.0)_, _httr(v.1.4.0)_, _xml2(v.1.2.0)_, _hms(v.0.4.2)_, _tidyselect(v.0.2.5)_, _grid(v.3.5.3)_, _webshot(v.0.5.1)_, _glue(v.1.3.1)_, _R6(v.2.4.0)_, _rmarkdown(v.1.12)_, _purrr(v.0.3.2)_, _readr(v.1.3.1)_, _magrittr(v.1.5)_, _codetools(v.0.2-16)_, _scales(v.1.0.0)_, _htmltools(v.0.3.6)_, _assertthat(v.0.2.1)_, _rvest(v.0.3.3)_, _colorspace(v.1.4-1)_, _stringi(v.1.4.3)_, _munsell(v.0.5.0)_ and _crayon(v.1.3.4)_
+_Rcpp(v.1.0.1)_, _later(v.0.8.0)_, _compiler(v.3.5.3)_, _pillar(v.1.3.1)_, _highr(v.0.8)_, _iterators(v.1.0.10)_, _tools(v.3.5.3)_, _digest(v.0.6.18)_, _packrat(v.0.5.0)_, _jsonlite(v.1.6)_, _evaluate(v.0.13)_, _tibble(v.2.1.1)_, _lattice(v.0.20-38)_, _viridisLite(v.0.3.0)_, _pkgconfig(v.2.0.2)_, _rlang(v.0.3.4)_, _rstudioapi(v.0.10)_, _yaml(v.2.2.0)_, _xfun(v.0.6)_, _stringr(v.1.4.0)_, _httr(v.1.4.0)_, _xml2(v.1.2.0)_, _hms(v.0.4.2)_, _tidyselect(v.0.2.5)_, _grid(v.3.5.3)_, _webshot(v.0.5.1)_, _glue(v.1.3.1)_, _R6(v.2.4.0)_, _rmarkdown(v.1.12)_, _xaringan(v.0.9)_, _servr(v.0.13)_, _purrr(v.0.3.2)_, _readr(v.1.3.1)_, _magrittr(v.1.5)_, _promises(v.1.0.1)_, _codetools(v.0.2-16)_, _scales(v.1.0.0)_, _htmltools(v.0.3.6)_, _rsconnect(v.0.8.13)_, _assertthat(v.0.2.1)_, _rvest(v.0.3.3)_, _mime(v.0.6)_, _colorspace(v.1.4-1)_, _httpuv(v.1.5.1)_, _stringi(v.1.4.3)_, _munsell(v.0.5.0)_ and _crayon(v.1.3.4)_"
NBISweden,workshop-mlbiostatistics,42b2de19e73b8b23bf3c03b5a621e580b3eee082,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-12T14:48:49Z,Warren W. Kretzschmar,wkretzsch@gmail.com,2019-05-12T14:48:49Z,Add regression II directory and gitignore for temp files,session-links.md;session-regression-I/.gitignore;session-regression-II/session-regression-II.Rmd;session-regression-II/session-regression-II.md,True,False,True,False,64,0,64,"---FILE: session-links.md---
@@ -13,6 +13,7 @@ title:  'Session links'
 ##### Regression I
 - [Linear regression and correlation](session-regression-I/session-regression-I.md)
 ##### Regression II
+- [Multiple linear regression](session-regression-II/session-regression-II.md)
 ##### Unsupervised learning: PCA, K-means, KNN
 ##### What comes next & introduction to machine learning
 ##### Wrap-up session & feedback

---FILE: session-regression-I/.gitignore---
@@ -0,0 +1 @@
+tmp*

---FILE: session-regression-II/session-regression-II.Rmd---
@@ -0,0 +1,35 @@
+---
+title: 'Session regression II: multiple linear regression'
+output:
+  html_document:
+    df_print: paged
+  md_document: 
+    variant: markdown_github
+header-includes: \usepackage{float}
+urlcolor: blue
+---
+
+
+```{r setup, include=FALSE}
+knitr::opts_chunk$set(echo = TRUE)
+knitr::opts_chunk$set(fig.path=""session-regression-II-files/figures/"")
+knitr::opts_chunk$set(fig.pos = 'H')
+```
+
+
+## Learning outcomes
+
+
+-------
+
+## Multiple linear regression
+### Estimating the Regression Coefficients
+### Estimating coefficients
+### Relationship between the response and predictors 
+### Model fit 
+### Predictions 
+### Qualitative predictors 
+### Interaction terms
+### Non-linear transformation of the predictors 
+### Potential problems: non-linearity, collinearity 
+### Logistic regression

---FILE: session-regression-II/session-regression-II.md---
@@ -0,0 +1,27 @@
+Learning outcomes
+-----------------
+
+------------------------------------------------------------------------
+
+Multiple linear regression
+--------------------------
+
+### Estimating the Regression Coefficients
+
+### Estimating coefficients
+
+### Relationship between the response and predictors
+
+### Model fit
+
+### Predictions
+
+### Qualitative predictors
+
+### Interaction terms
+
+### Non-linear transformation of the predictors
+
+### Potential problems: non-linearity, collinearity
+
+### Logistic regression"
NBISweden,workshop-mlbiostatistics,5837cca7b39006bc8c4baad3a111804ae5e50b68,Olga,olga.dethlefsen@nbis.se,2019-05-06T07:39:20Z,GitHub,noreply@github.com,2019-05-06T07:39:20Z,Fix equation,session-regression-I/session-regression-I.md,False,False,False,False,1,1,2,"---FILE: session-regression-I/session-regression-I.md---
@@ -41,7 +41,7 @@ red)
 
 The equation of the regression line is:
 
-\[y=beta_0 + beta_1x\]
+$$y=beta_0 + beta_1x$$
 
 ### Estimating the Coefficients
 "
NBISweden,workshop-mlbiostatistics,fcd66aa18ab2aedbbd493e08a0325d0ca07c3e89,Winni Kretzschmar,warrenk@kth.se,2019-05-05T10:42:04Z,GitHub,noreply@github.com,2019-05-05T10:42:04Z,Fix some typos,precourse.md,False,False,False,False,4,4,8,"---FILE: precourse.md---
@@ -18,24 +18,24 @@ There are few things **to do** before the course starts. These include both sett
 
 ### R & R-Studio <a name=""R""></a>
 
-During the course we will run scripts locally on laptops using `R` and `R-Studio`. To be able to follow exercises we ask you to install R and R-Studio on your laptop. Both of these work on computers running Linux, Windows and Macintosh operative systems. R-studio is a set of tools as well as an editor that facilitates the use of R. Over the last years it has become a very popular tool and in many ways become a de-facto standard for working with R.
+During the course we will run scripts locally on laptops using `R` and `R-Studio`. To be able to follow exercises we ask you to install R and R-Studio on your laptop. Both of these work on computers running Linux, Windows and Macintosh operating systems. R-studio is a set of tools as well as an editor that facilitates the use of R. Over the last years it has become a very popular tool and in many ways become a de-facto standard for working with R.
 
-Note that on same operative systems it will be easier to install and run R and R-studio if you are administrator of your own computer and hence are allowed to install software on your machine. If you do not have these privileges please ask your system administrator to install the latest version of R and R-studio.
+Note that on some operating systems it will be easier to install and run R and R-studio if you are administrator of your own computer and hence are allowed to install software on your machine. If you do not have these privileges please ask your system administrator to install the latest version of R and R-studio.
 
 ##### Install R
 
 Install version 3.5.0 or higher
 
 1.  Go to [CRAN](https://cran.rstudio.com)
-2.  Click on the link corresponding to your operative system
+2.  Click on the link corresponding to your operating system
 3.  Download the recommended files for your system.
 4.  Run the installer or move the downloaded files to suitable place on
     your computer.
 
 
 ##### Install R Studio
 
-Go to the web page [rstudio](https://www.rstudio.com/products/rstudio/download/) download the installer corresponding to your operative system. Unpack the installer and install the app on a suitable place on your system.
+Go to the web page [rstudio](https://www.rstudio.com/products/rstudio/download/) download the installer corresponding to your operating system. Unpack the installer and install the app on a suitable place on your system.
 
 ##### Test installations
 "
NBISweden,workshop-mlbiostatistics,e0ee970e450722a8ad79302cd1e105a92131b78b,olgadet,olga.dethlefsen@nbis.se,2019-05-03T11:29:36Z,olgadet,olga.dethlefsen@nbis.se,2019-05-03T11:29:36Z,Fix figure size,precourse.md,False,False,False,False,5,1,6,"---FILE: precourse.md---
@@ -40,7 +40,11 @@ Go to the web page [rstudio](https://www.rstudio.com/products/rstudio/download/)
 ##### Test installations
 
 If the installation above went without any problem you should be able to fire up R-studio and see something like the following:
-![](precourse/images/RStudio.png)
+<figure>
+<img src=""precourse/images/RStudio.png"" width=""400"" height=""300"">>
+<figcaption>
+</figcaption>
+</figure>
 
 ----------
 ### R libraries and datasets <a name=""Rlib""></a>"
NBISweden,workshop-mlbiostatistics,39d13ff458388a0c9a60b9fe8d9b76fea51f187a,Olga,olga.dethlefsen@nbis.se,2019-04-24T07:47:38Z,GitHub,noreply@github.com,2019-04-24T07:47:38Z,Fix header levels,CONTRIBUTING.md,False,False,False,False,5,6,11,"---FILE: CONTRIBUTING.md---
@@ -32,9 +32,6 @@ e.g. session-example
 
 *Btw, you get this structure by default if you use R-Studio, New file -> R markdown -> From Template -> GitHub Document (Markdown)*
 
-#### Add link(s) to the session to `index.md`
-_see example under Session Links_
-
 #### Code and commit
 ``` bash
  # Code & commit changes while working on the materials
@@ -54,17 +51,19 @@ _see example under Session Links_
  - The subject and body should be separated by a blank line, start with a capital letter, and the subject line should not end with a period.
  - More about [good commit messages][git-commits]
 
+### Add link(s) to the session to `index.md`
+_see example under Session Links_
 
-#### Add to Github
+### Add to Github
 
-##### Push feature branch to repo
+#### Push feature branch to repo
 
  ``` bash
   # Push to feature when ready
   git push
   ```
 
-##### Make a pull request to master branch when ready
+#### Make a pull request to master branch when ready
 Go to course repository [https://github.com/NBISweden/workshop-biostatistics.git](https://github.com/NBISweden/workshop-biostatistics.git) and create a pull request 
 
 ### Questions or feedback?"
NBISweden,workshop-mlbiostatistics,61b177f441e6bc31c3bd3c96cfbab9c0edc53220,Olga,olga.dethlefsen@nbis.se,2019-04-24T07:43:57Z,GitHub,noreply@github.com,2019-04-24T07:43:57Z,Fix link,index.md,False,False,False,False,1,1,2,"---FILE: index.md---
@@ -29,7 +29,7 @@ Room: E10:1308, BMC
 ###### Statistical theory and concepts II
 
 ######  Regression I
-[Session: regression](session-regression/session-regression-I)    
+[Session: regression](session-regression-I/session-regression-I)    
 
 ###### Regression II
 Session: regression"
NBISweden,workshop-mlbiostatistics,ba00db1d0d5b2dada4a6b2119f3b26d4a64f4ae1,olgadet,olga.dethlefsen@nbis.se,2019-04-24T07:14:08Z,olgadet,olga.dethlefsen@nbis.se,2019-04-24T07:14:08Z,Fix links,index.md,False,False,False,False,2,2,4,"---FILE: index.md---
@@ -29,10 +29,10 @@ Room: E10:1308, BMC
 ###### Statistical theory and concepts II
 
 ######  Regression I
-[Session: regression](session-regression/session-regression)    
+[Session: regression](session-regression/session-regression-I)    
 
 ###### Regression II
-[Session: regression](session-regression/session-regression)    
+Session: regression
 
 ----------
 "
NBISweden,workshop-mlbiostatistics,f7e328e7500883644ef59b1ced22b655e947c8d0,olgadet,olga.dethlefsen@nbis.se,2019-04-24T07:08:25Z,olgadet,olga.dethlefsen@nbis.se,2019-04-24T07:08:25Z,"Rename session-regression, split into 2 parts",session-regression/.Rhistory;session-regression/session-regression.Rmd;session-regression/session-regression.md;session-regression/session-regression_files/figure-gfm/fig-reg-1.png,True,False,True,False,0,710,710,"---FILE: session-regression/.Rhistory---
@@ -1,512 +0,0 @@
-6  14 85
-7  16 66
-8  16 79
-9  18 77
-10 19 91
-> # prediction
-> new.df <- data.frame(x=x)
-> predict(reg, my.data)
-1        2        3        4        5        6        7        8        9       10
-84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
-> # y - y.hat
-> sum(y-pred(reg(my.data)))
-Error in pred(reg(my.data)) : could not find function ""pred""
-> # y - y.hat
-> sum(y-predict(reg(my.data)))
-Error in reg(my.data) : could not find function ""reg""
-> # y - y.hat
-> sum(y-predict(reg, my.data)))
-Error: unexpected ')' in ""sum(y-predict(reg, my.data)))""
-> # y - y.hat
-> sum(y-predict(reg, my.data))
-[1] 3.410605e-13
-> RSS=sum((y-predict(reg, my.data))^2)
-> RSS
-[1] 777.5377
-> RSE=sqrt(RSS/8)
-> RSE
-[1] 9.858611
-> SE=RSE/(sum((x-mean(x))^2))
-> SE
-[1] 0.2325144
-> sqrt(SE)
-[1] 0.4821975
-> summary(reg)
-Call:
-lm(formula = y ~ x, data = my.data)
-Residuals:
-Min      1Q  Median      3Q     Max
--14.972  -7.434   1.028   7.939  12.028
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   30.104     23.824   1.264    0.242
-x              3.179      1.514   2.100    0.069 .
----
-Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-Residual standard error: 9.859 on 8 degrees of freedom
-Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
-F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
-> beta.0=30.104
-> beta.hat=30.104
-> alfa.hat=3.179
-> RSS=sum((y - beta.hat - alfa.hat*x)^2)
-> RSS
-[1] 777.5379
-> length(x)
-[1] 10
-> RSE=sqrt(RSS/(length(x-2)))
-> RSE
-[1] 8.817811
-> length(x-2)
-[1] 10
-> RSE=sqrt(RSS/(length(x)-2))
-> RSE
-[1] 9.858612
-> (length(x)-2)
-[1] 8
-> var=RSE
-> SE=RSE/(sum((x-mean(x))^2))
-> sqrt(SE)
-[1] 0.4821975
-> RSS=sum((y - beta.hat - alfa.hat*x)^2)
-> RSS
-[1] 777.5379
-> RSE=sqrt(RSS/(length(x)-2))
-> n=10
-> RSE*(1/n + (mean(x)^2)/(sum((x-mean(x)^2))))
-[1] -0.06752474
-> summary(reg)
-Call:
-lm(formula = y ~ x, data = my.data)
-Residuals:
-Min      1Q  Median      3Q     Max
--14.972  -7.434   1.028   7.939  12.028
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   30.104     23.824   1.264    0.242
-x              3.179      1.514   2.100    0.069 .
----
-Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-Residual standard error: 9.859 on 8 degrees of freedom
-Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
-F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
-> RSE*(1/n + (mean(x)^2)/(sum((x-mean(x)^2))))
-[1] -0.06752474
-> predict(reg, my.data)
-1        2        3        4        5        6        7        8        9       10
-84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
-> y.hat <- predict(reg, my.data)
-> y
-[1] 94 73 59 80 93 85 66 79 77 91
-> y.hat
-1        2        3        4        5        6        7        8        9       10
-84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
-> sqrt(sum((y - y.hat)^2))
-[1] 27.88436
-> sqrt((sum((y - y.hat)^2))/n-2)
-[1] 8.703664
-> summary(reg)
-Call:
-lm(formula = y ~ x, data = my.data)
-Residuals:
-Min      1Q  Median      3Q     Max
--14.972  -7.434   1.028   7.939  12.028
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   30.104     23.824   1.264    0.242
-x              3.179      1.514   2.100    0.069 .
----
-Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
-Residual standard error: 9.859 on 8 degrees of freedom
-Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
-F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
-> x <- 1:5
-> y <- c(2,4,5,4,5)
-> lm(y~x)
-Call:
-lm(formula = y ~ x)
-Coefficients:
-(Intercept)            x
-2.2          0.6
-> model <- lm(y~x)
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
-1    2    3    4    5
--0.8  0.6  1.0 -0.6 -0.2
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   2.2000     0.9381   2.345    0.101
-x             0.6000     0.2828   2.121    0.124
-Residual standard error: 0.8944 on 3 degrees of freedom
-Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
-F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
-> y.hat <- predict(model)
-> y.hat
-1   2   3   4   5
-2.8 3.4 4.0 4.6 5.2
-> y
-[1] 2 4 5 4 5
-> y.hat
-1   2   3   4   5
-2.8 3.4 4.0 4.6 5.2
-> y
-[1] 2 4 5 4 5
-> y.hat
-1   2   3   4   5
-2.8 3.4 4.0 4.6 5.2
-> model
-Call:
-lm(formula = y ~ x)
-Coefficients:
-(Intercept)            x
-2.2          0.6
-> names(model)
-[1] ""coefficients""  ""residuals""     ""effects""       ""rank""          ""fitted.values""
-[6] ""assign""        ""qr""            ""df.residual""   ""xlevels""       ""call""
-[11] ""terms""         ""model""
-> model$coefficients
-(Intercept)           x
-2.2         0.6
-> SE=sum((y.hat-y)^2)
-> SE
-[1] 2.4
-> SE=sum(y.hat-y)^2
-> SE
-[1] 9.663546e-30
-> n=length(x)
-> n=length(x)
-> SE=sqrt(sum((y.hat-y)^2)/(n-2))
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
-1    2    3    4    5
--0.8  0.6  1.0 -0.6 -0.2
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   2.2000     0.9381   2.345    0.101
-x             0.6000     0.2828   2.121    0.124
-Residual standard error: 0.8944 on 3 degrees of freedom
-Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
-F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
-> y.hat <- predict(model)
-> n=length(x)
-> SE=sqrt(sum((y.hat-y)^2)/(n-2))
-> SE
-[1] 0.8944272
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
-1    2    3    4    5
--0.8  0.6  1.0 -0.6 -0.2
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   2.2000     0.9381   2.345    0.101
-x             0.6000     0.2828   2.121    0.124
-Residual standard error: 0.8944 on 3 degrees of freedom
-Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
-F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
-> SE
-[1] 0.8944272
-> SE=sqrt(sum((y.hat-y)^2)/(n))
-> SE
-[1] 0.6928203
-> SE
-[1] 0.6928203
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
-1    2    3    4    5
--0.8  0.6  1.0 -0.6 -0.2
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   2.2000     0.9381   2.345    0.101
-x             0.6000     0.2828   2.121    0.124
-Residual standard error: 0.8944 on 3 degrees of freedom
-Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
-F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
-> SE=sqrt(sum((y.hat-y)^2)/(n-1))
-> SE
-[1] 0.7745967
-> SE.alfa <- SE/((x - mean(x))^2)
-> SE.alfa
-[1] 0.1936492 0.7745967       Inf 0.7745967 0.1936492
-> SE.alfa <- SE/((x - mean(x))^2)
-> SE.alfa
-[1] 0.1936492 0.7745967       Inf 0.7745967 0.1936492
-> SE.alfa <- SE/sum(((x - mean(x))^2))
-> SE.alfa
-[1] 0.07745967
-> summary(model)
-Call:
-lm(formula = y ~ x)
-Residuals:
-1    2    3    4    5
--0.8  0.6  1.0 -0.6 -0.2
-Coefficients:
-Estimate Std. Error t value Pr(>|t|)
-(Intercept)   2.2000     0.9381   2.345    0.101
-x             0.6000     0.2828   2.121    0.124
-Residual standard error: 0.8944 on 3 degrees of freedom
-Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
-F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
-> SE.alfa
-[1] 0.07745967
-> sqrt(SE.alfa)
-[1] 0.2783158
-> n=length(x)
-> SE=sqrt(sum((y.hat-y)^2)/(n-2))
-> SE.alfa <- SE/sum(((x - mean(x))^2))
-> sqrt(SE.alfa)
-[1] 0.2990698
-> n=length(x)
-> SE=sqrt(sum((y.hat-y)^2)/(n-1))
-> SE.alfa <- SE/sum(((x - mean(x))^2))
-> sqrt(SE.alfa)
-[1] 0.2783158
-> n=length(x)
-> SE=sqrt(sum((y.hat-y)^2)/(n))
-> SE.alfa <- SE/sum(((x - mean(x))^2))
-> sqrt(SE.alfa)
-[1] 0.2632148
-> SE.beta <- SE*(1/n + (mean(x)^2)/(sum(x-mean(x))^2))
-> sqrt(SE.beta)
-[1] Inf
-> SE.beta <- SE*(1/n + (mean(x)^2)/(sum(x-mean(x))^2))
-> SE.beta
-[1] Inf
-> SE
-[1] 0.6928203
-> n
-[1] 5
-> ((mean(x)^2)
-+ )
-[1] 9
-> mean(x)
-[1] 3
-> SE.beta <- SE*(1/n + ((mean(x)^2)/(sum(x-mean(x))^2)))
-> SE.beta
-[1] Inf
-> ((mean(x)^2)/(sum(x-mean(x))^2)))
-Error: unexpected ')' in ""((mean(x)^2)/(sum(x-mean(x))^2)))""
-> ((mean(x)^2)/(sum(x-mean(x))^2))
-[1] Inf
-> mean(x)
-[1] 3
-> mean(x)sum(x-mean(x))^2)
-Error: unexpected symbol in ""mean(x)sum""
-> mean(x)sum(x-mean(x))^2
-Error: unexpected symbol in ""mean(x)sum""
-> sum(x-mean(x))^2)
-Error: unexpected ')' in ""sum(x-mean(x))^2)""
-> sum(x-mean(x))^2))
-Error: unexpected ')' in ""sum(x-mean(x))^2)""
-> sum(x-mean(x))^2)
-Error: unexpected ')' in ""sum(x-mean(x))^2)""
-> sum(x-mean(x))^2)
-Error: unexpected ')' in ""sum(x-mean(x))^2)""
-> sum(x-mean(x))^2)
-Error: unexpected ')' in ""sum(x-mean(x))^2)""
-> sum(x-mean(x))
-[1] 0
-> x
-[1] 1 2 3 4 5
-> x - mean(x)
-[1] -2 -1  0  1  2
-> (x - mean(x))^2
-[1] 4 1 0 1 4
-> sum((x - mean(x))^2)
-[1] 10
-> SE.beta <- SE*(1/n + 9/sum((x - mean(x))^2))
-> SE.beta
-[1] 0.7621024
-> SE.beta <- SE*(1/n + x^2/sum((x - mean(x))^2))
-> SE.beta
-[1] 0.2078461 0.4156922 0.7621024 1.2470766 1.8706149
-> SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-> SE.beta
-[1] 0.7621024
-> sqrt(SE.beta)
-[1] 0.8729847
-> summary(model)
-# Coefficients:
-#             Estimate Std. Error t value Pr(>|t|)
-# (Intercept)   2.2000     0.9381   2.345    0.101
-# x             0.6000     0.2828   2.121    0.124
-x <- 1:5
-y <- c(2,4,5,4,5)
-model <- lm(y~x)
-summary(model)
-y.hat <- predict(model)
-n=length(x)
-SE=sqrt(sum((y.hat-y)^2)/(n))
-SE.alfa <- SE/sum(((x - mean(x))^2))
-sqrt(SE.alfa)
-SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-SE.beta
-sqrt(SE.beta)
-# Coefficients:
-#             Estimate Std. Error t value Pr(>|t|)
-# (Intercept)   2.2000     0.9381   2.345    0.101
-# x             0.6000     0.2828   2.121    0.124
-summary(model)
-sqrt(SE.alfa)
-sqrt(SE.beta)
-x <- 1:5
-y <- c(2,4,5,4,5)
-model <- lm(y~x)
-summary(model)
-y.hat <- predict(model)
-n=length(x)
-SE=sqrt(sum((y.hat-y)^2)/(n-1))
-SE.alfa <- SE/sum(((x - mean(x))^2))
-SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-SE.beta
-sqrt(SE.beta)
-sqrt(SE.alfa)
-# Coefficients:
-#             Estimate Std. Error t value Pr(>|t|)
-# (Intercept)   2.2000     0.9381   2.345    0.101
-# x             0.6000     0.2828   2.121    0.124
-x <- 1:5
-y <- c(2,4,5,4,5)
-model <- lm(y~x)
-summary(model)
-y.hat <- predict(model)
-n=length(x)
-SE=sqrt(sum((y.hat-y)^2)/(n-2))
-SE.alfa <- SE/sum(((x - mean(x))^2))
-SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-SE.beta
-sqrt(SE.beta)
-sqrt(SE.alfa)
-# Coefficients:
-#             Estimate Std. Error t value Pr(>|t|)
-# (Intercept)   2.2000     0.9381   2.345    0.101
-# x             0.6000     0.2828   2.121    0.124
-x <- c(17, 13, 12, 15, 16, 14, 16, 16, 18, 19)
-y <- c(94, 73, 59, 80, 93, 85, 66, 79, 77, 91)
-# correlation
-cor(x,y)
-# linear regression
-reg <- lm(y~x)
-# linear regression
-model <- lm(y~x)
-summary(model)
-# prediction
-y.hat <- predict(model)
-y
-y.hat
-n=length(x)
-SE=sqrt(sum((y.hat-y)^2)/(n-1))
-SE.alfa <- SE/sum(((x - mean(x))^2))
-SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-SE.beta
-x <- c(17, 13, 12, 15, 16, 14, 16, 16, 18, 19)
-y <- c(94, 73, 59, 80, 93, 85, 66, 79, 77, 91)
-# correlation
-cor(x,y)
-# linear regression
-model <- lm(y~x)
-summary(model)
-# prediction
-y.hat <- predict(model)
-n=length(x)
-SE=sqrt(sum((y.hat-y)^2)/(n-1))
-SE.alfa <- SE/sum(((x - mean(x))^2))
-SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-SE.beta
-sqrt(SE.beta)
-sqrt(SE.alfa)
-n=length(x)
-SE=sqrt(sum((y.hat-y)^2)/(n))
-SE.alfa <- SE/sum(((x - mean(x))^2))
-SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-SE.beta
-sqrt(SE.beta)
-sqrt(SE.alfa)
-sqrt(SE.beta)
-sqrt(SE.alfa)
-RSS <- sum((y.hat-y)^2)
-RSS
-RSE <- sqrt(RSS/(n-2))
-SE.alfa <- RSE/sum(((x - mean(x))^2))
-sqrt(SE.alfa)
-SE.alfa
-summary(model)
-SE.alfa
-sqrt(SE.alfa)
-x
-y
-sqrt(SE.alfa)
-SE.alfa
-SE.beta
-SE.beta <- RSE*(1/n + mean(x)^2/sum((x - mean(x))^2))
-SE.beta
-sqrt(SE.beta)
-sqrt(SE.alfa)
-SE.beta
-SE.alfa
-n
-weight=c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0)
-plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
-cor(weight, plasma)
-n=length(weight)
-cor(weight, plasma)
-# linear regression
-x <- weight
-y <- plasma
-model <- lm(y~x)
-summary(model)
-mean(x)
-mean(y)
-model$coefficients
-model$coefficients$x
-model$coefficients
-model$coefficients$x
-model$coefficients[,1]
-model$coefficients[1]
-model$coefficients[2]
-model$coefficients[2]*((x-mean(x))*^2)
-model$coefficients[2]*sum((x-mean(x))*^2)
-model$coefficients[2]*sum((x-mean(x))^2)
-(sum((y-mean(y))^2) - model$coefficients[2]*sum((x-mean(x))^2))/(n-2)
-(sum((y-mean(y))^2) - (model$coefficients[2])^2*sum((x-mean(x))^2))/(n-2)
-s <- sqrt((sum((y-mean(y))^2) - (model$coefficients[2])^2*sum((x-mean(x))^2))/(n-2))
-s
-se.1 <- s/(sqrt(sum(x-mean(x))^2))
-se.1
-se.1 <- s/(sqrt(sum(x-mean(x)^2)))
-se.1 <-sqrt(sum(x-mean(x))^2))
-se.1 <-sqrt(sum(x-mean(x))^2)
-se.1
-sum(x-mean(x))^2)
-sum(x-mean(x))^2
-sum((x-mean(x))^2)
-se.1 <-sqrt(sum((x-mean(x))^2))
-se.1
-se.1 <-s/sqrt(sum((x-mean(x))^2))
-se.1
-se.0 <- s*(1/n + mean(x)^2/(sum(x-mean(x))^2))
-se.0
-se.0 <- s*(1/n + mean(x)^2/(sum((x-mean(x))^2)))
-se.0
-se.0
-se.0 <- s*sqrt((1/n + mean(x)^2/(sum((x-mean(x))^2))))
-se.0
-?lm
-plot(weight, plasma, xlab = ""weight [kg]"", ylab=""plasma [l]"")
-plot(weight, plasma, pch=19, xlab = ""body weight [kg]"", ylab=""plasma [l]"")
-plot(weight, plasma, pch=19, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
-plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
-abline(lm(plasma~weight), col=""red"") # regression line (y~x)
-.10 * 1700
-.20 * 1700
-.15 * 1700
-255 / 2
-255 / 2 - 30 - 24
-255 / 2 - 30 - 24 - 16

---FILE: session-regression/session-regression.Rmd---
@@ -1,99 +0,0 @@
----
-title: ""Session regression I""
-output: github_document
-editor_options: 
-  chunk_output_type: console
----
-
-### Learning outcomes
-- to be able to fit regression line and interpret regression model
-- to be able to use `lm` function in R for model fitting, obtaining confidence interval and predictions
-
-## Linear Regression 
-Linear regression is a very simple approach for supervised learning, when numerical outcome and numerical exposure are concerned. The method of linear regression is used to estimate the best-fitting straight line to describe the association. With linear regression we can answer questions incl.:
-
-- is there a relationship between exposure (e.g. body weigth) and outcome (e.g. plasma volume)?
-- how strong is the relationship between the two variables
-- what will be a predicted value of the outcome given a new set of exposure values
-- how accurately can we predict outcome
-
-### Example data
-
-Example data contain the body weight and plasma volume for eight healthy men. 
-```{r}
-
-weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
-plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
-```
-
-Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)
-```{r, fig-reg, echo=F, fig.align=""center"", fig.height=5, fig.width=5}
-
-plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
-abline(lm(plasma~weight), col=""red"") # regression line
-
-```
-
-The equation of the regression line is: 
-
-$$y=beta_0 + beta_1x$$
-
-
-
-### Estimating the Coefficients
-### Assessing the Accuracy of the Coefficient Estimates
-### Asesssing the Accuracy of the Model
-
-
-
-```{r setup, include=FALSE}
-knitr::opts_chunk$set(echo = TRUE)
-```
-
-```{r reg, echo=F}
-
-weight=c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
-plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
-
-# correlation 
-cor(weight, plasma)
-
-# linear regression
-x <- weight
-y <- plasma
-model <- lm(y~x)
-summary(model)
-
-# prediction
-y.hat <- predict(model)
-
-n=length(x)
-RSS <- sum((y.hat-y)^2)
-RSE <- sqrt(RSS/(n-2))
-
-s <- sqrt((sum((y-mean(y))^2) - (model$coefficients[2])^2*sum((x-mean(x))^2))/(n-2))
-se.1 <-s/sqrt(sum((x-mean(x))^2))
-se.1
-
-se.0 <- s*sqrt((1/n + mean(x)^2/(sum((x-mean(x))^2))))
-se.0
-
-
-#             Estimate Std. Error t value Pr(>|t|)  
-# (Intercept)  0.08572    1.02400   0.084   0.9360  
-# x            0.04362    0.01527   2.857   0.0289 *
-```
-
-
-
-## Multiple linear regression
-### Estimating the Regression Coefficients
-### Estimating coefficients
-### Relationship between the response and predictors 
-### Model fit 
-### Predictions 
-### Qualitative predictors 
-### Interaction terms
-### Non-linear transformation of the predictors 
-### Potential problems: non-linearity, collinearity 
-### Logistic regression

---FILE: session-regression/session-regression.md---
@@ -1,99 +0,0 @@
-Session regression I
-================
-
-### Learning outcomes
-
-  - to be able to fit regression line and interpret regression model
-  - to be able to use `lm` function in R for model fitting, obtaining
-    confidence interval and predictions
-
-## Linear Regression
-
-Linear regression is a very simple approach for supervised learning,
-when numerical outcome and numerical exposure are concerned. The method
-of linear regression is used to estimate the best-fitting straight line
-to describe the association. With linear regression we can answer
-questions incl.:
-
-  - is there a relationship between exposure (e.g. body weigth) and
-    outcome (e.g. plasma volume)?
-  - how strong is the relationship between the two variables
-  - what will be a predicted value of the outcome given a new set of
-    exposure values
-  - how accurately can we predict outcome
-
-### Example data
-
-Example data contain the body weight and plasma volume for eight healthy
-men.
-
-``` r
-weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
-plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
-```
-
-Scatter plot of the data shows that high plasma volume tends to be
-associated with high weight and *vice verca*. Linear regrssion gives the
-equation of the straight line that best describes how the outcome
-changes (increase or decreases) with a change of exposure variable (in
-red)
-<img src=""session-regression_files/figure-gfm/fig-reg-1.png"" style=""display: block; margin: auto;"" />
-
-The equation of the regression line is:
-
-\[y=beta_0 + beta_1x\]
-
-### Estimating the Coefficients
-
-### Assessing the Accuracy of the Coefficient Estimates
-
-### Asesssing the Accuracy of the Model
-
-    ## [1] 0.7591266
-
-    ## 
-    ## Call:
-    ## lm(formula = y ~ x)
-    ## 
-    ## Residuals:
-    ##      Min       1Q   Median       3Q      Max 
-    ## -0.27880 -0.14178 -0.01928  0.13986  0.32939 
-    ## 
-    ## Coefficients:
-    ##             Estimate Std. Error t value Pr(>|t|)  
-    ## (Intercept)  0.08572    1.02400   0.084   0.9360  
-    ## x            0.04362    0.01527   2.857   0.0289 *
-    ## ---
-    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
-    ## 
-    ## Residual standard error: 0.2188 on 6 degrees of freedom
-    ## Multiple R-squared:  0.5763, Adjusted R-squared:  0.5057 
-    ## F-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893
-
-    ##          x 
-    ## 0.01526836
-
-    ##        x 
-    ## 1.023998
-
-## Multiple linear regression
-
-### Estimating the Regression Coefficients
-
-### Estimating coefficients
-
-### Relationship between the response and predictors
-
-### Model fit
-
-### Predictions
-
-### Qualitative predictors
-
-### Interaction terms
-
-### Non-linear transformation of the predictors
-
-### Potential problems: non-linearity, collinearity
-
-### Logistic regression"
NBISweden,workshop-mlbiostatistics,39fd2616606963b25cccbbd902b0de214a58e57f,olgadet,olga.dethlefsen@nbis.se,2019-03-18T10:06:00Z,olgadet,olga.dethlefsen@nbis.se,2019-03-18T10:06:00Z,First commit session-regression,session-regression/.Rhistory;session-regression/session-regression.Rmd;session-regression/session-regression.md;session-regression/session-regression_files/figure-gfm/fig-reg-1.png,True,False,True,False,710,0,710,"---FILE: session-regression/.Rhistory---
@@ -0,0 +1,512 @@
+6  14 85
+7  16 66
+8  16 79
+9  18 77
+10 19 91
+> # prediction
+> new.df <- data.frame(x=x)
+> predict(reg, my.data)
+1        2        3        4        5        6        7        8        9       10
+84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
+> # y - y.hat
+> sum(y-pred(reg(my.data)))
+Error in pred(reg(my.data)) : could not find function ""pred""
+> # y - y.hat
+> sum(y-predict(reg(my.data)))
+Error in reg(my.data) : could not find function ""reg""
+> # y - y.hat
+> sum(y-predict(reg, my.data)))
+Error: unexpected ')' in ""sum(y-predict(reg, my.data)))""
+> # y - y.hat
+> sum(y-predict(reg, my.data))
+[1] 3.410605e-13
+> RSS=sum((y-predict(reg, my.data))^2)
+> RSS
+[1] 777.5377
+> RSE=sqrt(RSS/8)
+> RSE
+[1] 9.858611
+> SE=RSE/(sum((x-mean(x))^2))
+> SE
+[1] 0.2325144
+> sqrt(SE)
+[1] 0.4821975
+> summary(reg)
+Call:
+lm(formula = y ~ x, data = my.data)
+Residuals:
+Min      1Q  Median      3Q     Max
+-14.972  -7.434   1.028   7.939  12.028
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   30.104     23.824   1.264    0.242
+x              3.179      1.514   2.100    0.069 .
+---
+Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
+Residual standard error: 9.859 on 8 degrees of freedom
+Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
+F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
+> beta.0=30.104
+> beta.hat=30.104
+> alfa.hat=3.179
+> RSS=sum((y - beta.hat - alfa.hat*x)^2)
+> RSS
+[1] 777.5379
+> length(x)
+[1] 10
+> RSE=sqrt(RSS/(length(x-2)))
+> RSE
+[1] 8.817811
+> length(x-2)
+[1] 10
+> RSE=sqrt(RSS/(length(x)-2))
+> RSE
+[1] 9.858612
+> (length(x)-2)
+[1] 8
+> var=RSE
+> SE=RSE/(sum((x-mean(x))^2))
+> sqrt(SE)
+[1] 0.4821975
+> RSS=sum((y - beta.hat - alfa.hat*x)^2)
+> RSS
+[1] 777.5379
+> RSE=sqrt(RSS/(length(x)-2))
+> n=10
+> RSE*(1/n + (mean(x)^2)/(sum((x-mean(x)^2))))
+[1] -0.06752474
+> summary(reg)
+Call:
+lm(formula = y ~ x, data = my.data)
+Residuals:
+Min      1Q  Median      3Q     Max
+-14.972  -7.434   1.028   7.939  12.028
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   30.104     23.824   1.264    0.242
+x              3.179      1.514   2.100    0.069 .
+---
+Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
+Residual standard error: 9.859 on 8 degrees of freedom
+Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
+F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
+> RSE*(1/n + (mean(x)^2)/(sum((x-mean(x)^2))))
+[1] -0.06752474
+> predict(reg, my.data)
+1        2        3        4        5        6        7        8        9       10
+84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
+> y.hat <- predict(reg, my.data)
+> y
+[1] 94 73 59 80 93 85 66 79 77 91
+> y.hat
+1        2        3        4        5        6        7        8        9       10
+84.15094 71.43396 68.25472 77.79245 80.97170 74.61321 80.97170 80.97170 87.33019 90.50943
+> sqrt(sum((y - y.hat)^2))
+[1] 27.88436
+> sqrt((sum((y - y.hat)^2))/n-2)
+[1] 8.703664
+> summary(reg)
+Call:
+lm(formula = y ~ x, data = my.data)
+Residuals:
+Min      1Q  Median      3Q     Max
+-14.972  -7.434   1.028   7.939  12.028
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   30.104     23.824   1.264    0.242
+x              3.179      1.514   2.100    0.069 .
+---
+Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
+Residual standard error: 9.859 on 8 degrees of freedom
+Multiple R-squared:  0.3553,	Adjusted R-squared:  0.2747
+F-statistic: 4.409 on 1 and 8 DF,  p-value: 0.06895
+> x <- 1:5
+> y <- c(2,4,5,4,5)
+> lm(y~x)
+Call:
+lm(formula = y ~ x)
+Coefficients:
+(Intercept)            x
+2.2          0.6
+> model <- lm(y~x)
+> summary(model)
+Call:
+lm(formula = y ~ x)
+Residuals:
+1    2    3    4    5
+-0.8  0.6  1.0 -0.6 -0.2
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   2.2000     0.9381   2.345    0.101
+x             0.6000     0.2828   2.121    0.124
+Residual standard error: 0.8944 on 3 degrees of freedom
+Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
+F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
+> y.hat <- predict(model)
+> y.hat
+1   2   3   4   5
+2.8 3.4 4.0 4.6 5.2
+> y
+[1] 2 4 5 4 5
+> y.hat
+1   2   3   4   5
+2.8 3.4 4.0 4.6 5.2
+> y
+[1] 2 4 5 4 5
+> y.hat
+1   2   3   4   5
+2.8 3.4 4.0 4.6 5.2
+> model
+Call:
+lm(formula = y ~ x)
+Coefficients:
+(Intercept)            x
+2.2          0.6
+> names(model)
+[1] ""coefficients""  ""residuals""     ""effects""       ""rank""          ""fitted.values""
+[6] ""assign""        ""qr""            ""df.residual""   ""xlevels""       ""call""
+[11] ""terms""         ""model""
+> model$coefficients
+(Intercept)           x
+2.2         0.6
+> SE=sum((y.hat-y)^2)
+> SE
+[1] 2.4
+> SE=sum(y.hat-y)^2
+> SE
+[1] 9.663546e-30
+> n=length(x)
+> n=length(x)
+> SE=sqrt(sum((y.hat-y)^2)/(n-2))
+> summary(model)
+Call:
+lm(formula = y ~ x)
+Residuals:
+1    2    3    4    5
+-0.8  0.6  1.0 -0.6 -0.2
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   2.2000     0.9381   2.345    0.101
+x             0.6000     0.2828   2.121    0.124
+Residual standard error: 0.8944 on 3 degrees of freedom
+Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
+F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
+> y.hat <- predict(model)
+> n=length(x)
+> SE=sqrt(sum((y.hat-y)^2)/(n-2))
+> SE
+[1] 0.8944272
+> summary(model)
+Call:
+lm(formula = y ~ x)
+Residuals:
+1    2    3    4    5
+-0.8  0.6  1.0 -0.6 -0.2
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   2.2000     0.9381   2.345    0.101
+x             0.6000     0.2828   2.121    0.124
+Residual standard error: 0.8944 on 3 degrees of freedom
+Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
+F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
+> SE
+[1] 0.8944272
+> SE=sqrt(sum((y.hat-y)^2)/(n))
+> SE
+[1] 0.6928203
+> SE
+[1] 0.6928203
+> summary(model)
+Call:
+lm(formula = y ~ x)
+Residuals:
+1    2    3    4    5
+-0.8  0.6  1.0 -0.6 -0.2
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   2.2000     0.9381   2.345    0.101
+x             0.6000     0.2828   2.121    0.124
+Residual standard error: 0.8944 on 3 degrees of freedom
+Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
+F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
+> SE=sqrt(sum((y.hat-y)^2)/(n-1))
+> SE
+[1] 0.7745967
+> SE.alfa <- SE/((x - mean(x))^2)
+> SE.alfa
+[1] 0.1936492 0.7745967       Inf 0.7745967 0.1936492
+> SE.alfa <- SE/((x - mean(x))^2)
+> SE.alfa
+[1] 0.1936492 0.7745967       Inf 0.7745967 0.1936492
+> SE.alfa <- SE/sum(((x - mean(x))^2))
+> SE.alfa
+[1] 0.07745967
+> summary(model)
+Call:
+lm(formula = y ~ x)
+Residuals:
+1    2    3    4    5
+-0.8  0.6  1.0 -0.6 -0.2
+Coefficients:
+Estimate Std. Error t value Pr(>|t|)
+(Intercept)   2.2000     0.9381   2.345    0.101
+x             0.6000     0.2828   2.121    0.124
+Residual standard error: 0.8944 on 3 degrees of freedom
+Multiple R-squared:    0.6,	Adjusted R-squared:  0.4667
+F-statistic:   4.5 on 1 and 3 DF,  p-value: 0.124
+> SE.alfa
+[1] 0.07745967
+> sqrt(SE.alfa)
+[1] 0.2783158
+> n=length(x)
+> SE=sqrt(sum((y.hat-y)^2)/(n-2))
+> SE.alfa <- SE/sum(((x - mean(x))^2))
+> sqrt(SE.alfa)
+[1] 0.2990698
+> n=length(x)
+> SE=sqrt(sum((y.hat-y)^2)/(n-1))
+> SE.alfa <- SE/sum(((x - mean(x))^2))
+> sqrt(SE.alfa)
+[1] 0.2783158
+> n=length(x)
+> SE=sqrt(sum((y.hat-y)^2)/(n))
+> SE.alfa <- SE/sum(((x - mean(x))^2))
+> sqrt(SE.alfa)
+[1] 0.2632148
+> SE.beta <- SE*(1/n + (mean(x)^2)/(sum(x-mean(x))^2))
+> sqrt(SE.beta)
+[1] Inf
+> SE.beta <- SE*(1/n + (mean(x)^2)/(sum(x-mean(x))^2))
+> SE.beta
+[1] Inf
+> SE
+[1] 0.6928203
+> n
+[1] 5
+> ((mean(x)^2)
++ )
+[1] 9
+> mean(x)
+[1] 3
+> SE.beta <- SE*(1/n + ((mean(x)^2)/(sum(x-mean(x))^2)))
+> SE.beta
+[1] Inf
+> ((mean(x)^2)/(sum(x-mean(x))^2)))
+Error: unexpected ')' in ""((mean(x)^2)/(sum(x-mean(x))^2)))""
+> ((mean(x)^2)/(sum(x-mean(x))^2))
+[1] Inf
+> mean(x)
+[1] 3
+> mean(x)sum(x-mean(x))^2)
+Error: unexpected symbol in ""mean(x)sum""
+> mean(x)sum(x-mean(x))^2
+Error: unexpected symbol in ""mean(x)sum""
+> sum(x-mean(x))^2)
+Error: unexpected ')' in ""sum(x-mean(x))^2)""
+> sum(x-mean(x))^2))
+Error: unexpected ')' in ""sum(x-mean(x))^2)""
+> sum(x-mean(x))^2)
+Error: unexpected ')' in ""sum(x-mean(x))^2)""
+> sum(x-mean(x))^2)
+Error: unexpected ')' in ""sum(x-mean(x))^2)""
+> sum(x-mean(x))^2)
+Error: unexpected ')' in ""sum(x-mean(x))^2)""
+> sum(x-mean(x))
+[1] 0
+> x
+[1] 1 2 3 4 5
+> x - mean(x)
+[1] -2 -1  0  1  2
+> (x - mean(x))^2
+[1] 4 1 0 1 4
+> sum((x - mean(x))^2)
+[1] 10
+> SE.beta <- SE*(1/n + 9/sum((x - mean(x))^2))
+> SE.beta
+[1] 0.7621024
+> SE.beta <- SE*(1/n + x^2/sum((x - mean(x))^2))
+> SE.beta
+[1] 0.2078461 0.4156922 0.7621024 1.2470766 1.8706149
+> SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+> SE.beta
+[1] 0.7621024
+> sqrt(SE.beta)
+[1] 0.8729847
+> summary(model)
+# Coefficients:
+#             Estimate Std. Error t value Pr(>|t|)
+# (Intercept)   2.2000     0.9381   2.345    0.101
+# x             0.6000     0.2828   2.121    0.124
+x <- 1:5
+y <- c(2,4,5,4,5)
+model <- lm(y~x)
+summary(model)
+y.hat <- predict(model)
+n=length(x)
+SE=sqrt(sum((y.hat-y)^2)/(n))
+SE.alfa <- SE/sum(((x - mean(x))^2))
+sqrt(SE.alfa)
+SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+SE.beta
+sqrt(SE.beta)
+# Coefficients:
+#             Estimate Std. Error t value Pr(>|t|)
+# (Intercept)   2.2000     0.9381   2.345    0.101
+# x             0.6000     0.2828   2.121    0.124
+summary(model)
+sqrt(SE.alfa)
+sqrt(SE.beta)
+x <- 1:5
+y <- c(2,4,5,4,5)
+model <- lm(y~x)
+summary(model)
+y.hat <- predict(model)
+n=length(x)
+SE=sqrt(sum((y.hat-y)^2)/(n-1))
+SE.alfa <- SE/sum(((x - mean(x))^2))
+SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+SE.beta
+sqrt(SE.beta)
+sqrt(SE.alfa)
+# Coefficients:
+#             Estimate Std. Error t value Pr(>|t|)
+# (Intercept)   2.2000     0.9381   2.345    0.101
+# x             0.6000     0.2828   2.121    0.124
+x <- 1:5
+y <- c(2,4,5,4,5)
+model <- lm(y~x)
+summary(model)
+y.hat <- predict(model)
+n=length(x)
+SE=sqrt(sum((y.hat-y)^2)/(n-2))
+SE.alfa <- SE/sum(((x - mean(x))^2))
+SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+SE.beta
+sqrt(SE.beta)
+sqrt(SE.alfa)
+# Coefficients:
+#             Estimate Std. Error t value Pr(>|t|)
+# (Intercept)   2.2000     0.9381   2.345    0.101
+# x             0.6000     0.2828   2.121    0.124
+x <- c(17, 13, 12, 15, 16, 14, 16, 16, 18, 19)
+y <- c(94, 73, 59, 80, 93, 85, 66, 79, 77, 91)
+# correlation
+cor(x,y)
+# linear regression
+reg <- lm(y~x)
+# linear regression
+model <- lm(y~x)
+summary(model)
+# prediction
+y.hat <- predict(model)
+y
+y.hat
+n=length(x)
+SE=sqrt(sum((y.hat-y)^2)/(n-1))
+SE.alfa <- SE/sum(((x - mean(x))^2))
+SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+SE.beta
+x <- c(17, 13, 12, 15, 16, 14, 16, 16, 18, 19)
+y <- c(94, 73, 59, 80, 93, 85, 66, 79, 77, 91)
+# correlation
+cor(x,y)
+# linear regression
+model <- lm(y~x)
+summary(model)
+# prediction
+y.hat <- predict(model)
+n=length(x)
+SE=sqrt(sum((y.hat-y)^2)/(n-1))
+SE.alfa <- SE/sum(((x - mean(x))^2))
+SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+SE.beta
+sqrt(SE.beta)
+sqrt(SE.alfa)
+n=length(x)
+SE=sqrt(sum((y.hat-y)^2)/(n))
+SE.alfa <- SE/sum(((x - mean(x))^2))
+SE.beta <- SE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+SE.beta
+sqrt(SE.beta)
+sqrt(SE.alfa)
+sqrt(SE.beta)
+sqrt(SE.alfa)
+RSS <- sum((y.hat-y)^2)
+RSS
+RSE <- sqrt(RSS/(n-2))
+SE.alfa <- RSE/sum(((x - mean(x))^2))
+sqrt(SE.alfa)
+SE.alfa
+summary(model)
+SE.alfa
+sqrt(SE.alfa)
+x
+y
+sqrt(SE.alfa)
+SE.alfa
+SE.beta
+SE.beta <- RSE*(1/n + mean(x)^2/sum((x - mean(x))^2))
+SE.beta
+sqrt(SE.beta)
+sqrt(SE.alfa)
+SE.beta
+SE.alfa
+n
+weight=c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+cor(weight, plasma)
+n=length(weight)
+cor(weight, plasma)
+# linear regression
+x <- weight
+y <- plasma
+model <- lm(y~x)
+summary(model)
+mean(x)
+mean(y)
+model$coefficients
+model$coefficients$x
+model$coefficients
+model$coefficients$x
+model$coefficients[,1]
+model$coefficients[1]
+model$coefficients[2]
+model$coefficients[2]*((x-mean(x))*^2)
+model$coefficients[2]*sum((x-mean(x))*^2)
+model$coefficients[2]*sum((x-mean(x))^2)
+(sum((y-mean(y))^2) - model$coefficients[2]*sum((x-mean(x))^2))/(n-2)
+(sum((y-mean(y))^2) - (model$coefficients[2])^2*sum((x-mean(x))^2))/(n-2)
+s <- sqrt((sum((y-mean(y))^2) - (model$coefficients[2])^2*sum((x-mean(x))^2))/(n-2))
+s
+se.1 <- s/(sqrt(sum(x-mean(x))^2))
+se.1
+se.1 <- s/(sqrt(sum(x-mean(x)^2)))
+se.1 <-sqrt(sum(x-mean(x))^2))
+se.1 <-sqrt(sum(x-mean(x))^2)
+se.1
+sum(x-mean(x))^2)
+sum(x-mean(x))^2
+sum((x-mean(x))^2)
+se.1 <-sqrt(sum((x-mean(x))^2))
+se.1
+se.1 <-s/sqrt(sum((x-mean(x))^2))
+se.1
+se.0 <- s*(1/n + mean(x)^2/(sum(x-mean(x))^2))
+se.0
+se.0 <- s*(1/n + mean(x)^2/(sum((x-mean(x))^2)))
+se.0
+se.0
+se.0 <- s*sqrt((1/n + mean(x)^2/(sum((x-mean(x))^2))))
+se.0
+?lm
+plot(weight, plasma, xlab = ""weight [kg]"", ylab=""plasma [l]"")
+plot(weight, plasma, pch=19, xlab = ""body weight [kg]"", ylab=""plasma [l]"")
+plot(weight, plasma, pch=19, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
+abline(lm(plasma~weight), col=""red"") # regression line (y~x)
+.10 * 1700
+.20 * 1700
+.15 * 1700
+255 / 2
+255 / 2 - 30 - 24
+255 / 2 - 30 - 24 - 16

---FILE: session-regression/session-regression.Rmd---
@@ -0,0 +1,99 @@
+---
+title: ""Session regression I""
+output: github_document
+editor_options: 
+  chunk_output_type: console
+---
+
+### Learning outcomes
+- to be able to fit regression line and interpret regression model
+- to be able to use `lm` function in R for model fitting, obtaining confidence interval and predictions
+
+## Linear Regression 
+Linear regression is a very simple approach for supervised learning, when numerical outcome and numerical exposure are concerned. The method of linear regression is used to estimate the best-fitting straight line to describe the association. With linear regression we can answer questions incl.:
+
+- is there a relationship between exposure (e.g. body weigth) and outcome (e.g. plasma volume)?
+- how strong is the relationship between the two variables
+- what will be a predicted value of the outcome given a new set of exposure values
+- how accurately can we predict outcome
+
+### Example data
+
+Example data contain the body weight and plasma volume for eight healthy men. 
+```{r}
+
+weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+```
+
+Scatter plot of the data shows that high plasma volume tends to be associated with high weight and *vice verca*. Linear regrssion gives the equation of the straight line that best describes how the outcome changes (increase or decreases) with a change of exposure variable (in red)
+```{r, fig-reg, echo=F, fig.align=""center"", fig.height=5, fig.width=5}
+
+plot(weight, plasma, pch=19, las=1, xlab = ""body weight [kg]"", ylab=""plasma volume [l]"")
+abline(lm(plasma~weight), col=""red"") # regression line
+
+```
+
+The equation of the regression line is: 
+
+$$y=beta_0 + beta_1x$$
+
+
+
+### Estimating the Coefficients
+### Assessing the Accuracy of the Coefficient Estimates
+### Asesssing the Accuracy of the Model
+
+
+
+```{r setup, include=FALSE}
+knitr::opts_chunk$set(echo = TRUE)
+```
+
+```{r reg, echo=F}
+
+weight=c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+
+# correlation 
+cor(weight, plasma)
+
+# linear regression
+x <- weight
+y <- plasma
+model <- lm(y~x)
+summary(model)
+
+# prediction
+y.hat <- predict(model)
+
+n=length(x)
+RSS <- sum((y.hat-y)^2)
+RSE <- sqrt(RSS/(n-2))
+
+s <- sqrt((sum((y-mean(y))^2) - (model$coefficients[2])^2*sum((x-mean(x))^2))/(n-2))
+se.1 <-s/sqrt(sum((x-mean(x))^2))
+se.1
+
+se.0 <- s*sqrt((1/n + mean(x)^2/(sum((x-mean(x))^2))))
+se.0
+
+
+#             Estimate Std. Error t value Pr(>|t|)  
+# (Intercept)  0.08572    1.02400   0.084   0.9360  
+# x            0.04362    0.01527   2.857   0.0289 *
+```
+
+
+
+## Multiple linear regression
+### Estimating the Regression Coefficients
+### Estimating coefficients
+### Relationship between the response and predictors 
+### Model fit 
+### Predictions 
+### Qualitative predictors 
+### Interaction terms
+### Non-linear transformation of the predictors 
+### Potential problems: non-linearity, collinearity 
+### Logistic regression

---FILE: session-regression/session-regression.md---
@@ -0,0 +1,99 @@
+Session regression I
+================
+
+### Learning outcomes
+
+  - to be able to fit regression line and interpret regression model
+  - to be able to use `lm` function in R for model fitting, obtaining
+    confidence interval and predictions
+
+## Linear Regression
+
+Linear regression is a very simple approach for supervised learning,
+when numerical outcome and numerical exposure are concerned. The method
+of linear regression is used to estimate the best-fitting straight line
+to describe the association. With linear regression we can answer
+questions incl.:
+
+  - is there a relationship between exposure (e.g. body weigth) and
+    outcome (e.g. plasma volume)?
+  - how strong is the relationship between the two variables
+  - what will be a predicted value of the outcome given a new set of
+    exposure values
+  - how accurately can we predict outcome
+
+### Example data
+
+Example data contain the body weight and plasma volume for eight healthy
+men.
+
+``` r
+weight <- c(58, 70, 74, 63.5, 62.0, 70.5, 71.0, 66.0) # body weight (kg)
+plasma <- c(2.75, 2.86, 3.37, 2.76, 2.62, 3.49, 3.05, 3.12) # plasma volume (liters)
+```
+
+Scatter plot of the data shows that high plasma volume tends to be
+associated with high weight and *vice verca*. Linear regrssion gives the
+equation of the straight line that best describes how the outcome
+changes (increase or decreases) with a change of exposure variable (in
+red)
+<img src=""session-regression_files/figure-gfm/fig-reg-1.png"" style=""display: block; margin: auto;"" />
+
+The equation of the regression line is:
+
+\[y=beta_0 + beta_1x\]
+
+### Estimating the Coefficients
+
+### Assessing the Accuracy of the Coefficient Estimates
+
+### Asesssing the Accuracy of the Model
+
+    ## [1] 0.7591266
+
+    ## 
+    ## Call:
+    ## lm(formula = y ~ x)
+    ## 
+    ## Residuals:
+    ##      Min       1Q   Median       3Q      Max 
+    ## -0.27880 -0.14178 -0.01928  0.13986  0.32939 
+    ## 
+    ## Coefficients:
+    ##             Estimate Std. Error t value Pr(>|t|)  
+    ## (Intercept)  0.08572    1.02400   0.084   0.9360  
+    ## x            0.04362    0.01527   2.857   0.0289 *
+    ## ---
+    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
+    ## 
+    ## Residual standard error: 0.2188 on 6 degrees of freedom
+    ## Multiple R-squared:  0.5763, Adjusted R-squared:  0.5057 
+    ## F-statistic:  8.16 on 1 and 6 DF,  p-value: 0.02893
+
+    ##          x 
+    ## 0.01526836
+
+    ##        x 
+    ## 1.023998
+
+## Multiple linear regression
+
+### Estimating the Regression Coefficients
+
+### Estimating coefficients
+
+### Relationship between the response and predictors
+
+### Model fit
+
+### Predictions
+
+### Qualitative predictors
+
+### Interaction terms
+
+### Non-linear transformation of the predictors
+
+### Potential problems: non-linearity, collinearity
+
+### Logistic regression"
NBISweden,workshop-mlbiostatistics,60f425808a494f005c3fb8af0614516349876308,olgadet,olga.dethlefsen@nbis.se,2019-03-14T13:12:59Z,olgadet,olga.dethlefsen@nbis.se,2019-03-14T13:12:59Z,Fix course title in index.md,index.md,False,False,False,False,1,1,2,"---FILE: index.md---
@@ -3,7 +3,7 @@ layout: default
 title:  'Biostatistics Essentials: a blackboard approach'
 ---
 
-# Biostatistics course
+## Biostatistics Essentials: a blackboard approach
 Course location: SciLifeLab, Uppsala University, BMC, Husargatan 3
 
 Room: E10:1308, BMC"
NBISweden,workshop-mlbiostatistics,eeba5fe57593fa4937c61637bfcf0cd05023a27d,olgadet,olga.dethlefsen@nbis.se,2019-03-14T12:35:35Z,olgadet,olga.dethlefsen@nbis.se,2019-03-14T12:35:35Z,Fix ul link,travel.md,False,False,False,False,1,1,2,"---FILE: travel.md---
@@ -18,5 +18,5 @@ title: 'Travel'
 
 
 
-[ul]: [https://www.ul.se/en]
+[ul]: https://www.ul.se/en
 [mazemap]: https://use.mazemap.com/#v=1&zlevel=1&left=17.5570095&right=17.7032085&top=59.8806680&bottom=59.8361040&campusid=49&campuses=uu&starttype=poi&start=383383&desttype=poi&dest=383419"
